[{"CaseStudyId":"597","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"2017370","Name":"Russia"}],"Funders":[],"ImpactDetails":"\u000d    Drug abuse in sport is a significant global problem. Exogenous GH was\u000d      banned by the IOC as long\u000d      ago as 1989, but it has continued to be used because of the lack of a\u000d      reliable anti-doping test. The\u000d      2012 WADA-approved test for exogenous GH is the culmination of many years\u000d      of multidisciplinary\u000d      work, in which statistical research conducted at Kent played an essential\u000d      role, and represents a\u000d      significant breakthrough in the fight against drug abuse in sport. The\u000d      impact lies both in improving\u000d      the integrity of sport, for the benefit of the millions of people who\u000d      follow it, and in discouraging the\u000d      use of exogenous GH, thereby reducing the associated health risks for\u000d      sportspeople.\u000d    The difficulty of devising a sensitive test for exogenous GH\u000d      administration is evidenced by the\u000d      range of official bodies which have become involved with or have supported\u000d      the research, and by\u000d      the amount of conference time devoted to the topic. One test (the\u000d      \"isoform\" test) for GH was\u000d      approved and has been in use since the Athens Olympics in 2004, but it has\u000d      had little success in\u000d      detecting dopers, and it is widely regarded as very easy to evade. This\u000d      feature has motivated the\u000d      intense interest in developing a more sensitive test [5.7].\u000d    The US Anti-doping Agency (USADA) views this area as sufficiently\u000d      important and long-lived that\u000d      after the 2004 Annual Symposium (Dallas,\u000d      on Detection of Human Growth Hormone Abuse), it was\u000d      again the topic of the 2011 event (London,\u000d      on Detection of Growth Factors). Two members of the\u000d      GH-2004 team gave invited talks at the Dallas meeting and three members\u000d      gave invited talks at\u000d      the 2011 London meeting (Holt, on Overview of the GH-2000 biomarkers\u000d        method; Bassett, on\u000d      Development and application of the discriminant function in the GH-2000\u000d        biomarker test; Cowan,\u000d      on The marker method: Inter-lab study). These Symposia are \"aimed\u000d      at bringing together anti-\u000d      doping experts from around the world ... to inform and shape meaningful\u000d      anti-doping research\u000d      programs for the future\" [5.9]. The 2011 London meeting \"gathered\u000d      over 100 scientists, laboratory\u000d      directors and sports administrators, representing renowned entities such\u000d      as research institutes,\u000d      international sport federations, universities, the World Anti-Doping\u000d      Agency, the International\u000d        Paralympic Committee, professional sports leagues, and the London\u000d        Olympic Organizing\u000d        Committee\", with participants from almost 30 countries [5.9].\u000d    Interactions between the GH-2004 team and various anti-doping agencies\u000d      are also evident through\u000d      their contributions to a special issue of the journal Growth Hormone\u000d        and IGF Research that\u000d      covered medical, legal and social aspects of GH abuse in sport [5.10].\u000d    The UK Anti-Doping Agency (UKAD,\u000d      then part of UK Sport) expressed interest in the project in late\u000d      2006 [5.5], having perceived a need for a more sensitive test of\u000d      GH administration. UKAD have\u000d      given practical support (for example, supplying athletes' blood samples\u000d      when permitted for\u000d      research purposes) [5.1]. The GH-2004 project was funded by WADA\u000d      and USADA [5.6].\u000d    The robustness of the biomarker test in the face of potential legal\u000d      challenges was established by\u000d      the end of 2011 and formal approval by the UK Accreditation Service to\u000d      conduct the test was\u000d      achieved in early July 2012. The announcement of the introduction of the\u000d      new test led to much\u000d      press comment; the Chief Science Officer of USADA, Larry Bowers, described\u000d      it as \"a terrific day\u000d      for anti-doping when you can put a test like this in place\" [5.8].\u000d      The test was used formally for the\u000d      first time in the run-up to the Olympic and Paralympic Games, and\u000d      successfully detected two\u000d        cases of GH doping in the Paralympic Games, leading to the\u000d      disqualification of the powerlifters\u000d      involved and their suspension for two years [5.1, 5.2, 5.4].\u000d    The leader of the GH-2004 project confirms that the \"test has been\u000d      endorsed by the World Anti-Doping\u000d        Agency and was introduced for the first time at the 2012 London\u000d      Olympic Games resulting\u000d      in two Russian powerlifters being disqualified at the Paralympic games\" [5.3].\u000d    Summary: The statistical research at Kent has had a significant\u000d      impact on the 2012 Olympics and\u000d      on the integrity of sport in general, through the design and validation of\u000d      a new internationally\u000d      recognised test to detect exogenous use of GH.\u000d    ","ImpactSummary":"\u000d    Statistical research at Kent\u000d      has made an essential contribution to the development of an effective\u000d      test to detect growth hormone (GH) abuse in sport. GH is naturally present\u000d      in the human body, but\u000d      its exogenous use has been banned in sport for many years. However, it is\u000d      believed to be widely\u000d      misused across a broad spectrum of sports and the research attracted a\u000d      wide range of support\u000d      from national anti-doping authorities in the UK\u000d      (UKAD) and USA\u000d      (USADA), from the EU, via the\u000d      Biomed 2 Programme, from the International\u000d        Olympic Committee (IOC) and the World\u000d        Anti-Doping\u000d        Agency (WADA), and from industry. The development of a reliable\u000d      anti-doping test (the \"biomarker\"\u000d      test), based on the biological and statistical research, is a breakthrough\u000d      that benefits the public, by\u000d      helping to ensure fairness in sporting events. There are also potential\u000d      health benefits to\u000d      sportspeople who are discouraged from using exogenous GH, which can have\u000d      serious side effects.\u000d      The new test was finalised and approved by WADA in July 2012. The test was\u000d      used for the first\u000d      time just prior to the 2012 London Games and led to two Paralympic\u000d      competitors being convicted\u000d      of GH doping.\u000d    ","ImpactType":"Legal","Institution":"\u000d    University\u000d          of Kent\u000d    ","Institutions":[{"AlternativeName":"Kent (University of)","InstitutionName":"University of Kent","PeerGroup":"B","Region":"South East","UKPRN":10007150}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"264371","Name":"Athens"},{"GeoNamesId":"4684888","Name":"Dallas"}],"References":"\u000d    One paper specifically on statistical methodology was published by the\u000d      GH-2000 research team:\u000d    \u000a[3.1] Brown, P.J., Kenward, M.G &amp; Bassett, E.E.\u000d      (2001). Bayesian discrimination with\u000d      longitudinal data. Biostatistics, 2(4), 417-432. doi:\u000d      10.1093\/biostatistics\/2.4.417\u000d    \u000aPapers which deal in detail with the follow up research, and which are\u000d      crucial to the formal\u000d      specification of the test are:\u000d    \u000a[3.2] Powrie J.K., Bassett E.E., Rosen T., Jorgensen J.O.,\u000d      Napoli R., Sacca L., et al. (2007)\u000d      Detection of growth hormone abuse in sport. Growth Horm IGF Res, 17(3),\u000d      220-226.\u000d      doi:10.1016\/j.ghir.2007.01.011\u000d    \u000a\u000a[3.3] Erotokritou-Mulligan, I.; Guha, N., Stow, M., Bassett,\u000d        E.E., Bartlett, C., Cowan, D.A.,\u000d      S&#246;nksen, P.H. &amp; Holt, R.I.G. (2012) The development of decision limits\u000d      for the\u000d      implementation of the GH-2000 detection methodology using current\u000d      commercial insulin-like\u000d      growth factor-I and amino-terminal pro-peptide of type III collagen\u000d      assays. Growth Horm IGF\u000d        Res, 22(2), 53-58. doi: 10.1016\/j.ghir.2011.12.005\u000d    \u000aThe following papers give examples of the work needed to ensure that the\u000d      test is robust to\u000d      logistical problems which may occur, and that it remains a fair test for\u000d      individuals with particular\u000d      characteristics; there is also a commentary on the use of statistical\u000d      reasoning in assay work:\u000d    \u000a[3.4] Holt R.I., Erotokritou-Mulligan I., Ridley S.A., McHugh\u000d      C.M., Bassett E.E., Cowan D.A., et al.\u000d      (2009) A determination of the pre-analytical storage conditions for\u000d      insulin like growth factor-I\u000d      and type III procollagen peptide. Growth Horm IGF Res, 19(1),\u000d      43-50. doi:\u000d      10.1016\/j.ghir.2008.06.001\u000d    \u000a\u000a[3.5] Erotokritou-Mulligan I., Bassett E.E., Cowan D.A.,\u000d      Bartlett C., McHugh C., Sonksen P.H., et\u000d      al. (2009) Influence of ethnicity on IGF-I and procollagen III peptide\u000d      (P-III-P) in elite athletes\u000d      and its effect on the ability to detect GH abuse. Clin Endocrinol\u000d        (Oxf), 70(1), 161-168.\u000d      doi: 10.1111\/j.1365-2265.2008.03319.x\u000d    \u000a\u000a[3.6] Bassett E.E., Erotokritou-Mulligan I. (2009). Statistical\u000d      issues in implementing the marker\u000d      method. Growth Horm IGF Res, 19(4), 361-365. doi:\u000d      10.1016\/j.ghir.2009.04.024\u000d    \u000a(Reference marked with a star best indicates the quality of the\u000d      underpinning research.)\u000d    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"3","Subject":"Clinical Sciences"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"}],"Sources":"\u000d    Main sources:\u000d    [5.1] The basis of the impact case study is summarised in a press\u000d    release from UK Anti-Doping\u000d    about the biomarker test. It explains the background to the test, gives\u000d    detail of the funding,\u000d    explains the role of the Kent statisticians and gives details of the\u000d    positive test results for two\u000d    powerlifters at the London 2012 Paralympics:\u000d    http:\/\/www.ukad.org.uk\/news\/article\/uk-research-leads-to-new-growth-hormone-test\u000d    [5.2] Email from the leader of the GH-2004 team confirming the\u000d      vital role of the statistical work\u000d      done at Kent in developing the test, and the disqualification of two\u000d      athletes at the London\u000d      2012 Paralympics Games thanks to this new test. (See Contact 1.)\u000d    [5.3] Final report for the GH-2000 project confirming the role of\u000d      Kent statisticians in this project.\u000d    Available at:\u000d    http:\/\/www.gh2004.soton.ac.uk\/GH-2000%20Final%20Report.pdf\u000d    [5.4] Announcement of first positive test at the Paralympic Games:\u000d    http:\/\/www.paralympic.org\/press-release\/latest-testing-methods-result-suspension-two-\u000d        russian-powerlifters-anti-doping\u000d    Additional sources:\u000d    [5.5] A statement from UKSport about the importance of their\u000d      active involvement with GH-2004:\u000d    http:\/\/www.gh2004.soton.ac.uk\/collaboration_with_uk_sport.htm\u000d    [5.6] Results and conclusion for the GH-2004 project available on\u000d      the WADA webpage confirming\u000d      in particular sources of funding:\u000d    http:\/\/www.wada-ama.org\/rtecontent\/document\/A1_2002_results.pdf\u000d    [5.7] Criticism of the then-current (\"isoform\") test, with hint at\u000d      a better test coming:\u000d    http:\/\/news.bbc.co.uk\/sport1\/hi\/olympics\/7339460.stm\u000d    [5.8] Announcement by WADA of the introduction of the biomarker\u000d      test:\u000d    http:\/\/sportsillustrated.cnn.com\/2012\/olympics\/2012\/writers\/david_epstein\/07\/27\/london-olympics-drug-testing\/index.html\u000d    [5.9] General information about the scope and purpose of USADA\u000d      Annual Symposia:\u000d    http:\/\/www.usada.org\/symposia\/\u000d    Details about the USADA Annual Symposium in London 2011:\u000d    http:\/\/www.usada.org\/symposia\/london\/\u000d    [5.10] A special issue of the journal Growth Hormone &amp; IGF\u000d        Research in August 2009 was devoted\u000d      to \"The Abuse of Growth Hormone in Sport and its Detection: a Medical,\u000d      Legal and Social\u000d      Framework\". Eight of the 24 papers were from the research team; other\u000d      contributors included\u000d      scientific staff of WADA and UKAD:\u000d    http:\/www.sciencedirect.com\/science\/publication?issn=10966374&amp;volume=19&amp;issue=4\u000d    If any of the links above is no longer working, pdf copies are\u000d        available on request.\u000d    \u000d    ","Title":"\u000d    Detection of growth hormone abuse in sport\u000d    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d    The aim of the research was to develop and validate an anti-doping test\u000d      to detect use of\u000d      exogenous GH. The multidisciplinary research team comprised specialists in\u000d      medicine, statistics\u000d      and bioassay technology. The statistical work was done at Kent or led by\u000d      Kent statisticians [5.1,\u000d        5.2, 5.3].\u000d    Because GH is present naturally in the body, detection of exogenous GH is\u000d      difficult and inevitably\u000d      relies on statistical techniques. The concentration of GH in blood varies\u000d      considerably; it depends\u000d      on age and it responds to exercise. GH is secreted in a pulsatile manner,\u000d      and is eliminated quickly\u000d      from the body. Research to find a marker to detect GH started in 1996 with\u000d      the GH-2000 project,\u000d      sponsored by the European Union (EU) and the IOC. The medical work centred\u000d      on St Thomas'\u000d        Hospital, London, with partners in Denmark, Sweden and Italy; and\u000d      all of the statistical work on the\u000d      project was undertaken at Kent by staff from the University of Kent: Bassett\u000d      (1970-2007; 2007-present,\u000d      Honorary Senior Lecturer, work done at Kent), Brown (1995-present)\u000d      and Kenward\u000d      (1995-1999) [5.3, p.7]. The project was successfully completed in\u000d      1999, and a novel statistical\u000d      approach (the \"trajectory\" method) which aimed to model the way in which\u000d      combinations of marker\u000d      levels varied over time following GH administration was published [3.1].\u000d    Four major studies were undertaken during the project: a Washout study, a\u000d      Double-blind study, a\u000d      Cross-sectional study and a Longitudinal study. This preliminary research\u000d      was needed to identify\u000d      and study hormones which could act as markers of GH abuse. To be suitable,\u000d      a biomarker had to\u000d      have a reasonably long half-life in the body, and it had to be responsive\u000d      to GH administration, but\u000d      relatively unresponsive to exercise. The research identified 8 possible\u000d      markers. A key component\u000d      of the University of Kent contribution to the project was the finding\u000d      that, for each sex separately, a\u000d      discriminant function based on only two of these markers, insulin-like\u000d      growth factor-I (IGF-I) and\u000d      N-terminal pro-peptide of type III collagen (P-III-NP), could distinguish\u000d      dopers from subjects given\u000d      a placebo. These statistical functions, derived during the GH-2000\u000d      project, form the basis of the\u000d      test; without this statistical component of the work, the test would have\u000d      not been economically\u000d      viable [3.2].\u000d    A follow-up project (GH-2004), involving Southampton\u000d        University (Medicine, project leader), the\u000d      University of Kent (Statistics), and Kings\u000d        College London (assay work), started in 2002, building on\u000d      the earlier work. The statistical component of the work was done or\u000d      supervised at Kent by Bassett\u000d        [5.1, 5.2]. Key statistically-based components of the work were the\u000d      specification of the score\u000d      function [3.2] and the decision limits; that is, the scores above\u000d      which an athlete would be\u000d      prosecuted [3.3].\u000d    To ensure that the test could withstand legal challenge, the GH-2004 team\u000d      needed to establish\u000d      robustness; for example, in respect of ethnicity [3.5], the\u000d      effects of injury, and differential changes\u000d      during adolescence. Validation of the method on independent sets of data\u000d      was also required, as\u000d      was confirmation that the test would give consistent results in different\u000d      laboratories [3.6]. Further,\u000d      because out-of-competition blood samples may need to be obtained far from\u000d      WADA-accredited\u000d      laboratories, research has been needed to determine stability of assay\u000d      results under varied storage\u000d      and transport conditions [3.4]. WADA policy also requires\u000d      concentrations obtained from assay work\u000d      to be estimated using two different types of assay. To meet all these\u000d      requirements, several detailed\u000d      studies were needed; the planning of these was achieved by collaboration\u000d      between the chemists\u000d      and statisticians, and the data analysis was the specific responsibility\u000d      of the statistical team.\u000d    The leader of the GH-2004 project acknowledged the statistical work done\u000d      at Kent and said that\u000d      the \"statistical analysis of the project, which has been a key part of the\u000d      work, was led by Dr Eryl\u000d      Bassett, University of Kent. This work has played a vital part of the\u000d      development of the test as the\u000d      test has to stand up to legal scrutiny. Dr Bassett has been involved in\u000d      the project since its inception\u000d      and designed the statistical analysis plan for the GH-2004 project and\u000d      preceding GH-2000 project\"\u000d      [5.3].\u000d    To summarise, the statistical research conducted at Kent by Bassett,\u000d      Brown and Kenward has\u000d      been crucial in designing the biomarker test and ensuring its viability\u000d      and validity.\u000d    "},{"CaseStudyId":"598","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000d\u000a    The current economic climate has emphasised the importance of resource\u000d\u000a      efficiency, which\u000d\u000a      presents a challenge for manufacturing companies. The KTP project has\u000d\u000a      developed a novel\u000d\u000a      calibration procedure, based on Kent research, that exploits historical\u000d\u000a      data that were already held\u000d\u000a      by the Company. The benefits of the new procedure are fourfold: (i) around\u000d\u000a      50% more meters can\u000d\u000a      be calibrated in a given period of time than with the previous method,\u000d\u000a      leading to increased profits\u000d\u000a      and more rapid fulfilment of orders; (ii) there are separate environmental\u000d\u000a      benefits, resulting from\u000d\u000a      less usage of energy; (iii) for some types of meter, certain secondary\u000d\u000a      measurements have been\u000d\u000a      found to be unnecessary and can be eliminated; (iv) the calibration is\u000d\u000a      more robust, reducing the\u000d\u000a      number of instruments that need to undergo costly and time-consuming\u000d\u000a      recalibration [5.1, item 6].\u000d\u000a      All these are confirmed by the Research and Development Manager of KROHNE\u000d\u000a      Ltd, who\u000d\u000a      declares that the \"new procedure based on your statistical theory\u000d\u000a        reduces the overall time and also\u000d\u000a        improves robustness\" and that it \"will not only benefit the\u000d\u000a        Company with thousands of instruments\u000d\u000a        produced more efficiently but also benefit our local economy and\u000d\u000a        environment since less energy\u000d\u000a        will be used in the production\" [5.6]. These benefits will\u000d\u000a      be substantial: for example the Company\u000d\u000a      anticipates that gross profits will have risen by around &#163;250,000 per\u000d\u000a      annum by 2015 [5.1, item 10].\u000d\u000a      The collaboration with KROHNE Ltd built on initial links with the Company\u000d\u000a      that were established by\u000d\u000a      X Wang. Support for building this into a more formal collaboration,\u000d\u000a      funded through a Knowledge\u000d\u000a      Transfer Partnership (KTP) in 2011, was provided by the Kent Innovation\u000d\u000a      and Enterprise unit.\u000d\u000a    At this stage, the primary impact has been to change the knowledge\u000d\u000a      and capability of staff\u000d\u000a      within the Company with regard to the use of historical data and\u000d\u000a      specialised Bayesian\u000d\u000a      methodology, so that they themselves can incorporate this more\u000d\u000a        efficient calibration method into\u000d\u000a      their manufacturing process. Previously, such methods were unknown to\u000d\u000a      staff at KROHNE Ltd.\u000d\u000a      The impact has been achieved through the KTP by adapting the Bayesian\u000d\u000a      regression techniques\u000d\u000a      developed at Kent to the specifics of the calibration problem, by running\u000d\u000a      many calibrations to\u000d\u000a      demonstrate the effectiveness of the method in practice, and by supporting\u000d\u000a      the implementation of\u000d\u000a      the new calibration method within the Company's core software. The\u000d\u000a      adoption of the new\u000d\u000a      production procedure based on the statistical developed by Kent\u000d\u000a      statisticians is confirmed by the\u000d\u000a      Research and Development Manager of KROHNE Ltd, who declares that \"this\u000d\u000a        new procedure has\u000d\u000a        been tested by our software engineers and it is being implemented in our\u000d\u000a        core software\" [5.6].\u000d\u000a    The Company has been extremely positive about the benefits of the KTP\u000d\u000a      throughout the project\u000d\u000a      (e.g. [5.2, 5.3, 5.5, 5.6]) and the project has received\u000d\u000a      consistently high ratings. The Company itself\u000d\u000a      rated the project as \"high\" (the top grade) in terms of \"improved\u000d\u000a        efficiency or productivity\" [5.1, item\u000d\u000a        13] and commented that \"The KTP exceeded our\u000d\u000a          expectations in finding a very effective\u000d\u000a        mathematical model to meet the Company's need.\" [5.1, item 18].\u000d\u000a      The project was awarded the\u000d\u000a      highest grade of \"Outstanding\" by the KTP Grading Panel for its\u000d\u000a      achievement [5.7]; only around\u000d\u000a      10% of projects nationally receive this rating.\u000d\u000a    The Company's commitment to the project is evidenced by the fact that\u000d\u000a      they have a software\u000d\u000a      engineer working full time on the implementation. This requires a full\u000d\u000a      understanding of the\u000d\u000a      Bayesian methodology and the Company employed the KTP Associate for an\u000d\u000a      additional five\u000d\u000a      months beyond the lifetime of the KTP project to facilitate this. Part of\u000d\u000a      the delivery of this\u000d\u000a      knowledge transfer has been the provision of a comprehensive document\u000d\u000a      detailing the statistical\u000d\u000a      methods used, and associated training of the software engineers by the KTP\u000d\u000a      Associate.\u000d\u000a    A secondary impact of the KTP has been to change the culture of\u000d\u000a      KROHNE Ltd, with regard to\u000d\u000a      the value of fundamental science, particularly industrial mathematics [5.1\u000d\u000a        item 12]. The Company\u000d\u000a      now recognises the value of historical data and sees the potential for\u000d\u000a      exploiting such data\u000d\u000a      elsewhere in the Company to \"improve manufacturing operation and\u000d\u000a        product quality\" [5.1, item 7].\u000d\u000a      It is also starting to address other issues in relation to historical\u000d\u000a      data, such as the question of\u000d\u000a      whether it is best to use all available data, or whether there may be\u000d\u000a      benefits in discarding some of\u000d\u000a      the oldest data.\u000d\u000a    The research undertaken by the University of Kent, with the KTP\u000d\u000a      Associate, has been presented to\u000d\u000a      a cross-section of staff at a seminar hosted by KROHNE Ltd [5.4, 5.5].\u000d\u000a      Bayesian methods were\u000d\u000a      completely new to the Company, but they have been quick to realise their\u000d\u000a      potential and describe\u000d\u000a      the KTP as \"significant to the Company's operation for enhancing its\u000d\u000a        competitiveness of precision\u000d\u000a        instrumentation manufacturing\" [5.1, item 4]. The Company's\u000d\u000a      2013 plan includes dissemination of\u000d\u000a      Bayesian statistical principles to Research &amp; Development personnel\u000d\u000a      and training of calibration\u000d\u000a      operators. Thus the impact on the Company was broad as it affected\u000d\u000a      personnel involved in\u000d\u000a      research and development, implementation through software development and\u000d\u000a      practical\u000d\u000a      calibration. The KTP Associate has also been involved in regular\u000d\u000a      discussions with KROHNE\u000d\u000a      Germany.\u000d\u000a    A further example of the increased awareness of the value of statistical\u000d\u000a      methods within\u000d\u000a      KROHNE Ltd is that a second software engineer is working full time on the\u000d\u000a      implementation of\u000d\u000a      another new statistically based procedure into the Company's software; the\u000d\u000a      KTP Associate also\u000d\u000a      contributed to development of this procedure.\u000d\u000a    Summary: Whilst there are various approaches to Bayesian\u000d\u000a      regression, the methods used here to\u000d\u000a      address the calibration problem successfully are firmly based in the\u000d\u000a      nonparametric approach of the\u000d\u000a      underpinning Kent research outlined in Section 2, which offers great\u000d\u000a      flexibility in the choice of prior\u000d\u000a      distribution; the need for this flexibility was apparent from the\u000d\u000a      historical data. The impact on the\u000d\u000a      Company has been to provide a new calibration procedure that utilises\u000d\u000a      historical data. The new\u000d\u000a      procedure reduces waste, lowers manufacturing costs and delivers more\u000d\u000a      reliable products that\u000d\u000a      require less re-calibration, and will lead to large increases in profits.\u000d\u000a      The KTP has provided the\u000d\u000a      knowledge and capability that the Company is now using to implement the\u000d\u000a      procedure into its\u000d\u000a      manufacturing software.\u000d\u000a    To conclude, this very successful KTP project has changed the practices\u000d\u000a      and protocols of the KTP\u000d\u000a      partner through transfer of knowledge and capability. The Company\u000d\u000a      anticipates a gross profit\u000d\u000a      increase of a quarter of a million pounds per annum in three years' time\u000d\u000a      as a direct consequence of\u000d\u000a      this KTP project [5.1, item 10]. This acknowledges the\u000d\u000a      significance of this project which has\u000d\u000a      already affected many departments of the Company.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This impact case study is based on a Knowledge Transfer Partnership (KTP)\u000d\u000a      between the School\u000d\u000a      of Mathematics, Statistics and Actuarial Science, University of Kent and KROHNE Ltd, a world\u000d\u000a      leading manufacturer of industrial measuring instruments. These precision\u000d\u000a      instruments (typically\u000d\u000a      flow meters and density meters) need to be calibrated accurately before\u000d\u000a      being used and this is an\u000d\u000a      expensive and time-consuming process.\u000d\u000a    The purpose of the KTP was to use Bayesian methodology developed by Kent\u000d\u000a      statisticians to\u000d\u000a      establish a novel calibration procedure that improves on the\u000d\u000a      existing procedure by incorporating\u000d\u000a      historical records from calibration of previous instruments of the same\u000d\u000a      type. This reduces\u000d\u000a      substantially the number of test runs needed to calibrate a new instrument\u000d\u000a      and will increase\u000d\u000a      capacity by up to 50%.\u000d\u000a    The impact of the KTP, which was graded as `Outstanding', has been to\u000d\u000a      change the knowledge\u000d\u000a      and capability of the Company, so that they can improve the\u000d\u000a        performance of their manufacturing\u000d\u000a      process by implementing this novel calibration method. This has been\u000d\u000a      achieved by adapting the\u000d\u000a      underpinning Kent research to the specific context of the calibration\u000d\u000a      problem, by running many\u000d\u000a      calibrations to demonstrate the effectiveness of the method in practice,\u000d\u000a      and by supporting the\u000d\u000a      implementation of the new calibration method within the Company's core\u000d\u000a      software.\u000d\u000a    Moreover, the project has changed the Company's thinking on\u000d\u000a      fundamental science, particularly\u000d\u000a      industrial mathematics. The value of historical data, and the usefulness\u000d\u000a      of Bayesian methods, is\u000d\u000a      now widely appreciated and training for staff in Bayesian Statistics is\u000d\u000a      being introduced. Thus the\u000d\u000a      project has not only changed the protocols of the Company, it has also\u000d\u000a      changed their practice.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of\u000d\u000a          Kent\u000d\u000a    ","Institutions":[{"AlternativeName":"Kent (University of)","InstitutionName":"University of Kent","PeerGroup":"B","Region":"South East","UKPRN":10007150}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a[3.1] Fuentes-Garcia, R., Mena, R.H. and Walker, S.G.\u000d\u000a      (2009). A nonparametric dependent\u000d\u000a      process for Bayesian regression. Statistics and Probability Letters,\u000d\u000a      79, 1112-1119.\u000d\u000a      doi: 10.1016\/j.spl.2009.01.005\u000d\u000a    \u000a\u000a[3.2] Fuentes-Garcia, R., Mena, R.H. and Walker, S.G.\u000d\u000a      (2010). A new Bayesian nonparametric\u000d\u000a      mixture model. Communications in Statistics, 39, 669-682.\u000d\u000a      doi: 10.1080\/03610910903580963\u000d\u000a    \u000a\u000a[3.3] Mena, R.H., Ruggiero, M. and Walker, S.G. (2011).\u000d\u000a      Geometric stick-breaking processes for\u000d\u000a      continuous-time Bayesian nonparametric modeling. Journal of\u000d\u000a        Statistical Planning and\u000d\u000a        Inference, 141, 3217-3230. doi: 10.1016\/j.jspi.2011.04.008\u000d\u000a    \u000a\u000a[3.4] Walker, S.G. (2007). Sampling the Dirichlet mixture model\u000d\u000a      with slices. Communications in\u000d\u000a        Statistics, 36, 45-54. doi: 10.1080\/03610910601096262\u000d\u000a    \u000a\u000a[3.5] Kalli, M., Griffin, J.E. and Walker, S.G.\u000d\u000a      (2011). Slice sampling mixture models. Statistics\u000d\u000a        and Computing, 21, 93-105. doi: 10.1007\/s11222-009-9150-y\u000d\u000a    \u000a\u000a[3.6] Wang, X. and Hussain, Y. (2010). Method to improve mass flow\u000d\u000a      measurement based on\u000d\u000a      statistical analysis of signals. This patent application was filed on\u000d\u000a        January 28, 2010 in\u000d\u000a        Germany (Patent DE102010006224A1), and subsequently in the USA, (Patent\u000d\u000a        US2011\/0184667A1).\u000d\u000a    \u000a(References marked with a star best indicate the quality of the\u000d\u000a      underpinning research.)\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    [5.1] KTP Final Report, 6th February 2013. This provides details\u000d\u000a      of the aims and objectives of the\u000d\u000a      project and assessment of performance and impact by both the industrial\u000d\u000a      partner and the\u000d\u000a      academic partner.\u000d\u000a    [5.2] Report LMC3 from the Technical Director, KROHNE Ltd, 2nd\u000d\u000a      November 2011 detailing\u000d\u000a      progress and noting the seminar given by the KTP Associate. (See Contact\u000d\u000a      4.)\u000d\u000a    [5.3] Report LMC4 from the Technical Director, KROHNE Ltd, 29th\u000d\u000a      February 2012, detailing\u000d\u000a      progress and highlighting the \"great benefits\" of the approach being\u000d\u000a      developed to the\u000d\u000a      calibration procedures. (See Contact 4.)\u000d\u000a    [5.4] Email from the Research and Metrology Manager, KROHNE Ltd,\u000d\u000a      7th October 2011,\u000d\u000a      announcing a forthcoming seminar by the KTP Associate.\u000d\u000a    [5.5] Email from the Research and Development Manager, KROHNE Ltd,\u000d\u000a      26th October 2011,\u000d\u000a      highlighting the progress of the project and in particular the success of\u000d\u000a      the seminar given by\u000d\u000a      the KTP Associate. (See Contact 3.)\u000d\u000a    [5.6] Letter from the Research and Development Manager, KROHNE\u000d\u000a      Ltd, 23rd July 2013,\u000d\u000a      explaining that the new calibration procedure \"will not only benefit\u000d\u000a        the Company with\u000d\u000a        thousands of instruments produced more efficiently but also benefit our\u000d\u000a        local economy and\u000d\u000a        environment since less energy will be used in the production.\" (See\u000d\u000a      Contact 3.)\u000d\u000a    [5.7] Certificate of excellence, (highest grade of \"Outstanding\"),\u000d\u000a      KTP Grading Panel.\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Bayesian calibration and verification of vibratory measuring devices\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The research in Bayesian methodology that underpins this impact case\u000d\u000a      study was conducted at\u000d\u000a      Kent by Griffin (2000-2004 and 2007-present), Kalli (PhD\u000d\u000a      student, 2004-2007), Walker (2004-2013) and X Wang\u000d\u000a      (2007-present).\u000d\u000a    Precision measuring instruments require careful calibration. For KROHNE\u000d\u000a      Ltd, the current\u000d\u000a      calibration procedures, which are a crucial step for the Company's\u000d\u000a      production line, are time-consuming and costly. However, large amounts of\u000d\u000a      historical data are available that can potentially\u000d\u000a      be exploited to improve the efficiency of the calibration process. For\u000d\u000a      example, for one class of\u000d\u000a      meters, over 800 instruments have already been calibrated, but before the\u000d\u000a      KTP project, these data\u000d\u000a      were not utilised in the calibration of new meters.\u000d\u000a    The existing calibration procedure used multiple regression, based on 30\u000d\u000a      test runs under varying\u000d\u000a      temperature and pressure conditions. The regression model is a\u000d\u000a      second-order model with five\u000d\u000a      parameters to be estimated. Bayesian methods provide a natural framework\u000d\u000a      for incorporating\u000d\u000a      historical data through introduction of a prior distribution and the basic\u000d\u000a      aim of the KTP was to\u000d\u000a      develop a Bayesian regression model for calibration.\u000d\u000a    The evidence from the historical data is that the distribution of the\u000d\u000a      parameter estimates is non-normal and exhibits multimodality and skewness.\u000d\u000a      This implies that a flexible family of distributions\u000d\u000a      is required to model the population distribution of the meter parameters.\u000d\u000a      A natural choice in a\u000d\u000a      Bayesian setting is the mixture of Dirichlet process model, where we mix a\u000d\u000a      multivariate normal\u000d\u000a      distribution for the five parameters with a Dirichlet process random\u000d\u000a      distribution function. The Kent\u000d\u000a      researchers, X Wang and Walker, therefore proposed an\u000d\u000a      infinite mixture model with weights\u000d\u000a      attached to a set of multivariate normal distributions for the calibration\u000d\u000a      process.\u000d\u000a    In this industrial application, the computational time for model fitting\u000d\u000a      needs to be kept to a\u000d\u000a      minimum. This led the Kent team to propose the use of simple geometric\u000d\u000a      weights. References\u000d\u000a      [3.1 - 3.3] provide the supporting theory for geometric weights.\u000d\u000a      This choice of weights allows fast\u000d\u000a      computation time due to the identifiability of the model. With more exotic\u000d\u000a      weight structures, such as\u000d\u000a      a stick-breaking prior for the weights, the model is unidentifiable and\u000d\u000a      much of the computing time\u000d\u000a      is due to the Markov chain Monte Carlo algorithm visiting different parts\u000d\u000a      of the model space which\u000d\u000a      produce the same density.\u000d\u000a    With geometric weights it is necessary to use slice sampling ideas for\u000d\u000a      dealing with the infinite\u000d\u000a      nature of the model, this effectively provides a random truncation of the\u000d\u000a      number of mixture\u000d\u000a      components, often at a small value; this computational technique was also\u000d\u000a      developed at Kent, see\u000d\u000a      references [3.4] and [3.5].\u000d\u000a    Because the new calibration procedure incorporates information from\u000d\u000a      previous calibration of\u000d\u000a      instruments of the same type, the number of test runs that are needed to\u000d\u000a      calibrate a new meter can\u000d\u000a      be reduced. The KTP Associate, overseen by X Wang and Walker,\u000d\u000a      has run many tests using the\u000d\u000a      proposed algorithm on data sets provided by KROHNE Ltd and the results\u000d\u000a      have proved to be\u000d\u000a      excellent. By applying the Bayesian pooling approach we have managed to\u000d\u000a      replicate the ordinary\u000d\u000a      least squares estimates of parameters for each meter using only 18\u000d\u000a      readings per meter rather than\u000d\u000a      the 30 that are used in the existing multiple regression approach.\u000d\u000a    A further benefit of the Bayesian approach is improved robustness of the\u000d\u000a      calibration procedure.\u000d\u000a      Although the multiple regression approach works well in general, it is\u000d\u000a      susceptible to occasional\u000d\u000a      anomalies in the calibration data. This may result in the need to\u000d\u000a      recalibrate the meter. Because the\u000d\u000a      Bayesian approach `borrows strength' from the historical data, data\u000d\u000a      anomalies are less influential\u000d\u000a      and this reduces the likelihood that a meter will have to be recalibrated.\u000d\u000a    Finally, the new statistical model has also highlighted an unanticipated\u000d\u000a      feature which is that one of\u000d\u000a      the secondary sensing devices thought to be important for calibration,\u000d\u000a      namely strain gauge, is\u000d\u000a      actually not required for at least two of the meter types [5.2, 5.3];\u000d\u000a      the elimination of this secondary\u000d\u000a      device from each instrument directly reduces manufacturing cost.\u000d\u000a    A document giving full details of the proposed Bayesian procedure has\u000d\u000a      been produced for the\u000d\u000a      Company by the KTP Associate.\u000d\u000a    The link with KROHNE Ltd was initiated in earlier collaborative research\u000d\u000a      work on mass flow\u000d\u000a      measurement involving X Wang at Kent that resulted in a patent [3.6].\u000d\u000a    "},{"CaseStudyId":"599","Continent":[{"GeoNamesId":"6255146","Name":"Africa"},{"GeoNamesId":"6255150","Name":"South America"},{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"1643084","Name":"Indonesia"},{"GeoNamesId":"2658434","Name":"Switzerland"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"1733045","Name":"Malaysia"},{"GeoNamesId":"2186224","Name":"New Zealand"},{"GeoNamesId":"3469034","Name":"Brazil"},{"GeoNamesId":"1605651","Name":"Thailand"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"1282988","Name":"Nepal"},{"GeoNamesId":"1062947","Name":"Madagascar"},{"GeoNamesId":"2077456","Name":"Australia"}],"Funders":["Engineering and Physical Sciences Research Council","Natural Environment Research Council"],"ImpactDetails":"\u000d\u000a    The impact of Statistical Ecology research at Kent has been to improve\u000d\u000a      substantially the\u000d\u000a      effectiveness of wildlife conservation policy and practice by providing\u000d\u000a      better methods of data\u000d\u000a      collection and analysis. The beneficiaries of the research are\u000d\u000a      individuals, charities and\u000d\u000a      environmental agencies that collect and analyse ecological data and the\u000d\u000a      organisations that plan\u000d\u000a      and implement management and conservation strategies that utilise these\u000d\u000a      data.\u000d\u000a      A variety of approaches are used to encourage the incorporation of our new\u000d\u000a      methodologies into\u000d\u000a      ecological practice. These include workshops and multidisciplinary\u000d\u000a      conferences, studentships with\u000d\u000a      collaborating bodies, joint research with external agencies and provision\u000d\u000a      of computer software.\u000d\u000a      Before detailing impact in (a) and (b) below, we expand on the context.\u000d\u000a    Multidisciplinary conferences: EURING meetings have been held\u000d\u000a      regularly since 1986 to\u000d\u000a      promote collaboration between ornithologists, statisticians and population\u000d\u000a      biologists. Kent\u000d\u000a      statisticians have contributed to all of them, editing or co-editing\u000d\u000a      proceedings of 6 of the 9\u000d\u000a      meetings. NCSE initiated the highly successful and influential series of\u000d\u000a      International Statistical\u000d\u000a      Ecology Conferences (St\u000d\u000a        Andrews 2008, Kent 2010, Oslo\u000d\u000a      2012, Montpellier\u000d\u000a      2014, British\u000d\u000a        Columbia\u000d\u000a      2016). Both conference series incorporate training workshops and attract\u000d\u000a      strong participation from\u000d\u000a      outside of academia; for example, approximately 1\/3 of the 146\u000d\u000a      participants at the 2010 ISEC at\u000d\u000a      Kent were from outside academia (e.g. Alaska Dept of Fish &amp; Game;\u000d\u000a      British Trust for Ornithology;\u000d\u000a      Microsoft; Proteus Consulting, New\u000d\u000a        Zealand; Snow Leopard Trust, India;\u000d\u000a      US Geological Survey;\u000d\u000a      Wildlife Conservation Society, Malaysia).\u000d\u000a    Studentships with collaborating bodies, some funded by CASE\u000d\u000a      awards, have addressed the\u000d\u000a      spread of an invasive species (CEH), the effect of dredging on benthic\u000d\u000a      organisms (Cefas) and the\u000d\u000a      phenology of overwintering wild birds (the Wildfowl and Wetlands Trust).\u000d\u000a      These close\u000d\u000a      collaborations ensure that research is directly relevant to ecologists.\u000d\u000a      Two research students have\u000d\u000a      produced introductory videos, targeted at an ecological audience, to\u000d\u000a      facilitate uptake of new\u000d\u000a      methods published in Methods in Ecology and Evolution, a journal\u000d\u000a      of the British Ecological Society\u000d\u000a      [5.1], and two students won a poster prize at the International\u000d\u000a        Congress in Conservation Biology,\u000d\u000a      Auckland, New Zealand,\u000d\u000a      2011.\u000d\u000a    Current projects with external agencies\/users include developing\u000d\u000a      models for the spread of ring-\u000d\u000a      necked parakeets (Psittacula krameri) with the British Trust for\u000d\u000a      Ornithology (BTO); devising new\u000d\u000a      models and methods for evaluating the effect of global warming on the\u000d\u000a      phenology, abundance and\u000d\u000a      distribution of British butterfly species, with Butterfly Conservation;\u000d\u000a      and investigating the causes of\u000d\u000a      the decline of sea birds in the North Sea, with CEH. These close links\u000d\u000a      promote the wider use of our\u000d\u000a      methodology within these non-academic institutions.\u000d\u000a    \u000d\u000a      \u000aEstimating the survival of wild animals and integrated population\u000d\u000a          modelling [3.1-3.4]\u000d\u000a        Discussion of the sheep model [3.1, 3.2] now appears in several\u000d\u000a        textbooks on ecological modelling\u000d\u000a        [5.2]. The estimate of the annual survival of shags developed in\u000d\u000a        [3.3] was incorporated in the\u000d\u000a        update of The Birds of the Western Palearctic [5.3]. Our\u000d\u000a        models demonstrated the probable reason\u000d\u000a        for the decline of the Northern Lapwing, a species that is now on the UK\u000d\u000a        red list of globally\u000d\u000a        threatened species [5.4]. Organisations such as the BTO use\u000d\u000a        information about population decline\u000d\u000a        \"to inform the public, opinion-formers and decision-makers... and to\u000d\u000a        make a significant contribution\u000d\u000a        to environmental policy.\" [5.5]\u000a\u000d\u000a      Our methodology for integrated population modelling (IPM) [3.4],\u000d\u000a        which allows the combined\u000d\u000a        analysis of data from different sources, is now widely used by\u000d\u000a        ecologists. For example, the\u000d\u000a        methodology has been used extensively by the BTO to \"understand the\u000d\u000a        demographic mechanisms\u000d\u000a        that drive population changes and hence to better identify the\u000d\u000a        ecological causes of change and the\u000d\u000a        management actions needed to alter population trajectories.\" [5.5].\u000d\u000a        This testimonial also highlights\u000d\u000a        the important contribution of other areas of statistical ecology\u000d\u000a        research at Kent, including \"analyses\u000d\u000a        of survival involving individual covariates\" [3.1, 3.2] and\u000d\u000a        \"occupancy modelling\" [3.5] and\u000d\u000a        concludes that our work \"makes a particularly important contribution to\u000d\u000a        ecological science within\u000d\u000a        the UK and\u000d\u000a        to the policy-relevant work that flows from this\". Another testimonial\u000d\u000a        indicating the\u000d\u000a        impact of IPM on ecological practice, from the Head of the Ecology\u000d\u000a        Department at the Swiss\u000d\u000a        Ornithological Institute [5.6], states that the paper [3.4]\u000d\u000a        \"changed the view of population ecologists\u000d\u000a        about the quantitative treatment of their data\" and leads to \"more\u000d\u000a        efficient conservation actions,\u000d\u000a        because management decisions can be reached earlier or with more\u000d\u000a        certainty\".\u000d\u000a      Design and analysis of occupancy studies [3.5-3.6]\u000d\u000a      In 2009, Guillera-Arroita visited Sumatra and provided training\u000d\u000a        on occupancy modelling to 30\u000d\u000a        people from the Indonesian Ministry of Forestry and 8 NGOs [5.7].\u000d\u000a        A training manual was\u000d\u000a        developed and translated into Indonesian [5.7]. Many of the\u000d\u000a        participants were co-authors of a\u000d\u000a        landmark 42-author article reporting the first island-wide assessment of\u000d\u000a        tigers in Sumatran\u000d\u000a        rainforests, based on occupancy surveys conducted in priority tiger\u000d\u000a        landscapes across Sumatra.\u000d\u000a        Guillera-Arroita led the analysis, with advice from Morgan\u000d\u000a        and Ridout. There is great public\u000d\u000a        interest in this flagship conservation species and this open-access\u000d\u000a        article, which is readily\u000d\u000a        accessible to the public via the Wikipedia page on Sumatran Tiger, plays\u000d\u000a        an important role in\u000d\u000a        informing the public, as well as those more actively involved in its\u000d\u000a        conservation, of the current\u000d\u000a        status of the species. From November 2011 to July 2013, this article has\u000d\u000a        been viewed 3583\u000d\u000a          times.\u000d\u000a        The survey informed the Indonesian Ministry of Forestry's National\u000d\u000a          Tiger Recovery\u000d\u000a          Programme which is using \"science-based adaptive management\" by\u000d\u000a        providing the baseline\u000d\u000a        against which future progress in conservation will be gauged. The survey\u000d\u000a        is highlighted in the\u000d\u000a        Global Tiger Recovery Program Implementation Report (Box 6, p.18) [5.8].\u000d\u000a        Priorities identified in\u000d\u000a        the report (p.19) include development of a $10million Global Environment\u000d\u000a        Facility grant for\u000d\u000a        transforming the effectiveness of biodiversity conservation in priority\u000d\u000a        Sumatran tiger landscapes,\u000d\u000a        and \"the occupancy approach of Guillera-Arroita will be the main\u000d\u000a        monitoring technique\" [5.7].\u000d\u000a        New methodology for camera trap data [3.6] was also motivated by\u000d\u000a        Sumatran tiger data, but has\u000d\u000a        since been applied in many other conservation and wildlife management\u000d\u000a        projects, including\u000d\u000a        human-tiger conflict in Nepal;\u000d\u000a        carnivores in South\u000d\u000a          Africa and Madagascar;\u000d\u000a        sympatric cat species in\u000d\u000a        Thailand; feral cats and dingoes in Australia;\u000d\u000a        and various cat species in Thailand,\u000d\u000a        Borneo and\u000d\u000a        Brazil. Rapid uptake\u000d\u000a        of the methodology was facilitated by (i) publication of a follow-up\u000d\u000a        paper in J.\u000d\u000a          Zoology, targeted at ecologists, with an accompanying podcast\u000d\u000a        discussing the work in non-\u000d\u000a        technical terms and (ii) provision of an R package (overlap) developed\u000d\u000a        with a scientist at Wildlife\u000d\u000a        Conservation Society, Malaysia, and available through the Central R\u000d\u000a        Archive Network (CRAN).\u000d\u000a        Occupancy surveys are widely used for amphibians and reptiles, for\u000d\u000a        example by fieldworkers\u000d\u000a        involved with National Amphibian and Reptile Recording Scheme (http:\/\/www.narrs.org.uk\/)\u000d\u000a        and it\u000d\u000a        is widely recognised that there is scope to improve survey technique.\u000d\u000a        For example, the government\u000d\u000a        is required by EU regulations to undertake surveillance to assess the\u000d\u000a        status of the great crested\u000d\u000a        newt (Triturus cristatus), and a recent report commissioned by\u000d\u000a        Natural England\u000d\u000a        (NECR080, 2011)\u000d\u000a        noted that \"attempts at assessing population status have been hampered\u000d\u000a        by problems with survey\u000d\u000a        data\". To address this need, Guillera-Arroita, Morgan, Ridout\u000d\u000a        and NERC fellow McCrea have\u000d\u000a        collaborated with scientists from Kent's Durrell\u000d\u000a          Institute of Conservation Ecology (DICE) to improve\u000d\u000a        the design and analysis of amphibian and reptile surveys, for example to\u000d\u000a        detect trends in\u000d\u000a        occupancy over time. Morgan was part of a NERC Knowledge\u000d\u000a        Transfer grant (\"Development of\u000d\u000a        standardised protocols for assessing reptile and amphibian populations\")\u000d\u000a        led by Griffiths (DICE),\u000d\u000a        aimed at improving current practice.\u000d\u000a    \u000d\u000a    As a final example of impact, our work on occupancy, and the accompanying\u000d\u000a      software SODA from\u000d\u000a      [3.5], has been used by staff of the Albany Pine Bush Preserve\u000d\u000a      Commission and the Wildlife\u000d\u000a      Diversity Unit of New York State Department of Environmental Conservation\u000d\u000a      to design occupancy\u000d\u000a      surveys to support a state recovery plan and monitoring program for the\u000d\u000a      Frosted Elfin butterfly\u000d\u000a      (Callophrys irus), a species that is threatened under New York\u000d\u000a      conservation law [5.9].\u000d\u000a    Summary: The beneficiaries of the research are individuals,\u000d\u000a      charities and environmental agencies\u000d\u000a      that collect and analyse ecological data and the organisations that plan\u000d\u000a      and implement\u000d\u000a      management and conservation strategies that utilise these data. The cited\u000d\u000a      examples illustrate how\u000d\u000a      novel statistical methodology developed at Kent has changed the practice\u000d\u000a      of wildlife managers and\u000d\u000a      conservation ecologists in collecting and analysing their data. As we have\u000d\u000a      described, considerable\u000d\u000a      effort is expended to encourage uptake. The impact has worldwide reach\u000d\u000a      because the methods\u000d\u000a      are generic and used by ecologists working on many different species, even\u000d\u000a      when the initial\u000d\u000a      methodological development was motivated by a particular application.\u000d\u000a      Impact is also significant,\u000d\u000a      because the species involved are often critically endangered and efficient\u000d\u000a      data collection and\u000d\u000a      analysis is essential to provide reliable inputs to management and\u000d\u000a      conservation programmes.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The impact of statistical ecology research at Kent\u000d\u000a      is on both the survey design of data collection\u000d\u000a      on wild animals, and the analysis of the resulting data. As a\u000d\u000a      result of our research, better quality\u000d\u000a      data are being collected more efficiently, and a wide range of new methods\u000d\u000a      of data analysis are\u000d\u000a      being used. This is essential for the conservation and management of wild\u000d\u000a      animal populations and\u000d\u000a      the preservation of biodiversity. New methods developed at Kent are now\u000d\u000a      standard tools used in\u000d\u000a      ecology. Examples of impact are improved understanding of the decline of\u000d\u000a      British farmland birds,\u000d\u000a      underpinning conservation action plans; and analysis of data from tiger\u000d\u000a      surveys, supporting the\u000d\u000a      Indonesian Government's National Tiger Recovery Plan.\u000d\u000a    ","ImpactType":"Environmental","Institution":"\u000d\u000a    University\u000d\u000a        of Kent\u000d\u000a    ","Institutions":[{"AlternativeName":"Kent (University of)","InstitutionName":"University of Kent","PeerGroup":"B","Region":"South East","UKPRN":10007150}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2193733","Name":"Auckland"},{"GeoNamesId":"5879092","Name":"Alaska"},{"GeoNamesId":"2992166","Name":"Montpellier"},{"GeoNamesId":"5128638","Name":"New York"}],"References":"\u000d\u000a    All references except [3.5] are papers in international journals.\u000d\u000a      The research reported in [3.5] is the\u000d\u000a      subject of six refereed papers in international journals.\u000d\u000a    \u000a[3.1] Catchpole, E.A., Morgan, B.J.T., Coulson, T.N., Freeman,\u000d\u000a        S.N. and Albon, S.D. (2000)\u000d\u000a      Factors influencing Soay sheep survival. Journal of the Royal\u000d\u000a        Statistical Society Series C-Applied\u000d\u000a        Statistics, 49, 453-472. doi: 10.1111\/1467-9876.00205\u000d\u000a    \u000a\u000a[3.2] Coulson, T., Catchpole, E.A., Albon, S.D., Morgan,\u000d\u000a        B.J.T., Pemberton, J.M., Clutton-Brock,\u000d\u000a      T.H., Crawley, M.J. and Grenfell, B.T. (2001) Age, sex, density, winter\u000d\u000a      weather, and\u000d\u000a      population crashes in Soay sheep. Science, 292, 1528-1531.\u000d\u000a      doi: 10.1126\/science.292.5521.1528\u000d\u000a    \u000a\u000a[3.3] Catchpole, E.A., Freeman, S.N., Morgan, B.J.T.\u000d\u000a      and Harris, M.P. (1998) Integrated\u000d\u000a      recovery\/recapture data analysis. Biometrics, 54, 33-46.\u000d\u000a      doi: 10.2307\/2533993\u000d\u000a    \u000a\u000a[3.4] Besbeas, P., Freeman, S.N., Morgan, B.J.T.\u000d\u000a      and Catchpole, E.A. (2002) Integrating mark-recapture-recovery\u000d\u000a      and census data to estimate animal abundance and demographic\u000d\u000a      parameters. Biometrics, 58, 540-547. doi:\u000d\u000a      10.1111\/j.0006-341X.2002.00540.x\u000d\u000a    \u000a\u000a[3.5] Guillera-Arroita, G. (2012) Occupancy modelling: study\u000d\u000a      design and models for data collected\u000d\u000a      along transects. PhD thesis, University of Kent. (Supervisors B.J.T.\u000d\u000a        Morgan and M.S.\u000d\u000a        Ridout).\u000d\u000a    \u000a\u000a[3.6] Ridout, M.S. and Linkie, M. (2009) Estimating overlap of\u000d\u000a      daily activity patterns from camera\u000d\u000a      trap data. Journal of Agricultural, Biological and Environmental\u000d\u000a        Statistics, 14, 322-337.\u000d\u000a      doi: 10.1198\/jabes.2009.08038\u000d\u000a    \u000a(References marked with a star best indicate the quality of the\u000d\u000a      underpinning research.)\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"5","Level2":"2","Subject":"Environmental Science and Management"}],"Sources":"\u000d\u000a    [5.1] Videos of PhD student research, including some of the\u000d\u000a      research from [3.5], help practicing\u000d\u000a      ecologists to rapidly familiarise themselves with novel methodology and\u000d\u000a      see how it should be\u000d\u000a      applied: http:\/\/www.methodsinecologyandevolution.org\/view\/0\/VideoPodcastArchive.html.\u000d\u000a    \u000d\u000a      [5.2] An example of textbook discussion of the Soay sheep models\u000d\u000a        [3.1, 3.2] is Chapter 1 of J.\u000d\u000a        Clark's Models for Ecological Data, Princeton University Press\u000d\u000a        (2007). ISBN:\u000d\u000a        9780691122625.\u000d\u000a      [5.3] The estimate of the survival of shags from [3.3]\u000d\u000a        for Birds of the Western Palearctic is in\u000d\u000a        Wanless, S. &amp; Harris, M.P.1997. Phalacrocorax aristotelis\u000d\u000a        Shag. BWP Update 1: 3-13.\u000d\u000a    \u000d\u000a    [5.4]\u000d\u000a        Probable causes of the decline in lapwings are discussed at\u000d\u000a      http:\/\/www.bto.org\/birdtrends2010\/wcrlapwi.shtml\u000d\u000a    [5.5] Letter from the Director of Science at the British Trust for\u000d\u000a      Ornithology confirming that BTO is\u000d\u000a      routinely using IPM and highlighting the impact of Kent's work on\u000d\u000a      \"ecological science within\u000d\u000a      the UK and the policy-relevant work that flows from this.\" (See Contact\u000d\u000a      1.)\u000d\u000a    [5.6] Email from the Head of the Ecology Department at the Swiss\u000d\u000a      Ornithological Institute\u000d\u000a      explaining how research at Kent on IPM has changed the practice of\u000d\u000a      population ecologists.\u000d\u000a      (See Contact 3.)\u000d\u000a    [5.7] Email from the Technical Manager, Fauna &amp; Flora\u000d\u000a      International, Program Aceh, Indonesia,\u000d\u000a      highlighting the importance of [3.5] to conservation work on the\u000d\u000a      Sumatran tiger and\u000d\u000a      confirming the impact of the training course run in Indonesia. (See\u000d\u000a      Contact 2.)\u000d\u000a    [5.8] The Global Tiger Recovery Program Implementation Report,\u000d\u000a      highlighting the impact of\u000d\u000a      occupancy modelling is at: http:\/\/www.globaltigerinitiative2013.org\/site\/wp-\u000d\u000a        content\/uploads\/2012\/07\/GTRP_Implementation_Report_2012.pdf\u000d\u000a    [5.9] Use of SODA software from [3.5] to plan occupancy\u000d\u000a      surveys for the Frosted Elfin butterfly is\u000d\u000a      reported in Bried et al (2012) Northeastern Naturalist, 19,\u000d\u000a      673-684.\u000d\u000a    If any of the links above is no longer working, pdf copies are\u000d\u000a        available on request.\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    New statistical tools for ecologists\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Research in statistical ecology undertaken at Kent over more than 25\u000d\u000a      years has involved many\u000d\u000a      PhD students and RAs, as well as permanent members of staff. The specific\u000d\u000a      underpinning\u000d\u000a      statistical ecology research cited here was conducted at Kent by Besbeas\u000d\u000a      (RA 1999-2002, lecturer\u000d\u000a      2003-2006, RA 2013-present), Freeman (RA 1994-1997), Guillera-Arroita\u000d\u000a      (PhD student, 2009-2012),\u000d\u000a      Morgan (1972-present), Ridout (2000-present).\u000d\u000a    In 2005, the National Centre for Statistical Ecology (NCSE, www.ncse.org.uk)\u000d\u000a      was established\u000d\u000a      as a joint research centre between the Universities of Kent, Cambridge\u000d\u000a      and St\u000d\u000a        Andrews, supported\u000d\u000a      by an EPSRC Mathematics Multidisciplinary Critical Mass Award. In\u000d\u000a      2010, NCSE expanded to\u000d\u000a      include the Universities of Bath,\u000d\u000a      Bristol,\u000d\u000a      Exeter,\u000d\u000a      Glasgow\u000d\u000a      and Sheffield,\u000d\u000a      together with the Centre for\u000d\u000a      Ecology and Hydrology (CEH), while the University of Cambridge dropped\u000d\u000a      out, following departure\u000d\u000a      of staff. This expansion was achieved with the aid of a joint EPSRC\/NERC\u000d\u000a      grant, and total funding\u000d\u000a      to date exceeds &#163;2,000,000.\u000d\u000a    To ensure that our research is relevant and responds to the changing\u000d\u000a      demands of the real world,\u000d\u000a      as technology advances, since 2010 NCSE has included four non-academic Project\u000d\u000a        Partners: Biomathematics and Statistics Scotland; the Centre for\u000d\u000a      Environment, Fisheries and\u000d\u000a      Aquaculture Science (Cefas); the Game and Wildlife Conservation Trust; and\u000d\u000a      Marine Scotland.\u000d\u000a      Estimating the survival of wild animals and integrated population\u000d\u000a        modelling. The use of\u000d\u000a        covariates in describing mortality of wild animals originated at\u000d\u000a      Kent. The ecological insights that\u000d\u000a      can result from this approach are illustrated by [3.1], which\u000d\u000a      models the mortality of Soay sheep\u000d\u000a      (Ovis aries), showing how population regulation depends in a\u000d\u000a      complex manner on measures of\u000d\u000a      climate, population density and individual characteristics, differentially\u000d\u000a      for animals of different ages.\u000d\u000a      This led on to a highly-cited Science paper [3.2].\u000d\u000a    Research at Kent has also pioneered methods of integrated population\u000d\u000a        modelling (IPM) [3.3,\u000d\u000a        3.4], which are now in widespread use by wildlife conservation\u000d\u000a      authorities, including the innovative\u000d\u000a      use of state-space models. This approach integrates different\u000d\u000a      types of information into a single\u000d\u000a      analysis, which can greatly reduce the uncertainty of estimates or indeed\u000d\u000a      make possible the\u000d\u000a      estimation of features that cannot be estimated from a single data source.\u000d\u000a      The method can also\u000d\u000a      deliver increased power for detecting the possibly subtle effects of\u000d\u000a      covariates, such as climatic\u000d\u000a      factors. This is important for understanding the effects of climate\u000d\u000a      change. Kent statisticians have\u000d\u000a      applied the methodology to several bird species, including northern\u000d\u000a      lapwings (Vanellus vanellus),\u000d\u000a      grey herons (Ardea cinerea), snow geese (Chen caerulescens)\u000d\u000a      and great cormorants\u000d\u000a      (Phalacrocorax carbo), and have continued to develop the\u000d\u000a      methodology; for example, by extending\u000d\u000a      it to multi-site models, where animals can move between sites.\u000d\u000a    Occupancy modelling and camera trap data. Occupancy surveys, which\u000d\u000a      aim to estimate the\u000d\u000a      proportion of study sites at which a species is present, are widely used\u000d\u000a      by practicing ecologists and\u000d\u000a      wildlife managers. Direct estimates of the proportion of sites occupied\u000d\u000a      are misleading, because\u000d\u000a      species are often not detected when present, and the statistical analysis\u000d\u000a      must account for this. The\u000d\u000a      work of Guillera-Arroita [3.5] covers both design and analysis of\u000d\u000a      occupancy studies, including the\u000d\u000a      development of hidden Markov models for occupancy data collected\u000d\u000a      along a transect. The\u000d\u000a      methods were developed for studying tigers in Sumatra, but are applicable\u000d\u000a      much more widely. One\u000d\u000a      method of assessing species presence at a site is the use of camera traps\u000d\u000a      &#8212; cameras linked to\u000d\u000a      detectors that trigger a photograph when an animal passes; these have\u000d\u000a      revolutionised the study of\u000d\u000a      cryptic animal species. The time of the photograph is recorded and [3.6]\u000d\u000a      provides a method of\u000d\u000a      using these data to estimate temporal overlap of competing species or\u000d\u000a      predator and prey species.\u000d\u000a    "},{"CaseStudyId":"938","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000a    The NIST Digital Library of Mathematical Functions makes the properties\u000a      of transcendental functions freely available to the general public. The\u000a      online, interactive format of the Digital Library is a new feature, with\u000a      its print companion, the new Handbook, providing further detailed\u000a      information for practitioners such as industrial scientists. This has a\u000a      commercial impact, in particular on the US economy and on the publisher of\u000a      the Handbook (Cambridge University Press). It also informs the practice of\u000a      teachers and researchers outside mathematics, and engineers and\u000a      professionals outside academia.\u000a    Professionals in all fields of science and engineering need reliable and\u000a      detailed technical information to be readily available, in order to\u000a      develop specific industrial applications and carry out research. In the\u000a      past, such information was collected into handbooks, which would be\u000a      revised and reprinted according to demand. Arguably the most successful\u000a      mathematical handbook of all time is the National Bureau of Standards\u000a      Handbook of Mathematical Functions (Abramowitz &amp; Stegun), first\u000a      published in 1964 [5.7]. Although still a classic, it has\u000a      gradually become outdated, as important new properties of special\u000a      functions, and new families of such functions, have been discovered over\u000a      the last fifty years. Nevertheless, it has served as a template for a new\u000a      project, the Digital Library of Mathematical Functions, led by NIST.\u000a    The aim of the NIST Digital Library is to make carefully selected and\u000a      accurate data on the elementary and higher mathematical functions\u000a      available to the public, with experienced professionals being the target\u000a      audience. The primary criterion for inclusion of information is usefulness\u000a      in disciplines outside mathematics. As well as the online hypertext\u000a      version, which includes interactive graphics and search tools, there is a\u000a      more traditional print version, the NIST Handbook of Mathematical\u000a      Functions, published by Cambridge University Press (2010). According to\u000a      the General Editor, Daniel Lozier at NIST, \"The Handbook will be used by\u000a      anyone who is doing a project in mathematics, science or engineering\" [5.7].\u000a    The fact that the Digital Library of Mathematical Functions is funded by\u000a      NIST, a US government agency, which is part of the US Department of\u000a      Commerce, clearly indicates the importance of this information to\u000a      scientific professionals. As well as their immediate uses in mathematics\u000a      and physics (for example, in describing the pattern of waves in the wake\u000a      of a moving ship [5.7]), mathematical functions are applied in\u000a      manufacturing technology, civil engineering, bioscience, electronics, and\u000a      computer networks. The impact of the NIST Digital Library on these areas\u000a      has so far been measured indirectly, in two different ways. Since the site\u000a      http:\/\/dlmf.nist.gov\/ was opened, NIST estimates that more than half\u000a        of the downloads were by non-academic users [5.1]. The web usage\u000a      data analysis of NIST [5.2] shows in particular that Chapter 32\u000a      written by Clarkson was downloaded more than 2000 times per year,\u000a      so that NIST estimates that more than 1000 non-academic users are\u000a      downloading Clarkson's chapter each year. At the request of NIST,\u000a      a citation analysis was performed to compare usage of the old NBS Handbook\u000a      (Abramowitz &amp; Stegun) and the new NIST Handbook [5.1]. The\u000a      latter analysis shows that while the NBS Handbook is still very widely\u000a      used, the amount of usage has gradually started to decline since the\u000a      appearance of the NIST Handbook: citations to the old Handbook increased\u000a      by 5% from 2007-2009, but decreased by 12% from 2010-2012; while citations\u000a      to the new Handbook more than doubled from 2011-2012. According to NIST,\u000a      15 of the top 20 journals that cite the new NIST Handbook of Mathematical\u000a      Functions are not Mathematical journals, confirming the role of\u000a      the NIST Handbook of Mathematical Functions as a tool for scientists\u000a        and engineers.\u000a    The new Handbook has already gained more than 600 citations (according to\u000a      Google Scholar on 31st July 2013 [5.8]), including in\u000a      the following disciplines: Chemistry, Forest Research, Optics, Water\u000a      Resources. According to Cambridge University Press (CUP), the NIST\u000a      Handbook is expected to become a bestseller just like the old Handbook,\u000a      and CUP receives considerable economic benefit and prestige by\u000a      publishing the Handbook. This is confirmed by the Editorial Director of\u000a      CUP: \"Since publication in 2010, the Handbook has seen strong sales\u000a      throughout the world, especially in North America and Europe, has\u000a      attracted much praise for quality and breadth of content, and for its high\u000a      production values. It is not just the print version of the book that users\u000a      appreciate; it is also available as a browsable CD that comes packaged\u000a      with the book and as an addressable database online. These features have\u000a      extended the usability of the Handbook over its predecessor and made it a\u000a      resource that can be readily used worldwide, not just by\u000a      mathematicians, but in fundamental science, applied science and\u000a      engineering.\" [5.10] This is backed up by several reviews in\u000a      industrial and engineering publications. For instance, the US Society for\u000a      Industrial and Applied Mathematics (SIAM) wrote that \"The NIST Handbook is\u000a      indeed a monumental achievement, and the many, many individuals who\u000a      participated in its creation and dissemination are to be congratulated and\u000a      thanked\" [5.11]. And Optics and Photonics News recognized that\u000a      \"The National Institute of Standards and Technology (NIST) and Cambridge\u000a      University Press are to be congratulated for publishing a treasury. It is\u000a      eminently readable with clear, sharp, high-contrast text, mathematical\u000a      notation and colored graphs and figures. People who work with functions\u000a      will delight in this handbook\" [5.11].\u000a    According to statements from a report to the US House of Representatives,\u000a      \"NIST carries out in a superb fashion an absolutely vital role in\u000a      supporting as well as facilitating the further development of the\u000a      technological base of the U.S. economy\" and, in particular, \"The Digital\u000a      Library of Mathematical Functions is without peer in the broader\u000a      community\" [5.9]. This acknowledges the significant economic\u000a      impact of the DLMF.\u000a    The innovative nature of the Digital Library, in the way that it makes\u000a      the properties of transcendental functions available online, has already\u000a      been acknowledged in US Government circles. The DLMF was one of only 10\u000a      projects (out of 200 nominated) to win a 2011 Government Computer News\u000a      (GCN) Award for Outstanding Information Technology Achievement in\u000a      Government [5.3]. This acknowledges the high quality and easy\u000a      accessibility of the information provided and further recognition by US\u000a      Government.\u000a    The Digital Library is now used not only to enhance academic knowledge\u000a      but also as a resource which informs the teaching practice of many\u000a      disciplines outside mathematics, hence changing practice in the HE sector\u000a      internationally. For instance, in the Department of Astronomy of the\u000a      University of Tokyo the Digital Library is used to teach Celestial\u000a      Mechanics [5.4]; and in the School of Engineering of the Catholic\u000a      University of America it is used to teach Acoustic Metrology [5.5].\u000a      (These examples are indicative of the immediate impact of the Handbook on\u000a      teaching practice in higher education; other examples include teaching at\u000a      Princeton [Astronomy, Observing and Modeling the Universe], and at\u000a      Colorado State University [Mechanical Engineering Problem Solving].) The\u000a      Digital Library is also used as a standard reference by the US army [5.6].\u000a    Summary: The research on Painlev&#233; equations at Kent has\u000a      contributed to the wealth of the US and other economies, and CUP, via the\u000a      DLMF and the new Handbook. The DLMF is used widely by scientists and\u000a      practitioners, and has attracted much praise contributing to the prestige\u000a      of CUP. It has also influenced teaching practice in disciplines other than\u000a      mathematics across the globe.\u000a    ","ImpactSummary":"\u000a    The US National Institute of Science &amp; Technology (NIST) \"Digital\u000a      Library of Mathematical Functions\" (DLMF) available at http:\/\/dlmf.nist.gov\/\u000a      is an online resource which informs the general public by making detailed\u000a      properties of elementary and higher functions freely available to all. The\u000a      DLMF together with its print companion, the new NIST Handbook of\u000a      Mathematical Functions, published by Cambridge University Press (CUP,\u000a      2010), replaces and updates the National Bureau of Standards Handbook of\u000a      Mathematical Functions (Abramowitz &amp; Stegun), a classic source which\u000a      is one of the best-selling mathematics texts of all time. The fact that\u000a      the 10-year DLMF project has been led and funded by NIST, part of the US\u000a      Department of Commerce, is a measure of the economic importance of making\u000a      this information publically available, so that science and engineering\u000a      practitioners can use it freely. In addition, CUP has already received\u000a      considerable benefit through the strong sales of the NIST Handbook since\u000a      its publication, and prestige from the praise that the DLMF has already\u000a      received from the US government and industrial societies.\u000a    A chapter of the new Handbook has been contributed by a member of the\u000a      School of Mathematics, Statistics and Actuarial Science, and is based on\u000a      research carried out at Kent. The provision of this information in an\u000a      interactive format raises awareness and understanding, and enhances the\u000a      work of practitioners such as industrial scientists and teachers in\u000a      disciplines outside mathematics, by allowing them easy and immediate\u000a      access to the most relevant and up to date research results in this area.\u000a    ","ImpactType":"Cultural","Institution":"\u000a    University of Kent\u000a    ","Institutions":[{"AlternativeName":"Kent (University of)","InstitutionName":"University of Kent","PeerGroup":"B","Region":"South East","UKPRN":10007150}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[3.1] A P Bassom, P A Clarkson, C K Law and J B McLeod, Application\u000a        of uniform asymptotics to the second Painlev&#233; transcendent, Archive\u000a      for Rational Mechanics and Analysis, 143 (1998) 241-271. doi:\u000a      10.1007\/s002050050105\u000a    \u000a\u000a[3.2] E L Mansfield and H N Webster, On one-parameter\u000a        families of Painlev&#233; III, Studies in Applied Mathematics, 101 (1998)\u000a      321-341. doi: 10.1111\/1467-9590.00096\u000a    \u000a\u000a[3.3] P A Clarkson and E L Mansfield, The second\u000a        Painlev&#233; equation, its hierarchy and associated special polynomials,\u000a      Nonlinearity, 16 (2003) R1-R26. doi: 10.1088\/0951-7715\/16\/3\/201\u000a    \u000a\u000a[3.4] P A Clarkson, Special polynomials associated with\u000a        rational solutions of the Painlev&#233; equations and applications to soliton\u000a        equations, Computational Methods and Function Theory, 6 (2006)\u000a      329-401. doi: 10.1007\/BF03321618\u000a    \u000a\u000a[3.5] P A Clarkson, Painlev&#233; equations - nonlinear special\u000a        functions, in \"Orthogonal Polynomials and Special Functions:\u000a      Computation and Application\", Editors F Marcell&#225;n and W van Assche,\u000a      Lecture Notes in Mathematics, 1883, Springer-Verlag, Berlin (2006) pp.\u000a      331-411. doi: 10.1007\/978-3-540-36716-1_7\u000a    \u000a(References marked with a star best indicate the quality of the\u000a      underpinning research.)\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    [5.1] Citation Analysis for the NIST Handbook of Mathematical\u000a      Functions, 2007-2012, from 24th July 2013, showing wide usage\u000a      outside academia during the REF period. (See Contact 2.)\u000a    [5.2] Downloads data analysis for Chapter 32 of the Digital\u000a      Library of Mathematical Functions, 2011-2012, from 29th May\u000a      2013, showing wide usage outside academia during the REF period. (See\u000a      Contact 2.)\u000a    [5.3] Award from the US Government Computer News magazine:\u000a    http:\/\/www.nist.gov\/itl\/math\/dlmf-102511.cfm\u000a    [5.4] Reading list for Celestial Mechanics, Department of\u000a      Astronomy, University of Tokyo:\u000a    http:\/\/www.s.u-tokyo.ac.jp\/en\/current\/syllabus\/pdf\/2012\/astron.pdf\u000a    [5.5] Syllabus in Acoustic Metrology, School of Engineering,\u000a      Catholic University of America:\u000a    http:\/\/faculty.cua.edu\/vignola\/Vignola_CUA\/ME_661_files\/ME%20661%20syllabus%20Summer%202011.pdf\u000a    [5.6] US army report on standards for measurements and\u000a      mathematical constants confirming that the Digital Library of Mathematical\u000a      Functions is a standard reference for the US army. This report is\u000a      available at:\u000a    http:\/\/www.wsmr.army.mil\/RCCsite\/Documents\/156-10_%20Physical%20Constants,%20Units,%20and%20Uncertainty%20Standard\/156-10_%20Physical%20Constants,%20Units,%20and%20Uncertainty%20Standard.pdf\u000a    [5.7] The Birth of a Classic...Take Two: NIST video available at\u000a    http:\/\/www.youtube.com\/watch?v=Exf02R1FnXY\u000a    showing that the original handbook is the most cited NIST resource and\u000a      the updated 2010 online version is more usable for science professionals.\u000a    [5.8] Google scholar entry for the Handbook of Mathematical\u000a      Functions showing the wide usage of the Handbook outside Mathematics:\u000a    http:\/\/scholar.google.com\/scholar?cluster=10873204128209726657&amp;hl=en&amp;as_sdt=2005&amp;\u000a        sciodt=0,5\u000a    [5.9] Statement of Professor Ross B. Corotis before the\u000a      Subcommittee on Technology Committee on Science, Space, and Technology, US\u000a      House of Representatives, March 20, 2013, confirming the unique role of\u000a      the DLMF in the broader community.\u000a    http:\/\/science.house.gov\/sites\/republicans.science.house.gov\/files\/documents\/HHRG-113-SY19-WState-RCorotis-20130320.pdf\u000a    [5.10] Email from an Editorial Director of CUP confirming the\u000a      benefits and prestige gained from CUP by publishing the NIST Handbook of\u000a      Mathematical Functions. (See Contact 3.)\u000a    [5.11] Reviews on the NIST Handbook of Mathematical Functions and\u000a      DLMF confirming the prestige gained by CUP in publishing this book are\u000a      available at:\u000a    http:\/\/www.cambridge.org\/pa\/knowledge\/isbn\/printView\/item6005277\/?site_locale=es_PA\u000a    If any of the links above is no longer working, pdf copies are\u000a        available on request. \u000a    ","Title":"\u000a    The Digital Library of Mathematical Functions as a public resource\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The research that contributed to the DLMF project and the NIST Handbook\u000a      was carried out at Kent by Clarkson (1995-), as well as by Mansfield\u000a      (1995-) and PhD student Webster (1995-1998).\u000a    The original National Bureau of Standards Handbook (Abramowitz &amp;\u000a      Stegun) was first published in 1964, and was subsequently reprinted many\u000a      times. One of the main reasons for producing the DLMF and the completely\u000a      new NIST Handbook was that research into mathematical functions has\u000a      advanced significantly, meaning not only that new information on classical\u000a      special functions needed to be presented, but also that new special\u000a      functions needed to be included, which were not even mentioned in the old\u000a      Handbook.\u000a    Chapter 32, Painlev&#233; Transcendents, contributed by Clarkson (sole\u000a      author), concerns a topic that was absent from Abramowitz &amp; Stegun's\u000a      Handbook, whose relevance to the modelling of scientific problems\u000a      involving nonlinearity has only begun to be appreciated in the last few\u000a      decades.\u000a    Almost all classical special functions are solutions of linear\u000a      differential equations, and indeed linear equations provide accurate\u000a      models in a huge variety of contexts; particular examples in the NIST\u000a      Handbook include the intensity profile of light in a rainbow (Airy\u000a      functions), and Kelvin's wave pattern in the wake of a ship. However, in\u000a      the latter half of the twentieth century, our understanding of nonlinear\u000a      models has increased substantially, and it is now well appreciated that\u000a      there are essential features of nonlinear systems that cannot be captured\u000a      by linear approximations.\u000a    Painlev&#233; transcendents are solutions of special nonlinear ordinary\u000a      differential equations, and they appear in models for many different\u000a      natural and man-made phenomena, including energy levels and scattering off\u000a      heavy nuclei, self-similar behaviour in wave-breaking, and the statistics\u000a      of the bus delivery system in Cuernavaca, Mexico (among many other\u000a      examples). Clarkson has worked on the detailed properties of\u000a      Painlev&#233; equations for several decades, including from 1995 at the\u000a      University of Kent, and is acknowledged as a world expert on the subject,\u000a      which is why he was chosen by NIST as the author of the chapter for the\u000a      DLMF and the NIST Handbook.\u000a    Of particular importance are explicit solutions of Painlev&#233; equations,\u000a      which occur for special parameter values. In research carried out at Kent,\u000a      Mansfield and Webster [3.2] constructed one- parameter\u000a      families of solutions of the third Painlev&#233; equation, while subsequent\u000a      work by Clarkson and Mansfield concerned the second\u000a      Painlev&#233; equation and the specific structure of its rational solutions [3.3],\u000a      which have applications in nonlinear wave equations admitting solitons [3.4].\u000a      Earlier work by Bassom, Clarkson, Law and McLeod concerned uniform\u000a      asymptotics for the second Painlev&#233; transcendent [3.1]. These\u000a      works were specifically cited in Chapter 32 of the NIST Handbook.\u000a    The general properties of Painlev&#233; equations (Hamiltonian structure,\u000a      asymptotics, special solutions) allow them to be viewed as nonlinear\u000a      special functions. The substantial survey article [3.5] highlights\u000a      these properties and pinpoints their relevance in a wide variety of\u000a      application areas. This article underpins the contribution by Clarkson to\u000a      the DLMF and the NIST Handbook.\u000a    "},{"CaseStudyId":"1718","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    When a sea wave hits a structure such as a seawall, a series of violent\u000d\u000a      fluid flows occur, as vividly illustrated at any harbour during high\u000d\u000a      winds. These impacts may damage the structure and cause hazardous splashes\u000d\u000a      and overtopping. These flows are complex, and demand sophisticated\u000d\u000a      mathematical tools to analyse and predict their movements, forces and\u000d\u000a      outcomes. Better understanding of violent flows leads to improved design,\u000d\u000a      engineering and repair of such coastal structures, and also allows\u000d\u000a      time-dependent safety advice for users of harbour walls to be provided\u000d\u000a      with greater confidence.\u000d\u000a    Engineers need to interpret the damage made by waves. For example, how do\u000d\u000a      storm waves withdraw blocks from a seawall? Violent flows due to\u000d\u000a      breaking-wave impacts can damage a structure by over-straining materials,\u000d\u000a      fragmenting components and undermining foundations. Violent flows are\u000d\u000a      inherently difficult to compute, particularly in domains with complicated\u000d\u000a      shapes of boundary. The International Conference on Coastal Engineering\u000d\u000a      (see [1] above) is a forum of industrial research, at which there is\u000d\u000a      recurring impetus from the designers to the theorists to improve design\u000d\u000a      methods, by developing accurate and efficient mathematical models that\u000d\u000a      predict wave impact forces. The work ultimately has many benefits. For\u000d\u000a      example, wave splashes are hazardous to persons and vehicles on top of a\u000d\u000a      harbour wall, and the harbour master's safety advice may depend on a\u000d\u000a      confident knowledge of the risks from the waves in that setting.\u000d\u000a    Below we provide three specific examples of the use of Cooker's models of\u000d\u000a      the effects of wave impact in this REF period:\u000d\u000a    Charles Scott and Partners: In 2006, Cooker began a collaboration\u000d\u000a      with Charles Scott and Partners, Glasgow (consulting engineers,\u000d\u000a      providing services in civil engineering) who wanted to understand how and\u000d\u000a      why a gantry had been knocked down by storm-waves in Shetland in November\u000d\u000a      2005. The gantry was a 5.5 metre high free-standing steel structure with\u000d\u000a      angled struts, designed to allow a person to work safely at an elevated\u000d\u000a      viewpoint, on a beach with high wave exposure. The unexpected failure of\u000d\u000a      the original design presented a safety problem. The company was at a loss\u000d\u000a      to explain the failure, and needed to design a safe replacement. The\u000d\u000a      company had no method to predict the forces on the gantry, so they\u000d\u000a      approached Cooker on the strength of his publications and expertise in\u000d\u000a      predicting wave forces. Drawing on his research, ([2], [4]) Cooker\u000d\u000a      described how and why the gantry was knocked down onto the beach. He\u000d\u000a      showed that the failure was due to the sudden huge loads (and moments\u000d\u000a      about the base) that were exerted on the cylindrical support when struck\u000d\u000a      by waves. The model was crucial to the company's understanding and their\u000d\u000a      subsequent design-work on the new gantry, to obtain the right number of\u000d\u000a      struts and their angles of inclination. Cooker was also able to dispel a\u000d\u000a      false notion that the two front struts of the structure could in some way\u000d\u000a      shelter the two rear struts from wave impact. Using these ideas, a\u000d\u000a      replacement gantry was built to a new design and successfully installed.\u000d\u000a    The company stated \"we would like to take this opportunity to offer our\u000d\u000a    sincere thanks for your invaluable help during the design period\"\u000d\u000a    (corroborating source [A]). Thus, Cooker's theoretical work has provided Charles\u000a      Scott and Partners with a successful predictive method to model, and\u000d\u000a    hence design, all future structures exposed to wave impact. \u000d\u000a\u0009HRWallingford Ltd: In the coastal wave-impact context, Cooker was a\u000d\u000a    partner in the EPSRC-funded ViFSNet group (2001-2004). This network\u000d\u000a    identified critical research problems that needed to be addressed post-2004,\u000d\u000a    as judged by professional engineers involved in the design and construction\u000d\u000a    of seawalls and harbours. This led to several collaborations, including with\u000d\u000a    HR Wallingford Ltd., who are a leading centre for (i)\u000d\u000a    coastal-engineering consultancy advice around the world, and (ii) the\u000d\u000a    modelling and computation of waves in the presence of coastal structures\u000d\u000a    (corroborating source [B]). This on-going commercial link ensures that\u000d\u000a    Cooker's research on wave impacts is used by industry.\u000a\u000d\u000a    \u000aAtkins UK: In addition, Cooker's expertise on wave impacts is\u000d\u000a      widely sought. For example, his expertise on offshore structures and\u000d\u000a      violent wave-structure interactions, has been applied to estimate the wave\u000d\u000a      impact forces on wave energy extraction devices. He has provided outcomes\u000d\u000a      of his research on wave impacts on several occasions in the past five\u000d\u000a      years to Atkins, global engineering and design consultants. The\u000d\u000a      interest by Atkins is in the need to account for the potentially\u000d\u000a      damaging and destructive wave forces, as well as the useful energy\u000d\u000a      available for extraction from steep and breaking waves, using devices of\u000d\u000a      novel design (corroborating source [C]).\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Mathematical models of violent flows developed by Dr Mark Cooker at UEA\u000d\u000a      have been adopted by industry. The work enhances the capabilities of\u000d\u000a      coastal engineers to design and repair seawalls and coastal structures,\u000d\u000a      and enhances their interpretation of damage inflicted by storm waves. The\u000d\u000a      research has direct industrial application, and is used to contain,\u000d\u000a      interpret and lessen sea-wave damage to structures. Commercial software\u000d\u000a      has proved inadequate in this field, compared with Cooker's mathematical\u000d\u000a      modelling, because computations alone cannot resolve the brief time-\u000d\u000a      scales and short length-scales over which there are large changes in\u000d\u000a      pressure, and sudden excursions of the liquid as splashes. An example of\u000d\u000a      this impact is the design of an observation gantry exposed to storm waves.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of East Anglia\u000d\u000a    ","Institutions":[{"AlternativeName":"East Anglia (University of)","InstitutionName":"University of East Anglia","PeerGroup":"B","Region":"East","UKPRN":10007789}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    (UEA authors in bold)\u000d\u000a    \u000a[1] Cooker, M.J. (2009) Theories of wave impact pressures on\u000d\u000a      coastal structures. Proceedings of the 31st International Conference\u000d\u000a        on Coastal Engineering, Hamburg 2008. World Scientific for ASCE,\u000d\u000a      3212-3223 ISBN: 9789814277402\u000d\u000a    \u000a\u000a[2] Cooker, M.J. and Peregrine, D.H. (1995) Pressure impulse\u000d\u000a      theory for liquid impact problems. Journal of Fluid Mechanics, 297,\u000d\u000a      193-214 DOI: 10.1017\/S0022112095003053\u000d\u000a    \u000a\u000a[3] Cooker, M.J. (2010) The flip-through of a plane inviscid jet\u000d\u000a      with a free surface. Journal of Engineering Mathematics, 67\u000d\u000a      (1-2), 137-152 DOI:10.1007\/s10665-009-9302-2\u000d\u000a    \u000a\u000a[4] Cox, S.J. and M.J. Cooker (1999) The motion of a\u000d\u000a      rigid body impelled by sea-wave impact. Applied Ocean Research, 21,\u000d\u000a      113-125 DOI: 10.1016\/S0141-1187(99)00005-X\u000d\u000a    \u000a\u000a[5] M&#252;ller, G., Hull, P., Allsop N.W.H., Bruce, T., Cooker, M.J. and\u000d\u000a      Franco, L. (2002) Wave effects on blockwork structures: model tests. Journal\u000a        of Hydraulic Research, 40 (2), 117-124 DOI:\u000d\u000a      10.1080\/00221680209499854\u000d\u000a    \u000a\u000a[6] M&#252;ller, G., W&#246;lters, G. and Cooker, M.J. (2003)\u000d\u000a      Characteristics of pressure pulses propagating through water-filled\u000d\u000a      cracks. Coastal Engineering, 49 (1), 83-98 DOI:\u000d\u000a      10.1016\/S0378-3839(03)00048-6\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"9","Level2":"11","Subject":"Maritime Engineering"},{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"4","Level2":"5","Subject":"Oceanography"}],"Sources":"\u000d\u000a    [A] Personal letter of thanks to Cooker from Charles Scott and\u000d\u000a        Partners, Glasgow. Held on file at UEA.\u000d\u000a    [B] Corroboration from HR Wallingford Ltd.\u000d\u000a    [C] Corroboration from Atkins UK. \u000d\u000a    ","Title":"\u000d\u000a    The Violent Forces on Coastal Structures due to Storm Waves\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2648579","Name":"Glasgow"},{"GeoNamesId":"8299621","Name":"Shetland"}],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    This study concerns the work of Dr Mark Cooker, appointed 1992.\u000d\u000a      The research was undertaken at UEA from 1993 up to the present.\u000d\u000a    In wave impact research Cooker has made significant applications of\u000d\u000a      pressure-impulse theory (which he co-developed) to practical situations\u000d\u000a      [1-4]. In particular the idea of pressure-impulse [2,4] is now widely used\u000d\u000a      (for example [5]) by coastal engineers for dynamic forces on seawalls, as\u000d\u000a      it is an efficient way of predicting loads on a structure during\u000d\u000a      breaking-wave impact and therefore has underpinned direct industrial\u000d\u000a      application [6].\u000d\u000a    The underlying mathematics presented in [2] was started while Cooker was\u000d\u000a      employed at Bristol University, but the paper required substantial further\u000d\u000a      work to satisfy reviewers. This additional theoretical work was carried\u000d\u000a      out by Cooker at UEA, and included a critical point of contention: the\u000d\u000a      occurrence of a singularity in some solutions where the boundary\u000d\u000a      conditions are discontinuous at a right angle in the boundary. This\u000d\u000a      revealed extreme violence in the corner of the fluid domain, and the need\u000d\u000a      for an inner solution to describe the splash-jet root. The resolution of\u000d\u000a      the primary importance of the outer solution was a crucial part of the\u000d\u000a      research, and its subsequent scientific development.\u000d\u000a    One of the underlying mathematical ideas that Cooker developed in [2],\u000d\u000a      and subsequently for example in [4], involves calculating the\u000d\u000a      pressure-impulse distribution in the water during impact. At each\u000d\u000a      geometric point the pressure-impulse is the time-integral of the large\u000d\u000a      transient pressure over the short duration of an impact. The pressure can\u000d\u000a      rise from ambient value to a peak of several atmospheres and decline\u000d\u000a      again, all within a few milliseconds. The time-integral of the pressure is\u000d\u000a      a mathematically flexible quantity with which to model significant changes\u000d\u000a      in the impacting wave flow. This idea is useful for design, because\u000d\u000a      simplified model equations and easily specified initial data predict the\u000d\u000a      sudden change in fluid velocity, the splash, the total impulse, and the\u000d\u000a      overturning moment on the structure that is hit. The theory [2,4] explains\u000d\u000a      the movement of debris along the seabed away from a seawall. The\u000d\u000a      impulses contribute to a wave-excavation of bed material that is then\u000d\u000a      pushed away from the foot of the wall by succeeding impacts. On a seawall,\u000d\u000a      the theory also predicts the forces made by the penetration of wave water\u000d\u000a      into confined spaces, such as cracks. From this the associated impulsive\u000d\u000a      forces on the interior of the structure can be estimated [6].\u000d\u000a    Experimental results have shown that high pressure coincides with the\u000d\u000a      start of a splash made by a wave impact. At a vertical wall, the forward\u000d\u000a      face of the wave is an accelerating and converging concave surface. From\u000d\u000a      the bottom of the surface, the splash jet emerges and climbs the wall,\u000d\u000a      accelerating as it ascends, ahead of the rest of the advancing wave face.\u000d\u000a      This ultra-violent flow is known as flip-through. Flip-through\u000d\u000a      coincides with the conditions for the highest impact pressures. Cooker's\u000d\u000a      analysis of this important phenomenon, modelled in [3], explains how and\u000d\u000a      where such large and damaging pressures occur.\u000d\u000a    "},{"CaseStudyId":"1719","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    During flight through clouds, suspended super-cooled water droplets\u000d\u000a      impact onto the wings and all\u000d\u000a      forward facing parts of an aircraft, and then turn to ice. Unchecked, this\u000d\u000a      icing can lead to loss of lift\u000d\u000a      and ultimately control, with very serious safety implications. The total\u000d\u000a      loss of an ATR72 of\u000d\u000a      American Eagle flight 4184 in 1995 at Roselawn which killed all 68 people\u000d\u000a      on board, was primarily\u000d\u000a      due to aircraft icing. This was despite the aircraft being certified as\u000d\u000a      safe to fly in the weather\u000d\u000a      conditions of the time, and the anti-icing systems being fully functional.\u000d\u000a      It was ultimately\u000d\u000a      understood that, while the existing certification and icing models were\u000d\u000a      effective in conditions with\u000d\u000a      clouds of small droplets (~20 microns), they were not valid for larger\u000d\u000a      droplets (up to 1500 microns).\u000d\u000a      Although rare, this is not an isolated incident. A similar fate befell\u000d\u000a      Aero Caribbean flight 883 in 2010\u000d\u000a      where icing caused by large droplets was again the primary cause of the\u000d\u000a      accident, with the loss of\u000d\u000a      68 lives.\u000d\u000a    The added complications of larger droplets, such as droplet distortion\u000d\u000a      and, especially, of splashing\u000d\u000a      meant the predictions of existing trusted models were wrong. They\u000d\u000a      significantly over-predicted the\u000d\u000a      rate of ice growth and did not predict ice formation as far back on the\u000d\u000a      wing as seen in practice.\u000d\u000a      Initial attempts to include simple empirical models failed to\u000d\u000a      significantly improve the situation. Since\u000d\u000a      2003, much research effort worldwide has been focussed towards\u000d\u000a      understanding the fundamental\u000d\u000a      problems in large droplet impacts and how they relate to icing.\u000d\u000a    The impact of the research described here has been primarily achieved\u000d\u000a      through collaboration with\u000d\u000a      AeroTex UK. This is an SME established out of the aircraft icing\u000d\u000a      and rotorcraft group at QinetiQ, a\u000d\u000a      large defence company that originated from the Defence Evaluation and\u000d\u000a      Research Agency\u000d\u000a      (DERA). AeroTex offer consultancy to the aircraft industry in the\u000d\u000a      field of aircraft icing, including ice\u000d\u000a      prediction, icing protection and design. They develop icing prediction\u000d\u000a      codes, and help to design\u000d\u000a      and certify ice protection systems. Whilst details of their customer base\u000d\u000a      are confidential, they have\u000d\u000a      confirmed that it includes several major aircraft manufacturers and\u000d\u000a      equipment suppliers (see\u000d\u000a      corroborating source [A]).\u000d\u000a    Our fundamental research on large droplet impacts and splashing has\u000d\u000a      enhanced understanding\u000d\u000a      and offered crucial insight into a critical, and previously little\u000d\u000a      understood, aspect of aircraft icing. It\u000d\u000a      has fed into and informed AeroTex expertise, helping them to\u000d\u000a      establish themselves as leading\u000d\u000a      consultants in their field. The knowledge garnered from our research has\u000d\u000a      enabled AeroTex to\u000d\u000a      improve the products they can offer their clients by more accurately\u000d\u000a      incorporating the influence of\u000d\u000a      splashing into their prediction codes.\u000d\u000a    The value of the UEA research to AeroTex is confirmed in a\u000d\u000a      supporting letter from the founder of\u000d\u000a      AeroTex UK and Aircraft Icing Consultant:\u000d\u000a    \"The research undertaken at UEA has contributed to an improved knowledge\u000d\u000a      of large\u000d\u000a      droplet and splashing dynamics and has enhanced AeroTex expertise.\u000d\u000a      It helped us to\u000d\u000a      produce improved numerical models. As a result we can offer ice prediction\u000d\u000a      and icing\u000d\u000a      protection design software which is better than our competitors.\u000d\u000a      Specifically we provide\u000d\u000a      prediction codes, AID (Aircraft Icing Design) and DRT (Droplet Residence\u000d\u000a      Time), which\u000d\u000a      include aspects of super-cooled large droplet behaviour that the UEA\u000d\u000a      research, along with\u000d\u000a      other theoretical and experimental investigations, has helped inform.\"\u000d\u000a    Additionally, Hicks spent a year working at AeroTex UK, helping\u000d\u000a      to develop further numerical\u000d\u000a      models. Hicks' research and expertise in aircraft icing was developed\u000d\u000a      during his postdoctoral\u000d\u000a      position at UEA, making him ideally placed to help AeroTex UK with\u000d\u000a      their product portfolio. This\u000d\u000a      resulted directly in the development of new design tools, particularly for\u000d\u000a      thermal and mechanical\u000d\u000a      anti-icing systems (see corroborating source [B]).\u000d\u000a    By providing fundamental insights into the basic physics of droplet\u000d\u000a      impacts and splashing, UEA\u000d\u000a      research has aided industry to further their understanding of aircraft\u000d\u000a      icing and to develop better\u000d\u000a      design tools, ultimately leading to improved air safety.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Aircraft icing is a significant factor in many aircraft accidents and\u000d\u000a      incidents. Ice accretion on the\u000d\u000a      wings has adverse aerodynamic effects, such as loss of lift and control,\u000d\u000a      and ice can also block\u000d\u000a      inlets into key flight sensors. Work by Richard Purvis and his Research\u000d\u000a      Associate, Peter Hicks at\u000d\u000a      UEA, in collaboration with AeroTex UK and QinetiQ, led to\u000d\u000a      better understanding of how the impacts\u000d\u000a      and splashing of water droplets influence the ice that forms on aircraft\u000d\u000a      wings. This led to improved\u000d\u000a      computer prediction codes, which are used by industry to improve design\u000d\u000a      and help satisfy\u000d\u000a      certification requirements.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a      University of East Anglia\u000d\u000a    ","Institutions":[{"AlternativeName":"East Anglia (University of)","InstitutionName":"University of East Anglia","PeerGroup":"B","Region":"East","UKPRN":10007789}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    Research Papers\u000d\u000a    (UEA authors in bold)\u000d\u000a    \u000a[1] M. Quero, D.W. Hammond, R. Purvis and F.T. Smith (2006)\u000d\u000a      Analysis of super-cooled water\u000d\u000a      droplet impact on a thin water layer and ice growth. Paper\u000d\u000a        AIAA-2006-466 of 44th AIAA\u000d\u000a        Aerospace Sciences Meeting and Exhibit 9-12 January 2006, Reno Nv, USA.\u000d\u000a      ISBN: 1563478072;978-156347807-9\u000d\u000a    \u000a\u000a[2] S.D. Howison, J.R. Ockendon, J.M. Oliver, R. Purvis and F.T.\u000d\u000a      Smith (2005) Droplet impact on\u000d\u000a      a thin fluid layer, Journal of Fluid Mechanics, 542, 1-23\u000d\u000a      doi:10.1017\/S0022112005006282.\u000d\u000a    \u000a\u000a[3] P.D. Hicks and R. Purvis (2010) Air cushioning and\u000d\u000a      bubble entrapment in three-dimensional\u000d\u000a      droplet impacts, Journal of Fluid Mechanics, 649, 135-163\u000d\u000a      doi:10.1017\/S0022112009994009.\u000d\u000a    \u000a\u000a[4] P.D. Hicks and R. Purvis (2011). Air cushioning in\u000d\u000a      droplet impacts with liquid layers and other\u000d\u000a      droplets. Physics of Fluids, 23, 062104\u000d\u000a      doi:10.1063\/1.3602505.\u000d\u000a    \u000a\u000a[5] P.D. Hicks, E.V. Ermanyuk, N.V. Gavrilov and R. Purvis\u000d\u000a      (2012) Air trapping at impact of a\u000d\u000a      rigid sphere onto a liquid, Journal of Fluid Mechanics, 695,\u000d\u000a      310-320\u000d\u000a      doi:10.1017\/jfm.2012.20.\u000d\u000a    \u000aExternal Funding\u000d\u000a    (Purvis was Principal Investigator on both grants)\u000d\u000a    Research Grant: EPSRC \"Three-dimensional droplet impacts and\u000d\u000a      aircraft icing.\" &#163;156,762 (2007-\u000d\u000a      2009)\u000d\u000a    Research Grant: Nuffield Foundation \"Air effects on high-speed\u000d\u000a      droplet impacts and aircraft\u000d\u000a      icing.\" &#163;5000 (2006-2007)\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"}],"Sources":"\u000d\u000a    [A] Letter of support from the Founder and Aircraft Icing Consultant, AeroTex\u000d\u000a        UK\u000d\u000a      (held on file at UEA)\u000d\u000a    [B] C. Hatch, R. Moser, R. Gent and P.D. Hicks (2011)\u000d\u000a      The Building Blocks for a Hybrid ElectroThermal-ElectroMechanical\u000d\u000a      Simulation Tool\u000d\u000a      SAE Technical Paper, 2011-38-0035\u000d\u000a      doi:10.4271\/2011-38-0035\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Improving Aircraft Safety in the Presence of Ice Build-up\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Mathematical research by Dr Richard Purvis and colleagues uses a\u000d\u000a      combination of numerical and\u000d\u000a      analytical techniques to gain a fundamental understanding of what happens\u000d\u000a      when a water droplet\u000d\u000a      hits a moving object, notably the wing of an aircraft. The objective was\u000d\u000a      to better understand the\u000d\u000a      splashing process, and thereby inform the empirical models of splashing\u000d\u000a      that were traditionally\u000d\u000a      used in icing prediction software.\u000d\u000a    Several important processes must be understood before an accurate model\u000d\u000a      of splashing can be\u000d\u000a      incorporated into more wide-ranging icing models. Two issues of particular\u000d\u000a      importance are (i)\u000d\u000a      predicting how much liquid gets splashed off the wing surface by a droplet\u000d\u000a      impact, and (ii)\u000d\u000a      establishing if, and where, that splashed liquid mass re-impinges further\u000d\u000a      back on the wing. The\u000d\u000a      research was partly supported by EPSRC and Nuffield Foundation grants to\u000d\u000a      Purvis (2007-2009).\u000d\u000a    The main underpinning research includes using a novel Volume-of-Fluid\u000d\u000a      numerical technique to\u000d\u000a      investigate which parameters influence the splash, focussing mainly on the\u000d\u000a      ratio of droplet size to\u000d\u000a      layer depth [1]. It offered a prediction of how much water is splashed\u000d\u000a      from the surface during an\u000d\u000a      impact event for a variety of droplet sizes, impact speeds and impact\u000d\u000a      angles. It was found that\u000d\u000a      much of the splashed water is displaced from the relatively warm water\u000d\u000a      lying on the wing rather\u000d\u000a      than the colder water originating in the droplet. This finding has\u000d\u000a      implications for the speed of ice\u000d\u000a      growth, especially because previous tentative splash models had assumed\u000d\u000a      that all the splashed\u000d\u000a      water would be cold; a deficiency that has now been corrected.\u000d\u000a      Additionally, the study [1] describes\u000d\u000a      experimental results carried out in the Cranfield icing wind tunnel and\u000d\u000a      draws comparisons with\u000d\u000a      Purvis' numerical solutions. Excellent agreement was found between the\u000d\u000a      experiments and\u000d\u000a      numerical predictions in the early stages after impact, until the\u000d\u000a      influence of the local airflow\u000d\u000a      becomes dominant. This, and other experimental findings, identified the\u000d\u000a      importance of air\u000d\u000a      behaviour during the droplet impact process.\u000d\u000a    A more theoretical basis was used in [2], to consider an analytical\u000d\u000a      asymptotic small-time study,\u000d\u000a      capturing the initial behaviour as a large droplet enters a thin water\u000d\u000a      layer. This is of interest as a\u000d\u000a      validation to the numerical approaches and for estimating the initial\u000d\u000a      impact pressure that may\u000d\u000a      erode any surface coating of the wing that might be used to prevent ice\u000d\u000a      build-up.\u000d\u000a    The research described in [3, 4 and 5] identified and examined the\u000d\u000a      importance of air cushioning in\u000d\u000a      droplet impacts. These studies focused on the entrapment of air bubbles as\u000d\u000a      impact is approached\u000d\u000a      onto both dry and wetted wings. Considering the local behaviour near\u000d\u000a      touchdown of a droplet onto\u000d\u000a      a substrate or water film, a coupled viscous-inviscid model was developed\u000d\u000a      by exploiting the large\u000d\u000a      density and viscosity differences between air and water. The research\u000d\u000a      identified the important\u000d\u000a      parameter ranges for air cushioning and these compare favourably to\u000d\u000a      experimental measurements.\u000d\u000a    Research Personnel\u000d\u000a    Lead academic: Dr Richard Purvis - UEA 2005 to date.\u000d\u000a    Postdoctoral Research Associate: Dr Peter Hicks who worked on droplet\u000d\u000a      impact and splashing at\u000d\u000a      the UEA from 2007-2009, funded by an EPSRC award to Purvis. He\u000d\u000a      subsequently moved to\u000d\u000a      University College London and then worked on a secondment in industry at AeroTex,\u000d\u000a      helping to\u000d\u000a      develop new icing codes. He now holds a lectureship in Engineering at the\u000d\u000a      University of Aberdeen.\u000d\u000a    "},{"CaseStudyId":"1720","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"3017382","Name":"France"}],"Funders":["Royal Society"],"ImpactDetails":"\u000d\u000a    By developing mathematical models of high-speed liquid impacts in a wide\u000d\u000a      variety of violent-flow applications, the research described above has\u000d\u000a      impact in the area of risk assessment for shipping. The adopted approach\u000d\u000a      to research ensures that there are clearly identified routes to\u000d\u000a      exploitation for the mathematical modelling developed within UEA and that\u000d\u000a      the research has a positive impact on industry.\u000d\u000a    Risk Assessment for Shipping\u000d\u000aThe importance of being able\u000d\u000a      to assess the risk of wave-inflicted damage to large ships was motivated\u000d\u000a      by the findings of the official report into the loss of the huge container\u000d\u000a      ship MSC Napoli in the English Channel on 18th January 2007. This\u000d\u000a      large (62,000 tons) and fairly modern (built 1991) ship was lost due to\u000d\u000a      excessive wave bending moments experienced by the ship in heavy seas.\u000d\u000a    Korobkin's research is utilised by service companies specialising in\u000d\u000a      quality, health &amp; safety and environment management for ships and\u000d\u000a      shipping. These companies are also responsible for the certification\u000d\u000a      (underwriting \/ insuring) of every ship for every route and cargo. In the\u000d\u000a      UK this is Lloyd's Register, and in France it is Bureau\u000d\u000a        Veritas. At any one time more than 10000 ships are classed\u000d\u000a      (underwritten \/ insured) by Bureau Veritas and all are subject to\u000d\u000a      periodical class and statutory surveys.\u000d\u000a    In order to successfully certify a ship, the companies need to be able to\u000d\u000a      assess all risk factors. Within this overall risk assessment, our research\u000d\u000a      is utilised to understand the effect of, and risks associated with,\u000d\u000a      differing sea conditions. The tools and methodologies developed at UEA\u000d\u000a      have been used to improve the guidelines used by Bureau Veritas\u000d\u000a      for the assessment of hydro-elastic interactions in severe sea conditions.\u000d\u000a      In addition, our research has improved the industrial computational tools\u000d\u000a      for safety and risk estimates.\u000d\u000a    The importance of the UEA research is clear from the supporting letter\u000d\u000a      from Bureau Veritas which states:\u000d\u000a    \"The results of the cooperation showed to be very useful to our company\u000d\u000a      and the tools and methodologies which were developed, are used on a daily\u000d\u000a      basis in practice.\"\u000d\u000a    \"... our Guidelines for assessment of hydro-elastic interactions during\u000d\u000a      impacts were updated thanks to the knowledge developed with these\u000d\u000a      cooperations. Due to the extreme complexity of the physical modelling\u000d\u000a      cooperation with Prof. Korobkin is still continuing and even increasing in\u000d\u000a      the recent years\".\u000d\u000a    (corroborating source [A])\u000d\u000a    Industrial Recognition\u000d\u000a    The contribution to this field by Korobkin is internationally recognised\u000d\u000a      by the Society of Naval Architects and Marine Engineers. In 2009, he was\u000d\u000a      awarded the prestigious Weinblum Memorial Lectureship, the highest award\u000d\u000a      in ship hydrodynamics, with the following citation:\u000d\u000a    \"In recognition of the many outstanding contributions to the field of\u000d\u000a      ship hydrodynamics which you have made in the course of a very successful\u000d\u000a      career in education and scientific research, it is our privilege to invite\u000d\u000a      you to present the 32nd Weinblum Memorial Lecture. The\u000d\u000a      Lectureship was established to honour individuals who exemplify the spirit\u000d\u000a      and ideals of Georg P. Weinblum. The lecturer is chosen annually by a\u000d\u000a      selection committee consisting of the Director of the Institute fur\u000d\u000a      Schiffbau der Universitat Hamburg, the Chairman of the Fachausschuss\u000d\u000a      Schiffshydrodynamik der Schiffbautechnischen Gesellschaft, the Chairman of\u000d\u000a      the Journal of Ship Research Committee and the Analytical Ship Wave\u000d\u000a      Relations Panel of the Society of Naval Architects and Marine Engineers.\"\u000d\u000a    (corroborating source [B])\u000d\u000a    Korobkin is only the fourth academic from the UK to be awarded this\u000d\u000a      Lectureship (G.E. Gadd, National Physical Laboratory, in 1982; F. Ursell,\u000d\u000a      University of Manchester, in 1985; E. Taylor, University of Oxford, in\u000d\u000a      2005).\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The safe operation of ships is a high priority task in order to protect\u000d\u000a      the ship, the personnel, the cargo and the wider environment. Research\u000d\u000a      undertaken by Professor Alexander Korobkin in the School of Mathematics at\u000d\u000a      UEA has led to a methodology for the rational and reliable assessment of\u000d\u000a      the structural integrity and thus safety of ships and their cargos in\u000d\u000a      severe sea conditions. Central to this impact is a set of mathematical\u000d\u000a      models, the conditions of their use, and the links between them, which\u000d\u000a      were designed to improve the quality of shipping and enhance the safety of\u000d\u000a      ships. The models, together with the methodology of their use, are\u000d\u000a      utilised by the ship certification industry bringing benefits through\u000d\u000a      recognised quality assurance systems and certification.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of East Anglia\u000d\u000a    ","Institutions":[{"AlternativeName":"East Anglia (University of)","InstitutionName":"University of East Anglia","PeerGroup":"B","Region":"East","UKPRN":10007789}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    (UEA authors in bold)\u000d\u000a    \u000a[1] Korobkin A.A. (2011) Semi-analytical approach in generalised\u000d\u000a      Wagner model, Proceedings of the 26th International Workshop on Water\u000d\u000a        Waves and Floating Bodies, Athens, Greece, 85-88 Available from: http:\/\/www.iwwwfb.org\/Abstracts\/iwwwfb26\/iwwwfb26_22.pdf\u000d\u000a    \u000a\u000a[2] Iafrati A. and Korobkin A.A. (2008) Hydrodynamic loads during\u000d\u000a      early stage of flat plate impact onto water surface, Physics of Fluids,\u000d\u000a      20, 082104 doi:10.1063\/1.2970776\u000d\u000a    \u000a\u000a[3] Iafrati A. and Korobkin A.A. (2011) Asymptotic estimates of\u000d\u000a      hydrodynamic loads in the early stage of water entry of a circular disk, Journal of Engineering Mathematics, 69, 199-224\u000d\u000a      doi:10.1007\/s10665-010-9411-y\u000d\u000a    \u000a\u000a[4] Ten I., Malenica S. and Korobkin A. (2011) Semi-analytical\u000d\u000a      models of hydroelastic sloshing impact in tanks of liquefied natural gas\u000d\u000a      vessels, Philosophical Transactions of the Royal Society A, 369\u000d\u000a      (1947), 2920-2941 doi:10.1098\/rsta.2011.0112\u000d\u000a    \u000a\u000a[5] Khabakhpasheva T.I., Korobkin A. A. and Malenica S. (2013)\u000d\u000a      Fluid impact onto a corrugated panel with trapped gas cavity, Applied\u000d\u000a        Ocean Research, 39, 97-112 doi:10.1016\/j.apor.2012.10.005\u000d\u000a    \u000a\u000a[6] Malenica S., Korobkin A.A., Ten I., Gazzola T., Mravak Z.,\u000d\u000a      De-Lauzon J. and Scolan Y.M. (2009) Combined semi-analytical and finite\u000d\u000a      element approach for hydro structure interactions during sloshing impacts\u000d\u000a      - SlosHel Project Proceedings of the 19th International\u000d\u000a        Offshore and Polar Engineering Conference, Osaka, Japan, ISOPE, 3,\u000d\u000a      143-152 Available from: www.veristar.com\/content\/static\/veristarinfo\/images\/4139.1.ISOPE2009-MALENICA.pdf\u000d\u000a    \u000aKey Research Funding and Industrial Partners\u000d\u000a    FP7: \"Tools for Ultra Large Container Ships (TULCS)\" (2009-2012)\u000d\u000a      Industrial partners: Bureau Veritas (France), MARIN\u000d\u000a      (Netherlands), CMA-CGM (France), Odense Steel Shipyard\u000d\u000a      (Denmark), CEHIPAR (Spain), INSEAN, SIREHNA\u000d\u000a      (France), WIKKI (UK) and HYDROCEAN (France). Total budget\u000d\u000a      &#163;2.75M - UEA budget &#163;120K\u000d\u000a    Royal Society International Joint Project: \"Free-surface separation\u000d\u000a        from a body which starts to move suddenly\" (2009-2011) Budget:\u000d\u000a      &#163;12,000\u000d\u000a    International Centre for Mathematical Sciences award for the workshop\u000d\u000a        on \"Mathematical challenges and modeling of hydroelasticity\"\u000d\u000a      Edinburgh, June 2010 Budget: &#163;21,000\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000d\u000a    [A] Letter from the Head of Hydro Structure Section, Deputy Director,\u000d\u000a      Research Department - Marine Division, Bureau Veritas - letter\u000d\u000a      held on file at UEA.\u000d\u000a    [B] Citation letter for the Weinblum Memorial Lectureship - letter held\u000d\u000a      on file at UEA. \u000d\u000a    ","Title":"\u000d\u000a    Safety on the Sea\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The underpinning research for this impact is concerned with mathematical\u000d\u000a      modelling of fluid- structure interaction. The research was primarily\u000d\u000a      undertaken by Professor Alexander Korobkin at UEA. The research\u000d\u000a      was further developed with the support of the EU funded project TULCS,\u000d\u000a      which had considerable industrial involvement.\u000d\u000a    The idea of the underpinning research is to identify the most typical\u000d\u000a      situations of violent interaction between a structure and a liquid and\u000d\u000a      then simplify in order to describe the interaction with idealised\u000d\u000a      geometries and including only the most important physical parameters [1].\u000d\u000a      The structural response and stresses are described by a finite-element\u000d\u000a      model of the structure, in which simplified models of the hydrodynamic\u000d\u000a      loads are used instead of fully nonlinear models of the hydrodynamics. In\u000d\u000a      view of all the assumptions and uncertainties in the estimation of such\u000d\u000a      situations and actual sea conditions, the semi-analytical models of\u000d\u000a      hydrodynamic loads devised by Korobkin provide a more reliable and\u000d\u000a      practical alternative to more complex and extremely computationally\u000d\u000a      expensive Computational Fluid Dynamics (CFD) models. There is little\u000d\u000a      advantage in using the complex models, when the conditions of interaction\u000d\u000a      are only known with large uncertainty. Korobkin and colleagues have shown\u000d\u000a      that the structural response is rather insensitive to details of the flow\u000d\u000a      and pressure distribution [4] and only global characteristics are\u000d\u000a      important [5]. Risk assessment is based on the stress distribution in the\u000d\u000a      structure, with the hydrodynamic forces being indicators of high stresses.\u000d\u000a      Korobkin's simplified models of hydrodynamics, which were developed with\u000d\u000a      the aim of understanding the fluid-structure interaction in critical\u000d\u000a      conditions, have been found to be of practical use once they are combined\u000d\u000a      with CFD analysis. This combined local-global modelling benefits from the\u000d\u000a      accuracy of CFD far from the violent parts of the flow and the flexibility\u000d\u000a      of semi-analytical models in the violent zones, where the CFD codes are\u000d\u000a      not so accurate. The combination of approaches gives more reliable\u000d\u000a      predictions of safety levels.\u000d\u000a    The models developed during this research were designed for use both\u000d\u000a      together with CFD and also on their own at the pre-design stage [4, 5].\u000d\u000a      The models were designed to be simple enough to be used by industry,\u000d\u000a      bringing not only numbers but also understanding to designers and\u000d\u000a      certification bodies. The simplified models are flexible in the sense that\u000d\u000a      they include a physical effect only where it is needed and neglect it\u000d\u000a      elsewhere. Korobkin's work provided a significant improvement over\u000d\u000a      existing models. In particular, the Modified Logvinovich Model (MLM) which\u000d\u000a      he developed, [1], is a very accurate and efficient way of predicting\u000d\u000a      loads on a solid body during slamming impact onto water. The model is\u000d\u000a      based on the results of collaborative research published in [2, 3].\u000d\u000a    In addition, Korobkin was involved in developing a research methodology\u000d\u000a      for the SLOSHEL consortium (includes Bureau Veritas, MARIN, the Lloyd's Register, and many other companies) as a subcontractor of Bureau Veritas. This consortium is interested in the sloshing of Liquefied\u000d\u000a      Natural Gas in tanks, in particular focussing on the hydroelasticity of\u000d\u000a      the tank walls during violent sloshing.\u000d\u000a    The work is based on papers [1-5]. The methodology for SLOSHEL work is\u000d\u000a      outlined in [6].\u000d\u000a    UEA personnel\u000d\u000aLead academic: Prof Alexander Korobkin (2007\u000d\u000a      to date)\u000d\u000aPhD Student: Alessandro Iafrati (UEA 2009)\u000d\u000a    "},{"CaseStudyId":"2433","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"660013","Name":"Finland"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":[],"ImpactDetails":"\u000d\u000aThe main route for delivering impact for the statistical QA tools of [1] is via a number of software\u000d\u000apackages [2]. These include affyPLM and affyQualityMetrics, available at bioconductor.org, a\u000d\u000arepository for free open source bioinformatics software, started in 2001 by an international team\u000d\u000aled by members of the Fred Hutchinson Cancer Research Center, and now the default software\u000d\u000aresource for the bioinformatics community. The reach of this impact is considerable. For example,\u000d\u000aduring the REF period, the two packages have been downloaded from Bioconductor over 380,000\u000d\u000atimes [2 i, ii]; Chipster's server in Finland (one of ten such servers worldwide) has 788 users [2 iii];\u000d\u000aand the Genevestigator web tool which uses the Bioconductor package affyQCReport\u000d\u000a(incorporating affyPLM) for the QC of datasets is used for [text removed for publication] [2 vi]. With\u000d\u000atwo of the co-authors working for biotech companies it was easy to establish further connections to\u000d\u000athe commercial world and spread the word about the methodology. Invitations to conferences such\u000d\u000aas JSM helped in sharing the methods, both with other statisticians and with users. More details\u000d\u000aabout the routes to impact are given in the explicit examples below.\u000d\u000a(A) Process improvement in microarray facilities and standardisation of data quality targets\u000d\u000aThe availability of QA tools [1] has enabled large microarray facilities as well as smaller\u000d\u000alaboratories to routinely monitor and improve their processes. This enables them to use high\u000d\u000aquality data for downstream analysis, ensuring validity of scientific conclusions. Two supporting\u000d\u000aletters [3, 4] confirm this:\u000d\u000a[3] CEO of Spheromics (formerly at Novartis and the Australian Genome Research Facility):\u000d\u000a\"Quality assessment and control for microarray data is an area where existing quality control\u000d\u000amethodology was not sufficiently developed [...]. The methodology [from [1]] has been helpful to\u000d\u000ame for both detecting outlier arrays and revealing systematics errors in the process. [...] I have\u000d\u000abeen using the tools proposed and the guidance about their interpretation, in particularly the NUSE\u000d\u000adistributions. They would highlight, at a glance, differences of chip quality caused by batch effects.\"\u000d\u000a[4] Senior Bioinformatics Officer at ICR (Institute of Cancer Research):\u000d\u000a\"We use quality assessment based on statistics methods developed by [1] implemented in R-packages.\u000d\u000aQuality assessment has been helpful to us in determining outlier arrays and detecting\u000d\u000aartefacts in the data. This makes sure our dataset is not distorted by poor quality microarrays.\"\u000d\u000aThe QA methods from [1] have contributed to changing attitudes in the genomics community, both\u000d\u000ain academia and beyond, towards giving higher priority to routine checking of data quality and to\u000d\u000aunderstanding of technically caused variation before drawing scientific conclusions. For example:\u000d\u000a&#8212; The quality landscapes feature provide a visual presentation of quality defects, thereby allowing\u000d\u000ascientists to identify, in the measurement process, specific causes of poor quality. The popularity\u000d\u000aof [5] (e.g., 10715 visits in the 12 months preceding February 2013) confirms the usefulness of this\u000d\u000aapproach.\u000d\u000a&#8212; Making use of the QA tools for microarray data has become a routine step in scientific studies to\u000d\u000aensure the reproducibility of research findings; see e.g. [6] for one of many examples. Many\u000d\u000aauthors cite the software or names of the QA tools (RLE, NUSE) rather than the publication [1].\u000d\u000aThe methods [1] have played important roles in setting professional standards for microarray data\u000d\u000aquality, as illustrated by an international initiative led by the US Food and Drug Administration\u000d\u000a(FDA). The Microarray Quality Control (MAQC) project aims at establishing standards to ensure\u000d\u000asuccessful and reliable use in clinical practice and regulatory decision-making. Phase II aimed to\u000d\u000aassess and establish \"best practices\" for development and validation of predictive models for\u000d\u000apersonalised medicine. CEO of Spheromics, who was involved in the project, states in his letter [3]\u000d\u000a\"In Phase II of that project, I have made frequent use of the assessment methods from your paper.\"\u000d\u000a(B) Individualisation of treatment decisions\u000d\u000aIn the last decade, many biotech companies have been pushing for the development of diagnostic\u000d\u000aand prognostic tools to support the optimisation of treatment choices. For example, individualised\u000d\u000arecurrence estimates help decide whether or not a patient would benefit from adjuvant\u000d\u000achemotherapy. Clearly the need for high quality standards becomes increasingly important the\u000d\u000acloser the use of a tool gets to a clinical setting. The QA methods developed in [1] are being used\u000d\u000ain the research units of companies developing tools for personalised medicine.\u000d\u000aWe demonstrate this in detail for Veracyte, a molecular diagnostics company and a pioneer in the\u000d\u000aemerging field of molecular cytology. Veracyte has developed a gene expression based test called\u000d\u000aAfirma that is expected to reduce massively the number of surgeries with their attendant morbidity\u000d\u000a(life-long follow-up treatments) in initially suspected thyroid cancer due to the occurrence of fairly\u000d\u000acommon thyroid nodules. A recent economic impact study in the Journal of Clinical Endocrinology\u000d\u000aand Metabolism concluded that routine use of Afirma in the USA would result in 74% fewer\u000d\u000asurgeries in patients with benign tumours, that is, tens of thousands of avoidable surgeries each\u000d\u000ayear corresponding to about $122 million medical savings [7]. The traditional diagnosis of thyroid\u000d\u000acancer produces up to 30% inconclusive cases which typically result in surgery, of which 70%-80%\u000d\u000aof patients turn out to have benign tumours. Afirma succeeds in avoiding the need for surgery in\u000d\u000aabout half of such cases, resulting in expected health care cost savings of $3000 per patient as\u000d\u000awell as improving patient health outcomes. Afirma has been developed and clinically validated as\u000d\u000apublished in 2012 in high-profile journals such as the New England Journal of Medicine (NEJM) [8].\u000d\u000aA crucial step for the commercial success of Afirma is to obtain medical insurance cover, e.g., it\u000d\u000awas approved by Medicare in 2012. For this, FDA software validation is key, a step that takes six\u000d\u000amonths and about $300,000 and after which the algorithm is locked. To ensure their success in\u000d\u000awinning over the clinicians, Veracyte needed the negative predictive value of their tool to be above\u000d\u000a94%, which required the highest possible quality standards. Veracyte's Chief Scientific Officer\u000d\u000astates in her supporting letter [9]:\u000d\u000a\"A recent study found that the Afirma Gene Expression Classifier demonstrated strong accuracy,\u000d\u000areliability and reproducibility under a range of conditions and variables. This includes maintenance\u000d\u000aof sample stability, analytic sensitivity and analytical specificity [...] A key step to these\u000d\u000aachievements is data quality assessment and control. We have been using quality assessment\u000d\u000amethods such as summaries of RLE distributions from your publication to shed light on the sources\u000d\u000aof variation in our custom-made gene expression microarrays. [They] have been serving us for\u000d\u000adetecting outliers as well as for revealing and removing artefacts and batch effects arising from\u000d\u000ainconsistencies in operator, protocol or sample condition.\"\u000d\u000aSeveral media stories from 2012 onwards highlight how particular patients, after having taken the\u000d\u000aAfirma test, have avoided surgery with long term health savings and improved patient welfare [10].\u000d\u000a","ImpactSummary":"\u000d\u000aDr Brettschneider and collaborators proposed a conceptual framework for high-dimensional gene\u000d\u000aexpression data quality assessment (QA) and developed a QA statistical toolbox tailored to short\u000d\u000aoligonucleotide microarray technology. The work has deepened understanding of sources of\u000d\u000avariation and has helped in removing noise and bias in microarray data sets. This has accelerated\u000d\u000athe invention of clinical instruments for molecular cancer diagnosis\/prognosis. The toolbox has\u000d\u000abeen applied widely, leading to impact through:\u000d\u000a(A) process improvement in microarray facilities saving running costs, and standardisation of data\u000d\u000aquality targets ensuring reproducible research;\u000d\u000a(B) individualisation of treatment decisions supported by enhanced data quality, thereby reducing\u000d\u000ahealthcare costs through avoidance of unnecessary surgery and improved patient welfare.\u000d\u000a","ImpactType":"Technological","Institution":"\u000d\u000aUniversity of Warwick\u000d\u000a","Institutions":[{"AlternativeName":"Warwick (University of)","InstitutionName":"University of Warwick","PeerGroup":"A","Region":"West Midlands","UKPRN":10007163}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000aKey publication: The bulk of the work was published in Technometrics, the leading journal for\u000d\u000astatistical methodology in sciences and engineering. The editors invited five commentaries and a\u000d\u000arejoinder, indicating the central nature of the work for the field of statistics in genomics and beyond.\u000d\u000aBrettschneider was invited to present the paper at JSM (Joint Statistical Meeting of the American\u000d\u000aStatistical Association), the world's largest gathering of statisticians, attracting over 5000 speakers\u000d\u000aand participants from academia, research institutes, government and the commercial sector, both\u000d\u000afrom within USA\/Canada and overseas.\u000d\u000a\u000a[1] Primary article: Brettschneider J, Collin F, Bolstad BM, and Speed TP, Quality assessment for\u000d\u000ashort oligonucleotide arrays, Technometrics 50 (2008) 241-264.\u000d\u000a(With five commentaries: Jones W, Bao L &amp; Hoeschele I, He W &amp; Bull SB, Kendall J &amp; Lakshmi B,\u000d\u000aGoldstein D, ditto pp.265-278, and Rejoinder, Brettschneider J et al, ditto pp.279-283).\u000d\u000aAll parts available at pubs.amstat.org\/toc\/tech\/50\/3 Cited 53 times to 16.10.2013.\u000d\u000a\u000a[1] was included, as an Internet Publication (arXiv preprint), in Warwick's RAE 2008 submission.\u000d\u000aSoftware: The statistical QA tools from [1] have been incorporated into software packages (see\u000d\u000anext Section and reference [2] for details).\u000d\u000a","ResearchSubjectAreas":[{"Level1":"6","Level2":"4","Subject":"Genetics"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"}],"Sources":"\u000d\u000a[2] QA tools from [1] have been incorporated into numerous software packages including:\u000d\u000a(i) Bioconductor R-package affyPLM for preprocessing Affymetrix type data. Part III of the package\u000d\u000avignette is devoted the QA tools; downloaded 344,040 times in REF period:\u000d\u000abioconductor.org\/packages\/release\/bioc\/html\/affyPLM.html\u000d\u000a(ii) Bioconductor R-package arrayQualityMetrics described in Kauffmann et al, arrayQualityMetrics\u000d\u000a&#8212; a bioconductor package for quality assessment of microarray data, Bioinformatics 25 (3): 415-\u000d\u000a416; downloaded 37 times in 2009, 180 times in REF period:\u000d\u000abioconductor.org\/packages\/release\/bioc\/html\/arrayQualityMetrics.html\u000d\u000aCorrespondence received providing download statistics.\u000d\u000a(iii) Chipster described in M Aleksi Kallio et al, Chipster: user-friendly software for microarray and\u000d\u000aother high-throughput data, BMC Genomics 12:507, 2011: chipster.csc.fi\/index.shtml\u000d\u000aCorrespondence received corroborates users.\u000d\u000a(iv) RobiNA described in Lohse M et al, Robin: An intuitive wizard application for R-based\u000d\u000aexpression microarray quality assessment and analysis. Plant Physiology, 153:642-51, 2010.\u000d\u000aIncorporates all QA tools from [1], see Section 5.1 in mapman.gabipd.org\/web\/guest\/robin\u000d\u000a(v) Maastricht Bioinformatics has built an online Affymetrix array QA webtool running from\u000d\u000aarrayanalysis.org which includes NUSE and RLE from [1].\u000d\u000a(vi) Nebion Gene expression search engine explains the use of QA from [1] in Chapter 8 at\u000d\u000agenevestigator.com\/userdocs\/manual\/qc.html. [text removed for publication]\u000d\u000a[3] Letter received: Founder and CEO of Spheromics spheromics.com, a company specialising in\u000d\u000aconsultancy for biomarker development and gene expression.\u000d\u000a[4] Letter received: Senior Bioinformatician at ICR: Royal Cancer Hospital, London.\u000d\u000a[5] Catalogue of technical artefacts: The website \"Chip Gallery\" plmimagegallery.bmbolstad.com\u000d\u000aset up by Bolstad provides QA case studies with extensive collections of quality landscapes. The\u000d\u000aemphasis is on spatial quality effects which lab scientists can compare with their own experimental\u000d\u000aresults to assign causes of poor quality. 10715 visits in the 12 months preceding February 2013.\u000d\u000a[6] Influencing methodology in the user community: Zhao H and Ma H, FacPad: Bayesian sparse\u000d\u000afactor modelling for the inference of pathway responsive to drug treatment. Bioinformatics (2012)\u000d\u000a28 (20): 2662-2670 bioinformatics.oxfordjournals.org\/content\/28\/20\/2662.short. The methods\u000d\u000asection reports all analysis steps starting with the raw data modelling transparency: \"...basic quality\u000d\u000acheck of the .CEL data was performed using the NUSE and RLE metrics (Brettschneider et al.,\u000d\u000a2008) [i.e. [1]], which are provided in the R package `affyPLM'. [...] boxplots of NUSE and RLE\u000d\u000awere drawn and arrays with bad quality were discarded.\"\u000d\u000a[7] Li H et al, Cost-Effectiveness of a Novel Molecular Test of Cytologically Indeterminate Thyroid\u000d\u000aNodules, JCEM 96(11): E1719 jcem.endojournals.org\/content\/96\/11\/E1719.full.pdf+html\u000d\u000a[8] Alexander EK et al, Preoperative Diagnosis of Benign Thyroid Nodules with Indeterminate\u000d\u000aCytology, NEJM, Aug 23, 2012 nejm.org\/doi\/full\/10.1056\/NEJMoa1203208#t=abstract\u000d\u000a[9] Letter received: Chief Scientific Officer, Senior Vice President of Research and Development at\u000d\u000aVeracyte veracyte.com, Bay Area Biotech pioneering in molecular cancer prognosis, among top 50\u000d\u000aVenture-capital companies by Wall Street Journal's Next Big Thing 2011.\u000d\u000a[10] Examples of news stories in US media highlighting that patients have benefitted directly from\u000d\u000athe Afirma test which relies on [1] as corroborated by [9]:\u000d\u000aProcedure at local doctor's office helps local woman avoid thyroid surgery (March 2012)\u000d\u000aheraldnews.com\/news\/x570358499\/Procedure-at-local-doctors-office-helps-Fall-River-woman-avoid-thyroid-surgery\u000d\u000aNew Thyroid Cancer Test (December 2012)\u000d\u000aabclocal.go.com\/kfsn\/story?section=news\/health\/health_watch&amp;id=8918522\u000d\u000aThyroid cancer test helps patients avoiding unnecessary surgery (May 2013) where a clinic notes\u000d\u000aseveral patients have benefitted from the test:\u000d\u000adesmoinesregister.com\/article\/20130520\/LIFE\/305200013\u000d\u000a\u000d\u000a","Title":"\u000d\u000aQuality assessment for high-throughput genomic data in research and clinical\u000d\u000apractice\u000d\u000a","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000aMicroarray based research. Genome-wide expression profiles have become central to\u000d\u000aunderstanding the genetic underpinning of cellular processes (e.g., cell division, circadian rhythm,\u000d\u000areaction to chemicals) and complex genetic diseases (e.g., cancers, neuropsychiatric disorders).\u000d\u000aThis approach was facilitated by the invention of microarrays two decades ago, which radically\u000d\u000achanged gene expression measurement. Instead of assessing genes one-by-one, the new\u000d\u000atechnologies enabled the screening of tens of thousands of them at once.\u000d\u000aReproducibility of research in genomics. After initial euphoria, many microarray studies turned\u000d\u000aout to be inconclusive or irreproducible, affecting industrial research in particular. In 2011, Bayer,\u000d\u000aone of the world's largest chemical companies, halted nearly two-thirds of its target-validation\u000d\u000aprojects because in-house experimental findings failed to match published literature claims; see\u000d\u000ablogs.nature.com\/news\/2011\/09\/reliability_of_new_drug_target.html. In the last decade, the need\u000d\u000afor rigorous experimental design, for appropriate statistical interpretation of results and for\u000d\u000acollaboration with statisticians has become widely recognised in the genomics community. Nature\u000d\u000aand other related research journals have published a series of articles about reproducibility and\u000d\u000ahave taken editorial steps to ensure transparency and robustness in articles published there; see\u000d\u000anature.com\/nature\/focus\/reproducibility.\u000d\u000aQA for high-throughput gene expression data. In a seminal paper [1] including five\u000d\u000acommentaries from researchers inside and outside academia, Brettschneider et al laid the\u000d\u000agroundwork for a new branch of research in statistical genomics. On the theoretical side, they\u000d\u000aprovided a general conceptual framework for quality assessment (QA) for data obtained by high-\u000d\u000athroughput molecular assays such as gene expression microarrays. On the practical side, they\u000d\u000aproposed a statistical QA toolbox for such gene expression measurement technologies. The\u000d\u000amethods include numerical chip quality measures such as RLE and NUSE distributions, as well as\u000d\u000aspatial representations of by-products of pre-processing algorithms, also called quality landscapes,\u000d\u000awhich enable scientists to connect technical artefacts to location on the slide. All measures were\u000d\u000astudied extensively on a variety of datasets, including spike-in calibration data, small lab and\u000d\u000amultisite studies. Some of the tools are specific to short oligonucleotide microarrays (including the\u000d\u000aindustrial standard made by Affymetrix); others can be used for different microarray platforms or\u000d\u000aeven for the most recent gene expression measurement technologies such as RNAseq.\u000d\u000aThe research team. Brettschneider joined the University of Warwick in 2007 and collaborated on\u000d\u000a[1] with Collin (Genomic Health, previously Affymetrix), Bolstad (Affymetrix) and Speed (UCB &amp;\u000d\u000aWEHI). The work reported in [1] had begun in 2006 while Brettschneider was working at Queen's\u000d\u000aUniversity in Ontario, and was completed in 2008 (with the aid of a visit to Warwick, from\u000d\u000aCalifornia, by her co-author Prof Speed).\u000d\u000a"},{"CaseStudyId":"2434","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Economic and Social Research Council"],"ImpactDetails":"\u000d    The impact occurred in 2010, and was the result of novel statistical\u000d      methods developed and published over the period 2004-2008. The impact was\u000d      made possible through:\u000d    (i) Firth's direct involvement as a consultant to the\u000d      broadcasters BBC and ITV in connection with the previous General Election\u000d      held in May 2005.\u000d    (ii) The fact that the methods developed for the 2005 election had\u000d      received high-profile attention from pollsters and broadcasters,\u000d      through their publication in 2007 (and ultimately in print in 2008) as one\u000d      of the Royal Statistical Society's prestigious Ordinary Meeting\u000d      papers.\u000d    (iii) The fact that Firth had also written fully-documented software\u000d      to implement the methods &#8212; software that could readily be used by\u000d      knowledgeable others at a later election (as indeed was the case at the\u000d      May 2010 election).\u000d    The research done by Firth (with Curtice) for the 2005 General Election\u000d      produced methods which have since become the industry standard. At the\u000d      most recent General Election in May 2010 those methods were used not only\u000d      by BBC and ITV, but also for the first time by the prominent satellite and\u000d      online broadcaster, Sky. The reasons are clear:\u000d    (i) The combination of methods described in Curtice and Firth (2008) is\u000d      substantially more sophisticated statistically than any of the relatively\u000d      simple approaches that had been used prior to 2005.\u000d    (ii) On the day of the 2005 election the analysis performed by Curtice\u000d      and Firth for the BBC and ITV networks had resulted in an unprecedented, perfect\u000d      prediction of the resulting 66-seat Labour majority in the House of\u000d      Commons. This accuracy was impressive in its own right. It put paid to the\u000d      myth that exit-poll-based predictions are always unreliable (a myth due\u000d      largely to a spectacular failure by both BBC and ITV forecasters at the\u000d      1992 election &#8212; see Payne(2003)[9]).\u000a      Moreover, the prediction of a 66-seats majority on 5 May 2005 was substantially\u000d      different from the 100-120 seats majority predictions that had been made\u000d      in many of the national newspapers on the strength of the latest\u000d      pre-election voting intention polls. The exit-poll analysis had provided\u000d      BBC and ITV with exactly what they wanted, a story-changing prediction\u000d      that strongly influenced their TV coverage from 10pm on election night.\u000d    By the time of the May 2010 election, Firth himself had moved on to other\u000d      projects; he was not available at that time as a consultant to the\u000d      broadcasters. He did, however, provide software (written in R, with a\u000d      user-friendly interface) for use by the BBC's and ITV's new generation of\u000d      statistical consultants (Dr Stephen Fisher of the University of Oxford,\u000d      and Dr Jouni Kuha of the London School of Economics and Political\u000d      Science). Fisher and Kuha, again with the political expertise of Professor\u000d      John Curtice on hand, applied the methods of Curtice and Firth (2008) to\u000d      the data from an exit poll conducted by market-research companies MORI and\u000d      NOP for BBC, ITV and Sky. The result was, again, a perfect\u000d      prediction of the all-important number of House of Commons seats for the\u000d      largest party; in 2010, again, it allowed the broadcasters to talk about\u000d      the \"right\" story from 10pm on election night, this time a hung parliament\u000d      with the Conservatives as largest party (on 307 seats, 19 seats short of\u000d      an overall majority).\u000d    The direct significance and reach of the impact, in connection\u000d      with the 2010 General election, are clear and substantial:\u000d    (i) The methods of Firth and Curtice (2008) were adopted\u000d        enthusiastically at the 2010 election by the major broadcasters (and\u000d      by their new generation of statistical consultants).\u000d    (ii) The methods had proven to be so effective at the 2005 election that\u000d      the consortium of broadcasters using them was enlarged to include also\u000d        Sky for the first time in 2010 (in addition to BBC and ITV).\u000d    (iii) The broadcasts made from 10pm on election night 6 May 2010 were\u000d      watched, heard and browsed (on TV, radio and the internet) by many\u000d        millions of people in the UK and abroad [e.g., source [8] says \"A\u000d        peak audience of 6.6 million watched David Dimbleby anchor Election 2010\u000d        on BBC One, BBC HD, BBC News Channel and on BBC Two in the nations, with\u000d        a 4.7 million average, 36.1 per cent share and 16.1 million reach.\"].\u000a      The exit-poll-based prediction of the final election outcome was central\u000d      to those broadcasts (see, e.g., source [5]) for at least the first couple\u000d      of hours &#8212; this being the peak viewing\/listening\/browsing period.\u000d    (iv) The fact that the methods delivered highly accurate\u000d      predictions to millions of people was a major contribution to the\u000d        immediate public understanding of a political outcome that was\u000d      likely to be complex and to require negotiation in advance of the\u000d      formation of a new Government.\u000d    (v) The exit-poll-based prediction was surprising to many,\u000d      including some prominent political commentators. In particular, the\u000d      prediction based on the exit poll was that the Liberal Democrats would\u000d      lose some seats in the House of Commons, whereas the pre-election voting\u000d      intention polls had quite consistently suggested an increased\u000d      number of seats for the Liberal Democrats. In the event, the 57 seats\u000d      actually won by the Liberal Democrats in 2010 (down from 62 seats in 2005)\u000d      was very close to, and even slightly worse for that party than, the\u000d      59-seats prediction that was broadcast from the exit poll.\u000d    Sources [2], [6] and [7] confirm the role played by the methods of\u000d      Curtice and Firth [1] at the 2010 election, including the inclusion for\u000d      the first time of Sky among the broadcasters using the methods. The letter\u000d      [6] to Firth from the BBC's Editor of Political Research includes this:\u000d      \"The importance of [the accuracy of the prediction] to the BBC,\u000d        and to the other broadcasters too, is hard to overstate. After the\u000d        disaster of the 1992 election, predictions based on UK exit polls were\u000d        for many years viewed with great scepticism. The methods you developed\u000d        for the 2005 election have completely turned this around. Our\u000d        election-night programming, on TV, radio and the internet, is some of\u000d        the most high-profile and important work that the BBC ever does. To have\u000d        authority restored to the election night broadcast is invaluable to us\u000d        (and, we like to think, also to the viewing\/listening\/browsing public,\u000d        in the UK and more widely); not least because in recent elections the\u000d        exit poll has become even more crucial to our flagship election night\u000d        programme. In 1992 &#8212; the last general election without local elections\u000d        held on the same day &#8212; the exit poll prediction announced at 10pm had\u000d        been displaced by 1am with a prediction based on 156 declared results.\u000d        In the 2010 election only 11 seats had been declared by 1am. In 1992, by\u000d        2am some 464 seats had declared: in 2010, the equivalent figure was 62.\u000d        As a result, the exit poll prediction carries our programme for much\u000d        longer than in past elections and its accuracy is therefore more closely\u000d        scrutinized than ever before. Without such effective exit-poll\u000d        methodology, the broadcasts on 6-7 May 2010 would have been\u000d        substantially less informative and less engaging to the audience(s).\u000d      Your methods are now certainly the `industry standard' for UK electoral\u000d        exit polling and associated prediction, and it is almost inconceivable\u000d        that they would not be used again at the next election due in 2015.\"\u000d      The letter [7] to Firth from the BBC's statistical consultants at the 2010\u000d      election includes:\u000d      \"The statistical methods that you developed for the 2005 election, and\u000d        which you reported and analysed in Curtice and Firth (2008), were\u000d        absolutely crucial to the success of our work at the 2010 election. Your\u000d        kind help in providing R software made it relatively easy for us to\u000d        adopt the same methods in 2010, and we did so &#8212; with great success, as\u000d        you know! The approach that you developed in Curtice and Firth (2008) is\u000d        now the `gold standard' in this area of work, and will surely be used\u000d        again at future elections.\"\u000d    Sources [4] and [5] exemplify the surprise, even disbelief, among\u000d      political commentators. At 10pm on election night, the BBC TV presenter\u000d      David Dimbleby himself [5] said of the prediction, on air:\u000d      \"If that's right the Liberal Democrats despite all that noise and fury\u000d        have actually dropped 3 seats, which could be one reason why we need to\u000d        be sceptical about this exit poll.\"\u000d      After the event, John Rentoul in the Independent on Sunday (9 May\u000d      2010) wrote:\u000d      \"The accurate prediction was so shocking, at 10pm on Thursday, that\u000d        large numbers of Conservatives flooded the internet to scorn it as\u000d        utterly implausible and to say that it could not possibly be right\u000d        because it failed to accord with what they felt in their bones. Most\u000d        incautious was Iain Dale, Tory blogger and would-be candidate, who said:\u000d        'It seems too incredible to be true that the Lib Dems are only predicted\u000d        to get 59 seats. I'll run naked down Whitehall if that turns out to be\u000d        true.' In the end, of course, the Lib Dems won even fewer seats, 57.\u000d        Dale's streak is eagerly awaited.\"\u000d    Source [3] is a web-based public engagement resource that was made\u000d      available by Firth soon after the 2010 election. It is designed to make\u000d      this story accessible to non-specialists (e.g., to non- statistical\u000d      journalists as well as to members of the general public). The main page\u000d      there gives a non-technical description of the methods used, as well as\u000d      some history, press quotes, references, etc. It is hoped that this very\u000d      public success story has a substantial, indirect impact additional to the\u000d      various direct impacts detailed above &#8212; namely, that it helps to inspire\u000d      school-age students to consider mathematical sciences as a field of study\u000d      and as a potentially interesting career choice.\u000d    ","ImpactSummary":"\u000d    This case study describes the impact achieved by novel methodology for\u000d      election exit-poll design and analysis, at the 2010 UK General Election.\u000d      The context is that the underpinning research had already led to success\u000d      in predicting the outcome of the May 2005 General Election for public\u000d      broadcasts by BBC and ITV. Its direct impacts at the 2010 General Election\u000d      were significant and far-reaching:\u000d    (i) The same statistical methods were used successfully in 2010 also by a\u000d      third major UK and international broadcaster, Sky (in addition to BBC and\u000d      ITV).\u000d    (ii) The TV, radio and internet audiences of the three channels combined\u000d      totalled many millions of people who were thereby informed of the likely\u000d      2010 election outcome immediately after the close of polling stations.\u000d    (iii) The prediction that was produced and broadcast at close of polls in\u000d      May 2010 was both surprising to political commentators and extremely\u000d      accurate.\u000d    As a result of these successes, the statistical methods we developed are\u000d      now the \"industry standard\" for electoral exit-poll design and analysis in\u000d      the UK.\u000d    ","ImpactType":"Technological","Institution":"\u000d    University of Warwick\u000d    ","Institutions":[{"AlternativeName":"Warwick (University of)","InstitutionName":"University of Warwick","PeerGroup":"A","Region":"West Midlands","UKPRN":10007163}],"Panel":"B         ","PlaceName":[],"References":"\u000d    The paper that reports all of the underpinning research is:\u000d    \u000a[1] Curtice, J and Firth, D (2008). Exit polling in a cold\u000d      climate: The BBC\/ITV experience in Britain in 2005 (with discussion). Journal\u000a        of the Royal Statistical Society A, 171, 509-539.\u000d      http:\/\/dx.doi.org\/10.1111\/j.1467-985X.2007.00536.x\u000d    \u000a\u000aThis paper was read before the Royal Statistical Society in 2007 at one\u000d      of the RSS Ordinary Meetings, and subsequently published with\u000d      discussion in the RSS Journal, Series A.\u000d    \u000a\u000aThis paper was also one of the reported highlights of Firth's output\u000d      during his ESRC Professorial Fellowship held at Warwick between 2003 and\u000d      2006 &#8212; for details see\u000d        http:\/\/tinyurl.com\/ovdy86h. The paper was one of Firth's four listed\u000d      outputs in RAE 2008 (UoA 22, Statistics and Operational Research,\u000d      University of Warwick). It also featured in the published citation for the\u000d      award to Firth of the Guy Medal in Silver (2012) &#8212; for details see\u000d      http:\/\/www.statslife.org.uk\/RSSN\/RSSNEWS_October2012.pdf.\u000d    \u000a","ResearchSubjectAreas":[{"Level1":"16","Level2":"6","Subject":"Political Science"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d    [2] Curtice, J, Fisher, S D and Kuha, J (2011). Confounding the\u000d      Commentators: How the 2010 Exit Poll Got it (More or Less) Right. Journal\u000a        of Elections, Public Opinion &amp; Parties A, 21, 211-235.\u000d      http:\/\/dx.doi.org\/10.1080\/17457289.2011.562612\u000d    [3] Firth, D. (2010) Exit polling explained. Public interest\u000d      web-page and glossary,\u000d      http:\/\/warwick.ac.uk\/dfirth\/exit-poll-explainer.\u000d      (Accessed more than 900 times from outside Warwick in the 180 days up to\u000d      16 October 2013.)\u000d    [4] Iain Dale's Diary, \"Are the BBC\/ITN\/Sky About to Have Egg on\u000d      Their Faces?\",\u000d      http:\/\/iaindale.blogspot.co.uk\/2010\/05\/are-bbcitnsky-about-to-have-egg-on.html\u000d    [5] YouTube clip of the opening few minutes of BBC TV election night\u000d      coverage on 6 May 2010.\u000d      http:\/\/www.youtube.com\/watch?v=hr2mojjmG1M\u000d    [6] Letter from the Editor of BBC Political Research.\u000d    [7] Letter from the BBC's statistical consultants for the May 2010\u000d      General Election.\u000d    [8] Official BBC viewing figures for election night 6 May 2010,\u000d      http:\/\/www.bbc.co.uk\/pressoffice\/pressreleases\/stories\/2010\/05_may\/07\/election_figures.shtml\u000d    [9] Payne, C D (2003), Election Forecasting in the UK: The BBC's\u000d      Experience. Euramerica 33, 193-234. http:\/\/www.ea.sinica.edu.tw\/eu_file\/12014268324.pdf\u000d    ","Title":"\u000d    Impact of novel methodology for election exit-poll design and analysis at\u000d      the 2010 UK General Election\u000d    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d    The underpinning research was application-specific statistical\u000d      methodology, developed by Professor David Firth at the University of\u000d      Warwick between October 2004 and February 2006. The research work was\u000d      carried out in preparation for the May 2005 General Election, and then\u000d      continued over several months after that election with post-election\u000d      analytical work to assess the performance of the methods in detail.\u000d    Firth, together with Professor John Curtice of the University of\u000d      Strathclyde (Department of Government), had been engaged by the BBC and\u000d      ITV jointly to design and analyse their election- day exit poll at the\u000d      next General Election (expected to take place in 2005 or 2006). A full\u000d      account of the methodological development was published in 2008 in the\u000d      jointly authored paper [1], detailed below.\u000d    The Curtice &amp; Firth (2008) paper [1] describes several strands of\u000d      novel statistical methodology developed for use at the 2005 General\u000d      Election, which would be the first time that BBC and ITV (and their\u000d      respective opinion-polling partner companies, NOP and MORI) were to\u000d      combine resources to run an exit poll and produce a single forecast that\u000d      would be broadcast by both organisations at 10pm on polling day. The novel\u000d      methods included, as the most important statistical ingredients:\u000d    (i) Design of the exit-poll via a panel of polling\u000d      stations (drawn from those that had been used by NOP and MORI in their\u000d      separate operations at the 2001 General Election).\u000d    (ii) Modelling of the exit-poll data through multivariate\u000d      regressions of electoral change. This included the detailed\u000d      development and testing of a completely new, coherent approach to the\u000d      treatment of multi-party shares of the vote.\u000d    (iii) Accurate calibration of constituency-level probability\u000d      forecasts through a new, non- standard method with tuning constants\u000d      determined through extensive experimentation with data from previous\u000d      elections.\u000d    The methodological (statistical) research was all carried out by Firth.\u000d      Curtice's role was to provide essential and detailed political and polling\u000d      knowledge.\u000d    Firth's involvement in election-night forecasting had begun in 1997 when\u000d      he worked at the University of Oxford: at the 1997 and 2001 General\u000d      Elections he served as assistant to the veteran psephologist Clive Payne,\u000d      who retired as the BBC's statistical consultant on election forecasting\u000d      after the 2001 election. Early developments in some parts of the new\u000d      methods of Curtice and Firth [1] were trialled at the 2001 election; the\u000d      bulk of the research work, though, was done after Firth moved to Warwick\u000d      on 1 October 2003 and was subsequently appointed by the BBC as Payne's\u000d      successor to work on the 2005 election.\u000d    "},{"CaseStudyId":"2435","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"2623032","Name":"Denmark"},{"GeoNamesId":"3057568","Name":"Slovakia"},{"GeoNamesId":"3190538","Name":"Slovenia"},{"GeoNamesId":"3017382","Name":"France"},{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"1861060","Name":"Japan"},{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"690791","Name":"Ukraine"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"3057568","Name":"Slovakia"},{"GeoNamesId":"2782113","Name":"Austria"},{"GeoNamesId":"798544","Name":"Poland"},{"GeoNamesId":"719819","Name":"Hungary"},{"GeoNamesId":"2264397","Name":"Portugal"},{"GeoNamesId":"660013","Name":"Finland"},{"GeoNamesId":"2510769","Name":"Spain"},{"GeoNamesId":"3077311","Name":"Czech Republic"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    \"RODOS is now used operationally for emergency preparedness\u000d\u000a        (planning), for training and for\u000d\u000a        emergency response in case of a nuclear accident in several European\u000d\u000a        countries including\u000d\u000a        Germany, Finland, Spain, Portugal, Austria, the Netherlands, Poland,\u000d\u000a        Hungary, Slovakia, Ukraine,\u000d\u000a        Slovenia, and the Czech Republic\" [18]. RODOS is in continual use in\u000d\u000a      these countries, and is also\u000d\u000a      implemented in research, training and planning centres in many other\u000d\u000a      countries (30 installations in\u000d\u000a      22 countries overall [18]), and used at European and international levels\u000d\u000a      to plan and run exercises,\u000d\u000a      providing the tools for a coherent, consistent and harmonised response.\u000d\u000a      India has developed a\u000d\u000a      version, IRODOS, for evaluation and research. The UK, while not planning\u000d\u000a      to use a full system\u000d\u000a      such as RODOS, uses many of its modules in stand-alone or smaller systems.\u000d\u000a      During the\u000d\u000a      Fukushima Crisis of 2011 RODOS was used in several studies for individual\u000d\u000a      countries and\u000d\u000a      internationally as part of the assessment of the impacts [16, 19].\u000d\u000a    The current version of RODOS incorporates many modules which implement\u000d\u000a      the designs and\u000d\u000a      methods proposed by Smith and French in the 1990s [7, 8, 9]. These\u000d\u000a      implementations have been\u000d\u000a      engineered into the operational version of RODOS by many partners to the\u000d\u000a      project during the last\u000d\u000a      decade. Comparing the 25 papers in the special issue of Radioprotection\u000d\u000a      [7], which summarises\u000d\u000a      much of the functionality of current RODOS implementations, with the early\u000d\u000a      design paper [3] shows\u000d\u000a      the strong influence that this paper has had in shaping the current\u000d\u000a      system.\u000d\u000a    The impact of the work by Smith and French has been to influence public\u000d\u000a      policy towards response\u000d\u000a      to nuclear contamination, increasing the security and safety of\u000d\u000a      populations living near nuclear plant\u000d\u000a      and, indeed, those further afield. Their research \"shaped the design of\u000d\u000a      uncertainty handling, data\u000d\u000a      assimilation and evaluation within the RODOS system\" which was important\u000d\u000a      because \"the\u000d\u000a      treatment of uncertainty in a consistent manner throughout the system was\u000d\u000a      (however) identified at\u000d\u000a      an early stage as a key area for development\" [16].\u000d\u000a    The current co-ordinator of the RODOS Development Activities states \"I\u000d\u000a      can confirm that many of\u000d\u000a      the concepts ideas and advice that you promulgated in the development\u000d\u000a      teams over the years\u000d\u000a      have been implemented within the operational versions of RODOS\" [17] and\u000d\u000a      this view is supported\u000d\u000a      by the Chairman of the International RODOS Users Group \"The current\u000d\u000a      version of RODOS\u000d\u000a      incorporates many modules which implement the designs and methods proposed\u000d\u000a      by Smith and\u000d\u000a      French\". Recently the RODOS system has \"performed well in predicting the\u000d\u000a      impact of the\u000d\u000a      accidental releases from the Fukushima Daiichi NPP in March 2011 &#8212; both in\u000d\u000a      Japan and more\u000d\u000a      widely &#8212; and in effectively informing relevant decision makers\" [16, 19].\u000d\u000a    Specifically, RODOS and related systems draw on the impetus given by\u000d\u000a      Smith and French in\u000d\u000a      using:\u000d\u000a    \u000d\u000a      Belief nets to estimate the probabilities of different source terms\u000d\u000a        during the threat stage\u000d\u000a        [10].\u000d\u000a      Kalman Filtering and related Bayesian forecasting methods in short-,\u000d\u000a        medium- and long-\u000d\u000a        range atmospheric dispersion models and in hydrological models [9, 11].\u000d\u000a      Bayesian spatio-temporal models to provide estimates of ground\u000d\u000a        contamination into the\u000d\u000a        long term [7].\u000d\u000a      MCDA modules to support the evaluation of different countermeasure\u000d\u000a        strategies including\u000d\u000a        an exploration of constraint satisfaction to construct feasible\u000d\u000a        strategies and an explanation\u000d\u000a        system to interpret the outputs into natural language [7-9, 12-14].\u000d\u000a    \u000d\u000a    In addition, many of the RODOS modules and design features are shared\u000d\u000a      with the Danish ARGOS\u000d\u000a      system, which is used within several Scandinavian countries and elsewhere\u000d\u000a      in the World. ARGOS\u000d\u000a      implements the same methodologies but using a different system\u000d\u000a      architecture. The underlying\u000d\u000a      understanding of nuclear emergency response and recovery, evident in the\u000d\u000a      designs of Smith and\u000d\u000a      French [3, 4], have also been incorporated into the motivation of the\u000d\u000a      NERIS platform (www.eu-neris.net),\u000d\u000a      which provides a forum for dialogue and methodological development between\u000d\u000a      all\u000d\u000a      European organisations and associations taking part in decision making of\u000d\u000a      protective actions in\u000d\u000a      nuclear and radiological emergencies and recovery in Europe. NERIS, in\u000d\u000a      which Warwick is a UK\u000d\u000a      partner along with Public Health England (PHE), is concerned, among other\u000d\u000a      things, to integrate\u000d\u000a      RODOS into local, national and international emergency management\u000d\u000a      processes and develop its\u000d\u000a      use with a broad range of stakeholders.\u000d\u000a    The use of Kalman filtering in atmospheric dispersion is implemented to\u000d\u000a      varying degrees at short,\u000d\u000a      medium and long ranges; some implementations are stand-alone [7, 9, 11].\u000d\u000a      Constraint satisfaction\u000d\u000a      approaches to the coarse expert system are not in the current\u000d\u000a      implementation, but the methods\u000d\u000a      augmented by case-based reasoning methodology are being evaluated in the\u000d\u000a      newly funded EU\u000d\u000a      FP7 PREPARE project (http:\/\/cordis.europa.eu\/projects\/rcn\/106584_en.html,\u000d\u000a      &#8364;6.4m). The early\u000d\u000a      issues relating to the tractability of Kalman filtering led to theoretical\u000d\u000a      developments of dynamic\u000d\u000a      belief nets and fast algorithms [5, 6]. There is a current recognition of\u000d\u000a      the need to develop\u000d\u000a      explanation systems for MCDA and other decision support tools, and the\u000d\u000a      success of the\u000d\u000a      explanation tool within the fine expert system of RODOS is being taken a\u000d\u000a      key example [15].\u000d\u000a    Smith and French are partners in a research project Management of\u000d\u000a        Nuclear Risk Issues:\u000d\u000a        Environmental, Financial and Safety (NREFS), funded by the EPSRC as\u000d\u000a      part of a UK-India Civil\u000d\u000a      Nuclear Research Collaboration. Its objective is to re-evaluate, following\u000d\u000a      Fukushima, some of the\u000d\u000a      fundamental thinking about the emergency management of a radiation\u000d\u000a      accident, particularly in\u000d\u000a      early phase decisions on evacuation and the establishment of exclusion\u000d\u000a      zones.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Professors Smith and French designed the overall information flows and\u000d\u000a      outline methodologies for\u000d\u000a      forecasting and decision analysis now incorporated into RODOS, a\u000d\u000a      widely-installed decision\u000d\u000a      support system for responding to nuclear emergencies. Their design uses:\u000d\u000a    \u000d\u000a      Bayesian statistics to forecast contamination spread and Bayesian\u000d\u000a        spatio-temporal models to\u000d\u000a        estimate contamination in the longer term;\u000d\u000a      multi-criteria decision analysis (MCDA) for the evaluation of\u000d\u000a        strategies;\u000d\u000a      an explanation system to translate the numerical outputs into plain\u000d\u000a        language.\u000d\u000a        RODOS and closely related systems are now installed for operational,\u000d\u000a        emergency planning and\u000d\u000a        training use by many national, regional and local European governments\u000d\u000a        and several other\u000d\u000a        countries world-wide (22 countries overall).\u000d\u000a    \u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Warwick\u000d\u000a    ","Institutions":[{"AlternativeName":"Warwick (University of)","InstitutionName":"University of Warwick","PeerGroup":"A","Region":"West Midlands","UKPRN":10007163}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    Publications:\u000d\u000a    \u000a1. D. Ranyard and J. Q. Smith, \"Building a Bayesian model in a\u000d\u000a        scientific environment: managing\u000d\u000a      uncertainty after an accident\" In S. French, and J. Q. Smith, Eds (1997).\u000d\u000a      The Practice of\u000d\u000a      Bayesian Analysis. London, Arnold, Chapter. 245 - 258. (1997) ISBN:\u000d\u000a      978-0340662403\u000d\u000a    \u000a\u000a2. J.Q. Smith and S. French \"Bayesian updating of atmospheric\u000d\u000a        dispersion models for use after\u000d\u000a        an accidental release of radiation\" The Statistician 42(5), 501-511.\u000d\u000a      (1993)\u000d\u000a    \u000a\u000a3. G. Caminada, S. French, K. Politis and J.Q. Smith, \"Uncertainty\u000d\u000a        in RODOS\". Revised RODOS\u000d\u000a      Research Report RODOS (B)-RP(94)05. FZK, Karlsruhe, Germany. (2000)\u000d\u000a      Available at http:\/\/www2.warwick.ac.uk\/fac\/sci\/statistics\/staff\/academic-\u000d\u000aresearch\/french\/research_interests\/crisismanagement\/uncertainty_in_rodos.pdf\u000d\u000a      This is an updated version of an earlier report: S. French, D. Ranyard and\u000d\u000a      J.Q. Smith,\u000d\u000a      Uncertainty in RODOS. RODOS Report RODOS (B)-RP(94)05, FZK, Karlsruhe,\u000d\u000a      Germany.\u000d\u000a      (1995)\u000d\u000a    \u000a\u000a4. S. French, K.N. Papamichail, D.C. Ranyard and J.Q. Smith `Design\u000d\u000a        of a decision support\u000d\u000a        system for use in the event of a radiation accident.' In F. Javier\u000d\u000a      Gir&#243;n and M. L. Mart&#237;nez (Eds)\u000d\u000a      Applied Decision Analysis. Boston, USA: Kluwer Academic Publishers p.\u000d\u000a      2-18. (1998) ISBN:\u000d\u000a      0792382501\u000d\u000a    \u000a\u000a5. J.Q. Smith, and K.N. Papamichail, \"Fast Bayes and the\u000d\u000a        dynamic junction forest\" Artificial\u000d\u000a      Intelligence 107(1) 99-124. (1999) DOI: 10.1016\/S0004-3702(98)00103-9\u000d\u000a    \u000a\u000a6. M. Drews, B. Larsen, H. Madsen and J.Q. Smith \"Kalman\u000d\u000a        Filtration of Radiation Monitoring\u000d\u000a        Data from Atmospheric Dispersion of Radioactive Materials\" Radiation\u000d\u000a      Protection Doisometry,\u000d\u000a      111(3) 257-269 (2004) DOI: 10.1093\/rpd\/nch339\u000d\u000a    \u000aGrants awarded:\u000d\u000a    J.Q Smith (PI): Proof of concept of Kalman Filtering, EU DG12,\u000d\u000a      1992-93, ECU 25,000 (Warwick\u000d\u000a      component)\u000d\u000a    J.Q. Smith (PI): Uncertainty handling in RODOS, EU DG12, 1996-99,\u000d\u000a      ECU 192,000\u000d\u000a    J.Q. Smith (PI): Dynamic Probabilistic Expert Systems, EPSRC\u000d\u000a      GR\/K72254\/01, 1996-99,\u000d\u000a      &#163;124,025\u000d\u000a    H.P. Wynn (PI), J.Q. Smith (CI): Decision Support in\u000d\u000a      Nuclear Incidents, EU DG12, 2000-2004,\u000d\u000a      &#163;142,200\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    \u000d\u000a      Special Issue of Radioprotection (2010) Volume 45 Issue 5.\u000d\u000a        Cambridge University Press. Table\u000d\u000a        of Contents at:\u000d\u000a        http:\/\/www.radioprotection.org\/action\/displayIssue?decade=2010&amp;jid=RAD&amp;volumeId=45&amp;issueId\u000d\u000a          =05&amp;iid=8803740\u000a\u000d\u000a      Ehrhardt, J. and Weiss, A. (2000). `RODOS: Decision Support'.\u000d\u000a        Off-Site Nuclear Emergency\u000d\u000a        Management in Europe. EUR19144EN. Luxembourg, European Community.\u000d\u000a      EURANOS Project final summary report: ftp:\/\/ftp.cordis.europa.eu\/pub\/fp6-\u000d\u000a          euratom\/docs\/euranos-publishable-summary-final_en.pdf\u000a\u000d\u000a      M. Zavisca, H. Kahlert, M. Khatib-Rahbar, E. Grindon and M. Ang (2004)\u000d\u000a        `A Bayesian Network\u000d\u000a        Approach to Accident Management and Estimation of Source Terms for\u000d\u000a        Emergency Planning.'\u000d\u000a        Paper Presented at the PSAM7\/ESREL'04 Conference 14-18 June 2004,\u000d\u000a        Berlin, Germany.\u000d\u000a      K. Politis and L. Robertson (2004) `Bayesian updating of atmospheric\u000d\u000a        dispersion after a nuclear\u000d\u000a        accident' Journal of the Royal Statistical Society C53(4)\u000d\u000a        583-600\u000d\u000a      Bertsch, V., French, S., Geldermann, J., H&#228;m&#228;l&#228;inen, R. P.,\u000d\u000a        Papamichail, K. N. and Rentz, O.\u000d\u000a        (2009). \"Multi-criteria decision support and evaluation of strategies\u000d\u000a        for environmental remediation\u000d\u000a        management.\" OMEGA 37(1): 238-251.\u000d\u000a      Papamichail, K. N. and French, S. (1999). \"Generating Feasible\u000d\u000a        Strategies in Nuclear\u000d\u000a        Emergencies &#8212; A Constraint Satisfaction Problem.\" J. Op. Res. Soc.\u000d\u000a        50: 617-626.\u000d\u000a      N. Papamichail and S. French (2013) `25 years of MCDA in Nuclear\u000d\u000a        Emergency Management'.\u000d\u000a        IMA Journal of Management Mathematics. In press and published\u000d\u000a        online.\u000d\u000a      Greco, S., Knowles, J. D., Miettinen, K. and Zitzler, E. (2012).\u000d\u000a        Learning in Multiobjective\u000d\u000a        Optimization. Report from Dagstuhl Seminar 12041. Dagstuhl Reports\u000d\u000a        Dagstuhl Publishing,\u000d\u000a        Germany, Schloss Dagstuhl &#8212; Leibniz-Zentrum f&#252;r Informatik. 2(1): 50-99.\u000d\u000a      Letter received from retired official of EU DG Research who led the EU\u000d\u000a        post Chernobyl Actions\u000d\u000a        within the European Commission from 1988 to 2008 quotes:\u000d\u000a        \"The current version of RODOS fully embodies their design of information\u000d\u000a        flows, uncertainty\u000d\u000a        handling and decision analytic support\"\u000d\u000a        \"The RODOS system is installed widely in emergency centres in many\u000d\u000a        European countries and\u000d\u000a        beyond. It performed well in predicting the impact of the accidental\u000d\u000a        releases from the Fukushima\u000d\u000a        Daiichi NPP in March 2011 - both in Japan and more widely &#8212; and in\u000d\u000a        effectively informing relevant\u000d\u000a        decision makers. Requests for its installation in China and in several\u000d\u000a        countries in South East Asia\u000d\u000a        have been made and are being given consideration under the auspices of\u000d\u000a        the European\u000d\u000a        Commission's programme on International Nuclear Safety Cooperation.\"\u000d\u000a      Letter received from current co-ordinator of all RODOS related R&amp;D\u000d\u000a        activities, quotes \"your\u000d\u000a        work with Professor French within the RODOS community in which you\u000d\u000a        helped shape our thinking\u000d\u000a        on uncertainty handling, data assimilation and decision support...has\u000d\u000a        been maintained as RODOS\u000d\u000a        has matured into implementation. Moreover, similar systems such as ARGOS\u000d\u000a        have developed\u000d\u000a        along the same sort of lines.\"\u000d\u000a      Letter received from Chairman of the International RODOS Users Group\u000d\u000a        quotes: \"Their\u000d\u000a        conceptual work and advisory role for many other developer groups mainly\u000d\u000a        covered the areas of\u000d\u000a        source term assessment based on belief nets; uncertainty handling; data\u000d\u000a        assimilation in\u000d\u000a        atmospheric dispersion models, food chain models and hydrological models\u000d\u000a        based on the Kalman\u000d\u000a        filter approach; multi-criteria decision analysis modules to support the\u000d\u000a        evaluation of different\u000d\u000a        countermeasure strategies. The current version of RODOS incorporates\u000d\u000a        many modules which\u000d\u000a        implement the designs and methods proposed by Smith and French.\"\u000d\u000a      See http:\/\/atmos.physic.ut.ee\/~muscaten\/YSSS2011\/YSSS2011-\u000d\u000a          Posters\/Poster_SvitlanaDidkivska.pdf\u000a\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    The design of methodologies for a nuclear emergency management system\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The 1986 Chernobyl Accident stimulated much European R&amp;D to become\u000d\u000a      better prepared for\u000d\u000a      handling nuclear emergencies. Central to this has been the development of\u000d\u000a      the RODOS system\u000d\u000a      (Real-time Online Decision suppOrt System\u000d\u000a      for nuclear emergency management). In 1990-91 it\u000d\u000a      became apparent that uncertainty handling, data assimilation and\u000d\u000a      evaluation were major issues in\u000d\u000a      its design. How could it track and predict the spread of contamination?\u000d\u000a      How reliable would such\u000d\u000a      estimates and forecasts be? In the longer term, how would it estimate the\u000d\u000a      spatial distribution of\u000d\u000a      contamination? How might it support the evaluation of countermeasures when\u000d\u000a      so many factors\u000d\u000a      needed to be taken into account? Professor Smith at Warwick, and Professor\u000d\u000a      French, who was\u000d\u000a      then at Leeds and Manchester, but joined Warwick in 2011, suggested that\u000d\u000a      Bayesian statistics and\u000d\u000a      decision analysis would provide the necessary framework. A\u000d\u000a      proof-of-concept project (begun in\u000d\u000a      1992, completed in 1993), to test Kalman filtering for assimilating\u000d\u000a      monitoring data for short-range\u000d\u000a      atmospheric dispersion prediction was funded under the EU Framework\u000d\u000a      R&amp;D programme and was\u000d\u000a      successful in demonstrating the efficacy of the Bayesian approach [1, 2].\u000d\u000a      Smith and French,\u000d\u000a      supported by several European Framework contracts through the 1990s,\u000d\u000a      joined the RODOS team\u000d\u000a      to develop further the Kalman filtering methodology, and design more\u000d\u000a      general methods for\u000d\u000a      uncertainty handling, data assimilation and evaluation within RODOS.\u000d\u000a    RODOS is a comprehensive system which deals with all timescales from\u000d\u000a      threat to long-term\u000d\u000a      recovery and across all geographical scales and thus its development has\u000d\u000a      involved very many\u000d\u000a      European and former Soviet Union Institutes. Much of French and Smith's\u000d\u000a      work was to draw on\u000d\u000a      their research on Bayesian statistics and decision analysis and advise\u000d\u000a      many other groups across\u000d\u000a      the RODOS project. By the end of the decade their involvement in many\u000d\u000a      aspects of the RODOS\u000d\u000a      design had extended their conceptual work from short range atmospheric\u000d\u000a      dispersion modelling into\u000d\u000a      medium and long range modelling, ground deposition, hydrological\u000d\u000a      modelling, as well as the\u000d\u000a      chaining of uncertainty throughout RODOS and further developing a\u000d\u000a      multi-criteria decision analysis\u000d\u000a      (MCDA) process to underpin evaluation [3, 4].\u000d\u000a    The implementation of the ideas stimulated theoretical advances:\u000d\u000a      recasting some of the problems\u000d\u000a      as dynamic belief nets stimulated the development of fast algorithms [5,\u000d\u000a      6]. A project, undertaken\u000d\u000a      by French and Smith joint with NNC Ltd, part of AMEC since 2005, showed\u000d\u000a      that Bayesian belief\u000d\u000a      nets could predict source terms during the threat phase of a radiation\u000d\u000a      accident [3]. Bayesian\u000d\u000a      spatio-temporal hierarchical models were developed to provide long term\u000d\u000a      models of ground\u000d\u000a      contamination [3]. Bayesian versions of deterministic hydrological models\u000d\u000a      were discussed and\u000d\u000a      have since been developed. French and Smith's design [3, 4] of the MCDA\u000d\u000a      process for evaluation\u000d\u000a      was novel for the mid 1990's in introducing three stages: a coarse\u000d\u000a        expert system based on\u000d\u000a      constraint satisfaction to construct countermeasure strategies which met a\u000d\u000a      range of constraints; an\u000d\u000a      evaluation system based on multi-criteria value theory to help rank\u000d\u000a      the strategies; and a fine expert\u000d\u000a        system which output the results into everyday language and terms\u000d\u000a      both to help user understanding\u000d\u000a      and to provide an audit trail.\u000d\u000a    "},{"CaseStudyId":"2454","Continent":[{"GeoNamesId":"6255146","Name":"Africa"},{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"6251999","Name":"Canada"},{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"953987","Name":"South Africa"}],"Funders":["Engineering and Physical Sciences Research Council","Medical Research Council"],"ImpactDetails":"\u000a    As a direct result of this body of research, Professor Hutton's estimates\u000a      of life expectancy for those afflicted by cerebral palsy and other\u000a      neurological injuries are widely requested and used internationally. The\u000a      impact of this has been broad in both reach and significance\u000a      to legal professionals, individuals and healthcare providers, in\u000a      particular in legal financial settlements.\u000a    Evidence for life expectancy in litigation\u000a    International reach:\u000a      Since 2008, Hutton has provided expert evidence for 73 cases related to\u000a      cerebral palsy life expectancy, and 30 cases concerning spinal cord and\u000a      brain injuries. Expert reports use data which are maintained and updated\u000a      by Hutton and refer to her published research. These include cases in the\u000a      UK, Australia, Canada, Hong Kong, South Africa and Eire. In adversarial\u000a      jurisdictions, two life expectancy reports are usually provided, although\u000a      in England, Wales and Australia, expert witnesses can receive joint\u000a      instructions from both sides. Hutton has provided reports for defendants\u000a      or plaintiffs, or in response to joint instructions. In Scotland, where\u000a      experts are only allowed to provide reports for one side, Hutton's reports\u000a      are respected by both pursuers and defenders (for example [9a, 10, 11]).\u000a      In Scotland, Hutton provided evidence for 33 cases during 2008-13,\u000a      estimated to be about two thirds of all cerebral palsy births subject to a\u000a      claim.\u000a    Within the UK, there are few expert witnesses instructed on life\u000a      expectancy after neurological injuries besides Hutton, and they all\u000a      typically cite Hutton's research in their evidence. Internationally, the\u000a      only other experts routinely instructed are the California-based Life\u000a        Expectancy Project. Their reports cite Hutton's research, and she\u000a      regularly appears in the same cases (for example [12]).\u000a    Hutton has been approached on behalf of the [text removed for\u000a      publication] to secure her services as expert witness for future cases and\u000a      instructions [13].\u000a    Significance and applications of life expectancy expert reports:\u000a      The majority of high-value medical negligence cases address allegations of\u000a      obstetric failures, which lead to cerebral palsy.\u000a    Annual payments by the NHS Litigation Authority (England) on clinical\u000a      claims increased from &#163;863M in 2010\/11 to &#163;1.28 billion in 2011\/12. 35% of\u000a      these claims result from cerebral palsy cases [9b].\u000a    Since settlements are generally confidential, it is not possible to give\u000a      precise figures to quantify the financial impact of Hutton's work. Minimum\u000a      disagreements between the pursuers and defendants are of the order of five\u000a      years, but are more usually ten or more years in cerebral palsy. As annual\u000a      costs for care for a severely disabled person are at least &#163;200,000 (see\u000a      for example [14]), this equates to &#163;1 million per case for a discrepancy\u000a      of 5 years, and to &#163;2m per case when the discrepancy is ten years.\u000a      Moreover, for a person who might require two paid staff in attendance for\u000a      24 hours a day, the annual cost of care would be twice as much. The\u000a      economic impact of Hutton's research in the years 2008-2013, for 100\u000a      cases, would therefore be conservatively estimated in the range\u000a      &#163;100M-450M.\u000a    Corroborating evidence of the majority of the 103 cases that Hutton has\u000a      provided expert evidence for (2008-2013) can be provided by the HEI but\u000a      due to limitations on space and references allowed, only one will be\u000a      described below.\u000a    One example of a motor and industrial case is the largest individual\u000a      settlement in the UK, awarded to Agnes Collier, who suffered spinal cord\u000a      injury in a car accident on 18 March 2009, resulting in a settlement\u000a      agreed on 19 November 2012. The sum awarded, &#163;23 million, was calculated\u000a      directly from Hutton's evidence on life expectancy. [15]\u000a    \"[text removed for publication]\u000a    [text removed for publication] \"[15].\u000a    Patient networks:\u000a      Hutton provided informatfion for a `CP factsheet on cerebral palsy\u000a      research' [16] provided by a Scottish charity, Capability Scotland, which\u000a      supports people with cerebral palsy and their families. This factsheet is\u000a      updated annually, and she has commented on the 2013 version, at the\u000a      request of Capability Scotland. The factsheet links to a four-page leaflet\u000a      on CP research providing information for the general public on Hutton's\u000a      website, which also attracts references from an internet forum of SCOPE,\u000a      the cerebral palsy charity for England [16]. She and Professor Pharoah\u000a      have also provided specific advice about life expectancy to a few\u000a      individual members of the public who have approached them (e.g., [17]).\u000a    Further practitioner reach:\u000a      Hutton's cerebral palsy research is cited in two textbooks for\u000a      paediatricians and epidemiologists (2010, 2009) [18].\u000a    Hutton has given seminars on life expectancy as part of continuing\u000a      professional education for lawyers, through the Association for Victims of\u000a      Medical Accidents (AVMA) (London, 2008) [19].\u000a    ","ImpactSummary":"\u000a    Professor Hutton has applied her research on statistical models for\u000a      survival analysis to cerebral palsy, a neurological disorder which\u000a      afflicts around 1 in 500 of newborn children globally. The body of\u000a      research has established medically-accepted norms for the life expectancy\u000a      of people with cerebral palsy. Her research extends to the study of life\u000a      expectancy for patients suffering from spinal cord injuries.\u000a    The impact of this work has been internationally substantial, influencing\u000a      medical and legal professionals, and informing lay people with involvement\u000a      in cerebral palsy. Her work is also widely cited by patient-networks and\u000a      textbooks.\u000a    Hutton is regularly called by both defence and plaintiff lawyers, as an\u000a      expert witness worldwide, assessing life expectancy for damages arising\u000a      from negligence in obstetric or paediatric care, or from accidents. Her\u000a      expertise is also used in brain and spinal cord injury cases, which also\u000a      result in substantial awards. The award of appropriate damages in legal\u000a      cases ensures that patients receive the best care for the rest of their\u000a      lives. From Jan 2008 to July 2013 Hutton has provided expert evidence in\u000a      103 such cases around the world, which had impact on decisions about\u000a      compensation totalling in the range &#163;100M-450M.\u000a    ","ImpactType":"Health","Institution":"\u000a    University of Warwick\u000a    ","Institutions":[{"AlternativeName":"Warwick (University of)","InstitutionName":"University of Warwick","PeerGroup":"A","Region":"West Midlands","UKPRN":10007163}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"1819729","Name":"Hong Kong"}],"References":"\u000a    Publications: (Warwick researchers in bold)\u000a    \u000a1. J. L. Hutton and P. F. Monaghan. Choice of Parametric\u000a      Accelerated Life and Proportional Hazards Models for Survival Data:\u000a      Asymptotic Results. Lifetime Data Analysis, 8(4) 375-393. (2002) DOI:\u000a      10.1023\/A:1020570922072\u000a    \u000a\u000a2. G. P. S. Kwong, and J. L. Hutton, Choice of parametric\u000a      models in survival analysis: applications to monotherapy for epilepsy and\u000a      cerebral palsy. Journal of the Royal Statistical Society: Series C\u000a      (Applied Statistics), 52: 153-168. (2003) DOI: 10.1111\/1467-9876.00395\u000a    \u000a\u000a3. K. Hemming and J. L. Hutton. Bayesian sensitivity models for\u000a      missing covariates in the analysis of survival data. J. Eval. Clin. Pract.\u000a      (2010) DOI: 10.1111\/j.1365-2753.2010.01569.x.\u000a    \u000a\u000a4. J. L. Hutton and P.O.D. Pharoah. Effects of cognitive, motor,\u000a      and sensory disabilities on survival in cerebral palsy. Arch.Dis.Child\u000a      86:84-89. (2002) DOI: 10.1136\/adc.86.2.84\u000a    \u000a\u000a5. K. Hemming, J. L. Hutton, A. Colver and M.J. Platt. Regional\u000a      variation in survival of people with cerebral palsy in the United Kingdom.\u000a      Paediatrics, 116(6) 1383-1390. (2005) DOI: 10.1542\/peds.2005-0259\u000a    \u000a\u000a6. K. Hemming, J. L. Hutton, S. Bonellie and J. Kurinczuk.\u000a      Intrauterine growth and survival in cerebral palsy. Arch Dis Child Fetal\u000a      Neonatal Ed. 93 F121-F126 (2008) DOI: 10.1136\/adc.2007.121129\u000a    \u000a\u000a7. J. L. Hutton, J.H.W. Watt and E. Wiredu. Letter commenting on\u000a      `Survival after short- or long-term ventilation after acute spinal cord\u000a      injury: a single-centre 25-year retrospective study.' Spinal Cord 50\u000a      859-860. (2012) DOI: 10.1038\/sc.2012.106.\u000a    \u000aGrant:\u000a      8. J. L. Hutton (PI) (including a sub-contract to Liverpool &#8212; CI\u000a      POD Pharoah), `Life expectancy in cerebral palsy: UK collaboration' MRC,\u000a      G9900630, Feb 2001-Jan 2005 (36 months, extended by maternity leave),\u000a      &#163;229,000\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"}],"Sources":"\u000a    9a. [text removed for publication]\u000a    [text removed for publication]\u000a    9b. NHSLA (http:\/\/www.nhsla.com\/Claims\/);\u000a      BMJ 2013;346:f978 doi: 10.1136\/bmj.f978\u000a    10. [text removed for publication]\u000a    11. [text removed for publication]\u000a    12. Strauss D and Brooks J (California Life Expectancy Project); Report\u000a        on Life Expectancy of Ryan Coyle, dated 25th April 2013.\u000a    13. [text removed for publication]\u000a    14. [text removed for publication]\u000a    15. Agnes Collier case: Settlement was widely reported in the media. For\u000a      example, http:\/\/www.dailymail.co.uk\/news\/article-2235334\/Agnes-Collier-17-paralysed-crash-killed-mother-awarded-23m-compensation-payout.html).\u000a    [text removed for publication]\u000a    16. Capability Scotland factsheet \"Cerebral Palsy Research\". http:\/\/www.capability-scotland.org.uk\/media\/57746\/cerebral_palsy_research_2013.pdf;\u000a      and SCOPE forum which refer to leaflet \"J L Hutton and K Hemming &#8212; Life\u000a      expectancy of children with cerebral palsy\" on Hutton's website at http:\/\/www2.warwick.ac.uk\/fac\/sci\/statistics\/staff\/academic-research\/hutton\/scope4.pdf\u000a    17. [text removed for publication]\u000a    18. Textbooks: `Textbook of perinatal epidemiology' (2010), E Shiener\u000a      (editor) `Paediatric Rehabilitation Principles &amp; Practices: 4th\u000a      Edition' (2009) MA Alexander, DJ Matthews\u000a    19. Event programmes with Hutton's name as a lecturer for AVMA\u000a      conferences can be requested from: http:\/\/www.avma.org.uk\/\u000a      (0845 123 23 52)\u000a    ","Title":"\u000a    The impact of research on life expectancy on people with cerebral palsy\u000a      and other neurological injuries\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"},{"GeoNamesId":"6269131","Name":"England"},{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research contains two main and symbiotic strands:\u000a      underpinning methodological statistical research, and applied statistical\u000a      analyses focused on the survival of patients with cerebral palsy and\u000a      related neurological disorders.\u000a    Between 2000 and 2005, Professor Hutton's statistical research in\u000a      survival analysis was focussed on reliable and robust estimation of\u000a      lifetime distributions. Her research with two PhD students [1, 2] derived\u000a      the distributions of estimators of regression coefficients under\u000a      mis-specified proportional hazard and accelerated life models, and thus\u000a      demonstrated the increased power and precision of accelerated life models.\u000a      Hutton with Hemming (research fellow at Warwick on an MRC grant [8])\u000a      proposed a Bayesian methodology for assessing assumptions about the missingness\u000a      status of covariate data, and the effects this has on inference for\u000a      accelerated failure time models [3].\u000a    Hutton's methodological statistical research determined the use of\u000a      accelerated life models for the substantive clinical analyses. The\u000a      increased power of inference using these models allowed more subtle\u000a      effects to be detected. In particular, in 2002, Hutton, with Pharoah\u000a      (Department of Public Health, University of Liverpool), published [4] the\u000a      first article to establish that visual disability is strongly associated\u000a      with survival of people with cerebral palsy, both univariately and after\u000a      allowing for motor and mental disabilities. The publication [4] also\u000a      demonstrated a decline in survival since 1966 for certain sub-groups.\u000a      Hutton, Hemming and two clinical colleagues established that after\u000a      disabilities were taken into account, survival did not differ between\u000a      regions of the UK, but it did differ by socio-economic status and birth\u000a      weight [5], and the same research team subsequently showed that the\u000a      socio-economic effect was best explained by intra-uterine growth [6].\u000a    In related applied research using the same underpinning statistical\u000a      methodology for survival analysis (from [1, 2, 4]) Hutton has also\u000a      analysed survival of people with spinal cord injuries [7].\u000a    Professor Hutton has been a full-time member of academic staff in the\u000a      Department of Statistics at Warwick, where she carried out this research,\u000a      since October 2000. She took the lead on all statistical research\u000a      described above and also initiated the clinical papers listed.\u000a    Dr Hemming was funded as a post-doctoral research fellow on MRC grant\u000a      G9900630 &#8212; Feb 2001 &#8212; June 2005 [8]. Dr GPS Kwong was a Statistics\u000a      Department-funded PhD student Oct 2000-June 2003. Dr K Boyd was an\u000a      EPSRC-funded PhD student Oct 2003-Feb 2007. Key clinical research\u000a      contributions contributing to this body of research have been made by\u000a      Professor POD Pharoah, Dr MJ Platt (University of Liverpool); Dr A Colver\u000a      (Newcastle University); and Prof J Kurinczuk (Oxford University).\u000a    "},{"CaseStudyId":"2455","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"2510769","Name":"Spain"}],"Funders":["Wellcome Trust","Biotechnology and Biological Sciences Research Council","Medical Research Council"],"ImpactDetails":"\u000d\u000a    Hutton's research on the effects of selection bias includes development\u000d\u000a      of new statistical methods, and consequently direct application of the\u000d\u000a      results to particular diseases and treatments, and to the implications for\u000d\u000a      good conduct and reporting of studies. The impact has therefore been in\u000d\u000a      three areas:\u000d\u000a    A) Specific clinical guidance on the treatment of epilepsy\u000d\u000a      Epilepsy is a common neurological disorder affecting over 500,000 people\u000d\u000a      within the UK (http:\/\/www.nhs.uk\/Conditions\/Epilepsy\/Pages\/Introduction.aspx).\u000d\u000a      In around 70% of cases, seizures are successfully controlled by AEDs\u000d\u000a      (anti-epileptic drugs) which are the 5th highest category of\u000d\u000a      expenditure on NHS England prescriptions. The 2004 NICE (National\u000d\u000a      Institute for Health and Clinical Excellence) Guidelines for the Diagnosis\u000d\u000a      and Management of Epilepsy highlighted inadequacies in care and treatment\u000d\u000a      of epilepsy patients. In 2007 a major multi-centre study SANAD (Standard\u000d\u000a      and New Antiepileptic Drugs), which compared the clinician's choice of\u000d\u000a      drug against new AEDs in over 2,000 patients, was published. The design of\u000d\u000a      this trial, led by UoL, was determined by Hutton's research. Her analyses\u000d\u000a      had highlighted uncertainty with regard to interactions between drugs, and\u000d\u000a      patient factors of age, type of epilepsy and seizure type related to\u000d\u000a      misclassification bias [6, 8]. The significance of the SANAD trial is\u000d\u000a      considerable, e.g.:\u000d\u000a    \u000d\u000a      SANAD played a central role in the construction of the 2012 NICE\u000d\u000a        Guidelines [12a], \"The primary scope of the guidelines was to\u000d\u000a          consider the role of antiepileptic drugs, especially given the impact\u000d\u000a          of important, real-world studies such as SANAD. The role of\u000d\u000a          established and newly licensed drugs has been considered using novel\u000d\u000a          statistical methods allowing comparison of cost effectiveness\"\u000d\u000a        (Preface P3). The Guidelines are important since they are \"expected\u000d\u000a        to be taken into full consideration by healthcare professionals and\u000d\u000a          organisations when deciding on treatments for patients\" [12b]. A\u000d\u000a        Consultant in Neuropsychiatry [12c] states that \"SANAD is the best\u000d\u000a          clinical trial and is the gold standard piece of work in relation to\u000d\u000a          the treatment of epilepsy. It is referred to in the NICE guidelines\u000d\u000a          for good reason. The other main strength of SANAD is that it was not\u000d\u000a          constrained by the needs of the pharmaceutical industry and has the\u000d\u000a          reputation of being relatively bias-free\".\u000d\u000a      The World Health Organisation (WHO) guidelines \"Evidence-based\u000d\u000a        recommendations for management of epilepsy and seizures ...\" cite the\u000d\u000a        meta-analysis [13a]. In addition, in [13b], the SANAD trial results were\u000d\u000a        explicitly highlighted when WHO deliberated whether to allow the\u000d\u000a        application for a new epilepsy drug treatment regime. SANAD trial\u000d\u000a        results were used in a WHO decision not to include Lamotrigine for\u000d\u000a        epilepsy in their Model List of Essential Medicines.\u000d\u000a      SANAD determined the recommendations of the German Association of\u000d\u000a        Scientific Medical Societies, for AEDs to be used to treat first\u000d\u000a        seizures and epilepsy in adults [14].\u000d\u000a      The Scottish Intercollegiate Guidelines Network [15] cites\u000d\u000a        meta-analyses (eg [8]) by Hutton as justification for its treatment\u000d\u000a        recommendations.\u000d\u000a      Hutton was instructed [text removed for publication], as an expert\u000d\u000a        witness for the claimants in a multi-party class action [text removed\u000d\u000a        for publication], after which the case was concluded with a confidential\u000d\u000a        out-of-court settlement.\u000d\u000a    \u000d\u000a    B) Generic guidance for the conduct and reporting of biomedical research\u000d\u000a    There are many examples where Hutton's research [3, 4, 5] has been cited and\u000d\u000a    used inter alia by policy makers, journal editors, and potential\u000d\u000a    study participants, to provide guidance on the ethical design and conduct of\u000d\u000a    cluster randomised trials, including the following examples.\u000d\u000a    1. Impacts on guidelines for statistical validity and ethics in cluster\u000d\u000a      randomised trials [4,5] include:\u000d\u000a    \u000d\u000a      One of the MRC's clinical trials guidelines \"Cluster Randomised\u000d\u000a        Trials: Methodological and Ethical Considerations\" [17] is based largely\u000d\u000a        on the research in [4].\u000d\u000a      Recommendations (from [4, 5]) are also incorporated into the\u000d\u000a        Consolidated Standards of Reporting Trials (CONSORT) Design extension to\u000d\u000a        cluster randomised trials [18]. CONSORT is in turn included within the\u000d\u000a        International Committee of Medical Journal Editors (ICMJE)\u000d\u000a        recommendations [19].\u000d\u000a      Recommendations (from [4, 5]) are also incorporated into the Ottawa\u000d\u000a        Statement on the Ethical Design and Conduct of Cluster Randomized Trials\u000d\u000a        [20].\u000d\u000a    \u000d\u000a    2. Preferred Reporting Items for Systematic Reviews and Meta-Analyses\u000d\u000a      (PRISMA) [21] is an evidence-based checklist for reporting systematic\u000d\u000a      reviews and meta-analysis, which has been translated into Spanish, Korean\u000d\u000a      and Russian. Its recommendations are also incorporated into the ICMJE [19]\u000d\u000a      to which most biomedical journals subscribe, and thus whose authors must\u000d\u000a      comply with each item on a specified checklist. Hutton's work in [3]\u000d\u000a      contributed to two check points related to selective reporting within\u000d\u000a      studies.\u000d\u000a    3. Research led by Hutton for the National Centre for the Replacement,\u000d\u000a      Refinement and Reduction of Animals in Research attracted media coverage,\u000d\u000a      and contributed to reporting guidelines in Animal Research: Reporting\u000d\u000a        of In Vivo Experiments (ARRIVE) [22a]. This is endorsed by many\u000d\u000a      journals and ten funders, including the Wellcome Trust and three research\u000d\u000a      councils [22b].\u000d\u000a    C) Evidence given in other trial cases as an expert witness\u000d\u000a      Based on her research on selection bias, ethics and meta-analysis, Hutton\u000d\u000a      has made recent appearances as an expert witness in legal cases. They\u000d\u000a      include:\u000d\u000a    \u000d\u000a      The General Medical Council instructed Hutton in May 2008 in a case\u000d\u000a        concerning the conduct, design, choice and reporting of outcome measures\u000d\u000a        of a clinical trial for which three doctors were accused of professional\u000d\u000a        misconduct [23]. The case 'collapsed because it had no sound\u000d\u000a          scientific evidence to support it' [23], as a direct result of\u000d\u000a        Hutton's discussions with the GMC and her report based on her research\u000d\u000a        including [1,2].\u000d\u000a      Based on her work including [3, 7, 9], Hutton was\u000d\u000a        [text removed for publication]\u000d\u000a        (over 100 cases) [24a, 24b, 24c].\u000d\u000a    \u000d\u000a    ","ImpactSummary":"\u000d\u000a    Professor Hutton's research considers the biasing effect of selection of\u000d\u000a      data due to consent procedures or selective reporting, and its\u000d\u000a      consequences for the validity of conclusions and reliability of results.\u000d\u000a      This research has had impacts on patients directly; on health and legal\u000d\u000a      professionals by informing and influencing national and international\u000d\u000a      guidelines for the treatment of epilepsy used by healthcare professionals\u000d\u000a      and practitioners; and has provided expert evidence to legal professionals\u000d\u000a      for the conclusion of civil litigations and a General Medical Council\u000d\u000a      professional misconduct trial. Hutton's research also informs ethical\u000d\u000a      debate associated with the validity and robustness of study results. This\u000d\u000a      work has determined guidelines for ethical conduct of research, and\u000d\u000a      requirements for publications, which are significant for all biomedical\u000d\u000a      researchers.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    University of Warwick\u000d\u000a    ","Institutions":[{"AlternativeName":"Warwick (University of)","InstitutionName":"University of Warwick","PeerGroup":"A","Region":"West Midlands","UKPRN":10007163}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"6094817","Name":"Ottawa"}],"References":"\u000d\u000a    \u000a1. J.L. Hutton and PR Williamson. Bias in meta-analysis with\u000d\u000a      variable selection within studies. JRSS C., Applied Statistics,\u000d\u000a      49:359-370, 2000. DOI: 10.1111\/1467-9876.00197\u000d\u000a    \u000a\u000a2. S. Hahn, P.R. Williamson, and J.L. Hutton. Investigation of\u000d\u000a      within-study selective reporting in clinical research: Follow-up of\u000d\u000a      applications submitted to an LREC. J.Eval.Clin.Pract. 8(3) 353-359. (2002)\u000d\u000a      DOI: 10.1046\/j.1365-2753.2002.00314.x\u000d\u000a    \u000a\u000a3. P.R. Williamson, C. Gamble, D.G. Altman, and J.L. Hutton.\u000d\u000a      Outcome selection bias in meta-analysis. Stat. Meth. Med. Res. 14(5)\u000d\u000a      515-524. (2005) DOI: 10.1191\/0962280205sm415oa\u000d\u000a    \u000a\u000a4. J.L. Hutton. Are distinctive ethical principles required for\u000d\u000a      cluster randomised controlled trials? Statist.Med. 20(3) 473-488. (2001).\u000d\u000a      DOI: 10.1002\/1097-0258(20010215)20:3&lt;473::AID-SIM805&gt;3.0.CO;2-D\u000d\u000a    \u000a\u000a5. J.L. Hutton, M. Eccles, and J.M. Grimshaw. Ethical issues in\u000d\u000a      implementation research: a discussion of the problems in achieving\u000d\u000a      informed consent. Implementation Science, 3:52. (2009) DOI:\u000d\u000a      10.1186\/1748-5908-3-52\u000d\u000a    \u000a\u000a6. P.R. Williamson, H. Clough, J.L. Hutton, A. Marson, and D.W.\u000d\u000a      Chadwick. Statistical issues in the assessment of the evidence for an\u000d\u000a      interaction between factors in epilepsy trials. Statist.Med. 21(18)\u000d\u000a      2613-2622. (2002) DOI: 10.1002\/sim.1044\u000d\u000a    \u000a\u000a7. P.R. Williamson, C. Tudur Smith, J.L. Hutton, and A.G. Marson.\u000d\u000a      Aggregate data meta-analysis with time-to-event outcomes. Statist.Med. 21\u000d\u000a      3337-3351. (2002) DOI: 10.1002\/sim.1303\u000d\u000a    \u000a\u000a8. AG Marson, PR Williamson, H Clough, J.L. Hutton and D W\u000d\u000a      Chadwick. Carbamazepine versus valproate monotherapy for epilepsy: a\u000d\u000a      meta-analysis. Epilepsia, 43:505-513, 2002. DOI:\u000d\u000a      10.1046\/j.1528-1157.2002.20801.x\u000d\u000a    \u000a\u000a9. K. Hemming, J.L. Hutton, M.J. Maguire, and A.G. Marson. Open\u000d\u000a      label extension studies and patient selection biases. J.Eval.Clin.Pract.\u000d\u000a      14(1) 141-144. (2008) DOI: 10.1111\/j.1365-2753.2007.00821.x\u000d\u000a    \u000a\u000a10. M. Maguire, K. Hemming, J.M. Wild, J.L. Hutton, and A.\u000d\u000a      Marson. Prevalence of visual field loss following exposure to vigabatrin\u000d\u000a      therapy: A systematic review. Epilepsia, 51 2423-2431. (2010) DOI:\u000d\u000a      10.1111\/j.1528-1167.2010.02772.x\u000d\u000a    \u000a11. J.L. Hutton (PI) `Models for selection bias, applied to\u000d\u000a      controlled trials and observational studies of antiepileptic drugs' MRC\u000d\u000a      G0400642 July 2005-Sept 2008 &#163;199,000\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    12a. Pharmacological Update of Clinical Guideline 20. \"The Epilepsies:\u000d\u000a      The diagnosis and management of the epilepsies in adults and children in\u000d\u000a      primary and secondary care\". Final Methods: Evidence and Recommendations\u000d\u000a      January 2012. Commissioned by the National Institute for Health and\u000d\u000a      Clinical Excellence.\u000d\u000a    See also the NICE Clinical Guideline 137, January 2012, pg 7: \".. a recent\u000a        large multicentre trial (the SANAD trial) evaluating newer drugs in\u000d\u000a        newly diagnosed epilepsy (accepting some limitations) suggested that\u000d\u000a        sodium valproate should the drug of choice...It was therefore considered\u000d\u000a        necessary to review new evidence regarding AEDs within an update of NICE\u000d\u000a        clinical guideline 20).\u000d\u000a    12b. See: http:\/\/www.nhs.uk\/NHSEngland\/thenhs\/healthregulators\/Pages\/nice.aspx\u000d\u000a    \u000d\u000a    12c. Consultant in Neuropsychiatry, National Centre for Mental Health,\u000d\u000a      Birmingham \u000d\u000a    13a. WHO guidelines `Evidence based recommendations for the\u000d\u000a        management of epilepsy and siezures in non-specialised health settings:\u000d\u000a        Standard antiepileptic drugs (phenobarbital, phenytoin, carbamazepine,\u000d\u000a        valproic acid) for management of convulsive epilepsy in adults and\u000d\u000a        children'\u000d\u000a      \u000d\u000a        http:\/\/www.who.int\/mental_health\/mhgap\/evidence\/resource\/epilepsy_q7.pdf\u000d\u000a       \u000d\u000a    13b. The Selection and Use of Essential Medicines. Report of the WHO\u000d\u000a      Expert Committee 2009, (WHO Technical report Series: 958), ISBN 978 92 4\u000d\u000a      120958 8 \u000d\u000a    14. German guidelines: Epileptic Shock and Epilepsy in Adults (AWMF No\u000d\u000a      030\/041, dated September 2012). For complete document see\u000d\u000a      \u000d\u000a        http:\/\/www.awmf.org\/leitlinien\/detail\/ll\/030-041.html \u000d\u000a    15. Diagnosis and Management of Epilepsy in Adults &#8212; A National Clinical\u000d\u000a      Guideline (Scottish Intercollegiate Guidelines Network. ISBN 1 899893 58\u000d\u000a      X, Updated October 2005. For full document see: http:\/\/www.sign.ac.uk\/guidelines\/fulltext\/70\/\u000a        \u000d\u000a    16. [text removed for publication] \u000d\u000a    17. \"Cluster randomised trials: Methodological and ethical\u000d\u000a      considerations\" MRC Clinical Trials Series, November 2002. The guidelines\u000d\u000a      continue to have impact &#8212; see current MRC Additional Terms and Conditions\u000d\u000a      'MRC requires research organisations to ensure that the research\u000d\u000a        undertaken under an award by the research organisation itself complies\u000d\u000a        with MRC terms and conditions including MRC's ethics and best practice.\"\u000d\u000a      See\u000d\u000a      http:\/\/www.mrc.ac.uk\/Utilities\/Documentrecord\/index.htm?d=MRC002406.\u000d\u000a    \u000d\u000a    18. See http:\/\/www.consort-statement.org\/consort-statement\/\u000d\u000a      Hutton is cited in Reference 248 \u000d\u000a    19. http:\/\/www.icmje.org\/icmje-recommendations.pdf.\u000d\u000a      See p12 for PRISMA and CONSORT. \u000d\u000a    20. Ottawa statement, PLoS Med 9(11): e1001346. DOI: 10.1371\/journal.pmed.1001346\u000d\u000a    \u000d\u000a    21. The PRISMA Statement for Reporting Systematic Reviews and\u000d\u000a      Meta-Analyses of Studies That Evaluate Health Care Interventions:\u000d\u000a      Explanation and Elaboration. Liberati A et al. PLoS Med\u000d\u000a        6(7): e1000100. DOI 10.1371\/journal.pmed.1000100.\u000d\u000a      Hutton is cited in references 122 and 152. \u000d\u000a    22a. Kilkenny C et al (2010). Improving Bioscience\u000d\u000a      Research Reporting: The ARRIVE Guidelines for Reporting Animal Research, PLoS\u000d\u000a        Biol 8(6): e1000412. DOI: 10.1371\/journal.pbio.1000412.\u000d\u000a      See Reference 5 to Hutton's research. \u000d\u000a    22b. See http:\/\/www.nc3rs.org.uk\/news.asp?id=1861\u000d\u000a      for the reach of the ARRIVE guidelines and http:\/\/www.nc3rs.org.uk\/news.asp?id=1798\u000d\u000a      for link to an open letter from the CEOs of the BBSRC, MRC and Wellcome\u000d\u000a      Trust. \u000d\u000a    23. Gornall J. Professional conduct &#8212; Three doctors and a GMC prosecution\u000d\u000a      BMJ 2008; 337:a907. Download at http:\/\/www.bmj.com\/content\/337\/bmj.a907\u000d\u000a    \u000d\u000a    24a. [text removed for publication] \u000d\u000a    24b. [text removed for publication] \u000d\u000a    24c. [text removed for publication] \u000d\u000a    ","Title":"\u000d\u000a    Impact of research into selection bias and ethical issues on published\u000d\u000a      medical guidelines and legal judgements\u000d\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Ideally, a statistical study should accurately sample from the entire\u000d\u000a      population of interest, but actual statistical trials may miss some types\u000d\u000a      of patients and may not report all outcomes. Therefore careful analysis of\u000d\u000a      resulting biases is required to ensure the statistical integrity of\u000d\u000a      conclusions.\u000d\u000a    The underpinning research analyses (a) the effects of biases caused by\u000d\u000a      incomplete and often selective reporting of data; (b) subsequently related\u000d\u000a      ethical issues; and (c) substantive application to the understanding of\u000d\u000a      epilepsy and drugs used to treat it (e.g., Vigabatrin). The body of\u000d\u000a      research was carried out at Warwick by Professor Hutton, Department of\u000d\u000a      Statistics since 2000, and involved collaboration with researchers at The\u000d\u000a      University of Liverpool (UoL). Specifically:\u000d\u000a    (a) Fundamental methodological research investigating how selection bias\u000d\u000a      depends upon correlations in data, with a specific focus on the effect of\u000d\u000a      incomplete reporting of sub-group analyses, was carried out in [1, 2, 3],\u000d\u000a      and these papers include substantive medical examples. This work involved\u000d\u000a      collaboration with researchers at UoL.\u000d\u000a    (b) Further research [4, 5] focuses on related ethical issues concerning\u000d\u000a      the design and analysis of cluster randomised trials. For example,\u000d\u000a      different methods of infection control applied to different hospitals are\u000d\u000a      assessed by results of individual patients, and therefore individual\u000d\u000a      consent can lead to a skewed population of patients with respect to which\u000d\u000a      biases need to be assessed.\u000d\u000a    (c) Collaboration with neurologists (at UoL) who specialise in epilepsy\u000d\u000a      provided the inspiration for much of the research in (a) and (b) including\u000d\u000a      the impact of missing data and misclassified factors [6, 7, 8], and\u000d\u000a      selection of patients into clinical trials and into follow-on studies\u000d\u000a      after clinical trials [3, 9]. Part of the research was carried out under\u000d\u000a      an MRC grant [11] supporting a Research Associate, Dr Hemming (Warwick)\u000d\u000a      and a Clinical Research Fellow, Dr Maguire (UoL). The Warwick team\u000d\u000a      developed sensitivity analyses to assess biases arising from patient\u000d\u000a      self-selection in open label extension studies, which reported very\u000d\u000a      different results from randomised clinical trials [9]. A further potential\u000d\u000a      source of bias in meta-analysis is missing information on study or patient\u000d\u000a      characteristics; Hemming and Hutton proposed a Bayesian approach to\u000d\u000a      assessing such bias [10]. This method demonstrated that rates of visual\u000d\u000a      field defect increased with dose and duration of Vigabatrin treatment.\u000d\u000a    In all this research, Professor Hutton took the lead on statistical and\u000d\u000a      ethical issues and methods.\u000d\u000a    "},{"CaseStudyId":"2456","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"3996063","Name":"Mexico"},{"GeoNamesId":"2750405","Name":"Netherlands"}],"Funders":["Wellcome Trust","Engineering and Physical Sciences Research Council","Medical Research Council"],"ImpactDetails":"\u000d\u000a    When the outbreak of H1N1 influenza was first identified in Mexico there\u000d\u000a      was worldwide concern that this might signify the start of a pandemic with\u000d\u000a      levels of mortality comparable to those of the 1918 flu outbreak. Keeling,\u000d\u000a      House and Danon have a reputation for both cutting-edge mathematical\u000d\u000a      modelling and applied quantitative epidemiology; this has led to their\u000d\u000a      research being used to advise and inform policy and decision-making for\u000d\u000a      pandemic influenza (in 2009 and for future outbreaks), as well as for\u000d\u000a      other infections such as the recent measles outbreak in Swansea.\u000d\u000a    Informing UK policy through membership of DoH Advisory Committees.\u000d\u000a      Keeling became an active member of the government's Joint Committee on\u000d\u000a      Vaccination and Immunisation (JCVI- influenza) [16] and during 2009 was\u000d\u000a      the designated independent chair of the Scientific Pandemic Influenza\u000d\u000a      Subgroup on Modelling (SPI-M) [15, 17]. He remains a member of both. In\u000d\u000a      both these roles Keeling has provided, and continues to provide, detailed\u000d\u000a      mathematical\/modelling advice to the DoH, based on his research described\u000d\u000a      above [1,2,14-16, 18, 19]. He has provided SPI-M with real-time modelling\u000d\u000a      updates on control and containment of pandemic influenza \"which played an\u000d\u000a      important and often critical role in formulating the Group's [SPI-M's]\u000d\u000a      consensus forecasts of what was happening and what would happen later in\u000d\u000a      the pandemic\" [15].   Both during and after the pandemic,\u000d\u000a      Keeling has provided analysis that \"has been essential in determining UK\u000d\u000a      pandemic policy\" [15]. As acknowledged by a Senior Principal Analyst at\u000d\u000a      DoH and HPA, Keeling \"played a very significant role in generating the\u000d\u000a      SPI-M consensus view which has been fundamental to the construction of the\u000d\u000a      UK's pandemic response and the management of the 2009 pandemic\" [15].\u000d\u000a      Since 2009, SPI-M has continued to meet and provide updated advice to the\u000d\u000a      Civil Contingencies Committee (commonly known as COBRA) in light of recent\u000d\u000a      scientific evidence. \u000d\u000a    A key issue during the early stages of any pandemic is determining the\u000d\u000a      true scale of the outbreak (whether all cases are severe and the outbreak\u000d\u000a      is currently small, or the outbreak is larger but fewer cases have severe\u000d\u000a      symptoms). This knowledge is vital in terms of effective public-health\u000d\u000a      planning. Danon and a team of researchers from around the world (USA,\u000d\u000a      Mexico, Hong Kong and the Netherlands) together with the CDC (Centres for\u000d\u000a      Disease Control, USA) provided early evidence that H1N1 was a mild\u000d\u000a      infection [1] in distinct contrast to all earlier reports. This\u000d\u000a      precipitated a change worldwide in the way the infection was considered.\u000d\u000a      The retrospective analysis undertaken with members of the HPA shows how\u000d\u000a      data from infected households during the early stage of any pandemic could\u000d\u000a      be used to infer the answers to many of these fundamental questions about\u000d\u000a      case severity [5]. As such, this strategy is being considered as part of\u000d\u000a      future planning both in the UK and overseas [14].\u000d\u000a    In addition to providing a quantitative estimate of the current\u000d\u000a      epidemiology, mathematical models were used to provide information about\u000d\u000a      the potential effect of control [3, 6]. During the pandemic there was\u000d\u000a      considerable debate around the optimal use of vaccination, in part driven\u000d\u000a      by poorly- posed findings in the early literature. Once vaccines were\u000d\u000a      available in the later stages of the pandemic, preliminary results (later\u000d\u000a      published as [3]) supported the general medical advice that vaccination\u000d\u000a      should be targeted first to those with severe health risks and then to\u000d\u000a      young children. Antivirals form the other pharmaceutical arm of control,\u000d\u000a      but they did not perform as well in practice during 2009 as indicated by\u000d\u000a      controlled tests; our results [3] showed that prompt antiviral treatment\u000d\u000a      was key to reducing population-level transmission and have informed debate\u000d\u000a      about future antiviral drug usage. A Senior Principal Analyst at DoH and\u000d\u000a      HPA, referring to the 2009 pandemic, acknowledged that Keeling's work\u000d\u000a      \"formed an important contribution to the overall analytical programme\u000d\u000a      which led to the savings of many hundreds of millions of pounds while\u000d\u000a      greatly increasing the health of the Nation\" [15].\u000d\u000a    Outreach to Public, Practitioners and Pupils. Our work in\u000d\u000a      this area has led to substantial impact in term of public interest and\u000d\u000a      engagement (impacts on society), impacts on practitioners and the\u000d\u000a      enhancement of science knowledge in school children both locally and\u000d\u000a      internationally. In all of these policy engagement events we highlight the\u000d\u000a      importance and significance of the impact of our mathematical models in\u000d\u000a      underpinning policy, and how we have substantially informed public debate\u000d\u000a      and stimulated public interest and engagement. Examples of public\u000d\u000a      activities include:\u000d\u000a    \u000d\u000a      Interviews with newspapers and media related to both H1N1 influenza\u000d\u000a        and subsequent research, including [20]:\u000d\u000a        Keeling's and Danon's Guardian interview (October 2011) (average daily\u000d\u000a        circulation of 265,000 in 2011) that linked their research work with the\u000d\u000a        film Contagion [20a];\u000d\u000a        Promoting the importance of childhood vaccination against influenza\u000d\u000a        through The Times of India [20b], the Mail Online (averages more than 8\u000d\u000a        million daily browsers) [20c], House interviewed by Reuters (January\u000d\u000a        2013) [20d], Danon on BBC Coventry &amp; Warwickshire radio (November\u000d\u000a        2011) and House (January 2013) on WAMC Northeast Public Radio (public\u000d\u000a        radio station in New York) [20e].\u000d\u000a      Presentations and interviews for NHS practitioners and coordinators\u000d\u000a        and associated services: For example, House presented at a public policy\u000d\u000a        exchange symposium (for public health officials, from NHS practitioners\u000d\u000a        and pandemic coordinators to local government officers in community\u000d\u000a        services and the Metropolitan Police) in April 2013 [21]. The company\u000d\u000a        Wellards, which provides training on all aspects of the sales\u000d\u000a        environment for professionals in the pharmaceutical and biotech\u000d\u000a        industries with over 16,000 users from 300 companies, has used the\u000d\u000a        research as one of its case studies on `Vaccines in the NHS' [22].\u000d\u000a      Presentations to Maths `A'-level students: Keeling presented how we\u000d\u000a        predict epidemics and control to \"two capacity audiences, each of 900\u000d\u000a        students engaging them with the significance of mathematics in his\u000d\u000a        research and the significance of his research to the general public\" in\u000d\u000a        a Mathematics in Action programme (2011-12) organised by The Training\u000d\u000a        Partnership [23] (who provide events complementary to the A-level\u000d\u000a        curriculum).\u000d\u000a    \u000d\u000a    ","ImpactSummary":"\u000d\u000a    Researchers in the Epidemiology Group at the University of Warwick have\u000d\u000a      an international reputation for high-quality mathematical modelling of\u000d\u000a      human infectious diseases, with particular emphasis on population\u000d\u000a      heterogeneity and variability. Such formulations and insights are an\u000d\u000a      important component of predictive modelling performed by Public Health\u000d\u000a      England (PHE), and are helping to shape national policy for a range of\u000d\u000a      vaccine-preventable infections.\u000d\u000a    The Warwick group was instrumental in providing a range of real-time\u000d\u000a      analyses and advice to UK authorities during the 2009 H1N1 (swine-flu)\u000d\u000a      pandemic, acknowledged by the Department of Health (DoH) to be\u000d\u000a      \"fundamental to the construction of the UK's pandemic response\" and making\u000d\u000a      an important contribution to the overall programme which \"led to the\u000d\u000a      saving of many hundreds of millions of pounds of taxpayers money, while\u000d\u000a      greatly increasing the health of the Nation\". Modelling and analysis\u000d\u000a      carried out at Warwick continue to provide insight into the control and\u000d\u000a      containment of future pandemics and are considered \"essential in\u000d\u000a      determining UK pandemic policy\".\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    University of Warwick\u000d\u000a    ","Institutions":[{"AlternativeName":"Warwick (University of)","InstitutionName":"University of Warwick","PeerGroup":"A","Region":"West Midlands","UKPRN":10007163}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"5128638","Name":"New York"},{"GeoNamesId":"1819729","Name":"Hong Kong"}],"References":"\u000d\u000a    Key publications\u000d\u000a    \u000a1. 1. Keeling, M.J., &amp; Rohani, P., Modeling Infectious\u000d\u000a      Diseases in Humans and Animals. Princeton University Press. 408 pp. (2007)\u000d\u000a      ISBN: 9780691116174\u000d\u000a    \u000a\u000a2. Lipsitch M, Lajous M, O'Hagan JJ, Cohen T, Miller JC, Goldstein E, Danon\u000a        L, Wallinga J, Riley S, Dowell SF, Reed C &amp; McCarron, M., Use\u000a        of cumulative incidence of novel influenza A\/H1N1 in foreign travellers\u000d\u000a        to estimate lower bounds on cumulative incidence in Mexico PLoS ONE,\u000d\u000a      4 (9) e6895. (2009) DOI: 10.1371\/journal.pone.0006895\u000d\u000a    \u000a\u000a3. Keeling, MJ, &amp; White, PJ, Targeting\u000a        vaccination against novel infections: risk, age and spatial structure\u000d\u000a        for pandemic influenza in Great Britain. J. R. Soc. Interface\u000d\u000a      8(58) 661-670. (2011) DOI: 10.1098\/rsif.2010.0474\u000d\u000a    \u000a\u000a4. House T, Baguelin M, van Hoek AJ, White PJ, Sadique Z, Eames\u000d\u000a      K, Read JM, Hens N, Melegaro A, Edmunds WJ &amp; Keeling, M.J., Modelling\u000a        the impact of local reactive school closures on critical care provision\u000d\u000a        during an influenza pandemic. Proc. R. Soc. Lond. B 278(1719)\u000a      2753- 2760 (2011) DOI: 10.1098\/rspb.2010.2688\u000d\u000a    \u000a\u000a5. House T, Inglis N, Ross JV, Wilson F, Suleman S, Edeghere O,\u000d\u000a      Smith G, Olowokure B &amp; Keeling, M.J., Estimation\u000a        of outbreak severity and transmissibility: Influenza A(H1N1)pdm09 in\u000d\u000a        households. BMC Medicine 10117 (2012) DOI:\u000d\u000a      10.1186\/1741-7015-10-117\u000d\u000a    \u000a\u000a6. Black A, House T, Keeling MJ &amp; Ross, J.V., Epidemiological\u000a        consequences of household- based antiviral prophylaxis for pandemic\u000d\u000a        influenza. J. R. Soc. Interface 10(81) 20121019.\u000d\u000a      (2013) DOI: 10.1098\/rsif.2012.1019\u000d\u000a    \u000aKey peer-reviewed grants and awards\u000d\u000a    7. Leach (HPA, PI), Grenfell (Cambridge), Keeling (Warwick)\u000d\u000a      \"Application of HE computing to public health\" EPSRC GR\/S43214\/01 Oct 2003\u000d\u000a      - Sept 2005 &#163;90,776.\u000d\u000a    8. Keeling (Warwick, PI), Read (Liverpool) \"Social contact survey\u000d\u000a      and modelling the spread of influenza\". MRC G0701256 Oct 2008 - Sept 2011\u000d\u000a      &#163;669,233.\u000d\u000a    9. Keeling (Warwick, PI) \"State of the art models for infectious\u000d\u000a      disease spread\". Wellcome Trust Jan 2010 - Sept 2013.\u000d\u000a    10. Keeling (Warwick, PI) \"Implications of clustering\u000d\u000a      (motif-structure) for network-based processes\". EPSRC EP\/H016139\/1 Jan\u000d\u000a      2010 - Jun 2013 &#163;290,372.\u000d\u000a    11. Leverhulme Trust \"Mobile phone data and infectious diseases\". Dec\u000d\u000a      2010 - Nov 2013. Danon (Warwick, Early Career Fellowship)\u000d\u000a    12. House (Warwick, Career Acceleration Fellowship) \"Disease\u000d\u000a      Transmission and Control in Complex, Structured Populations\" EPSRC\u000d\u000a      EP\/J002437\/1 Oct 2011 - Sep 2016 &#163;632,534\u000d\u000a    13. House (Warwick, PI) \"Robust Mathematical Modelling of\u000d\u000a      Household-Stratified Epidemic Time- Series\". EPSRC EP\/K026550\/1 Oct 2013 -\u000d\u000a      Sep 2016 &#163;216,448\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"11","Level2":"3","Subject":"Clinical Sciences"}],"Sources":"\u000d\u000a    14. Letter by Head of Bioterrorism and Emerging Disease Analysis, Public\u000d\u000a      Health England\u000d\u000a    15. Letter by Senior Principal Analyst, Department of Health, Health\u000d\u000a      Protection Analytical Team\u000d\u000a    Membership in DoH Advisory Committees\u000d\u000a    16. Member of Joint Committee on Vaccination and Immunisation:\u000d\u000a      https:\/\/www.gov.uk\/government\/policy-advisory-groups\/joint-committee-on-vaccination-and-immunisation\u000d\u000a    17. Member of Scientific Pandemic Influenza Subgroup on Modelling:\u000d\u000a      https:\/\/www.gov.uk\/government\/policy-advisory-groups\/scientific-pandemic-influenza-subgroup-on-modelling\u000d\u000a    Informing UK Government\u000d\u000a    18. Foresight Review \"Detection and Identification of Infectious\u000d\u000a      Diseases\" by Keeling commissioned by the Department for Business,\u000d\u000a      Innovations &amp; Skills of the UK Government, section S9:\u000d\u000a      \u000d\u000ahttp:\/\/www.bis.gov.uk\/foresight\/our-work\/projects\/published-projects\/infectious-diseases\/reports-and-publications\u000d\u000a    Underpinning UK policy\u000d\u000a    19. House and Keeling cited as evidence in UK Influenza Pandemic\u000d\u000a      Preparedness Strategy (p 39): https:\/\/www.gov.uk\/government\/uploads\/system\/uploads\/attachment_data\/file\/134747\/dh_131040.\u000a        pdf.pdf\u000d\u000a    Outreach to Public, Practitioners and Pupils\u000d\u000a    20. a\u000d\u000a      http:\/\/www.guardian.co.uk\/education\/2011\/oct\/17\/kate-winslet-contagion-fight-disease\u000d\u000a      b.\u000d\u000a      http:\/\/articles.timesofindia.indiatimes.com\/2009-06-18\/health\/28191041_1_vaccinate-kids-new-vaccine-potential-pandemic\u000d\u000a      c. \u000d\u000ahttp:\/\/www.dailymail.co.uk\/health\/article-1193839\/All-children-offered-swine-flu-vaccine-autumn-contain-spread-pandemic-say-researchers.html\u000d\u000a      d. http:\/\/uk.reuters.com\/video\/2013\/01\/22\/data-model-could-improve-future-\u000a        pandemic?videoId=240644097&amp;videoChannel=4000\u000d\u000a      e.\u000d\u000a      http:\/\/www.wamc.org\/post\/dr-thomas-house-university-warwick-mathematics-and-mapping-epidemics\u000d\u000a    21. http:\/\/publicpolicyexchange.co.uk\/events\/DD23-PPE.php\u000d\u000a    22. http:\/\/www.wellards.co.uk\/courses\/vaccines_in_the_nhs\/screen28_case.html\u000d\u000a    23. Letter received from Managing Director of The Training Partnership \u000d\u000a    ","Title":"\u000d\u000a    Mathematical modelling informing policy on human infectious diseases,\u000d\u000a      particularly pandemic influenza\u000d\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Statistical analysis of data and mathematical modelling of controls are\u000d\u000a      key tools in the modern public-health arsenal. Many decisions on health\u000d\u000a      policies must be shown to be cost-effective through mathematical and\u000d\u000a      economic modelling. Work within the Mathematics Institute at the\u000d\u000a      University of Warwick (performed by Keeling, House and Danon) has led to\u000d\u000a      both fundamental and applied insights in public health epidemiology. Here,\u000d\u000a      we highlight research on pandemic influenza, which addressed many pressing\u000d\u000a      issues during the 2009 pandemic and continues to influence policy planning\u000d\u000a      for future outbreaks.\u000d\u000a    One important component of the group's work has been methodological, and\u000d\u000a      the developments of mathematical techniques underlying modelling are used\u000d\u000a      extensively. Keeling's textbook [1] has become a standard text on\u000d\u000a      modelling infectious diseases (cited over 700 times), and as such is\u000d\u000a      developing the next generation of public-health researchers and career\u000d\u000a      academics.\u000d\u000a    During the 2009 influenza pandemic, there was considerable international\u000d\u000a      collaboration to understand the early infection dynamics. Danon worked\u000d\u000a      closely with researchers in the USA who were interested in the\u000d\u000a      public-health impacts in Mexico and the United States and published\u000d\u000a      several papers during that period. In particular, the number of influenza\u000d\u000a      incidents among foreign travellers was used to estimate the true burden of\u000d\u000a      disease in Mexico [2]. This work was one of the first to indicate that\u000d\u000a      there was a huge undiagnosed level of infection and that most infections\u000d\u000a      were mild.\u000d\u000a    A key issue in infectious disease policy is optimal use of a limited\u000d\u000a      vaccine supply. Work by Keeling, arising directly from a collaboration\u000d\u000a      with the health protection agency (HPA) during the pandemic, addressed the\u000d\u000a      trade-offs in vaccination between targeting of an epidemiologically\u000d\u000a      important group (such as primary school children) and a group in which the\u000d\u000a      outcomes of infection are severe [3]. Mathematical modelling verified the\u000d\u000a      decisions taken during the pandemic to first target vaccination at those\u000d\u000a      with severe health-risks and then at young children.\u000d\u000a    The hospital-based healthcare system was under severe stress in\u000d\u000a      particular localities during 2009, and it was suggested that localized\u000d\u000a      school closures might reduce transmission and, hence, relieve the local\u000d\u000a      burden. Our modelling and spatial statistics showed in 2011 that only\u000d\u000a      widespread extensive closure of schools (which is economically extremely\u000d\u000a      costly) would have had a substantial impact on cases of H1N1 [4].\u000d\u000a    As shown in [3], early case ascertainment (determining how many people\u000d\u000a      are infected when not all are severely ill) is key to understanding the\u000d\u000a      progression and control of an infection. In 2012, the group developed\u000d\u000a      techniques to quantify case ascertainment using the theory of\u000d\u000a      non-Markovian epidemic final outcomes and Bayesian MCMC (Markov Chain\u000d\u000a      Monte Carlo) together with household information on early cases in\u000d\u000a      Birmingham [5]. This work suggested that over 50% of cases were undetected\u000d\u000a      during the early stages of the 2009 pandemic, and provided a novel\u000d\u000a      framework for analysis of household data that our colleagues in the PHE\u000d\u000a      are considering for future outbreaks.\u000d\u000a    Clinical trials indicated that antivirals were effective against H1N1,\u000d\u000a      but their impact at the population level was not realized during the 2009\u000d\u000a      pandemic. In 2013, the group used Bayesian methods in conjunction with the\u000d\u000a      theory of path integrals for continuous time Markov chains to model\u000d\u000a      antiviral effects in a household-structured epidemic model [6]. This\u000d\u000a      showed that delays in delivery might account for the discrepancy between\u000d\u000a      clinical trials and population-level efficacy.\u000d\u000a    Key researchers at University of Warwick:\u000d\u000a    Prof. Matt Keeling (Lecturer 2002-05, Reader 2005-08, Professor, 2008-),\u000d\u000a      Dr. Leon Danon (Post- doctoral researcher &amp; Leverhulme fellow,\u000d\u000a      2006-2013), Dr Thomas House (Post-doctoral researcher &amp; EPSRC CAF\u000d\u000a      fellow, 2006-).\u000d\u000a    Key collaborators:\u000d\u000a    Dr Andrew Black &amp; Dr Josh Ross (University of Adelaide), Prof. Pej\u000d\u000a      Rohani (University of Michigan), Prof. Marc Lipsitch (Harvard University).\u000d\u000a    "},{"CaseStudyId":"2458","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"2802361","Name":"Belgium"},{"GeoNamesId":"6251999","Name":"Canada"}],"Funders":[],"ImpactDetails":"\u000a    Mathematics has always informed and influenced the creative arts.\u000a      Schleimer and Series' research has had impact on society, culture and\u000a      creativity, with additional educational and commercial aspects. Their\u000a      research has been used to create elegant objects and images which spread\u000a      general awareness of mathematics, and has been reproduced and modified by\u000a      the interested public.\u000a    Series' work has been disseminated through the book Indra's Pearls [3]\u000a      [text removed for publication] It has been translated into Russian (2011)\u000a      and Japanese (2013). There have been many favourable reviews and much\u000a      positive feedback, e.g. the Indra's Pearls entry on Amazon. http:\/\/www.amazon.co.uk\/product-\u000a        reviews\/0521352533\/ref=dp_top_cm_cr_acr_txt?ie=UTF8&amp;showViewpoints=1\u000a    The images made using [3] can be broadly categorized as fractal art. The\u000a      algorithms have not been copyrighted and Google searches for terms like\u000a      `fractal art Kleinian' lead to many original designs produced by\u000a      professional graphics artists and enthusiastic amateurs. Due to the\u000a      limitation on references we can only highlight a few examples to\u000a      demonstrate the reach of the impact of [3]. All were unsolicited and\u000a      acknowledge that they are based on [3]; all examples cited occurred in the\u000a      assessment period 2008-13.\u000a    a) A well-known Belgian mathematical graphics artist used [3] to create\u000a      prints, posters and book covers \"after becoming fascinated by the graphics\u000a      from the book\", (see his extensive website and online galleries [7]).\u000a      Highlights include a set of images for the offices of the London\u000a      Mathematical Society (2011) and the exhibition poster and other exhibits\u000a      in the Exposition Math&#233;matiques et Art, Paris, 2012; also an online\u000a      article explaining how to make the pictures 3-dimensional.\u000a    b) An American software developer incorporated algorithms from [3] as\u000a      \"Kleinian Group Orbit Traps'' into his Fractal Science Kit (v1 released\u000a      June 2008), thus providing an interactive programming environment for\u000a      creating fractals. In addition to selling this software, he also sells\u000a      fractal images via on-line galleries such as RedBubble, and displays his\u000a      work on Flickr and Facebook [8].\u000a    c) A British self-employed IT professional, incorporated algorithms from\u000a      [3] into his freeware fractal program Spirofractal creating publically\u000a      available artwork. He says that Indra's Pearls had an \"enormous impact\" on\u000a      him and that \"it has significantly improved my skills as a software\u000a      developer\"[9]. Other computer artists include an American programmer who\u000a      sells posters, cards and stationery inspired from [3]; a Canadian software\u000a      designer who posts video clips (2009 -2011); and an American artist\u000a      incorporated algorithms into his program Swirlique on his Mystic Fractal\u000a      website along with other examples found on the DeviantART gallery [9].\u000a    [3] has also been valued for its educational component. Series has given\u000a      many public lectures: in the assessment period, the LMS\/IMA Gresham\u000a      Lecture (2010) Indra's Pearls: Geometry and Symmetry, (available\u000a      at http:\/\/www.lms.ac.uk\/events\/lectures\/gresham-lectures)\u000a      and a similar talk at the Oxford Mathematical Institute Garden Party\u000a      (2011). [3] has significantly and directly impacted the extensive\u000a      popularization work of a German mathematician and computer visualisation\u000a      specialist (TU Munich). He created 25 interactive applets based on [3] for\u000a      a summer camp for mathematically gifted high school students. Now\u000a      available on line in three languages and frequently used, these also led\u000a      to enhancements of his geometry software Cinderella (2009) for schools and\u000a      the general public. The applets were the initial step in the creation of\u000a      his internet platform `mathe-vital.de' for mathematical experiments for\u000a      schools, peaking at 700 visits per day and winning the 2008 MedidaPrix.\u000a      The work also led to an installation at the Deutsche Museum, Munich\u000a      2011(1.4M visitors\/year), other art exhibitions and a prizewinning iOS app\u000a      iOrnament [10].\u000a    The reach of Indra's Pearls extends to other media. It drew attention to\u000a      the analogy between the iterative processes involved and eastern\u000a      philosophical traditions, and images have been used as jacket designs for\u000a      two books on eastern philosophy [11] Other designs based on [3] include a\u000a      widely exhibited silk quilt named Indra's Pearls by a quilt\u000a      designer (2009) [12], and an engagement ring for a lady from Arizona [12]\u000a      \"... a quick note to thank you all for your work on Indra's Pearls, and to\u000a      share the engagement ring that my partner had made for me based on the\u000a      math he learned in your book...''.\u000a    Schleimer-Segerman's 3D prints are for sale in attractive format from\u000a      Shapeways. [text removed for publication]\u000a    Public interest has been raised through the STL files for Triple gear\u000a      ([text removed for publication]) and a related sculpture, Triple\u000a        helix, available via Thingiverse [14]. With the STL file, a 3D\u000a      modelling program, and a 3D printer, the design can be modified and\u000a      replicated at will. Videos discussing each piece are posted on YouTube.\u000a      The video on One-half 120-cell has been watched over 17,000 times\u000a      since 9\/2011 and that on Triple gear more than 113,000 times since\u000a      12\/2012. Every time such a print is posted it generates online discussion\u000a      on blogs and aggregation websites, e.g. discussion on Triple\u000a        gear at Metafilter.\u000a    Segerman and Schleimer presented One-half 120 cell [4] at the\u000a      Bridges Art and Mathematics conference 2012 where it won \"Best Use of\u000a      Mathematics\", one of four awards out of 110 submissions [15]. Triple\u000a        gear, presented at Bridges 2013, was voted \"Most innovative\" of 136\u000a      submissions. There have been media articles in Scientific American\u000a      (reprinted in Nature), Smithsonian and Gizmodo [16].\u000a    The sculptures also featured in the 2013 Edinburgh International Science\u000a      Festival, while in 2012, five large versions of the sculptures were\u000a      installed at the Department of Mathematics at the University of Melbourne\u000a      [17].\u000a    The breadth of the examples above demonstrates the public appetite for,\u000a      and impact of, this kind of mathematical art. More than ten years after it\u000a      was first published the work by Series continues to inspire, while the\u000a      much more recent productions by Schleimer are rapidly attracting wide\u000a      attention.\u000a    ","ImpactSummary":"\u000a    This impact is on society, culture and creativity. Series and Schleimer\u000a      from Warwick's Geometry and Topology group have produced attractive\u000a      visualisations and physical realisations of mathematical objects arising\u000a      in their research. These have elicited a wide response from members of the\u000a      public, with designs being creatively used for commercial, aesthetic and\u000a      educational purposes.\u000a    Series popularised a novel form of fractal art, based on the geometry of\u000a      iterated M&#246;bius maps, in her book Indra's Pearls. This has inspired many\u000a      artists working in a variety of media ranging from posters to quilts.\u000a      Schleimer designs elegant yet mathematically accurate realisations of\u000a      three and four dimensional figures, such as knots and related surfaces,\u000a      suitable for 3D printing. These are being commercially manufactured, sold\u000a      and displayed publically.\u000a    ","ImpactType":"Cultural","Institution":"\u000a    University of Warwick\u000a    ","Institutions":[{"AlternativeName":"Warwick (University of)","InstitutionName":"University of Warwick","PeerGroup":"A","Region":"West Midlands","UKPRN":10007163}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2158177","Name":"Melbourne"},{"GeoNamesId":"2867714","Name":"München"},{"GeoNamesId":"2988507","Name":"Paris"},{"GeoNamesId":"2867714","Name":"Munich"}],"References":"\u000a    \u000a1. L. Keen, and C.Series. Pleating Coordinates for the Maskit\u000a      embedding of the Teichm&#369;ller space of punctured tori, Topology 32(4)\u000a      719-749. (1993) DOI: 10.1016\/0040-9383(93)90048-Z.\u000a    \u000a\u000a2. L. Keen, B. Maskit and C.Series. Geometric finiteness and\u000a      uniqueness for Kleinian groups with circle packing limit sets. J. reine\u000a      und angew. Mathematik 1993(436), 209 - 219. (1993) DOI: 10.1515\/crll.1993.436.209.\u000a    \u000a\u000a3. D. Mumford, C.Series and D. Wright, Indra's Pearls. Cambridge\u000a      Univ. Press. (2002) ISBN: 978-0-521-35253-6.\u000a    \u000a\u000a4. S. Schleimer and H. Segerman, Sculptures in S3.\u000a      Proceedings of Bridges 2012: Mathematics, Music, Art, Architecture,\u000a      Culture, (2012) 103 - 110. Available from: http:\/\/bridgesmathart.org\/2012\/cdrom\/proceedings\/53\/index.html\u000a    \u000a\u000a5. S. Schleimer and H. Segerman, Triple gear. Proceedings of\u000a      Bridges 2013: Mathematics, Music, Art, Architecture, Culture, (2013),\u000a      353-360. Available from: http:\/\/archive.bridgesmathart.org\/2013\/bridges2013-353.html\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000a    \u000a      [text removed for publication]\u000a      Letter from internationally known computer graphics artist whose\u000a        website, exhibits and exhibition posters show examples of his work\u000a        inspired by Indra's Pearls. www.josleys.com\/show_gallery.php?galid=318;\u000a          and his exhibits and exhibition posters:\u000a          www.josleys.com\/references.php\u000a\u000a      Letter from American software developer who confirms [3] \"extremely\u000a        useful in his software development and artistic endeavours\" which he\u000a        makes available on: www.fractalsciencekit.com\/index.htm;\u000a          http:\/\/fineartamerica.com\/art\/all\/kleinian+group+fractal\/all; www.redbubble.com\/shop\/steiner+chain+kleinian+circle+inversion;\u000a          www.fractalsciencekit.com\/gallery\/kleingp.htm; www.fractalsciencekit.com\/fractals\/large\/Fractal-Circle-Orbit-Trap-10.jpg\u000a\u000a      Website of IT professional and software developer: www.alunw.freeuk.com\/oldmonths.html\u000a        and http:\/\/alunw.deviantart.com\/gallery\/\u000a        (has also provided letter) American programmer (whose letter\u000a        corroborates use of [3]) sells through websites: www.bugman123.com\/Fractals\/index.html\u000a        and www.zazzle.com\/kleinian_double_1_15_cusp_group_poster-228240989450813261\u000a        Canadian software designer who has used [3] and posts video clips on www.brainjam.ca\/tessellations.htm\u000a        and www.vimeo.com\/album\/82895\u000a        American who incorporated algorithms into his program Swirlique on his\u000a        Mystic Fractal website www.mysticfractal.com\/swirlique_gallery\/index.htm\u000a        with further examples on DeviantArt Gallery: www.giovannigabrieli.deviantart.com\/art\/Kleinian-\u000a          Towers-193157543; www.riverfox1.deviantart.com\/art\/Mobius-Dragon-Iteration-349270804\u000a        http:\/\/theli-at.deviantart.com\/art\/Kleinian-drops-192676501\u000a\u000a      Letter from Mathematics Professor, Technical University Munich. See\u000a        also www-m10.ma.tum.de\/bin\/view\/MatheVital\/IndrasPearls\/WebHomeEn\u000a\u000a      Book jackets: \"Synchronicity: nature and Psyche in an Interconnected\u000a        Universe\" by J Cambray (ISBN 978-1-60344-143-8), &#169;2009; \"The Brahma Net\u000a        Sutra - Bodhisattva Precepts Handbook\" (ISBN 978-1-60236-010-5), &#169;2009.\u000a      Letter from former president of Quilters Guild of the British Isles.\u000a        See also www.vam.ac.uk\/microsites\/quilts\/detail\/800\/Indras_Pearls\u000a        Email from Arizona received December 2012 with pictures of the design of\u000a        an engagement ring and wedding stationery.\u000a      Website with Schleimer-Segerman's 3D prints: www.shapeways.com\/shops\/henryseg\u000a        [text removed for publication]\u000a      Websites: www.youtube.com\/user\/henryseg\u000a        (a) Video on One-half 120-cell: http:\/\/www.youtube.com\/watch?v=MyUfAs30yZk\u000a        (b) Video on Triple gear: http:\/\/www.youtube.com\/watch?v=I9IBQVHFeQs\u000a        Thingiverse: www.thingiverse.com\/henryseg\u000a        Triple gear http:\/\/www.thingiverse.com\/thing:66708\u000a        Triple helix http:\/\/www.thingiverse.com\/thing:89115\u000a        http:\/\/www.metafilter.com\/127408\/Triple-Gear\u000a\u000a      The Bridges Organisation: Art and Mathematics: http:\/\/gallery.bridgesmathart.org\/exhibitions\/2012-bridges-conference\/henrys\u000a        http:\/\/gallery.bridgesmathart.org\/exhibitions\/2013-bridges-conference\/schleimer-segerman\u000a\u000a      Short articles: http:\/\/blogs.scientificamerican.com\/observations\/2012\/10\/31\/mathematicians-at-play-3-d-puzzl\/ ; http:\/\/blogs.smithsonianmag.com\/artscience\/2013\/03\/fresh-off-the-3d-printer-henry-segermans-mathematical-sculptures\/ ; http:\/\/gizmodo.com\/5969758\/\u000a\u000a      Sculptures: Edinburgh International Science Festival-ASCUS\u000a        \u000awww.facebook.com\/media\/set\/?set=a.182872068531703.1073741827.148984691920441&amp;type=1\u000a        University of Melbourne http:\/\/www.youtube.com\/watch?v=qJkvlnaDLic\u000a\u000a    \u000a    ","Title":"\u000a    Mathematics in Digital Art\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"},{"GeoNamesId":"2650225","Name":"Edinburgh"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"},{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Caroline Series (Professor at Warwick since 1992) and Saul Schleimer\u000a      (Assistant Professor at Warwick 2007-11 and Associate Professor from 2011)\u000a      work in low dimensional topology and geometry. Their work uses modern\u000a      computing power to explore and exploit concrete visualisations of the\u000a      objects they study.\u000a    Series is a leading expert on hyperbolic geometry. She developed deep\u000a      mathematics behind some stunning computer graphics originally created by\u000a      David Mumford (Brown) and David Wright (Oklahoma) by iterating pairs of\u000a      M&#246;bius maps acting as conformal automorphisms of the Riemann sphere. By\u000a      analogy to a Julia set in complex dynamics, the limit set of the iteration\u000a      is the set of accumulation points of the orbit of any point under the\u000a      group generated by the pair of maps. Typically, limit sets are intricately\u000a      shaped fractals. Mumford and Wright discovered that certain limit sets\u000a      contain spiralling chains of circles in fascinating configurations.\u000a      Working with Linda Keen (City University New York), in 1992 Series used\u000a      3-dimensional hyperbolic geometry and invented \"pleating rays'' to\u000a      establish the mathematical framework behind these experimental discoveries\u000a      [1]. This justified the Mumford-Wright results mathematically and\u000a      suggested further pictures. Further work put everything into the context\u000a      of contemporaneous groundbreaking research in hyperbolic geometry. [2]\u000a      proves there is a unique pattern of spiralling circles for each rational\u000a      number.\u000a    Starting in 1995, Series joined Mumford and Wright to write a widely\u000a      accessible but mathematically detailed book (Indra's Pearls) about the\u000a      mathematics and computer algorithms behind the graphics which was\u000a      published in 2002 [3]. From 1992-2002 her research guided and informed the\u000a      exposition and also led to further graphics which are still being\u000a      explored.\u000a    Schleimer began collaborating with Henry Segerman (University of\u000a      Melbourne) on 3D prints in 2010. Both are experts on 3D topology. In\u000a      papers intended for a wide audience of mathematicians, engineers, and\u000a      artists [4, 5] they explain the mathematical tools necessary to build a\u000a      variety of geometric, topological, and combinatorial sculptures via 3D\u000a      printing.\u000a    In [4] they reviewed the basic theory of stereographic projection and its\u000a      relation to the geometry of the three-sphere, as well as relevant facts\u000a      from the theory of the quaternions. They then constructed conformally\u000a      correct 3D prints of the regular four-dimensional polytopes, including the\u000a      famous 120-cell. For this they had to deal with various engineering\u000a      constraints which arise, such as minimal feature size and maximal printed\u000a      diameter. They also gave conformally correct prints of minimal surfaces\u000a      contained in the three-sphere which were first found mathematically by\u000a      Lawson (1970).\u000a    Schleimer and Segerman also investigated linkages [5]. Inspired by\u000a      sculptures of Helaman Furguson and Oskar van Deventer, they invented a\u000a      3D-object they call \"Triple gear\". This is the first example of a\u000a      mechanism having three interlocking gears in which any one gear is able to\u000a      turn if and only if the others do, without the aid of a gearbox as found\u000a      in standard assemblies. The interlocking nature of the pieces, together\u000a      with their demanding tolerances, makes the triple gear all but impossible\u000a      to manufacture by traditional means. Triple gear answers a\u000a      question often presented in graphic designs: can an odd number of gears,\u000a      each in contact with the previous and the next, move? If the gears all lie\u000a      in a plane, the answer is \"no\": any such collection is frozen and cannot\u000a      move. Triple gear avoids this contradiction because it is not\u000a      planar.\u000a    "},{"CaseStudyId":"3387","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The mathematical models developed by the OR group at Cardiff University\u000d\u000a      have been applied to a wide range of healthcare services, in a variety of\u000d\u000a      hospitals, across the UK. The impacts directly arising from the research\u000d\u000a      are highly significant and include:\u000d\u000a    Health Benefits\u000d\u000a    The research has been used in a major new trauma centre that opened in\u000d\u000a      2010, at St George's Hospital in London (a capital investment of &#163;3.6m).\u000d\u000a      The models were used to create service level agreements (annual capacity\u000d\u000a      planning) with commissioners over several years and, importantly, informed\u000d\u000a      the required patient-flows and resourcing levels for the centre. This\u000d\u000a      enabled patient waiting time to be reduced and better levels of patient\u000d\u000a      care achieved through improved resource management and greater efficiency.\u000d\u000a      The result has been an observed increase in survival rates of 54%. Jen\u000d\u000a      Goddard, Business Analyst at St. George's Hospital, stated, \"The research\u000d\u000a      and tools that Cardiff University provided have been extremely useful to\u000d\u000a      St George's, enabling us to understand and model our data and use it to\u000d\u000a      significantly improve patient care and resource management. Mortality\u000d\u000a      rates have fallen as a consequence of the implementation of the Major\u000d\u000a      Trauma Centre and the Hyper Acute Stroke Unit.\" (5.1)\u000d\u000a    The models were similarly utilised by a new hyper-acute stroke unit\u000d\u000a      covering the South London area. The new unit and stoke care service were\u000d\u000a      rated as the best in the country by the National Sentinel Audit 2010\u000d\u000a      organised by the Royal College of Physicians. The outcomes were again\u000d\u000a      lifesaving; significantly lowering risk-adjusted mortality for stroke\u000d\u000a      patients to 60%. This is verified by data comparing mortality rates pre\u000d\u000a      and post the implementation of the research (provided by Jen Goddard, St.\u000d\u000a      George's Hospital). (5.1)\u000d\u000a    Patient care and hospital resources have also been significantly\u000d\u000a      improved, through the work of the OR team, at a major neurological\u000d\u000a      rehabilitation hospital in South Wales (Rookwood Hospital, Cardiff).\u000d\u000a      Previously the complex scheduling of inpatient physiotherapy sessions was\u000d\u000a      undertaken manually which was time consuming and amounted to a clinician\u000d\u000a      spending one day per week assembling the following week's timetable. The\u000d\u000a      research enabled the scheduling process to be automated; the computerised\u000d\u000a      new system has been implemented since January 2011. It produces a vastly\u000d\u000a      enhanced schedule within minutes, enabling clinicians to devote more time\u000d\u000a      to assessing patients' needs and delivering medical care. Jakko Brouwers,\u000d\u000a      Senior Service Improvement Programme Manager at the Hospital commented,\u000d\u000a      \"Your research work with our Department of Specialist Physiotherapy and\u000d\u000a      Rehabilitation at Rookwood Hospital has had a huge impact on how we\u000d\u000a      utilise our resources. ... The automated computer scheduling creates a\u000d\u000a      fairer system for patients as it takes into account what treatment the\u000d\u000a      patient received the previous week.\" (5.2)\u000d\u000a    The work at Rookwood Hospital also included a detailed queueing analysis\u000d\u000a      of their patient admission and discharge process, with particular emphasis\u000d\u000a      on tailoring physiotherapy treatment to the needs of the patient. This\u000d\u000a      optimised the length of stay in the unit and provided improved quality of\u000d\u000a      life for patients. Jakko Brouwers added, \"The service modelling work has\u000d\u000a      been a real asset in that it has opened the eyes of the operational\u000d\u000a      service manager to the issues regarding patient flow. These insights are\u000d\u000a      now used on a regular basis in waiting list management and admissions\u000d\u000a      meetings.\" (5.2)\u000d\u000a    The research has been particularly significant to the Welsh Ambulance\u000d\u000a      Service Trust (WAST). Like all Emergency Medical Services WAST is under\u000d\u000a      increasing pressure to provide wide geographical coverage and improve\u000d\u000a      performance to meet Government set response times. The work at Cardiff\u000d\u000a      University has helped WAST by forecasting 999 calls by category of call\u000d\u000a      and providing valuable evidence to confirm the importance of ambulance\u000d\u000a      allocations based on patient outcomes for different conditions (using\u000d\u000a      survival probabilities) as opposed to only those based on hard time\u000d\u000a      targets (e.g. 8 minute response). This means that extra lives can be\u000d\u000a      saved. Researchers at Cardiff University have quantified the relationship\u000d\u000a      between overall service performance and changes to durations in each\u000d\u000a      phase. Critically, the findings show that reducing the time of the patient\u000d\u000a      handover from ambulance to the emergency department at hospitals makes the\u000d\u000a      biggest impact on overall WAST performance. Richard Whitfield, the\u000d\u000a      Research and Development Manager at WAST stated that \"the work is an\u000d\u000a      extremely relevant contribution to implementing policy and procedural\u000d\u000a      changes at WAST.\" (5.3)\u000d\u000a    Financial Benefits\u000d\u000a    The research entailed modelling the Emergency Unit (EU) at the University\u000d\u000a      Hospital of Wales (UHW). This enabled the hospital to examine the\u000d\u000a      reconfiguration of services and identify problems in the system.\u000d\u000a      Subsequently major changes were implemented, based on the results of the\u000d\u000a      research, leading to a huge alteration in patient flows in the EU. This\u000d\u000a      meant that the hospital was able to reduce staffing levels and the number\u000d\u000a      of trolleys by 43%, amounting to a net efficiency gain of approximately\u000d\u000a      &#163;1.6m. Andrew Nelson, Director of Acute Services, Cardiff and Vale\u000d\u000a      University Health Board, commented, \"The modelling work provided by the\u000d\u000a      Maths school has been the basis of our plans for the current major\u000d\u000a      re-design and refurbishment of the Accident and Emergency Department at\u000d\u000a      the University Hospital of Wales. This is the second major change\u000d\u000a      resulting from the work undertaken by the maths school to improve our\u000d\u000a      knowledge of demand and capacity within the department and how we can best\u000d\u000a      equip the department to improve performance and overall cost\u000d\u000a      effectiveness\". (5.4)\u000d\u000a    The models were used to evaluate the impact of the South Wales\u000d\u000a      Development Programme, leading to a &#163;4.2 million investment in system wide\u000d\u000a      capacity. This has allowed Cardiff and Vale University Health Board to\u000d\u000a      better align resources with the needs of patients. Moreover, it has formed\u000d\u000a      the basis of a successful joint bid with Social Services for a &#163;2.5\u000d\u000a      million invest-to-save scheme with the Welsh Government to improve\u000d\u000a      non-emergency transport services for hospital patients. (5.4)\u000d\u000a    Public Engagement\u000d\u000a    The research has been widely disseminated in the UK and overseas and has\u000d\u000a      led to a significant level of public engagement. Examples of this include\u000d\u000a      a series of four radio programmes that were aired on BBC Radio Wales\u000d\u000a      (2010) as part of its weekly programme called `Science Cafe'. Seminars\u000d\u000a      have been given in America at Princeton University, New Jersey; Columbia\u000d\u000a      University, New York; Baylor University, Texas. The audiences comprised of\u000d\u000a      a mixture of nonspecialists, teachers and students. Professor Griffiths\u000d\u000a      has also spoken at the Cardiff Scientific Society, which seeks to extend\u000d\u000a      interest in the application of Science to everyday life. His latest talk\u000d\u000a      in October 2012 was entitled \"Some Queues I've enjoyed\". Professor Harper\u000d\u000a      has also given a wide range of public talks including speaking at the\u000d\u000a      Cardiff University's Innovation Network event in April 2013. This was\u000d\u000a      attended by 70 delegates, including health care professionals, who scored\u000d\u000a      the event 4.2 out of a possible 5. (5.5)\u000d\u000a    Outreach activities, where the healthcare modelling research has heavily\u000d\u000a      featured, have involved over 500 school children. Regular sessions are\u000d\u000a      given, for example, at the Monmouth Science Initiative\u000d\u000a      (www.monmouth-science.co.uk). (5.6) Online resources, concerning\u000d\u000a      healthcare modelling topics, have also been created for teachers and\u000d\u000a      students, and are also part of national initiatives such as that by the OR\u000d\u000a      Society (www.learnaboutor.co.uk). (5.7)\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research conducted at the School of Mathematics at Cardiff University has\u000d\u000a      engineered lifesaving, improvements to UK healthcare systems. New\u000d\u000a      mathematical models, accounting for the complexity and diversity of the\u000d\u000a      health system, have been created and applied in a variety of contexts to\u000d\u000a      markedly enhance the efficiency and effectiveness of a wide range of\u000d\u000a      healthcare services &#8212; at policy, commissioning and operational levels. The\u000d\u000a      extensive benefits include:\u000d\u000a    \u000d\u000a      Reducing the mortality of trauma patients across South London by 54%\u000d\u000a        (equating to 0.7 additional survivors out of every 100 patients for the\u000d\u000a        period 2010-2012, rising to 4.2 in 2013)\u000d\u000a      Reducing the mortality of stroke patients across South London by 60%\u000d\u000a        through the creation of a new Stroke Unit, based on the research\u000d\u000a        findings (the services were rated as the best in the country by the\u000d\u000a        National Sentinel Audit 2010 organised by the Royal College of\u000d\u000a        Physicians).\u000d\u000a      Realising net efficiency gains of &#163;1.6m per year in the emergency\u000d\u000a        department at University Hospital of Wales;\u000d\u000a       Provision of hospital capacity planning tools in use across the UK\u000d\u000a    \u000d\u000a    This work has been disseminated nationally and internationally, in the\u000d\u000a      media and at a range of events designed to engage the public with\u000d\u000a      Mathematics. Therefore the impacts claimed in this case study are health,\u000d\u000a      economic benefits and public engagement.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    Cardiff University\u000d\u000a    ","Institutions":[{"AlternativeName":"Cardiff University","InstitutionName":"Cardiff University","PeerGroup":"A","Region":"Wales","UKPRN":10007814}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2636841","Name":"Stoke-on-Trent"},{"GeoNamesId":"5128638","Name":"New York"},{"GeoNamesId":"5101760","Name":"New Jersey"},{"GeoNamesId":"2643743","Name":"London"}],"References":"\u000d\u000a    \u000a3.1. Griffiths JD, Leonenko G and Williams JE (2006), \"The\u000d\u000a      Transient Solution to M\/Ek\/1 Queue\". Operations Research Letters,\u000d\u000a      34(3): 349-354.\u000d\u000a      http:\/\/dx.doi.org\/10.1016\/j.orl.2005.05.010\u000d\u000a      Copy held by HEI, available on request.\u000d\u000a    \u000a\u000a3.2. Griffiths JD, Knight VA and Komenda I (2013), \"Bed\u000d\u000a      Management in a Critical Care Unit\". IMA Journal of Management\u000d\u000a        Mathematics. 24(2): 137-153.\u000d\u000a      http:\/\/dx.doi.org\/10.1093\/imaman\/dpr028\u000d\u000a      Copy held by HEI, available on request.\u000d\u000a    \u000a\u000a3.3. Harper PR, Knight VA and Marshall AH (2012), \"Discrete\u000d\u000a      Conditional Phase-type Models Utilising Classification Trees: Application\u000d\u000a      to Modelling Health Service Capacities\". European Journal of\u000d\u000a        Operational Research. 219(3): 522-530.\u000d\u000a      http:\/\/dx.doi.org\/10.1016\/j.ejor.2011.10.035\u000d\u000a        Copy held by HEI, available on request.\u000d\u000a    \u000a\u000a3.4. Knight VA and Harper PR (2013), \"Selfish Routing in Public\u000d\u000a      Services\". European Journal of Operational Research. 230(1):\u000d\u000a      122-132. http:\/\/dx.doi.org\/10.1016\/j.ejor.2013.04.003\u000d\u000a      Copy held by HEI, available on request.\u000d\u000a    \u000a\u000a3.5. Vieira IT, Cheng RCH, Harper PR and de Senna V\u000d\u000a      (2010), \"Small World Network Models of the Dynamics of HIV Infection\". Annals\u000a        of Operational Research. 178: 173-200.\u000d\u000a      http:\/\/dx.doi.org\/10.1007\/s10479-009-0571-y\u000d\u000a        Copy held by HEI, available on request.\u000d\u000a    \u000a\u000a3.6. Knight VA, Harper PR and Smith L (2012), \"Ambulance\u000d\u000a      Allocation for Maximal Survival with Heterogeneous Outcome Measures\". OMEGA:\u000a        The International Journal of Management Science. 40(6): 918-926. http:\/\/dx.doi.org\/10.1016\/j.omega.2012.02.003\u000d\u000a      Copy held by HEI, available on request.\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    5.1 Business Analyst, Strategy, St George's Healthcare Trust,\u000d\u000a      London. Corroborates the use and impact of the models at St George's\u000d\u000a        Hospital.\u000d\u000a    5.2 Senior Service Improvement Programme Manager, Rockwood\u000d\u000a      Hospital. Corroborates the use and impact of the models at Rockwood\u000d\u000a        Hospital.\u000d\u000a    5.3 Clinical R&amp;D Manager, Welsh Ambulance Service NHS Trust. Corroborates\u000a        the value of the research to WAST.\u000d\u000a    5.4 Director of Acute Services, Cardiff and Vale University Health\u000d\u000a      Board. Corroborates the use and impact of the models at the University\u000d\u000a        Hospital of Wales.\u000d\u000a    5.5\u000d\u000a      http:\/\/www.innovation-network.org.uk\/events\/innovation-in-healthcare-wed-17-apr-2013.aspx\u000d\u000a      Provides an example of the public talks given as a result of the\u000d\u000a        research.\u000d\u000a    5.6 http:\/\/www.monmouth-science.co.uk\/programme\/physics\/\u000d\u000a      Confirms Cardiff's involvement in outreach activities, stemming from\u000d\u000a        the research.\u000d\u000a    5.7 http:\/\/www.profpaulharper.com\/home\/school-outreach\u000d\u000a      Confirms that online resources for teachers have been developed from\u000d\u000a        the research. \u000d\u000a    ","Title":"\u000d\u000a    Mathematics and Healthcare: Saving Lives and Reducing Costs\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2653822","Name":"Cardiff"}],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Modelling health systems requires the development of suitable\u000d\u000a      mathematical models that are able to account for dynamic processes,\u000d\u000a      stochastic conditions, and high levels of complexity. This has been\u000d\u000a      achieved by the Operational Research (OR) group at Cardiff University from\u000d\u000a      1993-2012. The range of methodologies employed may be categorised as\u000d\u000a      follows:\u000d\u000a    Queueing Systems (Griffiths, Harper, Williams and\u000d\u000a        Knight)\u000d\u000a    Patient movements through healthcare systems (care-pathways) can be\u000d\u000a      represented by flows through networks of queues constrained by available\u000d\u000a      resources. Previous contributions in queueing theory have focussed on bulk\u000d\u000a      service queues and time-dependent queueing models. Theoretical work has\u000d\u000a      made significant progress with the transient solution of queueing systems\u000d\u000a      and the time-dependent nature of queues including multi-server queues with\u000d\u000a      priority (3.1) The stochastic nature of demand on acute hospital services\u000d\u000a      and the expected consequences on resource requirements, taking into\u000d\u000a      consideration both intra week and monthly seasonality, required the\u000d\u000a      development of a complex simulation packages by members of the OR group.\u000d\u000a      These packages enable simulation studies to be performed quickly and\u000d\u000a      efficiently, without highly specialised expertise and extensive\u000d\u000a      configuration times (3.2).\u000d\u000a    Exploiting known information on patient characteristics, such as age,\u000d\u000a      gender, socio-economic status etc, can allow for predictions on individual\u000d\u000a      patient resource needs and outcomes. Novel work on developing a discrete\u000d\u000a      conditional phase-type framework demonstrated the benefits of Coxian\u000d\u000a      phase-type distributions interfaced with classification tree algorithms.\u000d\u000a      Underpinning work also embedded Coxian phase-type service time fits within\u000d\u000a      M\/PH\/c\/NPRT priority queueing and simulation\u000d\u000a      models (3.3).\u000d\u000a    Behavioural Modelling (Harper, Griffiths, Williams and\u000d\u000a      Knight)\u000d\u000a    It is well observed that individual behaviour can affect the efficiency\u000d\u000a      of queueing systems. To address such issues, using theoretical insights\u000d\u000a      from routing game theory, algorithmic approaches for obtaining the price\u000d\u000a      of anarchy were proposed. Theoretical results were obtained as to the\u000d\u000a      demand and worth of service. It was shown that in a public service system,\u000d\u000a      with an adequate capacity to provide the perceived worth of service, a\u000d\u000a      high price of anarchy is to be expected (3.4).\u000d\u000a    Recognising that the structure of social networks also play an important\u000d\u000a      role in the dynamics of healthcare processes, research explored small\u000d\u000a      world principles to capture disease propagation dynamics as well as agent\u000d\u000a      based simulation investigations into the consequential effects of social\u000d\u000a      networks and behavioural (3.5). For the first time in the literature,\u000d\u000a      formal psychological models (such as theory of planned behaviour) were\u000d\u000a      integrated with physiological parameters within the same clinical model to\u000d\u000a      provide greater understanding of the importance of human behaviour in\u000d\u000a      healthcare simulations.\u000d\u000a    Facility Location (Harper and Knight)\u000d\u000a    The research considered the development of mathematical models for\u000d\u000a      locating facilities, including those within hierarchical systems. The\u000d\u000a      novelty and contribution of this work was to incorporate both equity and\u000d\u000a      efficiency parameters into proposed classes of optimisation model, thus\u000d\u000a      permitting decision making when considering both `fair' and `efficient'\u000d\u000a      locations. Other research proposed new mathematical formulations for\u000d\u000a      locating emergency medical services that incorporate the survival of\u000d\u000a      heterogeneous patient classes. An extended iterated method of solving the\u000d\u000a      model was developed that uniquely takes in to account the reactive nature\u000d\u000a      of the stochastic conditions (3.6).\u000d\u000a    Key staff contributing to the research: Professor Jeff Griffiths\u000d\u000a      (academic staff 1964-), Professor Paul Harper (academic staff 2007-), Dr\u000d\u000a      Janet Williams (academic staff 1988-2013) and Dr Vincent Knight (research\u000d\u000a      associate 2009-2011, academic staff 2011-).\u000d\u000a    "},{"CaseStudyId":"3389","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Cardiff University's contact with HP originated in 2007. Following the\u000a      research, the algorithms were subjected to extensive proof-of-concept\u000a      testing, in the Production Division in HP, where they were shown to\u000a      significantly improve the security of printing and network environments in\u000a      their prototype and pre-production printers and services. As a result of\u000a      this internal evaluation, the task of upgrading the key security features\u000a      in over 10 million electronic devices was initiated by HP in March 2013\u000a      (5.1). The algorithms are implemented in either the firmware or software\u000a      of devices, depending on their computing power.\u000a    Unusual behaviour detection and information extraction in streams of\u000a      short documents and files (emails, news, tweets, log files, messages,\u000a      etc.) are important problems in security applications and failure to\u000a      adequately protect printing and network environments has the potential to\u000a      adversely affect millions of users. The applications &#8212; which range from\u000a      automatic document classification to information extraction and\u000a      information visualisation, from automatic unusual behaviour detection to\u000a      security policy enforcement &#8212; all rely on automatically extracted features\u000a      (in data streams) or keywords (in documents) to perform a higher level of\u000a      analysis. Of paramount importance in these applications is the quality\u000a      (accuracy) and speed (efficiency) of keyword extraction algorithms. The\u000a      algorithms developed by Cardiff have been shown by HP to satisfy these\u000a      criteria.\u000a    Print Security:\u000a    The threat to printing and imaging devices and data has increased over\u000a      the last decade as a consequence of more sophisticated threats,\u000a      increasingly mobile workforces and changes in industry regulations. There\u000a      are a variety of means that data can be compromised in this fashion. These\u000a      include hardware theft which could expose documents sent to stolen\u000a      printers and multi-function printers for later printing, or unauthorized\u000a      changes to unprotected settings that will enable someone to reroute print\u000a      jobs and potentially access network and password information. Moreover\u000a      so-called network sniffers can obtain data that is transmitted between a\u000a      PC and a printer, revealing the print job. Similarly, unsecured cloud\u000a      connectivity could give unauthorized users access to the data at any time,\u000a      in any place. Data that is compromised can result in the loss of millions\u000a      of pounds due to employee and customer identity theft, private and\u000a      corporate lawsuits, industry violations or government fines. Subsequently,\u000a      means to combat unauthorised and illegal practises, whist enabling\u000a      innocent transactions or normal usage, are essential. The new approach to\u000a      rapid change detection in documents and text summarization and\u000a      classification developed at Cardiff, and implemented in electronic devices\u000a      by HP, identifies documents that are confidential and prevents them from\u000a      being printed by unauthorised users (5.1).\u000a    Mitigating Risks to Data Security:\u000a    Cardiff University's research has significantly improved the security of\u000a      printing and network environments. The sophisticated and efficient\u000a      algorithms for data mining, which were developed for HP, recognise normal\u000a      and abnormal patterns of data. Developments such as the new approach to\u000a      rapid change detection in data streams and log files, applied to the\u000a      problem of feature extraction, provide extremely fast and effective\u000a      techniques for the identification of meaningful features by parameter-free\u000a      methods. Likewise, the approach to an extractive summarization, by\u000a      modelling data as small-world networks, can be applied to the problem of\u000a      extracting the most important structures from data. The algorithms are\u000a      essentially valid safeguards for all data transmitted via printing and\u000a      network applications. The feature extractor developed by Cardiff\u000a      University has undergone extensive internal evaluation by HP and found to\u000a      be vastly superior to existing techniques (5.1, 5.2). In the evaluation HP\u000a      found that the feature extractor developed by Cardiff considerably\u000a      outperformed other feature extractors in current use. In quantitative\u000a      terms, the confidentiality accuracy was increased from 60% to 83%, thereby\u000a      reducing the error rate by more than 50% (5.1). The ability of the\u000a      extractor to detect unusual behaviour means that dangerous or unauthorised\u000a      behaviour can be prevented and therefore it provides enormous security\u000a      benefits for HP's extensive client base &#8212; this includes customers in\u000a      nearly every country in the world. HP services corporations such as\u000a      Barclays Bank, Fords and a plethora of multinational organisations in the\u000a      healthcare, life sciences and pharmaceutical industries. Data privacy is\u000a      paramount to these businesses; the research has enabled information\u000a      sharing in a markedly more secure IT environment.\u000a    Economic Gain:\u000a    HP has made it clear that it cannot divulge quantitative information\u000a      concerning economic gain from this research for reasons of commercial\u000a      sensitivity (in the context of sales) and state security (in the context\u000a      of consultancy).\u000a    Sales\u000a    The research has enabled HP to retain its position as the market leader\u000a      in the information technology industry, a fact officially recognised since\u000a      2007. In 2012 HP had the biggest share of the global market, 16%. The\u000a      company states, as part of its corporate aims, \"We lead in the marketplace\u000a      by developing and delivering useful and innovative products, services and\u000a      solutions.\" Instructively, the algorithms and resulting features\u000a      implemented in HP products are novel developments that outperform existing\u000a      attempts by competitors to address data security risks. They enable a\u000a      dynamic, as opposed to a static, response to protecting data. Dr. Steven\u000a      Simske, Director and Chief Technologist for Security Printing and Imaging\u000a      Engineering, commented that \"the algorithms developed by Cardiff\u000a      University are novel to data mining and are extremely valuable to our\u000a      organisation. They enable us to successfully compete within the industry\u000a      and drive technology forward to meet the evolving needs of our clients.\u000a      Without the research produced by Alex Balinsky our achievements in this\u000a      area would not have been possible.\" (5.1).\u000a    Consultancy\u000a    The algorithms have also been used to progress HP's security policy in\u000a      their security consultancy practice. This guarantees impact for their high\u000a      end clients, including security services and law enforcement agencies\u000a      across the globe. Dr. Simske continues to state that \"the research has\u000a      been integral to the development of our Big Data, Analytics and Security\u000a      themes. It is both unprecedented and highly creative work that is fuelling\u000a      the development of HP's entire security framework.\" (5.1)\u000a    ","ImpactSummary":"\u000a    The security of data in printing and network environments is an area of\u000a      increasing concern to individuals, businesses, government organisations\u000a      and security agencies throughout the world. Mathematical algorithms\u000a      developed at the School of Mathematics at Cardiff University represent a\u000a      significant step-change in existing data security techniques. The\u000a      algorithms enable greater security in automatic document classification\u000a      and summarisation, information retrieval and image understanding.\u000a      Hewlett-Packard (HP), the world's leading PC vendor, funded the research\u000a      underpinning this development and patented the resulting software, with\u000a      the aim of strengthening its position as the market leader in this sector\u000a      of the global information technology industry. Hewlett Packard has\u000a      incorporated the algorithms in a schedule of upgrades to improve the key\u000a      security features in over ten million of their electronic devices.\u000a      Accordingly, the impact claimed is mitigating data security risks for HP\u000a      users and clients and substantial economic gain for the company.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Cardiff University\u000a    ","Institutions":[{"AlternativeName":"Cardiff University","InstitutionName":"Cardiff University","PeerGroup":"A","Region":"Wales","UKPRN":10007814}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a3.1 A. Balinsky, H. Balinsky and S. J. Simske, \"On Helmholtz's\u000a        principle for Document Processing\", 10 ACM Symposium on Document\u000a      Engineering (DocEng2010), Manchester, UK, 21-24 September 2010.\u000a      http:\/\/doi.acm.org\/10.1145\/1860559.1860624 Copy held by HEI,\u000a      available on request.\u000a    \u000a\u000a3.2 A. Balinsky, H. Balinsky and S. J. Simske, \"On the Helmholtz\u000a      Principle for Data Mining\", HP Technical Report, HPL-2010-133, http:\/\/www.hpl.hp.com\/techreports\/2010\/HPL-2010-133.html Copy held by HEI, available on request.\u000a    \u000a\u000a3.3 A. Balinsky, H. Balinsky and S. J. Simske, Automatic Text\u000a      Summarization and Small-World Networks, ACM DocEng2011, Google, Mountain\u000a      View, California, 19-22 September 2011. http:\/\/doi.acm.org\/10.1145\/2034691.2034731\u000a      Copy held by HEI, available on request.\u000a    \u000a\u000a3.4 H. Balinsky, A. Balinsky, and S.Simske, Document\u000a      Sentences as a Small World, IEEE SMC 2011, October 9-12, 2011. doi:\u000a      10.1109\/ICSMC.2011.6084065 Copy held by HEI, available on request.\u000a    \u000a\u000a3.5 A. Balinsky, H. Balinsky and S. J. Simske, \"Rapid Change\u000a        Detection and Text Mining\", at the 2nd IMA Conference on Mathematics\u000a      in Defence, Defence Academy, Shrivenham, 20 October2011.\u000a      http:\/\/www.ima.org.uk\/conferences\/past_conferences\/2011\/maths_in_defence\/conference_papers.cfm Copy held by HEI, available on request.\u000a    \u000a\u000a3.6 A. Balinsky, H. Balinsky and S. J. Simske, Keyword\u000a      Determination based on a weight of meaningfulness, U.S. Patent 8,375,022,\u000a      12 February, 2013. Copy held by HEI, available on request.\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"},{"Level1":"11","Level2":"9","Subject":"Neurosciences"}],"Sources":"\u000a    5.1 HP Fellow, Director and Chief Technologist for Security\u000a      Printing and Imaging Engineering, Hewlett-Packard Laboratories. Corroborates\u000a        the use of the algorithms by HP and the resulting impact.\u000a    5.2 T. Bohne, S. R&#246;nnau, U. M. Borghoff, \"Efficient keyword\u000a        extraction for meaningful document perception\", ACM DocEng2011,\u000a      Google, Mountain View, California, 19-22 September 2011. Corroborates\u000a        that the algorithms are recognised as superior to existing techniques.\u000a    ","Title":"\u000a    Meeting the Challenges of Data Security\u000a    ","UKLocation":[{"GeoNamesId":"2653822","Name":"Cardiff"}],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research was undertaken at Cardiff University during the\u000a      period 2007-2011. The motivation for the research arose from the inability\u000a      of existing algorithms to reliably distinguish between confidential and\u000a      non-confidential documents. The research concerns the development of novel\u000a      algorithms that are crucially important in the field of data security.\u000a      Algorithms have been developed for rapid change detection in data\u000a        streams and documents, and text summarisation and classification\u000a      and used in partnership to perform a two-level analysis to create a secure\u000a      printing network environment. The rapid change detection algorithm can be\u000a      considered as a low-level analysis for extracting features or keywords.\u000a      Then these features or keywords are used to perform a higher level\u000a      analysis such as text summarisation or classification.\u000a    In the process of distinguishing between a confidential and\u000a      non-confidential document, a key indicator is the precise scientific\u000a      definition of the meaning of the document. Previous extraction algorithms\u000a      have not been robust in the sense that different algorithms produce\u000a      different outputs due to the non-existence of the scientific definition of\u000a      the meaning of a document.\u000a    The novel algorithms developed in (3.1, 3.2) represent the first attempt\u000a      to define document meaning based on the human perceptual model. Our\u000a      research is based on ideas from image processing and especially on the\u000a      Helmholtz Principle from the Gestalt Theory of human perception. When it\u000a      is applied to the problems of unusual behaviour detection and keywords\u000a      extraction, it delivers fast and effective tools to identify meaningful\u000a      keywords using parameter-free methods. A level of meaningfulness of the\u000a      keywords is also defined which can be used to modify the set of keywords\u000a      depending on application needs.\u000a    According to a basic principle of perception, due to Helmholtz, an\u000a      observed geometric structure is perceptually meaningful if it has a very\u000a      low probability of appearing in noise. As a common sense statement, this\u000a      means that \"events that could not happen by chance are immediately\u000a      perceived\". For example, a group of five aligned dots exists in both\u000a      images in Fig.1, but it can hardly be seen on the left-hand side image.\u000a      Indeed, such a configuration is not exceptional in view of the total\u000a      number of dots. Yet, in the right-hand image we immediately perceive the\u000a      alignment as a large deviation from randomness that would be unlikely to\u000a      happen by chance.\u000a    \u000a\u0009Fig. 1. The Helmholtz principle in human perception\u000a\u0009\u000a    The research makes novel contributions to knowledge extraction\u000a      technologies. It does so in the mining of unstructured data and detecting\u000a      unusual behaviour and in the content of streams of short documents and\u000a      files. In the context of data mining, the research defined the Helmholtz\u000a      Principle as the statement that meaningful features and interesting events\u000a      appear as large deviations from randomness (3.1, 3.2). In the cases of\u000a      textual, sequential or unstructured data qualitative measures were derived\u000a      for such deviations. Under unstructured data, data can be understood\u000a      without an explicit data model, but with some internal geometrical\u000a      structure. For example, sets of dots in Fig. 1 are not created by a\u000a      precise data model, but still have important geometrical structures:\u000a      nearest neighbours, alignments, concentrations in some regions, etc. A\u000a      good example is textual data where there are natural structures like\u000a      files, topics, paragraphs, documents etc. Sequential and temporal data\u000a      also can be divided into natural blocks like days, months or blocks of\u000a      several sequential events.\u000a    Over the years, the amount of text available electronically has grown\u000a      exponentially creating a huge demand for automatic methods and tools for\u000a      text summarisation. Based on the work on the detection of unusual\u000a      behaviour in text (3.1, 3.2), it was possible to model a document as a\u000a      one-parameter family of graphs, with its sentences (or paragraphs) as the\u000a      set of its nodes and edges defined by a carefully selected family of\u000a      meaningful words (using the Helmholtz principle form). We demonstrated\u000a      that, for some range of the parameter, there is a transition in which the\u000a      resulting graph becomes a small-world network. We exploited this\u000a      remarkable structure by modelling texts and documents as small-world\u000a      networks and applying many of the measures and tools from social network\u000a      theory to develop a novel approach to extractive text summarisation (3.3,\u000a      3.4, 3.5). The goal in extractive text summarisation is to extract the\u000a      most meaningful parts of documents (sentences, paragraphs, etc.) to\u000a      represent main concepts of the document. The algorithms based on our\u000a      research extract the most important sentences and structures from text\u000a      documents reliably and efficiently.\u000a    As a consequence of this work HP were provided with:\u000a    \u000a      An algorithm to detect changes in data streams, resulting in a US\u000a        patent for HP (3.6).\u000a      New algorithms for the Hewlett-Packard Secure Document Ecosystem\u000a        Portfolio, for automatic keyword extraction and significance evaluation.\u000a      Algorithms for extractive text summarisation and classification, using\u000a        a small-world network model &#8212; two US patents have been filed by HP.\u000a      New algorithms for the Hewlett-Packard Secure Document Ecosystem\u000a        Portfolio, which automatically summarise texts and extract their most\u000a        important features.\u000a    \u000a    Key staff: Prof. A. Balinsky (academic staff 2001-) assisted by two PhD\u000a      students (N. Mohammad (2007-2011, EPSRC CASE award with Hewlett-Packard)\u000a      and B. Dadachev (2011- , funded jointly by Cardiff University and\u000a      Hewlett-Packard)). Dr Mohammad was immediately employed by HP on\u000a      completion of his PhD.\u000a    "},{"CaseStudyId":"3390","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"3175395","Name":"Italy"},{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"3017382","Name":"France"},{"GeoNamesId":"2782113","Name":"Austria"},{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"2017370","Name":"Russia"}],"Funders":[],"ImpactDetails":"\u000d\u000aRoute to Impact:\u000d\u000aCardiff University agreed to undertake funded research on sales forecasting for Procter &amp; Gamble,\u000d\u000aas a result of a long standing relationship, from 1998 and 2006. The outcomes of this research\u000d\u000awere published in a series of confidential technical reports for Procter &amp; Gamble. In 2004, through\u000d\u000athe transfer of the Associate Director of Procter and Gamble, CMK (Phil Parker) to Nielsen, the\u000d\u000acompany approached Cardiff University to further develop statistical and mathematical models for\u000d\u000aforecasting purposes. The research has been applied by Nielsen and has led to the following\u000d\u000aimpacts:\u000d\u000aEconomic Gain:\u000d\u000aNielsen operates in Africa, Asia, Australia, Europe, Middle East, North America and South\u000d\u000aAmerica. From 2011-2013, the company has consistently ranked first in the Honomichl Top 25\u000d\u000alargest market research organisations in the World. In the group of top 4 global market research\u000d\u000aorganisations, Nielsen research revenue dominates (US$5.4 billion in 2013). Moreover it has\u000d\u000aincreased its global market share by 4% since 2011 outperforming all others in the group (5.1).\u000d\u000aThe products offered by Nielsen (Consumer information, Consumer research and Market\u000d\u000ameasurement) enable companies to understand consumers and consumer behaviour,\u000d\u000asubsequently increasing profits and expanding businesses. The research conducted at Cardiff\u000d\u000aUniversity, under an exclusivity agreement, has been integral to the development of Nielsen's\u000d\u000aservices.\u000d\u000aPredicting consumer buying behaviour, particularly regarding new or relaunched products, is key to\u000d\u000athe activities of the corporation. This involves the development of commercially viable analysis and\u000d\u000aforecasting systems. This is managed by a division called AC Nielson BASES. Prior to 2008\u000d\u000aNielsen was unable to justify a large investment based on the unproven NBD-Dirichlet theory since\u000d\u000aeven the basic mixed Poisson model had not been validated on large household panel data sets.\u000d\u000aNielsen required stronger confirmation of the applicability of mixed Poisson models to their data.\u000d\u000aThey commissioned work at Cardiff University by Professor Zhigljavsky and his team to perform an\u000d\u000ain-depth analysis of the very large data sets provided by Nielsen. The database contains records of\u000d\u000aevery transaction, through the scanning of individual items over a three year period with the\u000d\u000avariables including date of purchase, quantity, price per unit, flavour, brand and size. The database\u000d\u000acontains information on over 100 categories and subcategories of products ranging from cereals to\u000d\u000asoup, cosmetics to detergents and cough treatments to paper products. The statistical analysis of\u000d\u000athese very large data sets demonstrated that measured behaviour conformed to model predictions\u000d\u000aand therefore confirmed that the NBD-Dirichlet theory was a viable foundation for effective\u000d\u000aforecasting and market research analysis. Additionally (see section three) the research provided\u000d\u000avaluable insights into the most current and efficient mathematical modelling techniques in the\u000d\u000amarket research industry and particularly in consumer buying behaviour.\u000d\u000aNielsen has used the Cardiff research on forecasting consumer buying behaviour to provide\u000d\u000aservices to a host of major corporations. These include Unilever, Procter &amp; Gamble, Coca Cola,\u000d\u000aPepsi, Kraft and Nestl&#233;. Nielsen are unable to give explicit information on what services and\u000d\u000amethods are provided for what clients for reasons of commercial sensitivity. However, the research\u000d\u000ahas been foundational to the work conducted for these global organisations and has enabled the\u000d\u000aconsolidation and progression of corporate relationships. Subsequently, this has enabled Nielsen\u000d\u000ato compete on a worldwide scale and retain its dominant market position (5.2).\u000d\u000aRelated research by Professor Zhigljavsky and his team has had significant impact for other\u000d\u000acompanies, such as Procter &amp; Gamble. The models were applied to study sales data versus\u000d\u000apricing in several European countries. The models developed at Cardiff University provided more\u000d\u000areliable and accurate forecasts compared with the models that were previously employed by the\u000d\u000acompany. For example, the model was used to show that in Italy, sales of a particular Garnier\u000d\u000aproduct in drug stores were reduced by modestly increasing the price, whereas in other Italian\u000d\u000aoutlets increasing the price had minimal effect on sales. In the Netherlands sales of the same\u000d\u000aproduct were resilient to modest increases in price. Importantly, this has enabled Procter &amp; Gamble\u000d\u000ato selectively increase prices in different outlets and different countries without reducing sales. The\u000d\u000afinancial gain associated with this is sizeable.\u000d\u000aAs a direct consequence of this research Nielsen has invested US$10 million and a further US$20\u000d\u000amillion in research and forecasting operations. The Vice President of AC Nielsen Product\u000d\u000aDevelopment (Phil Parker) stated that the research Cardiff has provided has helped them to better\u000d\u000aunderstand and address their clients' needs and develop their businesses. He commented that\u000d\u000a\"The positive results of this work have confirmed the theory as a viable foundation for effective\u000d\u000aforecasting and related analyses, as well as providing insights on efficient methods for analysis\u000d\u000aand estimation.\" The subsequent economic rewards that have and will continue to be reaped from\u000d\u000athis extensive investment have remained confidential but are expected to significantly exceed\u000d\u000aUS$50 million (5.2).\u000d\u000aPublic\/Industry Engagement:\u000d\u000aThe research has been disseminated by Professor Zhigljavsky at over twenty national and\u000d\u000ainternational events, enhancing the knowledge and ability of leading organisations in the\u000d\u000amanufacturing and retail industries to implement more effective forecasting strategies and respond\u000d\u000ato a rapidly changing economic climate. These include presentations on the models in Russia,\u000d\u000aFrance, Austria, London and Glasgow. Audience members ranged from 30-100 people and\u000d\u000aincluded representatives from MindShare WW, MediaCom, DDB Matrix, Ninah, LMG, Billetts \/\u000d\u000aEbiquity, BrandScience, MacroAnalytica Limited, OHAL and D2D. Moreover, talks have been\u000d\u000agiven to senior staff at GlaxoSmithKline, Sainsbury's, Tesco and the Cooperative (5.3).\u000d\u000a","ImpactSummary":"\u000d\u000aThe School of Mathematics at Cardiff University has developed important statistical and\u000d\u000amathematical models for forecasting consumer buying behaviour. Enhancements to classical\u000d\u000amodels, inspired by extensively studying their statistical properties, have allowed us to exploit their\u000d\u000avast potential to benefit the sales and marketing strategies of manufacturing and retail\u000d\u000aorganisations. The research has been endorsed and applied by Nielsen, the #1 global market\u000d\u000aresearch organisation that provides services to clients in 100 countries. Nielsen has utilised the\u000d\u000amodels to augment profits and retain their globally leading corporate position. This has led to a\u000d\u000aUS$30 million investment and been used to benefit major consumer goods manufacturers such as\u000d\u000aPepsi, Kraft, Unilever, Nestl&#233; and Procter &amp; Gamble. Therefore the impact claimed is financial.\u000d\u000aMoreover, impact is also measurable in terms of public engagement since the work has been\u000d\u000adisseminated at a wide range of national and international corporate events and conferences.\u000d\u000aBeneficiaries include Tesco, Sainsbury's, GlaxoSmithKline and Mindshare WW.\u000d\u000a","ImpactType":"Economic","Institution":"\u000d\u000aCardiff University\u000d\u000a","Institutions":[{"AlternativeName":"Cardiff University","InstitutionName":"Cardiff University","PeerGroup":"A","Region":"Wales","UKPRN":10007814}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a\u000a3.1 Savani, V, Zhigljavsky, A.A. (2006) Efficient estimation of parameters of the negative binomial\u000d\u000adistribution. Communications in Statistics: Theory and Methods v. 35, No. 4-6, 767--783.\u000d\u000ahttp:\/\/dx.doi.org\/10.1080\/03610920500501346 Copy held by HEI, available on request.\u000d\u000a\u000a\u000a3.2 Savani, V., Zhigljavsky, A.A. (2007) Efficient parameter estimation for independent and\u000d\u000aINAR(1) negative binomial samples. Metrika, v. 65, No.2, 207-255.\u000d\u000ahttp:\/\/dx.doi.org\/10.1007\/s00184-006-0071-x Copy held by HEI, available on request.\u000d\u000a\u000a\u000a3.3 Savani V. and Zhigljavsky A. (2007) Asymptotic distributions of statistics and parameter\u000d\u000aestimates for mixed Poisson processes, Journal of Statistical Planning and Inference, v. 137, No.\u000d\u000a12, 3990-4002. http:\/\/dx.doi.org\/10.1016\/j.jspi.2007.04.016 Copy held by HEI, available on request.\u000d\u000a\u000a\u000a3.4 Leonenko N., Savani, V. and Zhigljavsky, A.A. (2007) Autoregressive negative binomial\u000d\u000aprocesses, Annales de l'Institut de Statistique de Universit&#233; de Paris, v. 51, No. 1-2, 25-47.\u000d\u000ahttp:\/\/cardiff.ac.uk\/maths\/resources\/ISUP_NBDPROCESSES_LeonenkoSavaniZhigljavsky.pdf\u000d\u000aCopy held by HEI, available on request.\u000d\u000a\u000a\u000a3.5 Zhigljavsky A. (2011) Statistical Modelling in Market Research. In: International Encyclopedia\u000d\u000aof Statistical Science, Springer, 1450-1452. http:\/\/dx.doi.org\/10.1007\/978-3-642-04898-2_548\u000d\u000aCopy held by HEI, available on request.\u000d\u000a\u000a\u000a3.6 Savani, V, Zhigljavsky, A.A. (2007) Confidential Technical Report (ACNielsen BASES):\u000d\u000aAnalyzing Consumer Purchases using Mixed Poisson Models. Copy held by HEI, available on\u000d\u000arequest.\u000d\u000a\u000a3.7 Research grants with A Zhigljavsky as Principal Investigator from\u000d\u000a&#8226; ACNielsen-BASES on Statistical Modelling in Consumer Studies; 2005, 2006-08, 2008-09,\u000d\u000a4 grants totalling US$100k.\u000d\u000a&#8226; Procter &amp; Gamble on Statistical Modelling in Marketing Research; 1998-2006, 5 grants\u000d\u000atotalling US$290k.\u000d\u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a5.1 http:\/\/www.marketingpower.com\/AboutAMA\/Pages\/2013_GlobalHonomichl25.pdf\u000d\u000aEvidence that Nielsen dominates the Market Research Industry.\u000d\u000a5.2 Vice President of AC Nielsen Product Development. Corroborates the use and impact of the\u000d\u000aresearch at Nielsen.\u000d\u000a5.3 Corporate Recruiter, Director Briefing Dinner Event (featuring a talk by Professor Zhigljavsky).\u000d\u000aPdf document. Provides an example of public talks given as a consequence of the research.\u000d\u000a\u000d\u000a","Title":"\u000d\u000aGrowing Businesses: Robust Models for Understanding Consumer Buying\u000d\u000aBehaviour\u000d\u000a","UKLocation":[{"GeoNamesId":"2643743","Name":"London"},{"GeoNamesId":"2653822","Name":"Cardiff"},{"GeoNamesId":"2648579","Name":"Glasgow"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"},{"GeoNamesId":"2634895","Name":"Wales"},{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000aThe key staff contributing to the research, carried out at Cardiff University during the period 1998-2009,\u000d\u000aare Professor A. Zhigljavsky (1997-) and Dr V. Savani (PhD student (2002-06), Lecturer\u000d\u000a(2006-08)). In 2008 Dr Savani was appointed as a full-time permanent statistician at Nielsen.\u000d\u000aThe basic statistical model we consider is the classical mixed Poisson process with gamma mixing\u000d\u000adistribution; the so-called Polya process. In the market research context, this model assumes that\u000d\u000athere are many buyers each purchasing according to a Poisson process with a given rate. The\u000d\u000apurchasing rates differ from buyer to buyer and are assumed random and mutually independent.\u000d\u000aFor the Polya process, the distribution of the purchase rates is the gamma distribution and the total\u000d\u000anumber of consumer purchase occasions is a negative binomial distribution (NBD). When\u000d\u000aconsidering several brands or categories we additionally assume that the buyer chooses the brand\u000d\u000aaccording to his or her personal propensities, which are random, independent for different buyers\u000d\u000aand follow the Dirichlet distribution. Various data sets have been analysed in the published\u000d\u000aliterature demonstrating that this statistical model is appropriate. The immense potential of this\u000d\u000amodel for market research applications had been debated for many years. However, this\u000d\u000adiscussion never progressed beyond a very basic level and as a consequence the model had\u000d\u000anever before been properly tested and used on data sets of practical value. At Cardiff we\u000d\u000aextensively studied the statistical properties of this model, in general, with particular emphasis on\u000d\u000amarket research applications for the first time. In particular, our original contributions to the\u000d\u000adevelopment of statistical models for market research include:\u000d\u000a\u000d\u000athe development of robust and efficient estimators for the NBD parameters for modelling\u000d\u000aconsumer purchase occasions based on an efficient implementation of the power method;\u000d\u000astandard methods for estimating the NBD parameters, such as the method of moments\u000d\u000aand the zero-term method, are inefficient for a significant number of categories. (3.1,3.2);\u000d\u000athe derivation of the joint asymptotic distributions of statistics, including parameter\u000d\u000aestimators, to study the dynamical behaviour of the mixed Poisson process and hence\u000d\u000aconfidence bounds for the forecasted values of various market research characteristics,\u000d\u000aincluding penetration, purchase frequency, mean repeat, repeat per repeater, etc. (3.3);\u000d\u000athis has required consideration of multivariate distributions with marginal NBD and is\u000d\u000aessential for assessing the forecasting ability of the basic model;\u000d\u000athe development of various statistical tests for assessing the validity of the model (3.3);\u000d\u000atesting the Polya process model against integer autoregression and other models with\u000d\u000aNBD as the marginal distribution (3.2,3.4);\u000d\u000athe development of variations of the basic mixed Poisson model allowing us to deal with\u000d\u000aseasonal effects, random non-response and the household flow-through problems (3.5,\u000d\u000a3.6).\u000d\u000a\u000d\u000aIn the course of developing the statistical methodology we tested the methods and models on the\u000d\u000alargest set of household panel data ever analysed, courtesy of Nielsen. The basic model, statistical\u000d\u000atests, estimation methods and forecasting procedures have been modified and adapted in relation\u000d\u000ato their performance on real data. It was shown that the basic model of mixed Poisson processes\u000d\u000aoften requires certain adjustments to combat problems associated with the presence of non-\u000d\u000abuyers, the non-stationarity of markets and the panel flow-through. We explored the range of\u000d\u000avalidity of the mixed Poisson model and variations thereof and provided evidence that the model is\u000d\u000ainadequate at the level of an individual consumer and for small time intervals. However, when\u000d\u000asufficient households are aggregated and a reasonable time interval such as a four week period is\u000d\u000aconsidered, either the basic model or a variation of it often becomes adequate.\u000d\u000aOur statistical findings outlined above have been published in statistical journals (3.1-3.4).\u000d\u000aFurthermore, in (3.5) we discussed the current state-of-the-art in mathematical modelling in market\u000d\u000aresearch, in general, and consumer buying behaviour, in particular. Results of our extensive tests\u000d\u000aon Nielsen household panel data and adaptations of the model to violations of the main\u000d\u000aassumptions are described in a series of technical reports submitted to Procter &amp; Gamble (P&amp;G)\u000d\u000aand Nielsen, for example (3.6). Research funding has come from both Nielsen and Procter &amp;\u000d\u000aGamble (3.7).\u000d\u000a"},{"CaseStudyId":"3672","Continent":[{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"298795","Name":"Turkey"}],"Funders":["Wellcome Trust","Biotechnology and Biological Sciences Research Council"],"ImpactDetails":"\u000d    The mathematical models and statistical inference techniques developed by\u000d      researchers at Warwick have been used by the UK government - through Defra\u000d      and Animal Health and Veterinary Laboratories Agency (AHVLA) - to underpin\u000d      policy relating to the control of FMD infection, particularly by\u000d      vaccination. Models of other infections by Warwick researchers have added\u000d      to the public debate [20] and UK government policy surrounding bTB control\u000d      transmission and control [21,22,]; the control and containment of avian\u000d      influenza [21]; and the spread and management of bee infections by the\u000d      Food and Environment Research Agency (FERA). Since 2008, the FMD modelling\u000d      framework has been extended to the USA and other countries, and has been\u000d      presented to different policy groups (such as the US Department of\u000d      Homeland Security [23] and the European Commission [24]), helping to shape\u000d      the expectations of what mathematical modelling can achieve during a\u000d      disease outbreak. We will discuss these three cases in turn.\u000d    FMD in the UK\u000d    Keeling has worked on mathematical models to predict the spread of FMD in\u000d      the UK since the devastating FMD outbreak of 2001. Together with\u000d      Tildesley, these early models have been refined and used to address\u000d      numerous applied questions about the optimal control of outbreaks. Work\u000d      since 2008 has focused on the potential benefits of control by localised\u000d      (contiguous premise) culling and control by vaccination bringing the most\u000d      impact, and suggested how the optimal use of these methods will vary\u000d      regionally. Predictions for optimal culling [5] suggest that the number of\u000d      farms losing livestock could be reduced by over 30%, while targeted\u000d      vaccination [2] could reduce losses by over 75%; these insights have\u000d      helped to shape government policy and are factored into current\u000d      contingency planning. A statement from the Head of Epidemiology at\u000d      AHVLA\/Defra states \"their research provides important scientific\u000d      underpinning of our current FMD contingency plan\" [21]. This plan now\u000d      contains vaccination as a key control measure against new outbreaks and\u000d      local reactive culling as an important potential method of control as a\u000d      result of its unpopularity with the farming community in 2001.\u000d    Following on from methodological developments in Warwick [4,7], the above\u000d      modelling is now being underpinned by more rigorous and systematic\u000d      Bayesian (MCMC) parameter inference, which enables Keeling and Tildesley\u000d      to rapidly tailor models more readily to any new outbreak. This important\u000d      innovation means that we can rapidly respond to requests for model\u000d      predictions from AHLVA\/Defra, leading to Keeling being made a member of\u000d      Defra's Quantitative Modelling Standing Capacity from 2011. In addition,\u000d      as a component of the inference software Keeling and Tildesley have been\u000d      working on GIS interface to display current cases and potential future\u000d      scenarios; AHVLA are \"hoping to use the GIS epidemic inference and\u000d      visualisation software directly produced from the InFER project in [their]\u000d      livestock epidemic planning and emergency procedures\" [21].\u000d    bTB in UK \u000d    The control of bTB is complicated by the emotive arguments about the role\u000d      of badgers in its transmission compared to transmission between cattle;\u000d      despite scientific uncertainty of the role of badgers 2013 saw a\u000d      large-scale cull of these animals. This contention necessitates a\u000d      mathematical model that is robustly fitted to data and can elucidate the\u000d      underlying transmission mechanisms and hence help determine adequate\u000d      control strategies. Work by Keeling has developed the first national-scale\u000d      predictive model of bTB that captures the spatial and temporal patterns of\u000d      bTB in the UK. This model has been under development for several years and\u000d      Defra have been kept fully informed of its findings; these outcomes have\u000d      \"shaped how Defra consider planning future control options\" [21]. Owing to\u000d      this work, Keeling has been invited to be a member of Defra's `bTB\u000d      modelling initiative' (see website and agenda papers where the terms of\u000d      reference of the group are minuted [22, 25]).\u000d    The continuing work in Warwick on modelling livestock epidemics in the UK\u000d      \"forms a valuable part of the scientific underpinning for the policy\u000d      decisions made on the control of livestock infections\" and has been\u000d      \"recognised by Keeling being a member of `Defra Quantitative Modelling\u000d      Standing Capacity', to be called upon to provide timely advice in case of\u000d      an outbreak situation.\" [21].\u000d    FMD in USA and Europe\u000d    Previous work on FMD in the USA primarily focused on localised results\u000d      using complex, parameter-rich models, and hence policy was based around a\u000d      limited set of potential scenarios. Keeling and Tildesley's parsimonious\u000d      model with its simpler mathematical formulation has enabled the\u000d      development of national-scale models that can be parameterised from early\u000d      epidemic data. Modelling results have been presented to policy-makers at\u000d      the US Department of Agriculture and Department of Homeland Security\u000d      (DHS). As a Program Manager at the DHS says [23], \"Tildesley and Keeling\u000d      through their academic publications and willingness to interact with\u000d      policy makers, have revolutionized the way in which DHS considers the use\u000d      of mathematical modelling for future outbreaks of livestock diseases\".\u000d    This modelling methodology is also being extended to Turkey where FMD is\u000d      endemic and therefore acts as a reservoir for infection into Europe. As\u000d      such \"the work of the Warwick group has had a high impact on our European\u000d      Member States and may provide the breakthrough needed in modelling control\u000d      efforts in affected countries\" states the Executive Secretary of the\u000d      European Commission for the Control of Foot-and-Mouth Disease (EuFMD)\u000d      [24]. The EuFMD is an intergovernmental body with 37 member states,\u000d      operating under the auspices of the UN Food and Agriculture Organisation.\u000d      [24] notes member states particularly use the Warwick group papers to\u000d      decide when and where to vaccinate and produce reactive strategies. The\u000d      Warwick group has led member states to recognise the importance of\u000d      national policies, \"...their willingness to address issues affecting\u000d      confidence in use of models is of huge importance and has been a major\u000d      contributor to the increased international acceptance of the use of\u000d      modelling to assist with FMD emergency planning\" as well as a \"tool kit\u000d      for predictive impact of changing movement controls, of enormous\u000d      importance to emergency planners in FMD free countries\" [24].\u000d    In summary, the mathematical modelling and statistical work in the\u000d      University of Warwick Mathematical Sciences Departments has left the\u000d      agencies responsible for infectious livestock disease control far better\u000d      prepared for future outbreaks and with a far better understanding of what\u000d      modelling and statistics can offer in terms of optimising control\u000d      strategies and responses.\u000d    ","ImpactSummary":"\u000d    Mathematical modelling of livestock infections and disease control\u000d      policies is an important part of planning for future epidemics and\u000d      informing policy during an outbreak of infectious disease. Researchers in\u000d      the Mathematics Institute, University of Warwick, are considered to be at\u000d      the cutting-edge of developing policy-orientated mathematical modelling\u000d      for a number of livestock infections. Such models have been used to inform\u000d      government policy for foot-and-mouth disease (FMD) and a range of other\u000d      infections including bovine tuberculosis (bTB) and bee infections. From\u000d      2008, their work with responsible national and international agencies has\u000d      focused on statistical inference from early outbreak data, formulating\u000d      models and inferring parameter values for bTB infection spread within and\u000d      between farms, developing predictive models of FMD outbreaks in the USA,\u000d      and extending such models to areas where FMD is endemic. This research has\u000d      helped to shape policy and determined how policy-makers perceive and use\u000d      predictive models in real-time.\u000d    ","ImpactType":"Environmental","Institution":"\u000d    University of Warwick\u000d    ","Institutions":[{"AlternativeName":"Warwick (University of)","InstitutionName":"University of Warwick","PeerGroup":"A","Region":"West Midlands","UKPRN":10007163}],"Panel":"B         ","PlaceName":[],"References":"\u000d    Publications:\u000d    \u000a1. Keeling, M.J., Woolhouse, M.E.J., May, R.M., Davies, G. and\u000d      Grenfell, B.T., Modelling Vaccination Strategies against Foot and Mouth\u000d      Disease. Nature 421 136-142 (2003) DOI:\u000a        10.1038\/nature01343\u000d    \u000a\u000a2. Tildesley, M.J., Savill, N.J., Shaw, D.J., Deardon, R.,\u000d      Brooks, S.P., Woolhouse, M.E.J., Grenfell, B.T. &amp; Keeling, M.J.,\u000d      Optimal reactive vaccination strategies for an outbreak of foot- and-mouth\u000d      disease in Great Britain, Nature 440, 83-86. (2006). DOI:\u000a        10.1038\/nature04324\u000d    \u000a\u000a3. Savill, N., St. Rose, S., Keeling, M., Woolhouse, M., Silent\u000d      spread of H5N1 in vaccinated poultry. Nature 442 757-757 (2006) DOI:\u000a        10.1038\/442757a\u000d    \u000a\u000a4. Jewell, C.P., Keeling, M.J. and Roberts, G.O., Predicting\u000d      undetected infections during the 2007 foot-and-mouth disease outbreak. J.\u000d      R. Soc. Interface. 6 1145-1151 (2009) DOI:\u000a        10.1098\/rsif.2008.0433\u000d    \u000a\u000a5. Tildesley, M.J., Bessell, P.R., Keeling, M.J. and Woolhouse,\u000d      M.E.J., The role of pre-emptive culling in the control of foot-and-mouth\u000d      disease. Proc. Roy. Soc. Lond. B. 276 3239-3248. (2009) DOI:\u000a        10.1098\/rspb.2009.0427\u000d    \u000a\u000a6. Vernon, M.C. and Keeling, M.J., Representing\u000d      the UK's cattle herd as static and dynamic networks. Proc. Roy. Soc. B.\u000d      276(1656) 469-476. (2009) DOI:\u000a        10.1098\/rspb.2008.1009\u000d    \u000a\u000a7. Jewell, C.P., Kypraios, T., Neal, P. and Roberts, G.O.,\u000d      Bayesian Analysis for Emerging Infectious Diseases. Bayesian Analysis\u000d      4(3) 465-496 (2009) DOI:\u000a        10.1214\/09-BA417\u000d    \u000a\u000a8. Brooks-Pollock, E. and Keeling, M.J. Herd size and bovine\u000d      tuberculosis persistence in cattle farms in Great Britain. Prev Vet.\u000d        Med. 92 360-365 (2009)DOI:\u000a        10.1016\/j.prevetmed.2009.08.022\u000d    \u000a\u000a9. Tildesley, M.J., House, T.A., Bruhn, M.C., Curry, R.J.,\u000d      O'Neil, M., Allpress, J.L., Smith, G. and Keeling, M.J., Impact of\u000d      spatial clustering on disease transmission and optimal control. Proc Natl\u000d      Acad Sci USA 107 1041-6 (2010) DOI:\u000a        10.1073\/pnas.0909047107\u000d    \u000a\u000a10. Tildesley, M.J. Smith, G. and Keeling, M.J., Modeling\u000d      the spread and control of foot-and- mouth disease in Pennsylvania\u000d      following its discovery and options for control, Prev Vet Med 104,\u000d      224-239. (2012) DOI:\u000a        10.1016\/j.prevetmed.2011.11.007\u000d    \u000a\u000a11. Jewell, J.C. and Roberts, G.O., Enhancing Bayesian risk\u000d      prediction for epidemics using contact tracing. BioStatistics\u000d      13(4) 567-579. (2013)DOI:\u000a        10.1093\/biostatistics\/kxs012\u000d    \u000a\u000a12. Datta, S., Bull, J., Budge, G. and Keeling, M.J.,\u000d      Modelling the spread of American Foulbrood in honey bees. J. Roy. Soc.\u000d        Interface 10(88) 20130650 (2013) DOI:\u000a        10.1098\/rsif.2013.0650\u000d    \u000aPeer-reviewed grants\/awards to Warwick researchers (unless stated\u000d          otherwise): \u000d    13. PI: Keeling \"Quantitative analysis of the spatio-temporal dynamics\u000d      and control of foot-and- mouth disease\" Wellcome Trust Oct 2002-Sep 2005\u000d      &#163;82,931.\u000d    14. PI: Keeling \"Preliminary modelling of AI epidemiology and control in\u000d      the UK\" Department for Environment, Food and Rural Affairs (Defra)\u000d      RMP\/2910 Dec 2005-Jun 2006 &#163;40,000.\u000d    15. PI: Smith (Penn State University) Co-Investigator: Keeling\u000d      \"Hierarchical models for the spatio- temporal dynamics of infectious\u000d      diseases\" NIH MIDAS Feb 2006-Jan 2010 &#163;217,000 (Warwick component).\u000d    16. PI: Keeling \"Spatio-temporal dynamics of livestock infections\"\u000d      Wellcome Trust Apr 2006-Mar 2009 &#163;166,432.\u000d    17. PI: Roberts \"InFER: Likelihood-based Inference for Epidemic Risk\"\u000d      BBSRC BB\/H00811X\/1. Oct 2009-Jun 2013 &#163;589,005.\u000d    18. PI: Keeling \"Modelling systems for managing bee disease: the\u000d      epidemiology of European foulbrood\" BBSRC BB\/I000615\/1 Oct 2010-Sep 2013\u000d      &#163;165,362 (Warwick component).\u000d    19. PI: Webb (Colorado State) Co-Investigator: Tildesley \"Spread of\u000d      animal disease within US Livestock: improving decisions and interventions\"\u000d      US Department of Homeland Security July 2011-Apr 2013 &#163;126,842 (Warwick\u000d      component).\u000d    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"11","Level2":"3","Subject":"Clinical Sciences"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"}],"Sources":"\u000d    \u000d      Open letter by leading scientists on the proposed badger cull.\u000d        http:\/\/www.theguardian.com\/theobserver\/2012\/oct\/14\/letters-observer\u000a\u000d      Statement from Head of the Epidemiology, Surveillance and Risk Group\u000d        in the Veterinary Directorate of the AHVLA, an Executive Agency of\u000d        Defra, UK.\u000d      Cross Departmental Review on 'Detection and Identification of\u000d        Infectious Diseases', chapter S9: http:\/\/www.bis.gov.uk\/foresight\/our-work\/projects\/published-projects\/infectious-\u000a          diseases\/reports-and-publications\"\u000d      Statement from Program Manager of the Chemical Biological Division for\u000d        the Department of Homeland Security, Science and Technology Directorate,\u000d        USA.\u000d      Statement from the Executive Secretary of the European Commission for\u000d        the Control of Foot- and-Mouth Disease (EuFMD).\u000d      Minutes of the SAC(13)024 on Bovine TB meeting, 26 June 2013 (p23) http:\/\/www.defra.gov.uk\/sac\/files\/sac-13-june-agenda-papers.pdf\u000a\u000d    \u000d    ","Title":"\u000d    Mathematical modelling of livestock infection to inform policy for future\u000d      epidemics and control of disease outbreaks.\u000d    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d    Livestock infections represent a serious threat to the farming industry,\u000d      national economic prosperity and food security. In the UK alone it has\u000d      been estimated that livestock infections have cost the economy over &#163;15\u000d      billion; hence a quantitative understanding of infection and an ability to\u000d      predict the impact of control measures is vital. Since the 2001 outbreak\u000d      of FMD in the UK (which directly affected over ten thousand farms),\u000d      mathematical modelling and statistical analysis have been viewed as\u000d      important tools in scenario planning and outbreak control for a range of\u000d      infectious diseases of livestock. Often, mathematical models are required\u000d      to untangle the complex trade-offs between control (culling potentially\u000d      infected animals) and minimising the size of an epidemic - too little\u000d      culling and the epidemic is uncontained, too much culling and the effect\u000d      of control could be worse than the infection. Researchers in the\u000d      Mathematics Institute and Statistics Department at the University of\u000d      Warwick (Keeling, Tildesley and Roberts) have been involved in the\u000d      development of new mathematical and statistical tools for tackling such\u000d      issues. Here, we primarily highlight their work on the transmission\u000d      dynamics of FMD [1,2,4,5,9,10,13], although we have also worked on, and\u000d      provided policy-makers with information about, other important livestock\u000d      infections including bovine tuberculosis [8], avian influenza in poultry\u000d      [3] and foulbrood in bees [12]. FMD is one of the most highly\u000d      transmissible of all livestock infections, so rapid, targeted control is\u000d      required to minimise economic losses. FMD is a worldwide problem and our\u000d      primary results focus on controlling novel outbreaks in the UK (based on\u000d      experience in 2001 and 2007 [1,2,4,5] and the USA (where data are more\u000d      limited) [9,10].\u000d    The research from Warwick has pioneered two separate, but related,\u000d      approaches. First, we have developed Bayesian MCMC (Markov Chain Monte\u000d      Carlo) techniques to infer underlying model parameters [4,7,11,12]; and\u000d      second we have constructed a range of mathematical models to elucidate the\u000d      expected epidemic behaviour and the implications of different control\u000d      options [1,2,6,12]. Inference of model parameters is vital if models are\u000d      to be accurately fitted to available data, and confidence in the\u000d      subsequent predictions is to be assessed. To infer model parameters, and\u000d      hence plausible mechanisms, for the spread of FMD [4,7] and\u000d      avian influenza [7,11], we developed novel MCMC methodology. In\u000d      particular, for a range of diseases it is vitally important to determine\u000d      occult (undetected and potentially undetectable) infections, and the\u000d      methods we developed are formulated around determining these hidden case.\u000d      These techniques are now becoming the accepted state-of-the-art for\u000d      inference of spatial epidemic data, and we have been applying them to\u000d      other infections [12].\u000d    Mathematical models, by necessity, are formulated to reflect the\u000d      perceived biological reality and, therefore, differ between particular\u000d      livestock infections and between different locations. However, work by\u000d      Warwick researchers from 2008 onwards has focused on two main transmission\u000d      pathways: local spatial transmission [1,2,5,9] and transmission through\u000d      the movement of livestock [6]. Models of local spatial transmission\u000d      (primarily related to FMD) have been used to investigate a range of\u000d      control measures (including localised culling [5,10,13,14] and vaccination\u000d      [1,2]) which provide clear insights into the role of targeted control in\u000d      livestock disease outbreaks and highlight the importance of local culling\u000d      as a rapid means of containment. These models have also been used to\u000d      demonstrate that knowing the exact location of farms may often be\u000d      unnecessary to determine near-optimal control policies [9] - this has\u000d      clear importance to countries (like the USA) where such location data is\u000d      unavailable. Network models have been extensively used to study the spread\u000d      of infection by movements; our research has highlighted that the dynamic\u000d      nature of the movements (and the identity of the individual moving) cannot\u000d      be subsumed in a static network approach but has important epidemiological\u000d      consequences [6]. These local and network transmission models have been\u000d      combined to allow us to study the spread and control of avian influenza\u000d      [11] and foulbrood [12].\u000d    Key Researchers at Warwick: Prof. Matt Keeling (Lecturer\u000d      2002-05, Reader 2005-08, Professor, 2008-), Prof. Gareth Roberts\u000d      (Professor, 2006-), Dr Michael Tildesley (Post-doctoral researcher\u000d      2003-2008, Warwick Zeeman Lecturer, 2011-2013).\u000d    "},{"CaseStudyId":"3915","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"2802361","Name":"Belgium"},{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"1814991","Name":"China"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Welding industry benefits from new design process software\u000a      The work described above has been successfully applied in the welding\u000a      industry through the OPTWELD project which ran through 2010. This project\u000a      resulted in substantial collaboration work with local industry (the TWI\u000a      Group and the ESI Group) on the implementation of distributed algorithms\u000a      in the commercial software application 'Sysweld'. An MSc research\u000a      dissertation, `Distributed algorithms for heat transfer related problems'\u000a      by S. Guo, documented the issues surrounding the work and has been widely\u000a      disseminated in industry. The embedding of the scientific computing\u000a      techniques, resulting from the research, in welding industry software has\u000a      led to an easy approach to coupling of subdomains which has significantly\u000a      enhanced the accuracy and applicability of the software. Research results\u000a      were also applied to: inverse problems, image processing,\u000a      pharmacokinetics, and stochastic optimisation and this clearly\u000a      demonstrated the impact of the numerical frameworks developed in\u000a      industrial mathematics.\u000a    Dissemination of findings in aero-acoustics and inverse problems\u000a      Work in the area of domain decomposition methods started with applications\u000a      in aero-acoustics and inverse problems in 1996 with Djambazov, and later\u000a      with Palansuriya, Wang, L. Lai, Siahaan, Tian, Kokulan. The most important\u000a      recent publication disseminating important results in the concept of\u000a      acoustic correction methods for the extraction of disparate scales can be\u000a      found in [3.2]. In relation to this work, C. Lai and other former students\u000a      explored the concept of a wider class of methods known as the defect\u000a      correction method for multi-scale problems. C. Lai solicited a special\u000a      issue, published in June 2008, entitled `Mathematical and Computational\u000a      Aspects of Multi-Scale Problems' with the Journal of Algorithms and\u000a        Technology, ISSN 1748-3018, while attending the World Congress on\u000a      Computational Mechanics, Los Angeles, 2006. During an academic visit to\u000a      China in July 2008, Lai gave an invited seminar entitled `Some new results\u000a      of the acoustic correction method for the extraction of sound signals' at\u000a      Dalian University of Technology. C. Lai visited Prof. P. Lin and Dr D.\u000a      Trucu, Dundee University, in order to exchange experiences in multi-scale\u000a      methods and gave a talk entitled `On the acoustic correction method for\u000a      extracting sound signals' during March 2009. In May 2009 C. Lai was in\u000a      discussion with Dr A. Rona, Leicester University, on a joint Marie Currie\u000a      Network proposal to FR7 fund, and gave a talk there entitled `On the\u000a      acoustic correction method for extracting sound signals'. This was\u000a      recently funded under FP7-People-2012-ITN AeroTraNet (Ref 317142) in which\u000a      Greenwich has secured a share of &#163;257,000 funding. Lai was also invited to\u000a      give an invited talk at the 2nd Belgian Mathematical Society\u000a      and London Mathematical Society Conference, Catholic University of Leuven,\u000a      the Netherlands on the topic `On high order schemes and defect correction\u000a      methods'. The work was summarised and presented at 16th\u000a      International Conference on Sound and Vibration, Krackow, July 2009, and\u000a      at World Congress on Computational Mechanics 2010, July 2010, Sydney. Lai\u000a      gave a talk to University of Essex in May 2011 entitled `On high order\u000a      schemes and defect correction methods in CFD software'. A joint Essex,\u000a      Greenwich, and Hertfordshire Workshop on Applied and Numerical Methods:\u000a      Multiscale Problems, June 2012, was funded through LMS Scheme 8 Grant\u000a      811062. The work in the acoustic correction method and high order schemes\u000a      mentioned above led to significant contribution to the book listed in\u000a      [3.3]. It is extremely exciting to see that this book is being used as one\u000a      of the texts and references for numerical techniques for LES at Lancaster\u000a      and Greenwich. The PhD student, L. Lai, recently completed his thesis in\u000a      this area with a generalised framework using defect correction.\u000a    Coupling of black box engineering software\u000a      Building on the early research experience in domain decomposition methods\u000a      before 2008, NAMU has been engaged in the TSB project described in [3a] on\u000a      a domain decomposition method for the coupling of black box engineering\u000a      software using the concept of the defect equation. Here the main\u000a      contribution is to allow work at different locations and sites using\u000a      different software being coupled to form a final piece of work. At the\u000a      same time Siahaan has been working on the theory of the convergence of a\u000a      quasi-Newton method showing that the coupling using the defect equation\u000a      converges to the solution, though very slowly. This is an extremely\u000a      encouraging theoretical result and Siahaan has also improved the method so\u000a      that convergence is speeded up as demonstrated in his PhD thesis in June\u000a      2011. The theoretical work is summarised in [3.1].\u000a    Temporal and scale parallel algorithms\u000a      NAMU has a long history of developing temporal and scale parallel\u000a        algorithms. In particular Lai and Parrott have been involved in such\u000a      work before joining the university in 1989 and 1993 respectively. Previous\u000a      work with an application in computational finance submitted in the last\u000a      RAE was demonstrated by Lai and Parrott in the paper entitled `A\u000a      distributed algorithm for European options with nonlinear volatility', Computers\u000a        and Mathematics with Applications, 49, 885-894, 2005. This\u000a      work helped to attract the funding described in R6 for ITN-STRIKE. Recent\u000a      activities and work concentrated on induced parallel and distributive\u000a      properties into mathematical models\/formulations so that a nonlinear\u000a      financial option problem which is intrinsically sequential may process\u000a      parallel properties. Typical techniques used here are transformation\u000a      methods summarised in two recent MSc dissertations at Greenwich:\u000a    1) N. Natkunam, Numerical analysis of the diffusion equation using\u000a      Laplace transform, 2009.\u000a    2) M.T. Tarawneh, Theoretical performance analysis of parallel algorithms\u000a      and applications to differential equations, 2009.\u000a    Lai visited the School of Mathematical Sciences, University of Liverpool,\u000a      in May 2010 while on an external examination role, and presented the above\u000a      work. He also took the opportunity of World Congress on Computational\u000a      Mechanics 2010 to visit the Mathematics Department, University of\u000a      Canberra, Australia, where he disseminated the above work in July 2010.\u000a      Further research work generated two book chapters in books, one with\u000a      applications to computational finance, as described in [3.4], and the\u000a      other a general concept on induced parallel properties, as below:\u000a    C.-H. Lai. On transformation methods and the induced parallel properties\u000a      for the temporal domain, in Substructuring Techniques and Domain\u000a        Decomposition Methods, ed F. Magoules, ISBN: 1759-3158 (doi:\u000a      10.4203\/csets.24.3), Saxe-Coburg Publications, Stirlingshire, UK, 45-70,\u000a      2010.\u000a    Lai was invited to be the external examiner for a PhD thesis on temporal\u000a      parallelisation at the University of Claude Bernard Lyon I in July 2011.\u000a      Knowing such impact in academia through the above work, financial and\u000a      market analysts are interested to know more about how temporal parallel\u000a      algorithms would affect the computational efficiency in derivative\u000a      analysis and prediction of prices. Discussion of collaboration with\u000a      Wuppertal University, Deutsche Postbank, and other financial organisations\u000a      resulted to the FR7 project linking these partners as described in [3b].\u000a    ","ImpactSummary":"\u000a    Spatial decomposition methods have been extended to apply to spatial,\u000a      scale, and temporal domains as a result of work at the Numerical and\u000a      Applied Mathematics Research Unit (NAMU) at the University of Greenwich.\u000a      This work has led to a numerical framework for tackling many nonlinear\u000a      problems which have been key bottlenecks in software design and scientific\u000a      computing. The work has benefitted the welding industry in the UK because\u000a      these concepts are now embedded, with parallel computing, in the\u000a      industry's modern welding design process software.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Greenwich\u000a    ","Institutions":[{"AlternativeName":"Greenwich (University of)","InstitutionName":"University of Greenwich","PeerGroup":"D","Region":"London","UKPRN":10007146}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"1814087","Name":"Dalian"},{"GeoNamesId":"2172517","Name":"Canberra"},{"GeoNamesId":"5368361","Name":"Los Angeles"},{"GeoNamesId":"2996944","Name":"Lyon"}],"References":"\u000a\u0009(REF 1 submitted staff in bold,** REF2 Output)\u000a    \u000a**3.1 Siahaan, A., Lai, C.-H., &amp; Pericleous, K. (2011). Local\u000a      convergence of an adaptive scalar method and its application in a\u000a      nonoverlapping domain decomposition scheme. Journal of Computational\u000a        and Applied Mathematics, 235(17), 5203-5212. http:\/\/dx.doi.org\/10.1016\/j.cam.2011.05.010\u000a      (Impact factor: 0.989)\u000a    \u000a\u000a**3.2 Wang, Z. K., Djambazov, G. S., Lai, C. H., &amp; Pericleous,\u000a      K. A. (2008). Numerical investigation of a source extraction technique\u000a      based on an acoustic correction method. Computers &amp; Mathematics\u000a        with Applications, 55(3), 441-458. http:\/\/dx.doi.org\/10.1016\/j.camwa.2004.08.017\u000a      (Impact factor: 2.069)\u000a    \u000a\u000a3.3 X. Jiang, &amp; Lai, C.-H. (2009). Numerical Techniques\u000a        for Direct and Large-Eddy Simulation. Boca Raton: CRC Press\/Taylor\u000a      &amp; Francis Group. ISBN 978-1-4200-7578-6 (print), 978-1-4200-7579-3\u000a      (ebook). http:\/\/dx.doi.org\/10.1201\/9781420075793\u000a      (17 Citations: Google Scholar.)\u000a    \u000a\u000a3.4 Lai, C.-H. (2008) Numerical solutions of certain nonlinear\u000a      models in European options on a distributed computing environment. In M.\u000a      Ehrhardt (Ed.). Nonlinear Models in Mathematical Finance: New Research\u000a        Trends in Option Pricing (pp. 305-320). New York: Nova\u000a      Science Publishers, Inc. ISBN: 978-1-60456-931-5, (9 citations: Google\u000a      Scholar)\u000a    \u000aResearch Grants\u000a      3a C.-H. Lai. OPTWELD - Real-time virtual prototyping tools for\u000a        OPTimising WELDed products (TSB\/CRD\/096 Q20688) http:\/\/www.optweld.org.uk\/.\u000a      TSB, Technology Programme. 01\/09\/2008 - 31\/08\/2011; Value funded to\u000a      Greenwich: &#163;67,000.\u000a    R1. 3b C.-H. Lai. Novel Methods in Computational Finance (Ref\u000a      304617). FP7-People-2012-ITN. Jan-2013 - Dec-2016. &#8364;3,582,470 (UoG\u000a      contribution &#163;329,000). The ITN network STRIKE is led by University of\u000a      Wuppertal, Germany.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000a    S.1 Collaboration work with local industry, TWI Group and ESI Group, on\u000a      the implementation of distributed algorithms for the commercial software\u000a      Sysweld.\u000a      http:\/\/www.optweld.org.uk\/home\/optweld_overview.pdf).\u000a      Techniques based in defect correction and coupling of subdomains have been\u000a      embedded into the parallel software design.\u000a    S.2 Lai was invited to be the external examiner for a PhD thesis on\u000a      temporal parallelisation at the University of Claude Bernard Lyon I in\u000a      July 2011. Discussion of collaboration with Wuppertal University and\u000a      several financial organisations, including Deutsche Postbank, resulted to\u000a      an industrial and academia collaborative project in FR7 as described in\u000a      [R6].\u000a    S.3 A special issue highlighting industrial collaborative work with\u000a      hospitals in pharmacokinetics and other applications in medical sciences:\u000a      Computer methods in pharmacy related research, Journal of Algorithms\u000a        and Computational Technology, 2 (1), 2011.\u000a      In relation to temporal scale decomposition, Lai applied inverse problem\u000a      techniques for some pharmacokinetic models and developed an intrinsic\u000a      parallel method:\u000a      L. Liu, C.-H. Lai, S.-D. Zhou, F. Xie, H.-W. Lu. PKAIN: An artificial\u000a      immune network for parameter optimisation in pharmacokinetics.\u000a      Modelling in Medicine and Biology VIII, 277 - 286, WIT\u000a      Press, ISSN 1747-4885, 2009.\u000a      L. Liu, C.-H. Lai, S.-D. Zhou, F. Xie, R. Lu. Two level time-domain\u000a      decomposition based distributed method for numerical solutions of\u000a      pharmacokinetic models. Computers in Biology and Medicine, 41, 221\u000a      - 227, 2011. (doi:10.1016\/j.compbiomed.2011.02.003)\u000a    S.4 DCABES (Distributed Computing and Algorithms for Business,\u000a      Engineering, and Sciences) is a research and industrial network exploring\u000a      modern mathematical tools related to distributed and parallel algorithms\u000a      and their roles in industry. The network was initiated by the University\u000a      of Greenwich and Wuhan University of Science and Technology, Wuhan, China,\u000a      and started long before NAMU was established and has been running since\u000a      2001 when the first international conference was held in Wuhan, China: http:\/\/www.dcabes.org. Lai is now one\u000a      of the two co- chairs of the steering committee of DCABES which looks\u000a      after the annual conference - International Conference on Distributed\u000a      Computing and Applications to Business, Engineering, and Sciences.\u000a      Proceedings of the annual conference are now being published by IEEE.\u000a    S.5 Lai is also a visiting professor at Buckingham University: http:\/\/www.buckingham.ac.uk\/directory\/professor-choi-hong-lai\/;\u000a      Jiangnan University, China; and Fuzhou University: http:\/\/cmcs.fzu.edu.cn\/glwz\/,\u000a      China. He is also an honorary professor at Wuhan University of Science and\u000a      Techology, Wuhan, China. The main role of such visiting professorships is\u000a      to promote joint research collaborations in various areas of numerical\u000a      analysis and scientific computing for industrial related applications.\u000a    S.6 NAMU has close ties with Fuzhou University in the area of applied\u000a      computing and applications. Lai is one of two co-directors of\u000a      Fuzhou-Greenwich Applied Computing R &amp; D Centre (http:\/\/cmcs.fzu.edu.cn\/glwz\/).\u000a      One of the roles that NAMU undertakes is to consolidate collaborations\u000a      with Chinese industry in image processing amongst other areas.\u000a    S.7 Recently NAMU has hosted the visit of Prof. Shidah Mohd-Ali,\u000a      Malaysian Science University to work in the area of domain decomposition\u000a      methods. Lai is nominated by Malaysian Science University as a member of\u000a      their external supervision team for one PhD student who works in domain\u000a      decomposition method.\u000a    S.8 Lai has taken up external supervision of a PhD student at University\u000a      of Hertfordshire in the area of computational finance.\u000a    S.9 The concept of defect correction method and multi-scale problems is\u000a      included in the final year course: Methods of Nonlinear Mathematics\u000a      (Course code MATH1133) which will be used in the training programme\u000a      provided through the FR7 project described in [R6]. \u000a    ","Title":"\u000a    Decomposition, defect correction, and related numerical methods\u000a    ","UKLocation":[{"GeoNamesId":"2636910","Name":"Stirling"}],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Spatially motivated decomposition (1997-2011)\u000a      Key outcomes from the research were formulation and solution of the\u000a      problem using a defect correction equation which produced an iterative\u000a      sequence of corrections. The concept of coupling was encapsulated in a\u000a      simple formulation, thereby giving convergence of the approximate\u000a      solutions sequence to the defect correction equation. This structure forms\u000a      the defect correction framework, which we initially applied to homogeneous\u000a      mathematical problems defined in different subdomains. The importance of\u000a      this framework is the extension to multi-mathematical models and\u000a      multi-scale problems, described later. The work was rated as\u000a      internationally leading by peers, and led to a TSB project [3a] where\u000a      Greenwich is a partner in a consortium led by TWI Ltd which has\u000a      demonstrated the convergence result of the method when applied to\u000a      industrial problems [3.1].\u000a    Key UoG personnel involved: M. Cross, Director of Centre for Numerical\u000a      Modelling and Process Analysis, left to Swansea in 2004; C.-H. Lai, Senior\u000a      Lecturer and later Professor of Numerical Mathematics; K. Pericleous,\u000a      Professor of CFD; A.L. Siahaan, researcher, joined in 2006 and left to\u000a      Cambridge in 2011.\u000a    Projects:\u000a      1) EPSRC GR\/50600 (1997) - Domain Decomposition Methods for Partial\u000a      Differential Equations;\u000a    2) British Council UK-Dutch Joint Scientific Research Grant\u000a      JRP433-AMS\/884\/4 (1997) - Coupling mathematical models.\u000a    Scale motivated decomposition (1999-2012)\u000a      The defect correction concept was used in the derivation of\u000a      multi-mathematical models with disparate scales for application to\u000a      aeroacoustic problems. An EPSRC grant (GR\/M60804) entitled `Computational\u000a      Aeroacoustics' was obtained for the work. At the continuum level the\u000a      formalism follows perturbation methods. At the discretised level the\u000a      defect correction method [3.2] offers a robust way of handling high order\u000a      schemes, large eddy simulations, and turbulence sub-grid modelling using\u000a      existing CFD software. A suitably constructed residual defect correction\u000a      term is obtained from existing CFD software output [3.3]. Further theory\u000a      appeared in the PhD thesis by L. Lai in June 2013.\u000a    Key UoG personnel involved: G.S. Djambazov, researcher and later senior\u000a      research fellow; C.-H. Lai, Senior Lecturer and later Professor of\u000a      Numerical Mathematics; L. Lai, researcher, (2004 - 2007); Z.-K. Wang,\u000a      researcher, (2001 - 2004).\u000a    Projects:\u000a      1) EPSRC GR\/M60804 (1999) - Computational Aeroacoustics;\u000a    2) LMS Workshop Grant EGR:hmc:00-0115010L0103 (2000) - Domain\u000a      Decomposition Methods for Fluid Mechanics;\u000a    3) British Council Alliance Franco-British Partnership Programme PN04.043\u000a      (2002) - Computational Acoustics;\u000a    4) LMS Scheme 8 Grant 81106 (2012) - Workshop on Applied and Numerical\u000a      Methods: Multiscale Problems.\u000a    Temporal motivated decomposition (2001-2012)\u000a      Decomposition of the temporal axis initially aimed to achieve\u000a      parallelisation in the time domain. Members of NAMU began to consider this\u000a      problem in 2001 with work submitted to RAE2008. Extension of the work\u000a      based on various transformation methods led to decoupled parametric\u000a      equations that may be solved concurrently with low complexity coupling at\u000a      the post-computation level. Parallelisation in time is achieved by\u000a      concurrent computation of the parametric equations. Two MSc dissertations\u000a      and two book chapters [3.4], demonstrate the method. Peers rated the work\u000a      as of internationally leading quality. This led to a collaborative project\u000a      in computational finance through FR7 STRIKE [3b] where Greenwich is\u000a      responsible for the development of distributive multi-scale solution\u000a      techniques for nonlinear financial problems in commodity markets.\u000a    Key UoG personnel involved: N. Kokulan, researcher, joined in 2010; C.-H.\u000a      Lai, Senior Lecturer and later Professor of Numerical Mathematics; A.K.\u000a      Parrott, Professor of Computational Science; S. Rout, researcher, (2001 -\u000a      2005).\u000a    "},{"CaseStudyId":"3916","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"3175395","Name":"Italy"},{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"3017382","Name":"France"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    Manufacturing, public services, humanitarian aid - many operations in\u000d\u000a      daily life depend on solving logistical challenges and optimising use of\u000d\u000a      resources. Some problems have been found over decades to be mathematically\u000d\u000a      intractable, not solvable in reasonable computation time. One set of\u000d\u000a      optimisation problems is defined by conditions of volume and value. Known\u000d\u000a      as the `knapsack problem', at its most simple it describes the burglar's\u000d\u000a      dilemma: which valuable things should be put in the loot bag? But it has\u000d\u000a      application wherever maximum return on investment, or getting maximum use\u000d\u000a      from limited resources, is an issue, including investment portfolios,\u000d\u000a      transport, pharmacy, retail etc. This case study describes the\u000d\u000a      breakthroughs that Strusevich and Kellerer have made at the intractable\u000d\u000a      end of knapsack problems. With research of this type, the route to impact\u000d\u000a      on the factory floor or field of operations is not always direct. In this\u000d\u000a      case, the impact lies in making the new knowledge available through\u000d\u000a      peer-reviewed publications and conferences - used by industry and\u000d\u000a      consultancies as well as fellow academics - which stimulates further\u000d\u000a      research and application. Here we outline the evidence that the research\u000d\u000a      is useful.\u000d\u000a    The research focuses on obtaining approximation algorithms and schemes\u000d\u000a      for knapsack and scheduling problems, as well as on determining classes of\u000d\u000a      non-linear knapsack problems that admit strongly polynomial algorithms for\u000d\u000a      their continuous relaxation. It is an example of successful collaboration\u000d\u000a      between Scheduling and Mathematical Programming specialists, with\u000d\u000a      beneficiaries from both these areas. Specialists in Scheduling Theory and\u000d\u000a      its applications will benefit from a unified and efficient framework for\u000d\u000a      handling several classes of hard-to-solve problems. On the other hand, the\u000d\u000a      researchers that specialise in Integer Non-linear Programming will receive\u000d\u000a      approx- imation techniques that the area definitely lacks at the moment.\u000d\u000a      Similar benefits will be provided by the fast algorithms for solving\u000d\u000a      continuous relaxations of non-linear problems. The techniques used for\u000d\u000a      designing the FPTASs will interest researchers in Theoretical Computer\u000d\u000a      Science.\u000d\u000a    The area covered by this case study is vibrant and developing fast, with\u000d\u000a      almost no history prior to 2005. Several established research teams are\u000d\u000a      competing in the area. The team consisting of Profs Strusevich and\u000d\u000a      Kellerer is an initiator and still can be seen as holding the leading\u000d\u000a      position.\u000d\u000a    They started reporting their findings related to this case study in 2005.\u000d\u000a      As an invited speaker, Prof Strusevich made a presentation at the\u000d\u000a      conference of the European Chapter of Combinatorial Optimization in 2005,\u000d\u000a      the main annual forum of European researchers in Combinatorial\u000d\u000a      Optimization. There is a strong interest in our work from applied\u000d\u000a      researchers and practitioners; for example, Prof Kellerer gave an invited\u000d\u000a      talk at the 39th International Conference on Computers and Industrial\u000d\u000a      Engineering in 2009. Besides, the members of the team have delivered\u000d\u000a      several invited talks at research meetings and seminars in the UK, France,\u000d\u000a      USA and Hong Kong.\u000d\u000a    There is strong evidence that the obtained results which link scheduling\u000d\u000a      problems with knapsack models have already had a noticeable impact on\u000d\u000a      Combinatorial Optimization and adjacent areas of research. For example, as\u000d\u000a      shown by the Strusevich-Kellerer research, in order to be able to convert\u000d\u000a      an FPTAS for the quadratic knapsack problem into an FPTAS for the\u000d\u000a      corresponding scheduling problem, the latter must admit a constant ratio\u000d\u000a      approximation algorithm. This fact has stimulated the interest to the\u000d\u000a      development of such approximation algorithms, and several papers by\u000d\u000a      various research teams that include academics from France, Germany, Italy\u000d\u000a      and the Netherlands appeared almost simultaneously in 2008-2010 [5.2, 5.4,\u000d\u000a      5.8, 5.9].\u000d\u000a    Since 2005 we have been reporting on various stages of the development of\u000d\u000a      an FPTAS for a single machine scheduling problem to minimize the weighted\u000d\u000a      sum of the completion times by adapting an FPTAS for the quadratic\u000d\u000a      knapsack problem. Several research teams have contributed towards the\u000d\u000a      design of faster, purpose-built schemes for the problem [5.3, 5.4, 5.6].\u000d\u000a    In 2006 Kellerer and Strusevich published a paper on a single machine\u000d\u000a      problem to minimise the total weighted tardiness with respect to a common\u000d\u000a      due date, where we reduced the problem to a version of a quadratic\u000d\u000a      knapsack problem and designed an FPTAS for its solution. This publication\u000d\u000a      was followed by Karakostas et al [5.6] who extended our approach to the\u000d\u000a      problem with a fixed number of due dates, and by Kacem [5.3], who\u000d\u000a      presented an FPTAS with an improved running time for the initial problem\u000d\u000a      with a single due date.\u000d\u000a    Along with the Strusevich-Kellerer team, academic groups from the\u000d\u000a      Netherlands and China independently use various mathematical programming\u000d\u000a      reformulations (linear and quadratic) for scheduling models with\u000d\u000a      speeding-up resources [5.1, 5.7], evidence of wider interest in this\u000d\u000a      field.\u000d\u000a    In most of the scheduling applications, Strusevich and Kellerer use a\u000d\u000a      version of the SQKP with a convex objective function to develop fast\u000d\u000a      approximation schemes. Recently Z. Xu has proved that in many case the\u000d\u000a      assumption of convexity can be relaxed, since the SKQP in the general form\u000d\u000a      admits an approximation algorithm with a polynomial worst-case ratio.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Many operations in daily life, from manufacturing to running a hospital,\u000d\u000a      need to optimise the return on use of resources where volume and value are\u000d\u000a      conditions. Scheduling theory tackles some of the hardest practical\u000d\u000a      optimisation problems, not known to be solvable in reasonable computation\u000d\u000a      time. Strusevich and Kellerer have been able to reformulate practical\u000d\u000a      scheduling challenges as `knapsack problems' - dealing with volume and\u000d\u000a      value constraints - and then design approximation algorithms which can be\u000d\u000a      applied back to the original challenge. The work has attracted EPSRC\u000d\u000a      funding, stimulated a new field of research which is developing fast, been\u000d\u000a      widely published, led to presentations at international conferences\u000d\u000a      including the 2009 Computers and Industrial Engineering conference\u000d\u000a      attended by industry practitioners and is impacting on Combinatorial\u000d\u000a      Optimisation research.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Greenwich\u000d\u000a    ","Institutions":[{"AlternativeName":"Greenwich (University of)","InstitutionName":"University of Greenwich","PeerGroup":"D","Region":"London","UKPRN":10007146}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a\u0009(REF1 Submitted staff in bold,**REF Output)\u000d\u000a    The work is described in the following publications, predominantly high\u000d\u000a      impact mathematics journals:\u000d\u000a    \u000a3.1 Kellerer, H., &amp; Strusevich, V. A. (2006). A fully\u000d\u000a      polynomial approximation scheme for the single machine weighted total\u000d\u000a      tardiness problem with a common due date. Theoretical Computer Science,\u000d\u000a      369(1-3), 230-238. http:\/\/dx.doi.org\/10.1016\/j.tcs.2006.08.030\u000d\u000a    \u000a\u000a3.2 Kellerer, H., &amp; Strusevich, V. A. (2008). Scheduling\u000d\u000a      parallel dedicated machines with the speeding- up resource. Naval\u000d\u000a        Research Logistics (NRL), 55(5), 377-389.\u000d\u000a      http:\/\/dx.doi.org\/10.1002\/nav.20292\u000d\u000a    \u000a\u000a3.3 Kellerer, H., Kubzin, M. A., &amp; Strusevich, V. A. (2009).\u000d\u000a      Two simple constant ratio approximation algorithms for minimizing the\u000d\u000a      total weighted completion time on a single machine with a fixed\u000d\u000a      non-availability interval. European Journal of Operational Research,\u000d\u000a      199(1), 111-116. http:\/\/dx.doi.org\/10.1016\/j.ejor.2008.11.003\u000d\u000a    \u000a\u000a**3.4 Kellerer, H., Soper, A. J., &amp; Strusevich, V. A.\u000d\u000a      (2010). Transporting Jobs through a Processing Center with Two Parallel\u000d\u000a      Machines, 6508(Chapter 33), 408-422.\u000d\u000a      http:\/\/dx.doi.org\/10.1007\/978-3-642-17458-2_33\u000d\u000a    \u000a\u000a3.5 Kellerer, H., &amp; Strusevich, V. A. (2011). Minimizing\u000d\u000a      Total Weighted Earliness-Tardiness On A Single Machine Around A Small\u000d\u000a      Common Due Date: An Fptas Using Quadratic Knapsack. International\u000d\u000a        Journal of Foundations of Computer Science, 21(03), 357-383.\u000d\u000a      http:\/\/dx.doi.org\/10.1142\/S0129054110007301\u000d\u000a    \u000a\u000a3.6 Kellerer, H., &amp; Strusevich, V. A. (2010). Fully\u000d\u000a      Polynomial Approximation Schemes for a Symmetric Quadratic Knapsack\u000d\u000a      Problem and its Scheduling Applications. Algorithmica, 57(4),\u000a      769-795. http:\/\/dx.doi.org\/10.1007\/s00453-008-9248-1\u000d\u000a    \u000a\u000a3.7 Kacem, I., Kellerer, H., &amp; Strusevich, V. A. (2011).\u000d\u000a      Single machine scheduling with a common due date: total weighted tardiness\u000d\u000a      problems. In A. R. Ahjoub (Ed.), Progress in Combinatorial\u000d\u000a        Optimization (pp. 391-421). Wiley-ISTE.\u000d\u000a    \u000a\u000a3.8 Kellerer, H., &amp; Strusevich, V. A. (2012). The symmetric\u000d\u000a      quadratic knapsack problem: approximation and scheduling applications. 4OR,\u000d\u000a      10(2), 111-161.\u000d\u000a      http:\/\/dx.doi.org\/10.1007\/s10288-011-0180-x\u000d\u000a    \u000a\u000a**3.9 Kellerer, H., Rustogi, K., &amp; Strusevich, V. A.\u000d\u000a      (2012). Approximation schemes for scheduling on a single machine subject\u000d\u000a      to cumulative deterioration and maintenance. Journal of Scheduling,\u000d\u000a      1-9. http:\/\/dx.doi.org\/10.1007\/s10951-012-0287-8\u000d\u000a    \u000a\u000a**3.10 Kellerer, H., Soper, A. J., &amp; Strusevich, V. A.\u000d\u000a      (2013). Preemptive scheduling on two identical parallel machines with a\u000d\u000a      single transporter. Journal of Combinatorial Optimization, 25(2),\u000a      279-307. http:\/\/dx.doi.org\/10.1007\/s10878-012-9511-x\u000d\u000a    \u000a\u000a**3.11 Kellerer, H., &amp; Strusevich, V. A. (2013). Fast\u000d\u000a      approximation schemes for Boolean programming and scheduling problems\u000d\u000a      related to positive convex Half-Product. European Journal of\u000d\u000a        Operational Research, 228(1), 24-32. http:\/\/dx.doi.org\/10.1016\/j.ejor.2012.12.028\u000d\u000a    \u000a\u000a3.12 Strusevich, V. A., &amp; Kellerer, H. (2013). Approximation\u000d\u000a      Schemes for Quadratic Boolean Programming Problems and Their Scheduling\u000d\u000a      Applications. In I. Kucukkoc &amp; N. Mustafee (Eds.), (pp. 73-88). OR55\u000a        Annual Conference - Keynotes and Extended Abstracts, University of\u000d\u000a      Exeter, Exeter, 3-5 September 2013. (Available at:\u000d\u000a      http:\/\/www.researchgate.net\/publication\/256440744_OR55_Keynotes_and_Extended_Abstracts_Book\/file\/504635228f972dfd2b.pdf#page=79)\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000d\u000a    This case study is strongly related to the LANCS initiative that contains\u000d\u000a      a main research cluster on discrete optimisation and non-linear\u000d\u000a      optimisation, with the stress on the connections between these two\u000d\u000a      branches of mathematical programming: see http:\/\/www.lancs-initiative.ac.uk\/\u000d\u000a      .\u000d\u000a    Papers by other research teams, which can be seen as initiated by this\u000d\u000a        case study:\u000d\u000a    5.1 A. Grigoriev, M. Uetz. Scheduling jobs with time-resource tradeoff\u000d\u000a      via nonlinear programming. Discrete Optimization, 2009, 6,\u000d\u000a      414-419.\u000d\u000a    5.2 I. Kacem. Approximation algorithm for the weighted flow-time\u000d\u000a      minimization on a single machine with a fixed non-availability interval. Computers\u000a        &amp; Industrial Engineering, 2008, 54, 401-410.\u000d\u000a    5.3 I. Kacem. Fully polynomial-time approximation scheme for the weighted\u000d\u000a      total tardiness minimization with a common due date. Discrete Applied\u000d\u000a        Mathematics, 2010, 158: 1035-1040.\u000d\u000a    5.4 I. Kacem, C. Chu. Worst-case analysis of the WSPT and MWSPT rules for\u000d\u000a      single machine scheduling with one planned setup period. European\u000d\u000a        Journal of Operational Research, 2008, 187, 1080-1089.\u000d\u000a    5.5 I. Kacem, A.R. Mahjoub. Fully polynomial time approximation scheme\u000d\u000a      for the weighted flow- time minimization on a single machine with a fixed\u000d\u000a      non-availability interval. Computers &amp; Industrial Engineering,\u000d\u000a      2009, 56, 1708-1712.\u000d\u000a    5.6 G. Karakostas, S.G. Kolliopoulos, J. Wang. An FPTAS for the total\u000d\u000a      weighted tardiness problem with a fixed number of distinct due dates.\u000d\u000a        Lecture Notes in Computer Science, 2010, 5609: 238-248\u000d\u000a    5.7 W. Luo, L. Chen, G. Zhang. Approximation algorithms for scheduling\u000d\u000a      with a variable machine maintenance. Lecture Notes in Computer Science,\u000d\u000a      2010, 6124, 209-219.\u000d\u000a    5.8 A. Marchetti-Spaccamela, N. Megow, M. Skutella, L. Stougie. Robust\u000d\u000a      sequencing on a single machine. Matheon Preprint 533, 2008.\u000d\u000a    5.9 N. Megow, J. Verschae. Short note on scheduling on a single machine\u000d\u000a      with one non-availability period. Matheon Preprint 557, 2009.\u000d\u000a    5.10 Z. Xu. A strongly polynomial FPTAS for the symmetric quadratic\u000d\u000a      knapsack problem. European Journal Operational Research, 2012,\u000d\u000a      218: 377-381 \u000d\u000a    ","Title":"\u000d\u000a    Solving Quadratic and Linear Knapsack Problems with Scheduling\u000d\u000a      Applications\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    This work has emerged as a continuation and extension of the studies done\u000d\u000a      by Prof Strusevich at the University of Greenwich, together with\u000d\u000a      Prof Hans Kellerer (Graz, Austria), over the past seven years prior to\u000d\u000a      July 2013. Prof Kellerer co-authored the most complete research monograph\u000d\u000a      on the knapsack problems, published by Springer in 2004.\u000d\u000a    Scheduling Theory is a problem area within Operational Research which\u000d\u000a      studies optimisation problems, normally arising in production and service\u000d\u000a      planning. Because the optimisation problems studied are not known to be\u000d\u000a      solvable in reasonable computation time, one of the main focuses of\u000d\u000a      scheduling research has been that of design and analysis of approximation\u000d\u000a      algorithms. For various scheduling problems, approximation algorithms can\u000d\u000a      be developed based on reformulations of the original problems in terms of\u000d\u000a      Integer Mathematical Programming. The purpose of this research project was\u000d\u000a      to:\u000d\u000a    (1) identify scheduling problems that allow a reformulation in terms of a\u000d\u000a      knapsack problem, including its non-linear versions and extended linear\u000d\u000a      versions;\u000d\u000a    (2) design fixed ratio approximation algorithms and approximation schemes\u000d\u000a      for the relevant knapsack models;\u000d\u000a    (3) adapt the algorithms and schemes developed for the knapsack problem\u000d\u000a      for solving the corresponding scheduling problems.\u000d\u000a    Profs Strusevich and Kellerer have identified a specific form of a\u000d\u000a      knapsack problem to minimize a quadratic non-separable objective function\u000d\u000a      that they have named the Symmetric Quadratic Knapsack Problem (SKQP). They\u000d\u000a      have shown that:\u000d\u000a    (1) the continuous relaxation of the problem can be solved in strongly\u000d\u000a      polynomial time provided that the function is convex;\u000d\u000a    (2) the integer version of the problem admits a constant ratio\u000d\u000a      approximation algorithm based on an appropriate rounding mechanism of the\u000d\u000a      fractional solution;\u000d\u000a    (3) the integer version can be solved in pseudo-polynomial time by\u000d\u000a      dynamic programming (DP);\u000d\u000a    (4) the DP algorithm can be converted into a fully polynomial-time\u000d\u000a      approximation scheme (FPTAS) for the problem, which is the best\u000d\u000a      approximation result that can be achieved.\u000d\u000a    No other form of the quadratic knapsack problem more general than SKQP is\u000d\u000a      known to possess all these properties [3.5, 3.6].\u000d\u000a    Further, they have identified a number of scheduling problems with\u000d\u000a      minisum objective functions which can be formulated in terms of the SQKP\u000d\u000a      [3.1, 3.3, 3.5, 3.6, 3.7, 3.10]. Prior to their study, none of these\u000d\u000a      problems has been known to admit an FPTAS; moreover, for some of them no\u000d\u000a      constant ratio approximation algorithms have been previously reported. The\u000d\u000a      researchers have shown how an FPTAS for the SQKP can be converted into a\u000d\u000a      scheme that behaves as an FPTAS for the corresponding scheduling problem.\u000d\u000a    A survey on this topic was invited by the Editor-in Chief of the journal\u000d\u000a      4OR, A Quarterly Journal of Operations Research, Professor Silvano\u000d\u000a      Martello (Bologna, Italy). The survey written by Profs Kellerer and\u000d\u000a      Strusevich was published in 2012, its journal version is 51 pages long and\u000d\u000a      it also covers approximation and scheduling applications of the\u000d\u000a      Half-Product Problem, which is a relaxation of the SQKP without a linear\u000d\u000a      constraint [3.8].\u000d\u000a    The research described in this case study also includes design of\u000d\u000a      approximation algorithms and schemes for scheduling problems based on the\u000d\u000a      Half-product reformulations (application to scheduling with machine\u000d\u000a      deterioration and maintenance, joint work with a Greenwich PhD student K.\u000d\u000a      Rustogi [3.9]; a number of applications of the convex Half-product with\u000d\u000a      and without the knapsack constraint [3.12], as well as on the use of\u000d\u000a      various versions of the knapsack problem with a linear objective,\u000d\u000a      including the sum-set problem (application to scheduling with\u000d\u000a      transportation, joint work with a Greenwich colleague Dr A. Soper [3.10]),\u000d\u000a      multi-index knapsack and the bicriteria knapsack problem (application to\u000d\u000a      scheduling with speeding-up resources [3.2]).\u000d\u000a    The study on this topic was funded by EPSRC grant \"Quadratic and Linear\u000d\u000a      Knapsack Problems with Scheduling Applications\" with Prof Strusevich as\u000d\u000a      principal investigator and Prof Kellerer as a collaborator. It ran from\u000d\u000a      January 2011 for two years; the value to the University of Greenwich was\u000d\u000a      &#163;23,306. The results have been reported in an invited Optimisation stream\u000d\u000a      key-note presentation at the 55th Conference of the Operational\u000d\u000a      Research Society (Exeter, September 2013) [3.12].\u000d\u000a    "},{"CaseStudyId":"4590","Continent":[],"Country":[],"Funders":["Science and Technology Facilities Council"],"ImpactDetails":"\u000d\u000aIn the area of STEM subjects, the general belief is that the trend of the steadily increasing\u000d\u000aimportance of High Performance Computing will continue for decades. It is likely that the highest\u000d\u000alevels of computing performance will be achieved through distributed hardware resources, such as\u000d\u000acomputer nodes or web-data storage. The performance of cloud computing critically hinges on\u000d\u000aboth the performance of individual nodes and the ability to pass information between them and, to\u000d\u000aa certain extent, on the performance of the network between head node and end user.\u000d\u000aBenchmark testing for supercomputer architectures, such as LINPACK, has been available for\u000d\u000amany years. Generically, these benchmark suites do not differentiate between the relative\u000d\u000aimportance of communication (RAM, inter-core, inter-node, network) or local computational power.\u000d\u000aWe have developed a benchmarking suite, BSMBench, which bridges this gap.\u000d\u000aThe unique feature of our benchmarking tool is that it allows the relative weight of memory\u000d\u000acommunication against single-core computational resource to be changed. This makes BSMBench\u000d\u000aa perfect tool for comparing different computing architectures, which may range from home\u000d\u000acomputers to large cluster installations. This versatility makes BSMBench an attractive tool for a\u000d\u000awide range of end-users. The code BSMBench is the only publicly available suite based on a BSM\u000d\u000aParticle Physics simulation environment. By changing fermion representation and colour number,\u000d\u000aBSMBench offers the unique possibility of changing the relative weight of memory communication\u000d\u000aagainst CPU computations.\u000d\u000aIBM (see source [1]) is using the code suite BSMBench to both assess the performance of new\u000d\u000ageneration machines, including BlueGene\/Q, and evolve future hardware.\u000d\u000aA third-party company, BSMBench Ltd [2,3] (featuring the name of our code suite), was created in\u000d\u000a2012 to promote and make the best use of know-how gained during the development of the code\u000d\u000aBSMBench. Currently, this company is developing general-purpose parallel software based on\u000d\u000aBSMBench that can be used in High Performance Computing applications in the finance,\u000d\u000aaerospace engineering, weather forecasting and oil extraction sectors, among others.\u000d\u000aIn early 2013, Fujitsu [4] started collaborating with the developer of the code suite BSMBench and\u000d\u000athe company BSMBench Ltd to adapt our code suite and create a benchmarking tool on\u000d\u000amainframes used for virtualization. The main idea is to unravel unused resources during a\u000d\u000avirtualization run, and to understand the underpinning restrictions of their usage.\u000d\u000aFujitsu is enhancing the BSMBench code suite by embedding a facility for measuring specific\u000d\u000ahardware metrics, such as the L2\/L3 filling level and the throughput cache.\u000d\u000aBSMBench has been made available to third parties and the general public via GitHub [5]. This has\u000d\u000abeen announced to the computing community in a presentation at the International\u000d\u000aSupercomputing Conference 2012 [6]. BSMBench has also featured in the IT magazine `Linux\u000d\u000aFormat', and been distributed to its readers via more than 20,000 copies on DVD [7,8,9].\u000d\u000aThe HPC-assisted research already carried out by our group was the subject of an INTEL case\u000d\u000astudy in 2010 [10]. This case study was also presented at the 21st meeting of the Machine\u000d\u000aEvaluation Workshop (MEW) organised by STFC in Runcorn, in November 2010. Here, the\u000d\u000aPlymouth HPC installation served as a showcase illustrating how supercomputer environments can\u000d\u000abe efficiently used for complex engineering problems.\u000d\u000a","ImpactSummary":"\u000d\u000aHigh Performance Computing (HPC) is a key element in our research. The Particle Physics Group\u000d\u000ahas accumulated expertise in the development and optimisation of coding paradigms for specific\u000d\u000asupercomputer hardware. Our codes are deployed on supercomputers around the world,\u000d\u000aproducing high-profile research results. We have developed a simulation environment, BSMBench,\u000d\u000athat is, on the one hand, flexible enough to run on major supercomputer platforms and, on the\u000d\u000aother hand, pushes supercomputers to their limits. These codes are used by IBM and Fujitsu\u000d\u000aSiemens for benchmarking their large installations and mainframes. The third party company\u000d\u000aBSMBench Ltd has commercialised the usage of our codes for analysing and optimising HPC\u000d\u000asystems of small and medium-sized enterprises.\u000d\u000a","ImpactType":"Technological","Institution":"\u000d\u000aPlymouth University\u000d\u000a","Institutions":[{"AlternativeName":"Plymouth (University of)","InstitutionName":"Plymouth University","PeerGroup":"C","Region":"South West","UKPRN":10007801}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000aRago and Patella are key developers for the code HiRep, which underpins this impact case study.\u000d\u000aWith the help of HiRep, some key questions in Particle Physics have been answered, and led to\u000d\u000ahigh-profile publications. Below, we list six indicative publications that have made use of HiRep.\u000d\u000aThe authors are listed below in alphabetical order. Bold font indicates authors from our unit of\u000d\u000aassessment. All publications are in international, top-level, and peer reviewed Particle Physics\u000d\u000ajournals. In this sector, the impact factors of high-profile journals communicating original research\u000d\u000aresults range from approximately 3 to 7. According to the classification scheme SPIRES, of the\u000d\u000ahigh-energy physics community, articles that attract 100-249 citations are `very well known papers',\u000d\u000awhile articles cited 50-99 times rank as `well-known papers'.\u000d\u000a\u000a[1] F. Bursa, L. Del Debbio, D. Henty, E. Kerrane, B. Lucini, A. Patella, et al.,\u000d\u000aImproved Lattice Spectroscopy of Minimal Walking Technicolor,\u000d\u000aPhys. Rev. D84 (2011) 034506, 17 citations so far, impact factor of journal: 4.964\u000d\u000a\u000a\u000a[2] B. Lucini, G. Moraitis, A. Patella and A. Rago,\u000d\u000aA Numerical investigation of orientifold planar equivalence for quenched mesons,\u000d\u000aPhys. Rev. D82 (2010) 114510, 5 citations so far, impact factor of journal: 4.964\u000d\u000a\u000a\u000a[3] L. Del Debbio, B. Lucini, A. Patella, C. Pica, A. Rago,\u000d\u000aThe infrared dynamics of Minimal Walking Technicolor,\u000d\u000aPhys. Rev. D82 (2010) 014510, 65 citations so far, impact factor of journal: 4.964\u000d\u000a\u000a\u000a[4] L. Del Debbio, B. Lucini, A. Patella, C. Pica and A. Rago,\u000d\u000aMesonic spectroscopy of Minimal Walking Technicolor,\u000d\u000aPhys. Rev. D82 (2010) 014509, 43 citations so far, impact factor of journal: 4.964\u000d\u000a\u000a\u000a[5] L. Del Debbio, A. Patella and C. Pica,\u000d\u000aHigher representations on the lattice: Numerical simulations. SU(2) with adjoint fermions,\u000d\u000aPhys. Rev. D81 (2010) 094503, 129 citations so far, impact factor of journal: 4.964\u000d\u000a\u000a\u000a[6] L. Del Debbio, B. Lucini, A. Patella, C. Pica, A. Rago,\u000d\u000aConformal versus confining scenario in SU(2) with adjoint fermions,\u000d\u000aPhys. Rev. D80 (2009) 074507, 79 citations so far, impact factor of journal: 4.964\u000d\u000a\u000a","ResearchSubjectAreas":[{"Level1":"2","Level2":"2","Subject":"Atomic, Molecular, Nuclear, Particle and Plasma Physics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"2","Level2":"99","Subject":"Other Physical Sciences"}],"Sources":"\u000d\u000a[1] Emerging Solutions Executive, Computational Science Center, IBM T.J. Watson Research,\u000d\u000aCambridge Research Center, Cambridge, MA USA. E-mail: kjordan@us.ibm.com.\u000d\u000a[2] Company Director, BSMbench Ltd., Dylan Thomas Centre, 1 Somerset Place, Swansea, SA1\u000d\u000a1RR. E-mail: m.heuberger@bsmbench.org.\u000d\u000a[3] http:\/\/www.cdrex.com\/bsmbench-limited-8459844.html\u000d\u000a[4] Director of ARCCA, Cardiff University, Redwood Building (Tower), King Edward VII Avenue,\u000d\u000aCardiff, Wales, CF10 3NB. E-mail: GuestMF@cardiff.ac.uk.\u000d\u000a[5] http:\/\/www.bsmbench.org\u000d\u000a[6] http:\/\/www.isc-events.com\/isc12_ap\/speakerdetails.php?t=speaker&amp;o=10426&amp;a=select&amp;ra=search\u000d\u000a[7] Linux Format, Issue 163, November 2012.\u000d\u000a[8] Editor, Linux Format, 30 Monmouth Street, Bath BA1 2BW, E-mail:\u000d\u000agraham.morrison@futurenet.co.uk.\u000d\u000a[9] Editor, Linux Pro, Via Torino, 51, I-20063 Cernusco sul Naviglio (MI), Italy. E-mail:\u000d\u000amassimilianozagaglia@sprea.it.\u000d\u000a[10] Intel Case Study: Exploring new energy sources and the fundamental structure of matter,\u000d\u000aMachine Evaluation Workshop (MEW), STFC in Runcorn, November 2010.\u000d\u000a\u000d\u000a","Title":"\u000d\u000aBenchmark Testing in High Performance Computing\u000d\u000a","UKLocation":[{"GeoNamesId":"2638960","Name":"Runcorn"},{"GeoNamesId":"2640194","Name":"Plymouth"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000aExplaining the origin of electroweak symmetry breaking is a theoretical problem of utmost\u000d\u000aimportance and a fundamental step in understanding the new data from the Large Hadron Collider\u000d\u000aat CERN that is trying to verify the existence of the Higgs particle. `Technicolor' is the framework\u000d\u000aaccording to which Electroweak symmetry breaking is due to the breaking of the chiral symmetry in\u000d\u000aa new strong interaction. The model proposes a different answer to the origin of mass, by means of\u000d\u000aa new mechanism to generate mass for the leptons. These ideas are inspired by the fact that a\u000d\u000asimilar mechanism is already at work in the theory of the strong interactions, i.e. Quantum\u000d\u000aChromodynamics (QCD). A fundamental requirement for any theory Beyond the Standard Model is\u000d\u000athat the framework does not spoil any lower energy prediction, i.e., that it is compatible with current\u000d\u000aobservations. This is a severe constraint, which in Technicolor is implemented by the mechanism\u000d\u000aof walking, i.e., the slow running of the gauge coupling in an intermediate range of energies. This\u000d\u000ahappens for near-conformal gauge theories. The question then becomes: is there a near-conformal\u000d\u000agauge theory that can account for the observed electroweak symmetry breaking?\u000d\u000aNumerical simulations of the theory, regularized on a space-time lattice, performed using Monte\u000d\u000aCarlo techniques, are the best tools for quantitative calculations. Given the experience of similar\u000d\u000acalculations in QCD accumulated over the last 30 years, it has been realised that this type of\u000d\u000aprogram would require a flexible, scalable code with the ability to run on state-of-the-art\u000d\u000asupercomputers.\u000d\u000aDr Antonio Rago and Dr Agostino Patella have been instrumental, as part of the collaborative work\u000d\u000aof particle physics groups at Plymouth University, University of Swansea, University of Edinburgh,\u000d\u000aCERN, and CP3 in Odense, in creating the innovative simulation suite HiRep to study a large class\u000d\u000aof gauge theories that inform particle physics experiments. The programs are written in standard C,\u000d\u000aand are parallelized using standard MPI and OpenMP libraries. The code is portable on a variety of\u000d\u000aarchitectures, and has been tested and used on several machines, from high-performance\u000d\u000aPC clusters to IBM BlueGene\/L and P, and the latest evolution BlueGene\/Q. Plymouth University\u000d\u000ahas a strong footprint in this project, since two of three core developers of the HiRep suite (Rago,\u000d\u000aPatella) are based at the university. Six indicative publications that made use of HiRep are listed in\u000d\u000athe next section.\u000d\u000aAt the moment, HiRep is the only code available in the particle physics community that simulates\u000d\u000agauge theories with an arbitrary number of colours and different fermion representation, making it\u000d\u000athe most versatile code to investigate the possibility of finding New Physics as an extension of the\u000d\u000aStandard Model. These numerical studies require extremely demanding calculations, carried out\u000d\u000aon the most powerful computers. We have worked in close collaboration with IBM to optimise our\u000d\u000acode for new generation machines and the evolution of future hardware. For these purposes, we\u000d\u000ahave developed a portable benchmarking suite, BSMBench, based on HiRep.\u000d\u000a"},{"CaseStudyId":"4591","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000a    The research carried out in the area of statistical disclosure control\u000a      has led to significant changes in the dissemination of micro data in\u000a      official statistics. The seminal paper by Franconi and Stander [3] and\u000a      subsequent work in the area of model-based micro-data protection (see [4])\u000a      have successfully resulted in the release of micro-data for research\u000a      purposes, with respect to the Italian sample of the Structure of\u000a        Earning survey. This survey, leading to a linked employer-employee\u000a      database harmonized at European level, is now going to reach its third\u000a      wave of release. Economists have carried out several studies, using the\u000a      micro data file for research distributed at national level by Istat, and\u000a      at EU level by Eurostat.\u000a    The ideas contained in another paper, published by researchers from\u000a      Plymouth University in the field of statistical disclosure control,\u000a      Burridge (2003) [2], have opened the field to the use of perturbation\u000a      methods showing sound statistical properties by construction. Again, a new\u000a      micro data file for research purposes on the system of account will be\u000a      released in the near future, stemming from the work carried out at\u000a      Plymouth University. Moreover, the IPSO method (Information Preserving\u000a      Statistical Obfuscation) has also been implemented in the mu-Argus\u000a      software (mu-Argus, 2013), which has been designed to create files of\u000a      individual data to be released either for research purposes or for the\u000a      public (1). Many national statistical offices in Europe and statistical\u000a      agencies around the world use this software. Some of the developments\u000a      contained in Polettini and Stander (2005), leading to an alternative\u000a      evaluation and a related accurate approximation of the risk of\u000a      re-identification, have also been implemented in the mu- Argus software.\u000a    Given a good understanding of administrative data systems, we have been\u000a      able to contribute to problem solving within the UK. For example, 13.9% of\u000a      the total primary care budget is directed to mental health, a total of &#163;8\u000a      billion. The indicative allocation of several billion pounds of public\u000a      money to GP practices, to provide services for mental health, is informed\u000a      by what the NHS refers to as the `Plymouth Model'. In the user guide [page\u000a      10](DoH, 2009):\u000a    `For mental health, the toolkit includes an entirely new methodology\u000a      developed by Plymouth University specifically for practice based\u000a      commissioning. This new approach moves away from modelling historic\u000a      utilisation and estimates need directly based on different person types'\u000a      (2).\u000a    In order to do this, we have required state-of-the-art epidemiological\u000a      models applied to the Health Survey for England, working on case mix\u000a      classifications as well as a statistical reconstruction of the census\u000a      data, known to be subject to statistical disclosure control. Quoting the\u000a      NHS (DoH, 2009):\u000a    `The new methodology has undergone extensive testing by the researchers\u000a      and DoH and we believe it provides a step-change improvement in the way we\u000a      model mental health need' (2).\u000a    This work has been reported to select committees in parliament and\u000a      recognised by MPs, all of whom have noted the size of GP commissioning\u000a      groups (3). It has also been fully considered by the National Audit\u000a      Office, influencing the way public sector funding may evolve in the future\u000a      (4). Finally, our expertise with national-scale small-area estimates has\u000a      had further impact on work carried out for the 2011 Skills for Life\u000a      Survey, which has been made available by the Department for Business,\u000a      Innovation and Skills (5).\u000a    ","ImpactSummary":"\u000a    Small area estimation (SAE) describes the use of Bayesian modelling of\u000a      survey and administrative data in order to provide estimates of survey\u000a      responses at a much finer level than is possible from the survey alone.\u000a      Over the recent past, academic publications have mostly targeted the\u000a      development of the methodology for SAE using small-scale examples. Only\u000a      predictions on the basis of realistically sized samples have the potential\u000a      to impact on governance and our contribution is to fill a niche by\u000a      delivering such SAEs on a national scale through the use of a scaling\u000a      method. The impact case study concerns the use of these small area\u000a      predictions to develop disease-level predictions for some 8,000 GPs in\u000a      England and so to produce a funding formula for use in primary care that\u000a      has informed the allocation of billions of pounds of NHS money. The value\u000a      of the model has been recognised in NHS guidelines. The methodology has\u000a      begun to have impact in other areas, including the BIS `Skills for Life'\u000a      survey.\u000a    ","ImpactType":"Societal","Institution":"\u000a    Plymouth University\u000a    ","Institutions":[{"AlternativeName":"Plymouth (University of)","InstitutionName":"Plymouth University","PeerGroup":"C","Region":"South West","UKPRN":10007801}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] Hewson P.J. and T.C. Bailey (2010): Modelling multivariate\u000a      disease rates with a latent structure mixture model, Statistical\u000a        Modelling 10(3): 241-164.\u000a    \u000a\u000a[2] Burridge, J. (2003): Information preserving statistical\u000a      obfuscation, Statistics and Computing 13:321-327.\u000a    \u000a\u000a[3] Franconi, L. and Stander, J. (2002): A model based method for\u000a      disclosure limitation of business microdata, Journal of the Royal\u000a        Statistical Society, Series D 51: 51-61.\u000a    \u000a\u000a[4] Polettini, S., Franconi, L. and Stander, J. (2002): Model\u000a      based disclosure protection. In Domingo-Ferrer, J. (Ed.) Inference\u000a        Control in Statistical Databases: from Theory to Practice. Berlin:\u000a      Springer-Verlag, pp. 83-96. (Peer reviewed book chapter)\u000a    \u000a\u000a[5] Asthana, S., Gibson, A., Bailey, T., Dibben, C., Hewson, P.,\u000a      Economou, T., Batchelor, D., Eastham, J., Craig, R., Scholes, S., Flowers,\u000a      J., Jenner, D. Person (2008): Based Resource Allocation (PBRA): The\u000a        Feasibility of Developing a Need-Based Approach to PBRA. Report to\u000a      the Department of Health (Policy Research Programme). University of\u000a      Plymouth. 118pp.\u000a    \u000a\u000a[6] S.Asthana, A. Gibson, P. Hewson, T. Bailey, C. Dibben (2011):\u000a      General practitioner commissioning consortia and budgetary risk: evidence\u000a      from the modelling of fair shares practice budgets for mental health', Journal\u000a        of Health Services Research &amp; Policy 16:95-101.\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000a    (1) Evidence of the impact of work on Disclosure control: Mu-Argus\u000a      Software http:\/\/neon.vb.cbs.nl\/casc\/..%5Ccasc%5Cmu.htm;\u000a      the manual is available at http:\/\/neon.vb.cbs.nl\/casc\/..%5Ccascprivate%5Cdeliverables%5CMUManual4.3.pdf),\u000a    (2) Evidence of the impact of the work on funding GPs for mental health\u000a      provision:\u000a    Department of Health (2009) Practice Based Commissioning: budget guidance\u000a      for 2009\/10: Methodological changes and toolkit guide DH (see\u000a      http:\/\/www.dh.gov.uk\/prod_consum_dh\/groups\/dh_digitalassets\/documents\/digitalasset\/dh\u000a      094392.pdf\u000a    (3) Asthana, S., Gibson, A. (2010), Funding Implications for Rural PCTs\u000a      of the new NHS Resource Allocation Methodology, in All Party Parliamentary\u000a      Group on Rural Services, The implications of national funding formulae for\u000a      rural health and education provision, Report, Written and Oral evidence.\u000a      London, House of Commons.\u000a    (4) National Audit Office (2011) `Cross-government landscape review\u000a      Formula funding of local public services ` REPORT BY THE COMPTROLLER AND\u000a      AUDITOR GENERAL\u000a    HC 1090 SESSION 2010-2012\u000a    (5) Small area estimation applied to skills for life\u000a    BIS Research Paper Number 81C (2012) 2011 Skills for Life Survey: Small\u000a      Area Estimation Technical Report, Department of Business, Innovation and\u000a      Skills \u000a    ","Title":"\u000a    Bayesian methods for large scale small area estimation (SAE).\u000a    ","UKLocation":[{"GeoNamesId":"2640194","Name":"Plymouth"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The Statistics Group at Plymouth specialises in Bayesian modelling\u000a      methods, with expertise in applying the necessary computationally\u000a      intensive methods, using modern High Performance Computing (HPC) where\u000a      necessary. Our work on survey methods, spatial statistics, and disclosure\u000a      control methods gives us a strong understanding of the sources and\u000a      structures of data that underpin our SAE work. Our applied modelling\u000a      interests have led to the development of a methodology for a range of\u000a      response types, including new frameworks for modelling multinomial\u000a      response and multivariate response types by means of latent structure\u000a      models, quantile regression methods, and spatial methods. This enables a\u000a      wide range of applications in such areas as disease mapping and financial\u000a      analysis. One early example of such modelling work was applied to cancer\u000a      mapping [Ref.1]. The underlying methodology has continuously been\u000a      developed at Plymouth within the Statistics group and has been applied to\u000a      other problems. Other established work by statisticians at Plymouth\u000a      concerns disclosure control. Administrative data are only released with\u000a      limitations on the ability to deduce details about individuals. Hence,\u000a      small area predictions require the reconstruction of the multiway tables\u000a      underlying a number of two and three way tables that have been released\u000a      under the limits for disclosure control. Our works in this area\u000a      (particularly [4]) underpin this reconstruction and therefore are key for\u000a      the estimation.\u000a    Our SAE work is exceptional and distinctive in that:\u000a    \u000a      We fit relatively complex models to survey data (which may consist of\u000a        several thousand person types), and can thus obtain a more sensitive\u000a        understanding of the risk surface [1],\u000a      Our understanding of disclosure control methods enables us to derive\u000a        usable information on these person types based on released\u000a        administrative data [2,3,4],\u000a      We simulate predictive distributions for the risk of disease for these\u000a        person types in 6,781 middle-level super-output areas in the whole of\u000a        England,\u000a      And we then re-apportion such estimates to some 8,000 GP practices.\u000a        [5,6]\u000a    \u000a    This is a computationally demanding task. There is a growing literature\u000a      on SAE but, without our ability to apply it to the whole of England, these\u000a      methods would not be trusted to inform decisions on a national scale. In\u000a      SAE, our work out-performs that of others. The sheer volume of\u000a      calculations requires the use of High Performance Computing to throughput\u000a      all the calculations. Given fine- grained predictions, we can re-aggregate\u000a      to whatever geography is of interest, which currently enables us to study\u000a      access to healthcare. In order to produce predictive distributions for\u000a      small areas, the necessary data are not easily available for data\u000a      protection reasons. For example, we require not only posterior\u000a      distributions from survey models but auxiliary data (individual level\u000a      data) on the 6,781 middle-level census output areas for the varied\u000a      demographic information that matches the census and other administrative\u000a      data with the survey data used in the modelling. For disclosure control\u000a      reasons, such data are not published. The research leading to impact here\u000a      involves the reconstruction of multiway tables for a number of two- and\u000a      three-way tables that have been released (with adjustments to limit\u000a      disclosure control). The underpinning work in this area (see [4])\u000a      therefore is a key part of the estimation process, enabling us to quantify\u000a      uncertainty in the auxiliary variables.\u000a    "},{"CaseStudyId":"4905","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The impact is economic: enhanced design capabilities based on the\u000a      research have resulted in a\u000a      superior product and substantial time and cost savings. The beneficiary is\u000a      Rolls-Royce, a world\u000a      leader in the design and manufacture of gas turbine engines for aircraft,\u000a      ships, power generation\u000a      and other applications. Rolls-Royce has been hailed as a star of the\u000a      manufacturing sector by the\u000a      UK Government, bucking the trend of many of its peers by achieving a total\u000a      revenue of over &#163;12.2\u000a      bn and record profits of &#163;1.4 bn in 2012 [A], with 85% of its sales abroad\u000a      [B] bringing valuable\u000a      income to the country, as well as providing employment for almost 20,000\u000a      in the UK alone [C].\u000a    From research to impact. Rolls-Royce uses CFD codes to simulate\u000a      the flow of fluids in and\u000a      around all products, including the flow of air through all components of\u000a      diesel or gas turbine\u000a      engines and their installations [D]. In 2004, Rolls-Royce received from\u000a      Oxford the first production\u000a      version of the HYDRA CFD code for testing. In 2006 it was established as\u000a      the company's\u000a      compressor design tool, and by 2009 it had become the design tool for\u000a      multiple businesses across\u000a      Rolls-Royce &#8212; including gas turbines, air and thermal systems, and power\u000a      generation [E]. It is one\u000a      of the few codes that the company uses for CFD [D].\u000a    The many uses of HYDRA within Rolls Royce are illustrated in this\u000a      diagram, taken from [F] and\u000a      used with permission.\u000a\u000a\u000a\u000a    The ability to simulate the flow of air through the engines is crucial to\u000a      Rolls-Royce's design\u000a      capability, as engines are now designed almost exclusively through\u000a      computer simulation with\u000a      experimental testing carried out afterwards to verify the performance of\u000a      the final design. Rolls-Royce uses HYDRA in various ways: to assess the\u000a      aerodynamic efficiency of a design; to assess\u000a      the unsteady aerodynamic forces acting on blades due to the passing of\u000a      neighbouring blade rows;\u000a      to assess the possibility of self-induced vibrations; and to quantify the\u000a      heat transfer from the very\u000a      hot gases coming out of the combustor into the high pressure turbine\u000a      blades [F].\u000a    HYDRA offers Rolls-Royce unprecedented accuracy because of its ability to\u000a      deal with shocks, and\u000a      the company reports that it is now able to rank designs to better than 1%\u000a      efficiency, consistently\u000a      more impressive than rival packages such as FLUENT. As an example, it\u000a      reports that calculated\u000a      loss coefficients for industrial exhaust systems now differ from measured\u000a      values by just 0.02%,\u000a      compared to 0.16% when calculated using FLUENT [E].\u000a    Nature and extent of the impact. Of the many impacts of HYDRA to\u000a      Rolls-Royce, the greatest\u000a      has been on the design of its gas turbine engines for aircraft. The\u000a      `soaring demand for more fuel-efficient engines for planes' [G] helped the\u000a      civil aerospace arm of the company deliver a 16%\u000a      increase in annual revenue to over &#163;6.4 billion in 2012 [A, G] and with\u000a      Trent engines, which are\u000a      designed using HYDRA, making up around 75% of all orders [A]. Furthermore,\u000a      `Rolls-Royce's\u000a      order book rose 4% to &#163;60.1bn thanks to strong demand for its Trent\u000a      aircraft engines' [B].\u000a    HYDRA has given Rolls-Royce a single tool for aerodynamic, aero-acoustic\u000a      and aero-elastic\u000a      applications. Its novel multi-grid solvers allows the company to\u000a      efficiently analyse complex engine\u000a      shapes which were previously difficult to assess. Furthermore, the\u000a      parallelisation of the software\u000a      has cut analysis time, and Rolls-Royce attributes the decrease in design\u000a      time for an intermediate\u000a      turbine test rig, from [text removed for publication], to the use\u000a      of HYDRA's adjoint CFD code [E].\u000a      In total, the code has helped save Rolls-Royce at least [text removed\u000a        for publication], in test rig\u000a      expenses [E]. Rolls-Royce technological development webpage openly credits\u000a      HYDRA as one of\u000a      the key pieces of technology that make up The Rolls-Royce Engineering\u000a      System [D]; for example,\u000a      \"HYDRA has been used extensively in the design of recent RR products such\u000a      as the Trent 1000.\"\u000a    The code was used to design Rolls-Royce's Trent 1000 series of engines,\u000a      which power Boeing 787\u000a      aeroplanes. By way of illustration of the benefits of improved accuracy\u000a      and design capability,\u000a      improvements to the latest iteration of the Trent 1000, unveiled in 2012,\u000a      over its predecessors\u000a      include: [text removed for publication] more efficient\u000a      intermediate pressure compressor; [text\u000a        removed for publication] more efficient intermediate pressure\u000a      turbine; shortened Boeing flight\u000a      testbed schedule thanks to design being ahead of time; and a fan assessed\u000a      as having \"world class\u000a      performance\" in a Boeing Audit [E]. The Trent 1000 is also the quietest\u000a      mode of powering the 787\u000a      &#8212; some 6 dB quieter than its competitors [C].\u000a    The newer Trent XWB, announced in 2007 and then first flown in 2012,\u000a      powers the new Airbus\u000a      A350 XWB and was also designed using HYDRA. As of May 2012, it was\u000a      Rolls-Royce's fastest-selling engine to date, having achieved 1,100 orders\u000a      from 34 customers worldwide [C]. Once\u000a      again, HYDRA contributed to the improvement of its design, with its high\u000a      pressure compressor\u000a      seeing an improvement in efficiency of [text removed for publication],\u000a      and its intermediate\u000a      pressure compressor seeing a [text removed for publication]\u000a      improvement, both over the Trent\u000a      1000. All told, the Trent XWB is [text removed for publication]\u000a      more efficient than the first\u000a      generation Trent engines of 1995, making it the most efficient Trent\u000a      engine to date.\u000a    ","ImpactSummary":"\u000a    Rolls-Royce uses the HYDRA computational fluid dynamics (CFD) code for\u000a      the design of all of its\u000a      new gas turbine engines. The HYDRA CFD package, including the mathematical\u000a      theory behind it,\u000a      was developed by Professor Mike Giles and his research team in the period\u000a      1998-2004 at the\u000a      University of Oxford, and subsequently transferred to Rolls-Royce, forming\u000a      the basis of the RR\u000a      corporate CFD strategy with an investment of over 100 person years in\u000a      development.\u000a    Since 2009, HYDRA has become the standard aerodynamic design tool across\u000a      Rolls-Royce, and\u000a      has been used to design Rolls-Royce's Trent 1000 engine and the newer\u000a      Trent XWB. HYDRA has\u000a      enabled Rolls-Royce to save over [text removed for publication] in\u000a      test rig expenses, provides\u000a      superior accuracy compared to its competitors such as FLUENT, and has\u000a      contributed to increases\u000a      in engine efficiency of up to [text removed for publication],\u000a      which in turn has led to higher sales\u000a      and increased revenue for Rolls-Royce.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Oxford\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] P.I. Crumpton and M.B. Giles. `Multigrid aircraft computations using\u000a      the OPlus parallel\u000a      library'. pp.339-346 in Parallel Computational Fluid Dynamics:\u000a      Implementations and Results\u000a      Using Parallel Computers, A. Ecer, J. Periaux, N. Satofuka, and S. Taylor,\u000a      editors, North-Holland, 1996. DOI: 10.1.1.48.9819.\u000a    \u000aKey paper on OPlus parallel framework on which HYDRA is built.\u000a    \u000a* [2] M.B. Giles, M.C. Duta, J.-D. Muller and N.A. Pierce. `Algorithm\u000a      developments for discrete\u000a      adjoint methods'. AIAA Journal, 41(2):198-205, 2003. DOI: 10.1.1.10.262.\u000a    Key paper, in international journal, on adjoint algorithms in HYDRA;59\u000a        Citations (Web of\u000a        Science), 108 citations (Google Scholar).\u000a    \u000a\u000a* [3] M.S. Campobasso and M.B. Giles. `Stabilization of a linear flow\u000a      solver for turbomachinery\u000a      aeroelasticity by means of the recursive projection method', AIAA Journal,\u000a      42(9) 1765-1774,\u000a      2004. DOI: 10.1007\/978-3-540-74460-3_24.\u000a    Key paper, in international journal, on linearisation problem; 13\u000a      citations (Web of Science),\u000a      23 citations (Google Scholar).\u000a    \u000a\u000a* [4] M.B. Giles and NA Pierce. `An introduction to the adjoint approach\u000a      to design', Flow,\u000a      Turbulence and Combustion, 65(3-4):393-415, 2000. DOI: 10.1.1.135.6053.\u000a    Overview paper in international journal; 130 citations (Web of\u000a        Science), 280 citations\u000a        (Google Scholar).\u000a    \u000aThe three asterisked outputs best indicate the quality of the\u000a      underpinning research. All these\u000a      papers report research performed exclusively at the University of Oxford.\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"}],"Sources":"\u000a    [A] http:\/\/www.rolls-royce.com\/investors\/financial_reporting\/financial_results\/index.jsp\u000a    [B] http:\/\/www.guardian.co.uk\/business\/2011\/nov\/11\/rolls-royce-engine-recovery-economy\u000a    [C] http:\/\/www.rolls-royce.com\/Images\/RR_full_AR_2011_tcm92-34435.pdf\u000a    [D] http:\/\/www.rolls-royce.com\/about\/technology\/systems_tech\/design_systems_tools.jsp\u000a    [E] A Brief History of HYDRA, Rolls-Royce internal presentation, supplied\u000a      by Chief Design\u000a      Systems Architect at Rolls-Royce (who can be contacted), showing the\u000a      significance of the\u000a      impact of Hydra at Rolls-Royce. Copy held by Oxford University.\u000a    [F] Rolls-Royce presentation at ICFD meeting, reading University, 2008,\u000a      www.icfd.rdg.ac.uk\/ICFD25\/Talks\/LLapworth.pdf\u000a    [G] http:\/\/uk.reuters.com\/article\/2013\/02\/14\/uk-rolls-royce-idUKBRE91D0B720130214?feedType=RSS&amp;feedName=businessNews\u000a    [C]-[F] give data about Hydra and its use at Rolls-Royce. [A], [B] &amp;\u000a      [G] give evidence of the\u000a      economic success of Rolls-Royce engines designed using Hydra.\u000a    \u000a    ","Title":"\u000a    Computational fluid dynamics: the Rolls-Royce HYDRA code for jet engine\u000a      design\u000a    ","UKLocation":[{"GeoNamesId":"2640729","Name":"Oxford"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    In 1993, Professor Mike Giles of the University of Oxford established the\u000a      Rolls-Royce University\u000a      Technology Centre (UTC) in Computational Fluid Dynamics. Part of the\u000a      Numerical Analysis Group,\u000a      it was created to investigate and develop mathematical and computational\u000a      techniques for use in\u000a      the analysis and design of turbo-machinery.\u000a    The development of HYDRA, a programme of work at the University of Oxford\u000a      running from 1998\u000a      to 2004, was led by Professor Mike Giles and was sponsored by Rolls Royce\u000a      with supporting\u000a      funding from EPSRC. Based on identified Rolls Royce requirements, Giles\u000a      set four key goals for\u000a      his team: that they should use efficient multi-grid iterative solvers on\u000a      complex unstructured grids to\u000a      model problems with complex engine geometries; that they should enable the\u000a      efficient computation\u000a      of linear and nonlinear unsteadiness due to blade flutter and forced\u000a      response; that they should use\u000a      adjoint design techniques to improve the speed of design optimisation; and\u000a      that the whole\u000a      computation should execute very efficiently on distributed-memory parallel\u000a      clusters.\u000a    Turbomachinery flows have hydrodynamic shocks which can be modelled\u000a      mathematically as\u000a      discontinuities in the flow properties. In prior work, Giles was the first\u000a      to show that linear\u000a      perturbation methods could be used to analyse the effect of shock\u000a      oscillations in inviscid flows.\u000a      This was an important background result for the development of HYDRA at\u000a      the University of\u000a      Oxford, but new research was required to develop efficient multi-grid\u000a      solvers for linear unsteady\u000a      viscous flows [3].\u000a    Giles and his team also developed new adjoint techniques [2,4] to improve\u000a      the efficiency of\u000a      optimisation calculations. The team moved away from existing adjoint\u000a      techniques, instead\u000a      developing their own so-called \"discrete\" approach in which the nonlinear\u000a      discretisation is\u000a      linearised and then the transposed matrix defines the discrete adjoint\u000a      equations. While other\u000a      research groups were also working on the subject, HYDRA research pioneered\u000a      many of the\u000a      developments in the area, including the use of Automatic Differentiation\u000a      software to construct\u000a      discrete adjoint equations.\u000a    Taken as a whole, these advances were combined by Giles' team into a\u000a      complete CFD package\u000a      called HYDRA which offered:\u000a    \u000a      use of complex unstructured grids composed of a mix of different\u000a        element types, to give\u000a        maximum geometric flexibility to handle complex turbomachinery\u000a        geometries, including tip\u000a        gaps, disk cavities, cooling slots, and internal cooling passages;\u000a      an efficient multigrid solver for time-averaged steady flow\u000a        calculation, and for solving the\u000a        implicit nonlinear system of equations which comes from approximating\u000a        nonlinear unsteady\u000a        flow calculations;\u000a      the ability to analyse linearised harmonic unsteady flow perturbations\u000a        for both forced\u000a        response and flutter analysis;\u000a      an \"adjoint\" design capability to efficiently compute the sensitivity\u000a        of output quantities, such\u000a        as engine efficiency, to changes in any one of possibly hundreds of\u000a        design variables.\u000a    \u000a    The parallelisation aspects were handled by building on preparatory\u000a      research undertaken between\u000a      1993 and 1998. This made it possible to hide the parallelism, from both\u000a      the HYDRA CFD users\u000a      and crucially the HYDRA developers in Oxford, allowing them to concentrate\u000a      their efforts on\u000a      developing new features within HYDRA [1]. This was a forerunner of modern\u000a      high-level abstraction\u000a      techniques which are an active research topic today in computer science\u000a      addressing the\u000a      challenges of many-core computing.\u000a    Key researchers from the University of Oxford:\u000a      Mike Giles: Reader (1992-1997), Professor (1997-present); Paul Crumpton:\u000a      PDRA (1993-1997);\u000a      Niles Pierce: PDRA (1997-1998); Mihai Duta: PDRA (2002-2005); Jens Muller:\u000a      PDRA (1997-2002)\u000a    "},{"CaseStudyId":"4906","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The research has given rise to significant economic and environmental\u000a      impact, benefiting a significant percentage of the major (so-called \"Tier\u000a      1\") banks, the software industry that supports the banks, and reducing\u000a      significantly the energy consumption per Monte Carlo simulation performed\u000a      by financial institutions.\u000a    From research to impact\u000a    The largest investment banks in London each have thousands of servers\u000a      largely devoted to Monte Carlo simulations, and to quantify their risks\u000a      (the amount of money they could potentially lose due to uncertain and\u000a      unpredictable events) and satisfy regulatory demands they need to be able\u000a      to calculate huge numbers of sensitivities efficiently. The \"Smoking\u000a      adjoints\" paper [2] was recognised immediately as a significant advance on\u000a      the state-of-the-art. In January 2007 the paper was voted by the finance\u000a      industry readers of RISK as the best paper of 2006, with Mike\u000a      Giles and Paul Glasserman being jointly named \"Quant of the Year\" by the\u000a      magazine [A]. Quant of the Year is \"the award practical quants most care\u000a      about\", according to Laurie Carver of RISK magazine who leads the\u000a      survey of \"authors, referees and other industry and academic quants\" to\u000a      decide the winner each year. This indicates the broad importance of this\u000a      work to the finance industry, as does the fact that Giles has been asked\u000a      to give a one-day course on \"Adjoint Methods for Option Pricing\" at Global\u000a      Derivatives Trading and Risk Management 2013, the leading international\u000a      conference for the industry.\u000a    Nature and extent of the impact\u000a    One bank, Credit Suisse, was very quick to adopt the approach and\u000a      published two papers on the subject. One in RISK in 2011 entitled `Real\u000a      Time Counterparty Credit Risk Management in Monte Carlo' [B] builds on the\u000a      work of [2-6] and states \"Adjoint algorithmic differentiation can be\u000a        used to implement efficiently the calculation of counterparty credit\u000a        risk. We demonstrate how this powerful technique can be used to reduce\u000a        the computational cost by hundreds of times, thus opening the way\u000a        to real time risk management in Monte Carlo.\" In 2008, Credit Suisse\u000a      filed a US patent application (pending) [C] \"to protect our use of it\"\u000a      and informed us that the adjoint technique is \"very well appreciated\u000a        within the company because of its benefits to better risk management and\u000a        enhanced profitability through improved hedging\" and a Director at\u000a      Credit Suisse Group states that [D] the impact is far reaching as \"it's\u000a        clear that many banks are now using the methodology\". Support for\u000a      the broad impact of the \"Smoking adjoints\" paper [2] comes from the\u000a      Numerical Algorithms Group (NAG, http:\/\/www.nag.co.uk\/),\u000a      a company which develops mathematical libraries and other software. The\u000a      Vice-President of Sales at NAG talks extensively to the banks. His\u000a      supporting letter [E] states that \"possibly 20% of the Tier 1 banks are\u000a        using adjoints on a daily basis as part of their routine option pricing\u000a        processes, and most of the others have tested and are developing the\u000a        technology\". He further comments [E] that leading quants have stated\u000a      that \"because of regulatory and internal risk management concerns, many\u000a        more Greeks are being required for a wide range of other pricing\u000a        applications as well, and this is driving the growth in the use of\u000a        adjoints, to minimise the computational costs of generating all of these\".\u000a      NAG currently provide consultancy services to banks to help them generate\u000a      adjoint codes, and in response to requests from banks have also developed\u000a      adjoint versions of some of their key mathematical library routines using\u000a      their DCO software which was released in 2010. The Vice-President of Sales\u000a      at NAG states [E] \"We see this as a growth area for us and we plan\u000a        further investment next year.\"\u000a    Ingo Schneider, the head of financial engineering at Dekabank, wrote a\u000a      paper in RISK in 2009 with two academic collaborators on \"Fast\u000a      Monte Carlo Bermudan Greeks\" [F]. The abstract of the paper describes it\u000a      as extending \"the pioneering work of Giles &amp; Glasserman (2006)\".\u000a      The method was described in RISK's regular technical section [G]\u000a      and commented upon by an anonymous credit derivatives trader at a European\u000a      bank: \"It means you can do far more simulations, for far more risk\u000a        parameters, with far less computational cost. If you want to do it in\u000a        real time &#8212; and with the speed news travels now there's really no other\u000a        option &#8212; there's not really another way to do it.\"\u000a    Credit Suisse confirm that when used in credit valuation adjustment\u000a      (CVA), for example, the saving in computing costs is considerable. \"We're\u000a        talking orders of magnitude ... Something that would have taken hours or\u000a        overnight by bumping can be calculated hundreds of times quicker, and\u000a        you can manage the CVA intra-day, in real time.\" [H].\u000a    As indicated by Credit Suisse, the computational benefits of the adjoint\u000a      approach can be very substantial. For Correlation Greeks, and Greeks for\u000a      Fixed Income products which depend on a large set of future interest\u000a      rates, the savings can be up to a factor of 100 compared with the old\u000a      approach (known as `bumping') of simply perturbing each input parameter\u000a      individually and re-simulating to find the consequential change in the\u000a      value. In practical terms, this means that banks can either reduce their\u000a      computing costs and energy consumption, or increase the number of\u000a      simulations they can afford to perform.\u000a    The scale of the calculations by the banking sector is indicated by the\u000a      table below which highlights 8 of the world's top 500 supercomputers [I]\u000a      that are in the UK and are likely to be performing financial Monte Carlo\u000a      simulations.\u000a    \u000a      \u000a        \u000a          rank\u000a          company\u000a          size\u000a        \u000a        \u000a          #185\u000a          IT service provider\u000a          17,280 cores\u000a        \u000a        \u000a          #275\u000a          financial institution\u000a          24,672 cores\u000a        \u000a        \u000a          #317\u000a          IT service provider\u000a          10,288 cores\u000a        \u000a        \u000a          #354\u000a          IT service provider\u000a          14,592 cores\u000a        \u000a        \u000a          #417\u000a          IT service provider\u000a          16,288 cores\u000a        \u000a        \u000a          #461\u000a          financial institution\u000a          18,080 cores\u000a        \u000a        \u000a          #465\u000a          bank\u000a          17,952 cores\u000a        \u000a        \u000a          #476\u000a          IT service provider\u000a          6,560 cores\u000a        \u000a      \u000a    \u000a    The IT providers in the list are typically providing computing resources\u000a      to large financial institutions. Note also that the computing facilities\u000a      of many banks are probably not included in this list. The information is\u000a      provided by the IT vendors (banks are very secretive about their computing\u000a      facilities).\u000a    An indication of the power consumption of financial data centres is given\u000a      by a 2008 Guardian article (http:\/\/www.guardian.co.uk\/technology\/2008\/may\/29\/energy.olympics2012)\u000a      on the negative impact that the Olympics would have on power provision for\u000a      the banks based in Canary Wharf. What happened subsequently is that major\u000a      banks built new data centres around the M25 where power was available, or\u000a      used new data centres set up by IT providers. This indicates the\u000a      computational power of the major financial institutions, and it is thought\u000a      that over half of this power is devoted to Monte Carlo simulations. Given\u000a      that the costs of renting supercomputer time is upwards of thousands of\u000a      dollars per hour [J], in this context, even a factor 2 reduction in the\u000a      cost of computing Monte Carlo sensitivities equates to a major reduction\u000a      in cost as well as energy consumption, or enables many more calculations\u000a      giving improved modelling and mitigation of risk through considering many\u000a      more different risk scenarios.\u000a    ","ImpactSummary":"\u000a    The largest investment banks in London each have thousands of servers\u000a      largely devoted to Monte Carlo simulations, and to quantify their risks\u000a      and satisfy regulatory demands they need to be able to calculate huge\u000a      numbers of sensitivities (defined below) known collectively as \"Greeks\".\u000a      An adjoint technique developed by Professor Mike Giles in 2006 greatly\u000a      reduced the computational complexity of these calculations. The technique\u000a      is used extensively by Credit Suisse and other major banks, reducing their\u000a      computing costs and energy consumption. It has also led to the Numerical\u000a      Algorithms Group developing new software to support the banks in\u000a      exploiting this new adjoint approach to computing sensitivities.\u000a    ","ImpactType":"Economic","Institution":"\u000a    University of Oxford\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a*[1] MB Giles, NA Pierce. `An introduction to the adjoint approach to\u000a      design', Flow, Turbulence and Combustion, 65(3): 393-415, 2000, http:\/\/people.maths.ox.ac.uk\/gilesm\/files\/ftc00.pdf.\u000a      130 citations according to Web of Science, 280 according to Google Scholar\u000a    \u000a\u000a*[2] MB Giles, P Glasserman. `Smoking adjoints: fast Monte Carlo Greeks',\u000a      RISK, 19(1):88-92, January 2006,\u000a      http:\/\/www2.gsb.columbia.edu\/faculty\/pglasserman\/Other\/RiskJan2006.pdf.\u000a    \u000a\u000a*[3] MB Giles. `Collected matrix derivative results for forward and\u000a      reverse mode algorithmic differentiation'. pp. 35-44 in Advances in\u000a      Automatic Differentiation, Springer, 2008, http:\/\/people.maths.ox.ac.uk\/gilesm\/files\/AD2008.pdf.\u000a    \u000a\u000a[4] MB Giles, `Vibrato Monte Carlo sensitivities', pp. 369-392 in Monte\u000a        Carlo and Quasi Monte Carlo Methods 2008, Springer, 2009,\u000a      http:\/\/people.maths.ox.ac.uk\/gilesm\/files\/mcqmc08.pdf.\u000a    \u000a\u000a[5] L Capriotti, MB Giles. `Fast correlation Greeks by adjoint\u000a      algorithmic differentiation', RISK, 23(4):77-83, 2010, http:\/\/people.maths.ox.ac.uk\/gilesm\/files\/risk_cg10.pdf.\u000a    \u000a\u000a[6] L Capriotti, MB Giles. `Algorithmic differentiation: adjoint Greeks\u000a      made easy', RISK, 25(10), 2012, http:\/\/papers.ssrn.com\/sol3\/papers.cfm?abstract_id=1801522.\u000a    \u000aThe three asterisked outputs best indicate the quality of the\u000a      underpinning research. Papers [5] and [6] are refereed papers in one of\u000a      the leading practitioner publications.\u000a    Research funding\u000a    Research was partially funded by a 15-month EPSRC Springboard Fellowship:\u000a      Development of Multilevel Monte Carlo Algorithms for Mathematical Finance,\u000a      EP\/E031455\/1, 01\/01\/07-31\/03\/08, &#163;74,574.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"15","Level2":"2","Subject":"Banking, Finance and Investment"}],"Sources":"\u000a    [A] Risk Quant of the Year award:\u000a      http:\/\/www.risk.net\/risk-magazine\/feature\/1498251\/quants-paul-glasserman-michael-giles\u000a    [B] L Capriotti, J. Lee, M. Peacock, `Real Time Counterparty Credit Risk\u000a      Management in Monte Carlo', RISK 24(6):86-90, 2011\u000a      http:\/\/papers.ssrn.com\/sol3\/papers.cfm?abstract_id=1824864&amp;rec=1&amp;srcabs=1801522\u000a    [C] Credit Suisse US patent application, based on Giles' adjoint\u000a      techniques:\u000a      https:\/\/www.google.com\/patents\/WO2008151098A1?cl=en&amp;dq=luca+capriotti&amp;hl=en&amp;sa=X&amp;ei=9O4EUsSFEuqG0AWq24FI&amp;sqi=2&amp;pjf=1&amp;ved=0CDsQ6AEwAQ.\u000a    [D] Letter from a Director at Credit Suisse Group, Investment Banking\u000a      Division, describing the nature and extent of the impact at Credit Suisse.\u000a      Copy held by the University of Oxford, 2013\u000a    [E] Letter from the Vice-President, Sales, NAG, describing the nature and\u000a      extent of the impact on NAG and the financial sector. Copy held by the\u000a      University of Oxford, 2013\u000a    [F] M. Leclerc, Q. Liang, I. Schneider, `Fast Monte Carlo Bermudan\u000a      Greeks', RISK, 22(7):84- 88, 2009, http:\/\/people.maths.ox.ac.uk\/gilesm\/files\/risk_greeks09.pdf\u000a    [G] L Carver Cutting Edge introduction: Computation, computation,\u000a      computation', RISK Technical paper, 06 Sep 2012, commenting on the\u000a      significance of Adjoint methods in the financial industry:\u000a      http:\/\/www.risk.net\/risk-magazine\/technical-paper\/2203043\/cutting-edge-introduction-computation-computation-computation\u000a    [H] L Carver, `Algorithmic gymnastics', RISK, 25(8):52, 2012,http:\/\/www.risk.net\/risk-magazine\/profile\/2194286\/credit-suisse-algorithmic-gymnastics\u000a    [I] The Top 500 supercomputer list http:\/\/top500.org\/\u000a\u0009(downloaded on 3\/9\/13), showing extent of computational resources\u000a      used by banks\u000a    [J] Price of supercomputing time can be verified at:\u000a      http:\/\/arstechnica.com\/business\/2012\/04\/4829-per-hour-supercomputer-built-on-amazon-cloud-to-fuel-cancer-research\/\u000a    References [B], [F] and [H] are by practitioners, indicating the reach of\u000a      Giles' adjoint research \u000a    ","Title":"\u000a    Adjoint sensitivities in computational finance bring orders-of-magnitude\u000a      runtime improvements\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Adjoint techniques are a well-established set of mathematical methods\u000a      that have been extensively used in engineering design optimisation to\u000a      simultaneously and efficiently compute the sensitivity of a single output\u000a      quantity with respect to a large number of input parameters.\u000a    Mike Giles (Oxford faculty, 1992 to date) has been a leading researcher\u000a      in the use of adjoints in engineering design optimisation; his\u000a      introductory article [1] on the subject with Niles Pierce (Oxford\u000a      postdoctoral researcher, 1997-1998) has been well cited. When he switched\u000a      research fields from computational fluid dynamics to computational\u000a      finance, Giles recognised the opportunity to apply the adjoint technique\u000a      to Monte Carlo option pricing in finance to more efficiently compute\u000a      option price sensitivities (known in the industry as \"Greeks\") which are\u000a      used to estimate, and thereby minimise, possible future losses due to\u000a      changes in stock prices, interest rates, exchange rates, etc. In January\u000a      2006, together with Professor Paul Glasserman from Columbia University, he\u000a      published the paper \"Smoking adjoints: fast Monte Carlo Greeks\" [2] in\u000a      RISK, the leading publication for those working in quantitative finance\u000a      within investment banks and other financial institutions. This is the key\u000a      paper underpinning this Impact case study.\u000a    Manual implementation of discrete adjoint methods can be time-consuming\u000a      and error-prone. Fortunately, much of this can be automated using forward\u000a      and reverse mode automatic differentiation methods developed in computer\u000a      science. In this context, one piece of research by Mike Giles was on a\u000a      higher-level linear algebra approach that is relevant to key steps in\u000a      Monte Carlo simulation (such as the Cholesky decomposition of a\u000a      correlation matrix) and time-marching in financial PDE simulations [3].\u000a    A key technical limitation was the fact that the whole approach requires\u000a      differentiability, but many financial option payoffs are discontinuous.\u000a      This was addressed by inventing the \"vibrato\" Monte Carlo method [4],\u000a      which is a hybrid mix of the pathwise sensitivity method (which the\u000a      adjoint treatment is based on) and the alternative, less efficient,\u000a      Likelihood Ratio Method.\u000a    The specific requirements of correlation Greeks (which are the\u000a      sensitivity of an option price to two or more inputs simultaneously\u000a      varied) to compute the sensitivity to each of the many elements in the\u000a      correlation matrix was addressed in [5], a RISK paper written with\u000a      Luca Capriotti, a Director at Credit Suisse Group, Investment Banking\u000a      Division, and probably the leading proponent of adjoint techniques within\u000a      the industry. Forward and reverse mode automatic differentiation was\u000a      introduced in [6], a further RISK paper with Luca Capriotti.\u000a    "},{"CaseStudyId":"5569","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000a    It is crucially important for public policy to identify clinical\u000a      treatments which combine cost-effectiveness\u000a      with clinical efficacy and minimal adverse effects, and this requires an\u000a      ongoing\u000a      process of evaluation of new pharmaceutical products as they become\u000a      available. This evaluation\u000a      requires large scale clinical trials of the kind described above. The\u000a      research described here has\u000a      made a significant impact in this regard at both the national and\u000a      international level.\u000a    Firstly, the SANAD trial was the driver for a major update in 2012 of the\u000a      clinical guidelines for the\u000a      treatment of epilepsy published by NICE (the National Institute for\u000a      Clinical Excellence). NICE is the\u000a      main source of guidance on treatment for healthcare professionals in the\u000a      UK. In 2008 it\u000a      commissioned the National Clinical Guidelines Centre for Acute and Chronic\u000a      Conditions\u000a      (NCGCACC) to partially update the 2004 NICE guidelines on epilepsy\u000a      treatment, NICE Clinical\u000a      Guideline 20 (2004). The first stage in the update was the publication in\u000a      2009 of a \"Scope\"\u000a      document [5.1] (references listed in Sect 5) which defined the remit for\u000a      the guideline update.\u000a      Section 3 of this document (3.2 section d), headed \"Clinical need for the\u000a      guideline\", stated\u000a      \"However, a recent large multicentre trial (the SANAD trial) evaluating\u000a        newer drugs in newly\u000a        diagnosed epilepsy (accepting some limitations) suggested that sodium\u000a        valproate should\u000a        be the drug of choice in generalised and unclassifiable epilepsies, and\u000a        lamotrigine in partial\u000a        epilepsies. We therefore consider it necessary to review new evidence\u000a        regarding anti-epileptic\u000a        drugs within an update of the NICE clinical guideline,\" This\u000a      demonstrates that the\u000a      SANAD research was the most significant trigger for the update.\u000a    Moreover it is clear from the draft consultation version of the\u000a      guidelines [5.2] published in 2010 that\u000a      the SANAD researchers were asked by NCGCACC to supply additional\u000a      unpublished research. The\u000a      final updated version of the guidelines was published in 2012 as NICE\u000a      Clinical Guideline 137\u000a      (2012) [5.3]. The Introduction to these guidelines repeats the quotation\u000a      above, confirming that\u000a      SANAD was the major motivation for the update; this introduction appears\u000a      as p7 of the full PDF\u000a      document and as the \"Abstract\" on the webpage for the updated guideline.\u000a      It is clear from Sect. 1.9\u000a      of the guideline, especially 1.9.4, 1.9.12 and 1.9.14, that the treatment\u000a      recommendations have\u000a      been modified in 2012 to place a greater emphasis on sodium valproate and\u000a      lamotrigine, in line\u000a      with the SANAD findings.\u000a    Secondly, the International League Against Epilepsy (ILAE) Guidelines\u000a      cited SANAD in 2009 [4] as\u000a      an important influence on work on updating their guidelines for\u000a      monotherapy (i.e. the treatment by\u000a      a single drug). The updated guidelines were finally published in 2013\u000a      [5.5]. The ILAE is the major\u000a      international professional organisation representing all those interested\u000a      in the field of epilepsy. The\u000a      ILAE publishes an official journal, Epilepsia,\u000a      which is the leading, most authoritative source for\u000a      current clinical and research results on all aspects of epilepsy. Epilepsia\u000a      periodically publishes the\u000a      ILAE Guidelines which represent the current international standard on\u000a      epilepsy treatment.\u000a      Ref.[5.4] above was also published in this journal.\u000a    Finally German Epilepsy guidelines published in 2008 [5.6] cite SANAD\u000a      underpinning the\u000a      recommendation for lamotrigine as first line treatment.\u000a    Beneficiaries of this work include\u000a    \u000a      People with epilepsy, through better informed treatment choices.\u000a      The NHS and other health systems caring for people with epilepsy,\u000a        through providing\u000a        effective and cost effective care.\u000a      Guideline developers including NICE, which have used these data to\u000a        underpin their\u000a        guidance.\u000a    \u000a    Naturally it will be some time before impacts on public health resulting\u000a      from such recent changes in\u000a      the guidelines can be measured.\u000a    ","ImpactSummary":"\u000a    A team at the University of Liverpool has undertaken research that has\u000a      informed practice and\u000a      policy worldwide in the management of patients presenting with newly\u000a      diagnosed epilepsy, which\u000a      has achieved international impact on health. Seizures are common and 3-5%\u000a      of the population will\u000a      be given a diagnosis of epilepsy during their lifetime. Decisions about\u000a      when to start treatment, and\u000a      if so with which drug are crucial and can have a significant effect on\u000a      outcomes for the individual\u000a      and have significant economic consequences for society. The research\u000a      includes the undertaking\u000a      and analysis of data from randomised controlled trials. The data analysis\u000a      is based on the statistical\u000a      research initiated by Dr Paula Williamson while in the Department of\u000a      Mathematical Sciences at the\u000a      University of Liverpool between 1996 and 2000. The research identified the\u000a      most appropriate first\u000a      line treatments for patients with newly diagnosed epilepsy, addressing\u000a      both clinical and cost\u000a      effectiveness. This work has underpinned national policy and triggered the\u000a      most recent update of\u000a      the NICE (National Institute for Clinical Excellence) epilepsy guidelines\u000a      in 2012.\u000a    ","ImpactType":"Health","Institution":"\u000a    University of Liverpool\u000a    ","Institutions":[{"AlternativeName":"Liverpool (University of)","InstitutionName":"University of Liverpool","PeerGroup":"A","Region":"North West","UKPRN":10006842}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[3.1] Williamson PR, Marson AG, Tudur C, Hutton JL and Chadwick D. (2000)\u000a      Individual patient\u000a      data meta-analysis of randomised anti-epileptic drug monotherapy trials.\u000a      Journal of Evaluation in\u000a      Clinical Practice, 6(2): 207-214, DOI: 10.1046\/j.1365-2753.2000.00236.x\u000a    \u000a\u000a[3.2] Williamson PR, Clough HE, Hutton JL, Marson AG and Chadwick DW.\u000a      (2002) Statistical\u000a      issues in the assessment of the evidence for an interaction between\u000a      factors in epilepsy trials.\u000a      Statistics in Medicine, 21: 2613-2622, DOI: 10.1002\/sim.1044\u000a    \u000a\u000a[3.3] Preston CL, Marson AG, Williamson PR. Lamotrigine versus\u000a      carbamazepine monotherapy for\u000a      epilepsy. The Cochrane Database of Systematic Reviews 2002, Issue 4. Art.\u000a      No.: CD001031. DOI:\u000a      10.1002\/14651858.CD001031.\u000a    \u000a\u000a[3.4] Marson AG, Williamson PR, Wilson H, Hutton JL, Chadwick DW, on\u000a      behalf of the epilepsy\u000a      monotherapy group. Carbamazepine versus valproate monotherapy for\u000a      epilepsy: A meta-analysis.\u000a      Epilepsia 2002;43(5):505-513, DOI: 10.1046\/j.1528-1157.2002.20801.x.\u000a    \u000a\u000a[3.5] Marson AG, Al-Kharusi AM, Alwaidh M, Appleton R, Baker GA, Chadwick\u000a      DW, Cramp C,\u000a      Cockerell OC, Cooper PN, Doughty J, Eaton B, Gamble C, Goulding PJ, Howell\u000a      SJL, Hughes A,\u000a      Jackson M, Jacoby A, Kellett M, Lawson GR, Leach JP, Nicolaides P, Roberts\u000a      R, Shackley P,\u000a      Shen J, Smith DF, Smith PEM, Tudur Smith C, Vanoli A, Williamson PR on\u000a      behalf of the SANAD\u000a      Study group. Carbamazepine, gabapentin, lamotrigine, oxcarbazepine or\u000a      topiramate for partial\u000a      epilepsy: results from the SANAD trial. Lancet 2007;369:1000-1015, DOI: 10.1016\/S0140-6736(07)60460-7.\u000a    \u000a\u000a[3.6] Marson AG, Al-Kharusi AM, Alwaidh M, Appleton R, Baker GA, Chadwick\u000a      DW, Cramp C,\u000a      Cockerell OC, Cooper PN, Doughty J, Eaton B, Gamble C, Goulding PJ, Howell\u000a      SJL, Hughes A,\u000a      Jackson M, Jacoby A, Kellett M, Lawson GR, Leach JP, Nicolaides P, Roberts\u000a      R, Shackley P,\u000a      Shen J, Smith DF, Smith PEM, Tudur Smith C, Vanoli A, Williamson PR on\u000a      behalf of the SANAD\u000a      Study group. Valproate, lamotrigine or topiramate for generalized and\u000a      unclassifiable epilepsy:\u000a      results from the SANAD trial. Lancet 2007;369:1016-1026, DOI: 10.1016\/S0140-6736(07)60461-9.\u000a    \u000aThese publications describe the research undertaken by Prof Williamson\u000a      while a member of the\u000a      Department of Mathematical Sciences; the Lancet publications represent\u000a      reports on the SANAD\u000a      trial in its entirety.. Many of the publications listed above, where the\u000a      research carried out in the\u000a      Department of Mathematical Sciences is described, are highly cited, with\u000a      Refs [3.1], [3.4-6]\u000a      accruing 28, 52, 229 and 218 citations respectively by September 2013\u000a      (according to the Web of\u000a      Knowledge). The publications appear in high-impact journals; Statistics in\u000a      Medicine is rated A* in\u000a      the ARC list of journals. The Lancet has an Article Influence score of\u000a      13.6, ranked 2 out of 153\u000a      medical journals; the Journal of Evaluation in Clinical Practice has an\u000a      Article Influence score of\u000a      0.468, ranked 55 out of 153 medical journals; and Epilepsia has an Article\u000a      Influence score of 1.2,\u000a      ranked 30 out of 191 journals of clinical neurology.\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"9","Subject":"Neurosciences"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"}],"Sources":"\u000a    NICE Guidelines\u000a    [5.1] National\u000a        Institute for Health and Clinical Excellence (NICE) Scope document.\u000a      Section 3.2 (d)\u000a      references the SANAD trial as a major driver for update of the guidance.\u000a    [5.2] Draft\u000a        consultation version of the NICE guidelines (2010). SANAD\u000a      researchers were asked by\u000a      NCGCACC to supply additional unpublished research (Page 60, Line 23) to\u000a      inform the guidance\u000a      updates.\u000a    [5.3] NICE\u000a        Clinical Guideline 137 published 2012; see The epilepsies: the\u000a      diagnosis and\u000a      management of the epilepsies in adults and children in primary and\u000a      secondary care at\u000a    International League Against Epilepsy (ILAE) guidelines:\u000a    [5.4] See p66 of http:\/\/www.ilae.org\/Visitors\/Documents\/ILAEAnnual-Report2009_000.pdf\u000a      showing\u000a      inclusion of SANAD data in updated guidance.\u000a    [5.5] Updated ILAE\u000a        evidence review of antiepileptic drug efficacy and effectiveness as\u000a      initial\u000a      monotherapy for epileptic seizures and syndromes (Epilepsia 2013), further\u000a      references role of\u000a      SANAD trial on International guidance.\u000a    German guidelines\u000a    [5.6] Leitlinien f&#252;r Diagnostik und Therapie im der Neurologie.\u000a      Stuttgart: Georg Thieme Verlag,\u000a      2008. Document\u000a        link is original German guidance, but SANAD trial is referenced on\u000a      page 11 and\u000a      the base of page 21 references SANAD data\u000a    \u000a    ","Title":"\u000a    Informing clinical policy on epilepsy treatment\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The HTA (Health Technology Assessment) funded study of Standard and New\u000a      Antiepileptic Drugs\u000a      (SANAD) compared standard and new antiepileptic drugs for patients with\u000a      newly diagnosed\u000a      epilepsy and compared clinical and cost effectiveness. The grant\u000a      application for SANAD was\u000a      submitted in 1998 and recruitment for the trials started in 1999. The\u000a      underpinning research\u000a      included the meta-analysis of randomised controlled trials and statistical\u000a      method development. The\u000a      rationale and framework for individual patient data meta-analysis of\u000a      anti-epileptic drug trials was\u000a      described, developed and implemented by Paula Williamson while a staff\u000a      member in the\u000a      Department of Mathematical Sciences at the University of Liverpool\u000a      (between 1996 and\u000a      2000)[3.1,3.2,3.3,3.4] (references listed in Sect. 3), and continued when\u000a      she joined the Department\u000a      of Biostatistics in 2000. This work included advice on the selection of,\u000a      and data management for,\u000a      patient outcomes as well as the optimal statistical methods for data\u000a      analysis.\u000a    The research outcomes of the SANAD trial provide the best evidence\u000a      worldwide to inform\u000a      treatment decisions. The trial identified lamotrigine (a new drug) as a\u000a      first line treatment for newly\u000a      diagnosed focal epilepsy as it was as effective as the standard treatment\u000a      (carbamazepine) but\u000a      better tolerated, and was cost effective[3.3,3.5]. Valproate (the standard\u000a      treatment) was identified\u000a      as the most effective treatment for patients with generalised epilepsy\u000a      [3.4,3.6]. The analysis of\u000a      SANAD[3.5,3.6] exploited the statistical methods developed by Williamson\u000a      to assess competing\u000a      risks of anti-epileptic drug failure, namely drug withdrawal due to\u000a      inadequate seizure control or\u000a      unacceptable adverse effects. Williamson proposed and applied a competing\u000a      risks failure time\u000a      model, assuming cause-specific proportional hazards, appropriate to this\u000a      setting. Subsequent work\u000a      demonstrated that in this particular setting, if such competing risks are\u000a      ignored, there is an\u000a      unacceptably high likelihood that important differential effects of the\u000a      newer anti-epileptic drugs\u000a      would be missed. In subsequent work, prognostic modelling of these data\u000a      identified the\u000a      characteristics of patients with differing seizure outcomes. This allows a\u000a      stratified approach to\u000a      patient counselling and treatment decisions.\u000a    The design of SANAD (e.g. sample size and outcomes) was informed by\u000a      individual patient data\u000a      meta-analysis (IPDMA) undertaken by Williamson and her group in the\u000a      Department of\u000a      Mathematical Sciences (which included H.E.Wilson (now Clough), C. Tudur\u000a      (now Tudur-Smith)\u000a      and C. Gamble)[3.1,3.2,3.3 and 3.4]. The parameter estimates for the\u000a      sample size calculation for\u000a      SANAD were taken from the results of a pivotal IPDMA [3.4]. The choice of\u000a      outcomes was\u000a      informed by the definitions proposed and applied by Williamson [3.1].\u000a      Subsequently data from\u000a      SANAD were included in an updated meta-analysis. The SANAD project\u000a      involved a team of 12 co-investigators\u000a      (including Prof Williamson) led by Prof AG Marson of the Department of\u000a      Molecular\u000a      and Clinical Pharmacology as PI. Other key members of the team were Profs\u000a      G Baker and A\u000a      Jacoby.\u000a    "},{"CaseStudyId":"5571","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000a    Since July 2011, our stochastic infectious disease simulator has been\u000a      installed on computers at\u000a      Cefas, with a user-friendly interface. Since then, Cefas have used this\u000a      simulator to model potential\u000a      epidemics in the aquaculture industry of England and Wales, and develop\u000a      contingency planning to\u000a      control them.\u000a    By using simulations of infectious disease outbreaks, Cefas have gained a\u000a      much better\u000a      understanding of the dynamics of such diseases in the aquaculture system\u000a      and are able to\u000a      determine the efficacy of potential control strategies. This very detailed\u000a      quantitative analysis was\u000a      not previously available to them and has had an impact on their practice\u000a      by significantly increasing\u000a      their analytic capacity and capability. Our collaborators at Cefas\u000a      state[5.1]: \"This research has\u000a        enabled us to better understand the potential impact of exotic disease\u000a        introduction to the salmonid\u000a        aquaculture industry in England and Wales.\" and: \"The information\u000a        obtained provides support for\u000a        our methods of control and, in conjunction with other qualitative and\u000a        quantitative inputs, provides\u000a        evidence to underpin contingency plans for combatting infectious\u000a        diseases in the UK. The\u000a        simulator is currently being used in further development of contingency\u000a        planning contributing to the\u000a        drafting of disease-specific annexes which inform government policy on\u000a        the control of infectious\u000a        diseases in the aquaculture industry.\"\u000a    Indeed, a primary extension of analytic capacity available to Cefas\u000a      provided by the simulator lies in\u000a      its ability to study specific infectious diseases. Cefas say that \"This\u000a        (the simulator) has significantly\u000a        impacted on our capacity to tailor our analysis towards specific\u000a        diseases as opposed to more\u000a        generic measures of control. Our use of the simulator has so far focused\u000a        on the infectious diseases\u000a        that are likely to cause the most damaging outbreaks in the UK including\u000a        IHN, VHS and Gs.\"\u000a    Another key advantage brought by the simulator is a much greater degree\u000a      of confidence in\u000a      understanding the dynamics of the system than can be gained from a\u000a      qualitative analysis alone.\u000a      Cefas say that \"By unifying all of the major routes of transmission\u000a        into a single assimilated model,\u000a        we have a better understanding of the scenarios we would face given an\u000a        outbreak. The level of\u000a        detail incorporated within the model has also increased our confidence\u000a        in measures to control\u000a        infectious diseases in the aquaculture industry. \"\u000a    Combining the complexities of several modes of transmission together with\u000a      the impact of\u000a      interventions was not previously possible in a quantitative way. Increased\u000a      understanding of the\u000a      pattern of spread is also vital, as well as highlighting the most\u000a      important generic aspects for\u000a      targeting which are found to be time to detection, laboratory testing\u000a      capacity during an outbreak,\u000a      delays in implementing control measures on infected sites and the time\u000a      until fish-farm restocking\u000a      should be allowed.\u000a    Another major impact is the increase in capability for Cefas to\u000a      investigate different control\u000a      strategies including proactive and reactive strategies. This is\u000a      particularly important as many fish\u000a      diseases can spread without obvious symptoms, complicating control\u000a      efforts.\u000a    The cost of an outbreak of a notifiable infectious disease becoming\u000a      endemic is potentially millions\u000a      of pounds per year (see section 2, first paragraph), and so the impact of\u000a      any percentage increase\u000a      in the chance of successfully controlling outbreaks can measured on this\u000a      scale. Reducing the\u000a      impact of these notifiable diseases is central to the viability of many\u000a      aquaculture businesses in a\u000a      global market environment, with some diseases inevitably leading to\u000a      unsustainable losses for fish\u000a      farmers. Additionally it should be noted that the export market to any\u000a      country which does not have\u000a      these notifiable diseases will immediately disappear until the infectious\u000a      disease is demonstrably no\u000a      longer present.\u000a    ","ImpactSummary":"\u000a    In July 2011, a fish disease simulator developed in the Department of\u000a      Mathematical Sciences at\u000a      the University of Liverpool was installed on computers at the Centre for\u000a      Environment, Fisheries &amp;\u000a      Aquaculture Science (Cefas), an executive agency of the UK government\u000a      Department for\u000a      Environment, Food and Rural Affairs (Defra).\u000a    Since this date, the simulator has significantly improved the capability\u000a      available to Cefas for\u000a      understanding the likely spread of infectious diseases in the aquaculture\u000a      industry of England and\u000a      Wales, and enabled the optimisation of methods for the prevention and\u000a      control of outbreaks.\u000a      Specifically, a user-friendly interface enables Cefas to focus on\u000a      particular diseases of concern,\u000a      understand their specific pattern of spread and optimise methods for their\u000a      control. The simulator is\u000a      currently being used to develop contingency planning for outbreaks.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University of Liverpool\u000a    ","Institutions":[{"AlternativeName":"Liverpool (University of)","InstitutionName":"University of Liverpool","PeerGroup":"A","Region":"North West","UKPRN":10006842}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[3.1] Sharkey KJ, Fernandez C, Morgan KL, Peeler E, Thrush M, Turnbull\u000a      JF, Bowers RG, Pair-\u000a      level approximations to the spatio-temporal dynamics of epidemics on\u000a      asymmetric contact\u000a      networks J Math Biol vol 53 pp 61-85 (2006) doi:10.1007\/s00285-006-0377-3\u000a      (peer reviewed\u000a      journal, impact factor 2.4).\u000a    \u000a\u000a[3.2] Jonkers ART, Sharkey KJ, Thrush MA, Turnbull JF, Morgan KL,\u000a      Epidemics and control\u000a      strategies for diseases of farmed salmonids: a parameter study, Epidemics\u000a      vol. 2 pp 195-206\u000a      (2010). doi:10.1016\/j.epidem.2010.08.001 (peer reviewed journal, impact\u000a      factor 2.3).\u000a    \u000aResearch grants:\u000a    Part of this work was supported by a grant awarded to. Prof. Kenton\u000a      Morgan (PI): Stochastic\u000a      spatially explicit models of the likely spread of IHN, VHS and G. salaris\u000a      in farmed and wild UK fish\u000a      populations; DEFRA (grant code: FC1153); &#163;205,741\u000a    ","ResearchSubjectAreas":[{"Level1":"7","Level2":"4","Subject":"Fisheries Sciences"}],"Sources":"\u000a    [5.1] Group Manager, Aquatic Pests and Pathogens (Cefas) has provided a\u000a      letter of support to\u000a      corroborate the critical impact of our fish disease simulator upon the\u000a      capacity of CEFAS to analyse\u000a      and control fish disease.\u000a    [5.2] Pages from the Cefas\u000a        website showing the role of our modelling within Cefas statutory\u000a      remit.\u000a    [5.3] DEFRA\u000a        Policy planning document. Page 3 indicates the role of the modelling\u000a      work in\u000a      informing UK \"development of contingency plans for disease outbreaks and\u000a      biosecurity\" Aquatic\u000a      Animal Health Evidence Plan. Policy portfolio: Animal Health: Global trade\u000a      and aquaculture health.\u000a      Policy area within portfolio: Aquaculture Health. (DEFRA 2013)\u000a    \u000a    ","Title":"\u000a    Control of epidemics in the aquaculture industry of England and Wales\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"},{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    In 1998, it was estimated (Giorgetti, 1998) that the single notifiable\u000a      aquatic disease Viral\u000a      Haemorrhagic Septicaemia Virus (VHS) costs the Western European\u000a      aquaculture industry US$ 60\u000a      million annually. Iversen et al 2005 assessed the general cost of disease\u000a      to the Norwegian fish\u000a      farming industry to be US$150 million annually. The UK has so far been\u000a      lucky in excluding major\u000a      infectious diseases such as VHS, Gyrodactylus salaries (GS) and\u000a      Infectious Haematopoietic\u000a      Necrosis (IHN) virus among others. Our collaborators at Cefas state that\u000a      the economic impact of\u000a      one of these diseases becoming endemic in the UK \"could easily amount\u000a        to millions of pounds per\u000a        year with significant impact on the economic viability of large sections\u000a        of the aquaculture industry.\"\u000a    The underpinning research carried out at the University of Liverpool was\u000a      conducted between April\u000a      2004 and August 2010 in collaboration with Cefas. This work was partly\u000a      funded by Defra grant\u000a      FC1153 with Prof Kenton Morgan, of the Veterinary school at the University\u000a      of Liverpool, as PI.\u000a      The core mathematical modelling research was initiated by Prof Roger\u000a      Bowers of the Department\u000a      of Mathematical Sciences at the University of Liverpool and by Dr Kieran\u000a      Sharkey (at this time a\u000a      post-doc in this department, April 2004-September 2007). The work was\u000a      continued by Dr Sharkey\u000a      when he returned to the submitting unit as a lecturer, between September\u000a      2009 and publication of\u000a      the results in September 2010. Dr Sharkey was primarily responsible for\u000a      design of the model and\u000a      for coordinating the project across all project partners. Dr Sharkey and\u000a      Dr Jonkers (the latter time-\u000a      split between the University of Liverpool and the University of Munster,\u000a      Germany) collaborated on\u000a      producing the computer simulator.\u000a    The research generated the first detailed stochastic simulation model of\u000a      the British aquaculture\u000a      system, integrating detailed transportation and river networks in addition\u000a      to control measures [3.2].\u000a      It involved a particularly detailed assessment of the issues surrounding\u000a      water-borne spread of\u000a      pathogens, incorporating information about river speeds and fish farm\u000a      locations. The transportation\u000a      networks were built up using confidential data collected by Cefas via\u000a      recorded \"live fish\u000a      movements\"&#8212;i.e. commercial transportation of live fish. By law, each live\u000a      fish movement must be\u000a      recorded and this represents the most serious route of contamination. This\u000a      enabled them to obtain\u000a      a network structure for disease spread with unprecedented detail in terms\u000a      of epidemic modelling.\u000a      During the development of the work, regular meetings were held with Cefas\u000a      in addition to\u000a      communication by telephone and email to ensure that the research was\u000a      directly relevant to them.\u000a    The potential control measures depend on the type of infection,\u000a      particularly because many aquatic\u000a      diseases can be asymptomatic for long periods, requiring a shift towards\u000a      more preventative rather\u000a      than reactive strategies. Hybrid strategies combining preventative\u000a      measures and reactive\u000a      measures were therefore considered. General quantitative guidelines for\u000a      preventing and controlling\u000a      future infection outbreaks were published [3.2] based on the simulation\u000a      model.\u000a    The research enabled a unified modelling of several mechanisms of\u000a      transmission, generating a\u000a      complex system with emergent properties which could not be investigated by\u000a      looking at each mode\u000a      of transmission separately. Methods of control anticipating these complex\u000a      interactions could also\u000a      be implemented for the first time.\u000a    The stochastic simulation model was designed with a user-friendly\u000a      interface and incorporates\u000a      sufficient tuneable parameters to be adaptable to many specific infectious\u000a      diseases which threaten\u000a      the aquaculture industry of England and Wales. This model is currently\u000a      installed on computers at\u000a      Cefas.\u000a    References:\u000a      Giorgetti, The cost of disease, FAO EastFish Mag., 1 (1998), pp. 40-41\u000a      Iversen et al. Stress responses in Atlantic salmon (Salmo salar L.)\u000a      smolts during commercial well\u000a      boat transports, and effects on survival after transfer to sea\u000a      Aquaculture, 243 (1-4) (2005), pp.\u000a      373-382\u000a    "},{"CaseStudyId":"5572","Continent":[],"Country":[],"Funders":["Natural Environment Research Council"],"ImpactDetails":"\u000d\u000a    Note: All superscript references within this section refer to\u000d\u000a      corroborating sources listed in Section 5\u000d\u000a    Most projects studying recent environmental change as recorded in natural\u000d\u000a      archives such as lake\u000d\u000a      sediments involve teams of scientists working on a range of environmental\u000d\u000a      indicators. Decisions\u000d\u000a      on environmental issues are generally based on findings from many\u000d\u000a      different sources, and may\u000d\u000a      have long-term impacts. Ongoing work with colleagues studying sediment\u000d\u000a      records in Siberia is\u000d\u000a      one of many investigations informing the debate on climate change in the\u000d\u000a      Arctic. Below are brief\u000d\u000a      accounts, supported by letters from lead contacts, of three recent\u000d\u000a      projects with more limited\u000d\u000a      objectives and readily identifiable impacts.\u000d\u000a    Western Airborne Contaminants Assessment Project (WACAP)1\u000d\u000a      The lead contact for this USEPA funded project, carried out during 2002-7,\u000d\u000a      was Dr Dixon H.\u000d\u000a      Landers, Senior Research Environmental Scientist at the U.S. Environmental\u000d\u000a      Protection Agency.\u000d\u000a      The main results were reported in 20081 and a summary\u000d\u000a      (including initial impacts and actions) in\u000d\u000a      20092. According to a letter from Dr Landers3, the\u000d\u000a      project has had wide ranging impacts on\u000d\u000a      chemical registration decisions, new research, conservation actions and\u000d\u000a      decisions relating to\u000d\u000a      regulation of pollution sources. The knowledge framework has been used to\u000d\u000a      inform various\u000d\u000a      regulatory bodies3 concerned with the impact of emissions from\u000d\u000a      industrial sources and agricultural\u000d\u000a      uses of pesticides. The Liverpool role in providing dated profiles made a\u000d\u000a      major contribution to two\u000d\u000a      main elements of the project. First, to compare current contaminant\u000d\u000a      deposition rates with those\u000d\u000a      from historic, pre-industrial times. Second, to explicitly link the\u000d\u000a      sediment records to increases in\u000d\u000a      airborne contaminants from \"regional\" sources. Dr Landers describes\u000d\u000a      Appleby and Piliposyan's\u000d\u000a      contribution as \"authoritative\" and confirms that their \"careful\u000d\u000a        and thorough work was a key\u000d\u000a        component in the high level of success that WACAP has enjoyed\".\u000d\u000a      To be precise, according to Dr Landers3, the sediment work by\u000d\u000a      Appleby and Piliposyan \"provided\u000d\u000a        strong evidence on the extent to which recent increases in Hg\u000d\u000a        contamination ... were due to\u000d\u000a        emissions from local or regional sources, as opposed to higher global\u000d\u000a        background concentrations\".\u000d\u000a      There are presently a number of research efforts funded by the NPS\u000d\u000a      (National Parks Service) and\u000d\u000a      other government and non-government organizations building on these\u000d\u000a      results. As Dr Landers\u000d\u000a      states3, \"the WACAP data has been used to help select which\u000d\u000a        parks to target for this research\" by\u000d\u000a      identifying the sites most at risk from local Hg emissions.\u000d\u000a      Moreover the sediment records resulting from Appleby and Piliposyan's work\u000d\u000a      for WACAP \"have\u000d\u000a        been most useful in helping NPS understand which Historic Use Pesticides\u000d\u000a        continue to persist in\u000d\u000a        park ecosystems often well after they have been banned, and which\u000d\u000a        Current Use Pesticides are\u000d\u000a        beginning to increase in park ecosystems over time.\" In particular,\u000d\u000a      data supplied by Appleby and\u000d\u000a      Piliposyan on the residual incidence of the pesticide Endosulfan in the\u000d\u000a      natural environment was a\u000d\u000a      major part of the evidence base which resulted in the UN decision to ban\u000d\u000a      it. As Dr Landers states,\u000d\u000a        \"Data on the temporal record of the presence of the widely-used\u000d\u000a        insecticide Endosulfan obtained\u000d\u000a        from WACAP sediment records was entered as evidence of the long term\u000d\u000a        persistence and\u000d\u000a        widespread occurrence of this substance in hearings that preceded the\u000d\u000a        [United Nations] decision to\u000d\u000a        ban it\". In 2011, Endosulfan was added to the UN list of persistent\u000d\u000a      organic pollutants to be\u000d\u000a      eliminated worldwide4.\u000d\u000a    Restoration of mosquito control ditches on Fire Island, New York\u000d\u000a      This US National Park Service funded project, led by Dr Roman (NPS) and\u000d\u000a      Professor John King\u000d\u000a      (University of Rhode Island), was designed to provide evidence concerning\u000d\u000a      plans to restore\u000d\u000a      marshes on the Fire Island National Seashore that decades earlier had been\u000d\u000a      ditched for mosquito\u000d\u000a      abatement. In a letter5, Prof King says that the work of\u000d\u000a      Appleby and Piliposyan \"provided important\u000d\u000a        information for environmental managers at the National Park Service\"\u000d\u000a      and \"supported a\u000d\u000a        management option that will save considerable money\". Specifically,\u000d\u000a      the NPS needed to make a\u000d\u000a      decision whether to actively fill in the ditches, \"a costly and major\u000d\u000a        undertaking\" according to\u000d\u000a      Professor King5, or leave them alone to fill in naturally. Dr\u000d\u000a      Roman confirmed in an e-mail6 that\u000d\u000a      Appleby and Piliposyan's research was \"instrumental in the NPS decision\u000d\u000a        to allow human made\u000d\u000a        ditches at Fire Island National Seashore to fill naturally\". The\u000d\u000a      Liverpool role was to analyse and\u000d\u000a      date sediment cores from various locations on Fire Island. 210Pb\u000d\u000a      records in salt-marsh sediment\u000d\u000a      cores were used to determine the rate of sea level rise7. 210Pb\u000d\u000a      records in sediment cores from the\u000d\u000a      ditches were used to determine current natural sediment accumulation rates8.\u000d\u000a      The results\u000d\u000a      (published in 20128) suggested that natural inundation would\u000d\u000a      best achieve the desired goal. In\u000d\u000a      consequence management has subsequently decided on the non-intervention\u000d\u000a      option; Professor\u000d\u000a      King states in his letter5 that \"the paper which resulted from\u000d\u000a      this work [Ref. 8 below] is being\u000d\u000a      applied now\". This has resulted in a considerable financial saving to the\u000d\u000a      NPS.\u000d\u000a    Use of antifoulant paints in UK inland waters\u000d\u000a      The lead contact for this English Nature, DEFRA and NERC funded project,\u000d\u000a      carried out during\u000d\u000a      2000-3, was Dr Carl Sayer of University College London (UCL). The main\u000d\u000a      purpose was to\u000d\u000a      determine the cause of the ecological degradation of the Norfolk Broads\u000d\u000a      shallow lake system, one\u000d\u000a      of the suspects being the use of toxic tributyltin (TBT) in boat\u000d\u000a      antifouling paints. The Liverpool role\u000d\u000a      was to date sediment cores from the Broads. The resulting chronology was\u000d\u000a      used by UCL to\u000d\u000a      reconstruct the historical record of TBTs in the lake waters. The results,\u000d\u000a      widely reported in the\u000d\u000a      media, demonstrated not only that TBT was a key contributory factor but\u000d\u000a      also that previously\u000d\u000a      contaminated sediments acted as a long-term reservoir for pollution in\u000d\u000a      spite of the world-wide ban\u000d\u000a      on the use of TBTs that came into force in 2008. The Senior Ecologist at\u000d\u000a      the Broads Authority, Dr\u000d\u000a      Andrea Kelly, confirms in a letter9 that \"as a consequence\u000d\u000a        of this work and subsequent media\u000d\u000a        coverage the Broads Authority initiated a campaign to promote\u000d\u000a        environmentally friendly antifoulant\u000d\u000a        use in the Broads system\". This ongoing campaign informs their\u000d\u000a      current advice to boat users10, and\u000d\u000a      as Dr Kelly states9, \"this research... has been influential\u000d\u000a        in informing the authorities policy and\u000d\u000a        strategic conservation direction in regard to policy on ecoboating and\u000d\u000a        anti-foulant paint use on the\u000d\u000a        Broads\" and developing literature for the 2013 Green Boat Show on\u000d\u000a      the Broads10.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Environmental management decisions are frequently based on records of\u000d\u000a      environmental change\u000d\u000a      recovered from natural archives such as lake sediments. Key to deciphering\u000d\u000a      these records is a\u000d\u000a      reliable technique for dating sediment sequences. Researchers in the\u000d\u000a      Liverpool University\u000d\u000a      Mathematical Sciences Department have played a major role in the\u000d\u000a      development of dating\u000d\u000a      techniques using natural (210Pb) and artificial (137Cs)\u000d\u000a      fallout radionuclides. Working with\u000d\u000a      environmental scientists they have been responsible for the implementation\u000d\u000a      of these techniques in\u000d\u000a      research programs that have resulted in national and international\u000d\u000a      controls on e.g. emissions from\u000d\u000a      power stations, the use of persistent organic pollutants and climate\u000d\u000a      change. In particular, the US\u000d\u000a      National Parks Service (NPS) is using their research to monitor pollution\u000d\u000a      levels at sensitive\u000d\u000a      locations in their National Parks and this research has also been a key\u000d\u000a      factor in the UN decision in\u000d\u000a      2011 to ban the widely-used insecticide Endosulfan. Their research also\u000d\u000a      enabled the NPS in 2012\u000d\u000a      to identify the most effective solution for marsh restoration off Long\u000d\u000a      Island, New York, resulting in a\u000d\u000a      considerable financial saving to the NPS; and finally their research on\u000d\u000a      pollutants in the Norfolk\u000d\u000a      Broads has led to the current campaign by the Broads Authority to promote\u000d\u000a      environmentally\u000d\u000a      friendly anti-fouling paints.\u000d\u000a    ","ImpactType":"Environmental","Institution":"\u000d\u000a    University of Liverpool\u000d\u000a    ","Institutions":[{"AlternativeName":"Liverpool (University of)","InstitutionName":"University of Liverpool","PeerGroup":"A","Region":"North West","UKPRN":10006842}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"5128638","Name":"New York"},{"GeoNamesId":"5224323","Name":"Rhode Island"}],"References":"\u000d\u000a    \u000a1. Piliposyan G &amp; P G Appleby, 2003. A simple model of the origin and\u000d\u000a      transport of 222Rn and\u000d\u000a      210Pb in the atmosphere. Continuum Mech. Thermodyn. 15:\u000d\u000a      503-518. (Impact factor 1.091).\u000d\u000a      DOI:10.1007\/s00161-003-0129-1\u000d\u000a    \u000a\u000a2. Appleby P G, E Y Haworth, H Michel, D B Short, G Laptev &amp; G T\u000d\u000a      Piliposyan, 2003. The\u000d\u000a      transport and mass balance of fallout radionuclides in Blelham Tarn,\u000d\u000a      Cumbria (UK). J\u000d\u000a      Paleolimnology 29: 459-473. (Impact factor 2.209). DOI:\u000d\u000a      10.1023\/A:1024437426878\u000d\u000a    \u000a\u000a3. Piliposyan G &amp; P G Appleby, 2003. A model of the impact of winter\u000d\u000a      ice cover on pollutant\u000d\u000a      concentrations and fluxes in mountain lakes. Water, Air &amp; Soil\u000d\u000a      Pollution 44: 101-115. (Impact\u000d\u000a      factor 1.748). DOI: 10.1023\/A:1022994812659\u000d\u000a    \u000a\u000a4. Klaminder J, Appleby P, Crook P &amp; Renberg I (2012).\u000d\u000a      Post-deposition diffusion of 137Cs in lake\u000d\u000a      sediment: implications for radiocesium dating. Sedimentology,\u000d\u000a      59:2259-2267. (Impact factor\u000d\u000a      2.601). DOI:10.1111\/j.1365-3091.2012.01343.x\u000d\u000a    \u000a\u000a5. Wischnewski J, Mackay AW, Appleby PG, Mischke S &amp; Herzschuh U,\u000d\u000a      2011. Modest diatom\u000d\u000a      response to regional warming on the southeast Tibetan Plateau during the\u000d\u000a      last two centuries, J\u000d\u000a      Paleolimnology, 46:215-227. (Impact factor 2.209). DOI\u000d\u000a      10.1007\/s10933-011-9533-x\u000d\u000a    \u000a\u000a6. Appleby P G &amp; G T Piliposyan, 2004. Efficiency corrections for\u000d\u000a      variable sample height in well-type\u000d\u000a      germanium detectors. NIMB 225: 423-433. (Impact factor 1.266).\u000d\u000a      DOI:10.1016\/j.nimb.2004.05.020\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"4","Level2":"2","Subject":"Geochemistry"},{"Level1":"4","Level2":"3","Subject":"Geology"},{"Level1":"4","Level2":"6","Subject":"Physical Geography and Environmental Geoscience"}],"Sources":"\u000d\u000a    \u000d\u000a      A summary of the findings of the WACAP project can be found at\u000d\u000a        http:\/\/www.nature.nps.gov\/air\/studies\/air_toxics\/docs\/2008FinalReport\/08_FactSheet_HiRes_letter.pdf\u000a\u000d\u000a      \u000aBulletin\u000d\u000a          issued by NPS to summarise study findings and impacts,\u000d\u000a        corroborating the use of\u000d\u000a        Liverpool research.\u000d\u000a      Senior Research Environmental Scientist, U.S. Environmental Protection\u000d\u000a        Agency, can\u000d\u000a        corroborate, in a statement of support, the impact of the Liverpool\u000d\u000a        contribution on chemical\u000d\u000a        registration and regulation of pollution sources policy.\u000d\u000a      An announcement of the ban on Endosulfan may be found at\u000d\u000a        http:\/\/cep.unep.org\/repcar\/prohibicion-del-uso-de-endosulfan-en?set_language=en\u000a\u000d\u000a      Professor of Oceanography at Paleomagnetics and Coastal Mapping\u000d\u000a        Laboratory, University of\u000d\u000a        Rhode Island, has provided a letter of support to corroborate the impact\u000d\u000a        on Environmental\u000d\u000a        Management in the United States.\u000d\u000a      Coastal Ecologist at National Park Service, University of Rhode\u000d\u000a        Island, USA, can corroborate,\u000d\u000a        in a letter of support, that the manuscript `Journal of Coastal\u000d\u000a        Research', co-authored by\u000d\u000a        Appleby, has been instrumental in the National Parks decision making on\u000d\u000a        human-made\u000d\u000a        mosquito ditches\u000d\u000a      A National Park Service report on which the mosquito ditch management\u000d\u000a        decision was based\u000d\u000a        may be found at\u000d\u000a        http:\/\/www.nps.gov\/nero\/science\/FINAL\/FIIS_marsh\/FIIS_marsh_sealevel_final_July07_v2.pdf\u000a\u000d\u000a      The second report on which the mosquito ditch management decision was\u000d\u000a        based was Corman\u000d\u000a        SS, Roman CT, John W. King JW and Appleby PG, 2012. Salt Marsh\u000d\u000a        Mosquito-Control\u000d\u000a        Ditches: Sedimentation, Landscape Change, and Restoration Implications,\u000d\u000a        J Coastal Research\u000d\u000a        28:874-880. DOI: 10.2112\/JCOASTRES-D-11-00012.1.\u000d\u000a      Senior Ecologist, Broads Authority can corroborate the role research\u000d\u000a        played in informing the\u000d\u000a        Authorities policy on ecoboating and anti-foulant paint use on the\u000d\u000a        Broads.\u000d\u000a      \u000aBroads\u000d\u000a          factsheet on cleaner and greener boating, citing \"recent research\u000d\u000a        on past use of\u000d\u000a        antifouling paints\".\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Radiometric dating of environmental records in natural archives\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Note: All superscript references within this section refer to research\u000d\u000a      listed in Section 3\u000d\u000a    The 210Pb dating methodology is underpinned by research\u000d\u000a      carried out by Professor P G Appleby\u000d\u000a      and Dr G T Piliposyan (University of Liverpool Department of Mathematical\u000d\u000a      Sciences) on the 210Pb\u000d\u000a      cycle1,2,3. 210Pb is a naturally occurring fallout\u000d\u000a      radionuclide created in the atmosphere by the decay\u000d\u000a      of 222Rn gas, a daughter product of 226Ra that\u000d\u000a      enters the atmosphere by diffusion from land\u000d\u000a      surfaces. The 210Pb atoms are then quickly attached to\u000d\u000a      atmosphere particulates and deposited on\u000d\u000a      the landscape principally during rain. A fraction of this fallout 210Pb\u000d\u000a      deposited on a lake and its\u000d\u000a      catchment is transported through the water column and incorporated in\u000d\u000a      sediments forming on the\u000d\u000a      bed of the lake. Each layer so formed is in time buried by subsequent\u000d\u000a      deposits, and the initial\u000d\u000a      concentration reduced by radioactive decay. By measuring the present day\u000d\u000a      concentrations it is\u000d\u000a      then possible to calculate the age of each layer provided accurate\u000d\u000a      estimates can be made of the\u000d\u000a      original concentrations.\u000d\u000a    Although the general nature of this cycle has been reasonably well known\u000d\u000a      at a qualitative\u000d\u000a      level, mathematical models in recent papers by Piliposyan and Appleby1,2,3\u000d\u000a      have led to a much\u000d\u000a      better quantitative understanding of these processes, and in a more\u000d\u000a      reliable dating methodology in\u000d\u000a      which the 210Pb calculations for each site can be calibrated\u000d\u000a      against chronostratigraphic dates\u000d\u000a      determined from sediment records of 137Cs4, an\u000d\u000a      artificial radionuclide fallout of which originated\u000d\u000a      from the atmospheric testing of thermo-nuclear weapons, and nuclear\u000d\u000a      accidents such as\u000d\u000a      Chernobyl. The method has been used in numerous environmental studies,\u000d\u000a      e.g. climate change in\u000d\u000a      Tibet5\u000d\u000a    Key to the widespread use of these methods was the development of\u000d\u000a      non-destructive\u000d\u000a      methods for determining 210Pb, 226Ra and 137Cs\u000d\u000a      in environmental samples by gamma assay using\u000d\u000a      hyper-pure germanium well-type gamma spectrometers, pioneered at\u000d\u000a      Liverpool. One of the\u000d\u000a      inherent problems with this technique, particularly when analysing low\u000d\u000a      energy gamma photons\u000d\u000a      such as those emitted by 210Pb, is the effect of\u000d\u000a      self-absorption within the sample. Appleby and\u000d\u000a      Piliposyan have developed models for making corrections for such losses in\u000d\u000a      samples of varying\u000d\u000a      size, mass and composition6. These models have been\u000d\u000a      incorporated into a software suite used to\u000d\u000a      analyse data from the University of Liverpool Environmental Radiometric\u000d\u000a      Laboratory operated\u000d\u000a      jointly with colleagues in the Department of Physics and recognised as\u000d\u000a      internationally one of the\u000d\u000a      leading facilities of its kind in the world.\u000d\u000a    Because of their expertise both with the modelling and radio-analytical\u000d\u000a      techniques,\u000d\u000a      research groups from all around the world continue to seek the advice of,\u000d\u000a      and collaborate with\u000d\u000a      Appleby and Piliposyan on the implementation of the 210Pb\u000d\u000a      dating methodology in the context of a\u000d\u000a      wide range of projects concerned with recovering records of environmental\u000d\u000a      change stored in lake\u000d\u000a      sediments, salt-marshes and peat bogs. Recent projects in which they have\u000d\u000a      been partners\u000d\u000a      include\u000d\u000a    \u000d\u000a      US EPA (2003-6) Western Airborne Contaminants Assessment Project\u000d\u000a        studying trans-pacific\u000d\u000a        transport of pollutants from the Asian land mass.\u000d\u000a      US EPA (2005-8) Study of salt marsh sedimentation on Long Island and\u000d\u000a        its impact on\u000d\u000a        landscape structures.\u000d\u000a      Spanish Ministry of the Environment (2006-9) Study of reservoir\u000d\u000a        sedimentation rates in Spain\u000d\u000a      Alfred-Wegener-Institute, Potsdam, Germany (2009-12) Studies of recent\u000d\u000a        environmental\u000d\u000a        change in Tibet and Arctic Russia.\u000d\u000a    \u000d\u000a    "},{"CaseStudyId":"5573","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2017370","Name":"Russia"},{"GeoNamesId":"3175395","Name":"Italy"}],"Funders":["Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000a    Most modern 21st-century structures are designed so that damaging\u000a      low-frequency elastic vibrations can be avoided. But unexpected external\u000a      loads can still trigger unwanted shake and rattle, with some dangerous\u000a      consequences. Well-known examples are the Millennium Bridge in London and\u000a      the Volga Bridge in the Russian city of Volgograd. The first of these\u000a      opened as a footbridge across the Thames in 2000 but had to be shut soon\u000a      after for a major redesign after members of the public complained about it\u000a      moving excessively when they walked across it. The 7.1km road bridge over\u000a      the river Volga had similar problems when a resonant vibration caused\u000a      sections of the bridge to shake in May 2011, less than a year after it had\u000a      opened. The fact that problems arose, even though the Volga and Millennium\u000a      bridges were designed using fully certified packages, shows that it is all\u000a      too easy with large and complicated structures to overlook vibrations that\u000a      may cause structural problems under practical conditions [5.1].\u000a    During 2011-12, our proposed approach and the results of our novel\u000a      asymptotic analysis have been fully adopted by the design engineer, Dr\u000a      Gian Felice Giaccu and the design and construction company ICOSTRADE\u000a      S.R.L. in their working practices, combined with the industrial grade FEM\u000a      package. The use of this novel approach has made the design of elongated\u000a      structures more efficient and reliable by helping to avoid unwanted\u000a      low-frequency vibrations of the bridge systems, and possible design errors\u000a      similar to the ones that occurred in the Millennium Bridge and the Volga\u000a      Bridge. The financial implications of correcting a design error in a\u000a      bridge are substantial, as multi- million investments are required on\u000a      every occasion when such errors occur, and the new method has already\u000a      provided a very efficient practical tool. The company has written a letter\u000a      confirming the material benefits of this research on their practical\u000a      operations. To quote from the letter of support [5.2]: \"This new method\u000a        has now been adopted in our working practices, and we acknowledge a high\u000a        positive impact of this research on our business. The recent novel idea\u000a        of phase transition waves, originated from this research on long\u000a        bridges, we also recognized as a realistic mechanism of potential\u000a        failure of bridges and hence has been taken into account in the\u000a        definition of the safety margins for our industrial designs.''\u000a    The approach leading to routing of elastic waves around unwanted regions\u000a      has also delivered new multi-scale designs, adopted by the international\u000a      software development company ENGINSOFT, which specialises in industrial\u000a      computation and design. The work at ENGINSOFT was led by the project\u000a      manager, Mr Giovanni Borzi, and the algorithms are linked to structured\u000a      wave shields around stress concentrators such as voids or entrant corners,\u000a      as well as problems of thermal striping in structured solids. This work\u000a      was done in the framework of EU-funded grant [G2], valued at 2.34M Euro,\u000a      that develops Industry-Academia Partnerships and Pathways. The value of\u000a      the impact is high, as it has led to enhanced hybrid computational\u000a      algorithms of ENGINSOFT for analysis for dynamic response of multi-scale\u000a      structural systems with defects. This has advanced the capability of the\u000a      company, which has a multi-million turnover of industrial research\u000a      projects. A letter [5.3] has been provided by ENGINSOFT to confirm the\u000a      significant impact on their industrial work made by the research of A.B.\u000a      Movchan and his group. They say that Prof Movchan's new methods have \"offered\u000a        revolutionary new perspectives\" which \"have proved to be\u000a        extremely effective\". They confirm that these new methods \"have\u000a        been incorporated into our working practices and boosted our competitive\u000a        position\" with \"an immediate financial benefit already estimated\u000a        at around 250,000 Euros.\"\u000a    Efficient knowledge exchange events have been put in place to make sure\u000a      that the results and ideas of design for wave by-pass systems for\u000a      structured solids were exposed to the academic and industrial communities.\u000a      These include the International Workshop \"Elasticity Day\", held in\u000a      Liverpool in May 2012, and three industrial workshops, all organized and\u000a      held in Liverpool, on Modelling of Defects in Welds and Heterogeneous\u000a      Media (May 2008), Waves in Structured Media and Localisation (June 2009)\u000a      and Asymptotic &amp; Computational Models of Fracture and Wave Propagation\u000a      in Structured Media (May 2010) as well as an interdisciplinary workshop on\u000a      Metamaterial Structures and Dynamic Localisation Effects (December 2011).\u000a      This knowledge exchange has been also enhanced by the recent publication\u000a      of the article \"Bypassing Shake Rattle and Roll \" [5.1] in Physics World\u000a      [May, 2013]\u000a    ","ImpactSummary":"\u000a    It is well-known that certain bridges are susceptible to potentially\u000a      dangerous uncontrolled vibrations; recent examples include London's\u000a      Millennium Bridge and the Volga Bridge in Volgograd. Correcting such\u000a      problems after the construction of the bridge can be extremely expensive\u000a      and time-consuming. Research in the Department of Mathematical Sciences at\u000a      the University of Liverpool has led to a novel approach for predicting\u000a      such behaviour in advance and then modifying the bridge design so as to\u000a      avoid it. During the period 2011-12 this research has been incorporated\u000a      into standard design procedures by industrial companies involved in bridge\u000a      design. There is an economic impact for the companies concerned (avoiding\u000a      costly repairs after bridge construction) and a societal impact\u000a      (improvements in public safety and also avoiding the inconvenience of\u000a      long-term closure of crucial transport links).\u000a    The research is based on a novel, highly non-trivial approach that has\u000a      been developed to study properties of elastic waves in complex engineered\u000a      structures with a multi-scale pattern. The work has been taken up by the\u000a      industrial construction company ICOSTRADE S.R.L. Italy, whose design\u000a      engineer Dr Gian Felice Giaccu integrated the innovative research ideas\u000a      into their standard design procedures for complex structures such as\u000a      multiply supported bridges. Novel designs of wave by- pass systems\u000a      developed by the Liverpool group have also been embedded in standard\u000a      algorithms by the industrial software company ENGINSOFT, in the framework\u000a      of a project led by their project manager Mr. Giovanni Borzi.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Liverpool\u000a    ","Institutions":[{"AlternativeName":"Liverpool (University of)","InstitutionName":"University of Liverpool","PeerGroup":"A","Region":"North West","UKPRN":10006842}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"472757","Name":"Volgograd"}],"References":"\u000a    Research publications:\u000a    \u000a1. Brun M., Giaccu G.F., Movchan, A.B. &amp; Movchan, N.V. (2012)\u000a      Asymptotics of eigenfrequencies in the dynamic response of elongated\u000a      multi-structures. Proceedings of the Royal Society A: Mathematical,\u000a        Physical and Engineering Sciences, 468 (2138), 378-394.\u000a      doi:10.1098\/rspa.2011.0415\u000a    \u000a\u000a2. Colquitt, D. J., Jones, I. S., Movchan, N. V., &amp; Movchan, A. B.\u000a      (2011). Dispersion and localization of elastic waves in materials with\u000a      microstructure. Proceedings of the Royal Society A: Mathematical,\u000a        Physical and Engineering Sciences, 467(2134),\u000a      2874-2895. doi:10.1098\/rspa.2011.0126\u000a    \u000a\u000a3. Colquitt, D.J., Jones, I.S., Movchan, N.V., Movchan, A.B., Brun, M.\u000a      and McPhedran, R.C. (2013) Making waves round a structured cloak:\u000a      lattices, negative refraction and fringes. Proceedings of the Royal\u000a        Society A: Mathematical, Physical and Engineering Sciences, 469\u000a        (2157), 20130218. doi: 10.1098\/rspa.2013.021.\u000a    \u000aThe journal Proceedings of the Royal Society A: Mathematical,\u000a        Physical and Engineering Sciences has an Impact Factor of 2.34 and\u000a      is ranked 11 out of 56 journals in its sector by Web of Knowledge.\u000a    External grant support:\u000a    G1. European Commission FP7 Grant. Industry-Academia Partnerships and\u000a      Pathways. PIAP-GA- 284544-PARM-2, 1.78M Euro, Liverpool share &#163;180,000.\u000a    G2. European Commission FP7 Grant. Industry-Academia Partnerships and\u000a      Pathways. PIAP-GA- 286110-INTERCER2, 2.34M Euro, Liverpool share &#163;160,000.\u000a    \u000a    G3. EPSRC research grant EP\/D035082\/1 of &#163;144,744 on \"Thermal vibrations\u000a      and localization for solids with singularly perturbed boundaries\".\u000a    G4. European Commission Marie Curie grant on Modelling of smart composite\u000a      interfaces and dynamically resistant systems,\u000a      PIEF-GA-2011-302357-DYNAMETA, &#163;220,257.\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"5","Subject":"Civil Engineering"},{"Level1":"11","Level2":"9","Subject":"Neurosciences"},{"Level1":"9","Level2":"1","Subject":"Aerospace Engineering"}],"Sources":"\u000a    5.1 Bypassing shake, rattle and roll, Feature Article by M. Brun, A.\u000a      Movchan, I. Jones and R. McPhedran, Physics World, May 2013.\u000a    5.2 The Design Engineer at ICOSTRADE S.R.L. and colleagues were primary\u000a      users of the new findings for by-pass metamaterial systems in the design\u000a      of bridges and routing low frequency vibrations away from the main deck\u000a      and corroborate this in a statement of support.\u000a    5.3 The Project Manager at ENGINSOFT, can corroborate, in a letter of\u000a      support that \"the research results of the Liverpool group have made a\u000a        significant impact on the development of novel hybrid algorithms...\"\u000a    \u000a    ","Title":"\u000a    Metamaterial systems and routing of elastic waves in engineered\u000a      structures\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"},{"GeoNamesId":"2644210","Name":"Liverpool"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research was produced by the research group in Waves and\u000a      Solid Mechanics at the University of Liverpool. The personnel engaged in\u000a      the work includes Professors A.B Movchan, N.V. Movchan, R.C. McPhedran and\u000a      Dr Michele Brun, as well as the PhD students Mr Stewart Haslinger and Mr\u000a      Daniel Colquitt. The work has attracted external support in the form of an\u000a      EPSRC grant [G3], large scale EU-funded Industry-Academia Partnerships and\u000a      Pathways grants [G1, G2] involving industrial partners in Italy, Poland,\u000a      Israel and Ukraine, the Duncan Norman Memorial Fellowship, and the EU\u000a      Commission Marie Curie grant [G4]. It should be emphasised that in line\u000a      with our Impact strategy, one of the end-users, Dr Giacci, has been\u000a      closely involved with the research (and indeed is a co-author on one of\u000a      the research papers). This has enabled us to tailor the research closely\u000a      to the end-users' requirements but also implies that the research and\u000a      impact are closely intertwined. For completeness, but at the risk of some\u000a      repetition later, we shall describe the whole process here.\u000a    The concepts of waveguides and wave bypass structures are useful in a\u000a      wide variety of contexts, ranging from natural systems which have\u000a      biologically useful visual properties, to the optical fibre technologies\u000a      which power the internet. These powerful ideas have not yet been\u000a      effectively implemented in other areas of wave science. In particular,\u000a      they can prove very effective in providing simple analytic models and\u000a      results for the vibrations occurring in large multiply supported\u000a      structures such as bridges, but seem to have been largely overlooked in\u000a      this context; an omission which has now been remedied by our research.\u000a    An important simplifying concept which has proved extremely successful is\u000a      to replace the large multiply supported structure by a single unit cell of\u000a      an appropriate periodic structure, as described in [1]. This replacement\u000a      permits the use of techniques familiar in Solid State Physics, known as\u000a      Bloch Mode Analysis. These Bloch Modes give the allowed vibration patterns\u000a      in the building blocks (unit cells) of periodic structures. There are well\u000a      developed techniques for finding these Bloch Modes, and also for detecting\u000a      whether such vibration modes have been overlooked. Addition of a high\u000a      contrast wave by-pass system leads to new dispersion properties of Bloch\u000a      waves which are exploited to divert wave energy away from the bridge deck.\u000a      Our background work on the analysis of Bloch waves in complex periodic\u000a      systems is described in Refs. [1-3] listed below. Of particular relevance\u000a      here is the localisation in multi-scale engineered systems studied in\u000a      Refs. [2, 3].\u000a    These ideas have been developed into a method of analysing large multiply\u000a      supported bridge structures in order to identify and avoid unwanted\u000a      vibrations. This method does not replace the results of the complex\u000a      industrial design packages which are currently used to provide final\u000a      designs. However, it does provide a realistic appreciation of the\u000a      characteristics and frequency ranges of vibrational modes which are likely\u000a      to prove troublesome to the reliable performance of the bridge over a wide\u000a      variety of environmental conditions. Importantly, it also provides an\u000a      immediate indication as to whether any important vibrational modes have\u000a      been overlooked in the voluminous results provided by commercial design\u000a      packages. The knowledge of these troublesome frequencies is then used in\u000a      the design of a lightweight \"wave bypass\" structure that diverts the\u000a      vibrations away from load-bearing elements. Further \"dumping\" of unwanted\u000a      vibrational modes is applied as appropriate. The bypass structure\u000a      represents a highly directive system that re-routes the waves around the\u000a      bridge deck, which is then shielded from vibrations within the unwanted\u000a      frequency range. The design involves considering the deck of a bridge as a\u000a      slender solid lying on pillars placed at regularly spaced intervals. By\u000a      analysing the vibration of each repeating element or \"unit cell\" of the\u000a      bridge, deflection of the unwanted modes away from the bridge deck is\u000a      achieved by adding a system of linked resonators. The advantage of this\u000a      approach is that the total mass of each resonator is several orders of\u000a      magnitude less than the bridge itself, while the bars linking the\u000a      resonators will have a relatively low stiffness. Such structures are\u000a      easily predesigned by evaluating their frequencies of vibration when they\u000a      are isolated from the bridge. A crucial feature is that this design does\u000a      not require any change in the way the main deck is attached to the\u000a      supporting pillars.\u000a    "},{"CaseStudyId":"5574","Continent":[{"GeoNamesId":"6255150","Name":"South America"},{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"3469034","Name":"Brazil"},{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"2921044","Name":"Germany"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The impacts described here were developed by Professor Roberto Cipolla,\u000d\u000a      based on the fundamental research with Professor Giblin described earlier.\u000d\u000a      As Prof. Cipolla states in a letter [5.1], \"this work with Giblin and\u000d\u000a        Astrom laid the essential groundwork for later developments by myself\u000d\u000a        and my students and other collaborators, leading to important practical\u000d\u000a        applications with significant external impact, such as those to\u000d\u000a        scultpure (Anthony Gormley) and online retailing (Metail)\".\u000d\u000a    Applications in Sculpture.\u000d\u000a    Renowned sculptor Antony Gormley (creator of the \"Angel of the North\" in\u000d\u000a      Gateshead and \"Another Place\" in Crosby) designed and built a\u000d\u000a      25-metre-high statue of a crouching human body, entitled \"Exposure\", which\u000d\u000a      was unveiled in 2010 [5.3] in Lelystad in the Netherlands. The statue is\u000d\u000a      built from interlocking pylon-like structures. This was achieved using\u000d\u000a      software developed by Professor Cipolla at Cambridge, based on the\u000d\u000a      fundamental research on singularity theory described above, carried out\u000d\u000a      with Peter Giblin and published in joint work. This allowed 3-dimensional\u000d\u000a      models to be reconstructed from a small number of ordinary camera images,\u000d\u000a      in this case from 2-dimensional photographs of a cast of the artist's\u000d\u000a      body. According to Gormley [5.4], this software is \"unique in the world:\u000d\u000a      it's extraordinary to get a fully rotational model from a standard\u000d\u000a      single-lens digital camera\".\u000d\u000a    The sculpture \"Exposure\" has been the subject of international news\u000d\u000a      coverage [5.3], and is now an important landmark on the coast of the\u000d\u000a      Netherlands. The impact here is cultural, since this work of art would not\u000d\u000a      exist without the software developed from research completed in our UoA,\u000d\u000a      and the public, especially in the Netherlands, have benefitted from this\u000d\u000a      thought-provoking and impressive work.\u000d\u000a    Applications in Retail.\u000d\u000a    Metail is a company, founded in 2008 [5.5] and based in London and\u000d\u000a      Cambridge, that offers \"virtual fitting rooms\". It now has 40 employees.\u000d\u000a      The website allows users to easily generate a 3- dimensional personal body\u000d\u000a      model to see how clothes would fit them online, prior to purchase.\u000d\u000a      Shoppers are able to visualize themselves wearing complete outfits. The\u000d\u000a      company's founder and CEO, Tom Adeyoola, first explored the possibilities\u000d\u000a      of computer vision in the context of card recognition for on-line gaming.\u000d\u000a      An internet search led to Roberto Cipolla and a personal contact with him.\u000d\u000a      Tom was particularly impressed by the current preparations for Gormley's\u000d\u000a      \"Exposure\" sculpture. Subsequently an ex-PhD student of Cipolla's was\u000d\u000a      commissioned for the project of developing software for the virtual\u000d\u000a      fitting room, based on the underpinning joint research with Peter Giblin\u000d\u000a      on singularity theory described above.\u000d\u000a    Metail has raised over &#163;4m in investment, filed patents and won a &#163;100k\u000d\u000a      grant from the Government's Technology Strategy Board in 2012 and a\u000d\u000a      further &#163;250k grant in August 2013. Metail launched to the public with its\u000d\u000a      first trial commercial partner, Tesco, in February 2012 [5.6]. The trial\u000d\u000a      was a success and went on to win an internal Tesco award for \"Best on-line\u000d\u000a      innovation\" (voted for by the CEO Phil Clarke and the executive team). At\u000d\u000a      the end of September 2012 Metail launched commercially on Tesco's main\u000d\u000a      website, opening the \"F&amp;F Virtual Fitting Room\". This website enable\u000d\u000a      Tesco customers to try clothing online before they buy. The website is run\u000d\u000a      in association with Facebook and the Home Page says \"The F&amp;F Virtual\u000d\u000a      Fitting Room has been so successful on Facebook that we have made it part\u000d\u000a      of our main website!\". Subsequently Metail has launched with the clothing\u000d\u000a      retailer Warehouse, and also with Zalando Germany (Germany's fastest\u000d\u000a      growing clothing retailer), and the UK's largest on-line retailer, Shop\u000d\u000a      Direct. Metail went on to launch their first live show TV experience 'Take\u000d\u000a      over the Makeover' with ITV's programme \"This Morning\" in December 2012;\u000d\u000a      this feature has been running monthly since March 2013. They launched\u000d\u000a      their second international client of Dafiti in Brazil in February. Metail\u000d\u000a      is currently generating around &#163;70k per month in revenue and in March 2013\u000d\u000a      registered 80,000 users on their website. Over 300,000 people have created\u000d\u000a      \"MeModels\" using their websites since February 2012. The impact here has\u000d\u000a      therefore been economic, since Metail owes its existence and current\u000d\u000a      success to the software developed from our research; and societal, since a\u000d\u000a      large number of consumers in several countries have been enabled to\u000d\u000a      \"virtually\" try on clothing before purchasing online. The CEO of Metail\u000d\u000a      can confirm these details of company performance [5.7].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Professor Peter Giblin (Department of Mathematical Sciences at the\u000d\u000a      University of Liverpool), together with collaborators, used methods from\u000d\u000a      singularity theory to develop an approach for recovering 3-d information\u000d\u000a      from 2-d images, such as photos. In the past decade, these have been\u000d\u000a      implemented and built upon by software engineers, leading to significant\u000d\u000a      cultural, economic and societal impacts. These include the creation of an\u000d\u000a      innovative 25m high sculpture of the human body in the Netherlands by the\u000d\u000a      sculptor Antony Gormley and the virtual modelling of clothing on online\u000d\u000a      clothing websites such as Tesco's (Virtual Changing Room by\u000d\u000a      Tesco\/F&amp;F). These have reached thousands of consumers worldwide and\u000d\u000a      represent a significant commercial success for the company which developed\u000d\u000a      the software.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Liverpool\u000d\u000a    ","Institutions":[{"AlternativeName":"Liverpool (University of)","InstitutionName":"University of Liverpool","PeerGroup":"A","Region":"North West","UKPRN":10006842}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2751738","Name":"Lelystad"}],"References":"\u000d\u000a    \u000a[3.1] P.J.Giblin, F.E.Pollick and J.E.Rycroft, `Recovery of an unknown\u000d\u000a      axis of rotation from the profiles of a rotating surface', J.Optical Soc.\u000d\u000a      America 11A (1994), 1976--1984, DOI: 10.1364\/JOSAA.11.001976.\u000d\u000a    \u000a\u000a[3.2] K. &#197;str&#246;m, R.Cipolla and P.J.Giblin, `Generalised epipolar\u000d\u000a      constraints', International J. Computer Vision 33 (1999) 51&#8212;72, DOI: 10.1023\/A:1008113231241.\u000d\u000a      This is an expanded version, with more detail of the methods and results,\u000d\u000a      of the article with the same name in Proc. European Conference on Computer\u000d\u000a      Vision (ECCV) 1996, Lecture Notes in Computer Science 1065,\u000d\u000a      Springer-Verlag, 97--108.\u000d\u000a    \u000a\u000a[3.3] R.Cipolla and P.J.Giblin, Visual Motion of curves and surfaces,\u000d\u000a      viii + 184pp. Cambridge University Press, 2000.\u000d\u000a    \u000aEarly results were published in [3.4] R.Cipolla, K. &#197;str&#246;m and\u000d\u000a      P.J.Giblin, `Motion from the frontier of curved surfaces', Proc. Int.\u000d\u000a      Conf. on Computer Vision (ICCV) June 1995 (IEEE Computer Society Press),\u000d\u000a      pp. 269--275.\u000d\u000a    The research is published in leading international journals devoted to\u000d\u000a      (often theoretical) computer vision. It has been well-cited, with [3.1]\u000d\u000a      accruing 19 citations and [3.2] 14 citations as at September 2013.\u000d\u000a      According to the Web of Science, the Journal of the Optical Society of\u000d\u000a      America A has an Impact Factor of 1.67 and an Article Influence Index of\u000d\u000a      0.62 and is ranked 30th out of 80 journals on Optics; the\u000d\u000a      International Journal of Computer Vision has an Impact Factor of 3.62 and\u000d\u000a      an Article Influence Index of 3.01 and is ranked 9th out of 115\u000d\u000a      journals on Computer Science: Artificial Intelligence.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"}],"Sources":"\u000d\u000a    [5.1] Collaborating Professor from University of Cambridge, has provided\u000d\u000a      a statement of support to corroborate the connection between the\u000d\u000a      theoretical work with Peter Giblin and the impacts described above.\u000d\u000a    [5.2] Publications\u000d\u000a      from Cipolla's group directly related to the impact, such as continue to\u000d\u000a      refer to the fundamental mathematical work, establishing its foundational\u000d\u000a      nature for the impact.\u000d\u000a    [5.3] A\u000d\u000a        report on Antony Gormley's sculpture, including a description of\u000d\u000a      Prof. Cipolla's involvement, Archived at: http:\/\/www.webcitation.org\/69XxyH01M\u000d\u000a      Other coverage: http:\/\/www.mymodernmet.com\/profiles\/blogs\/antony-gormley-exposure\u000d\u000a    [5.4] Gormley quote, emphasising the critical role of the software based\u000d\u000a      on our singularity research; in an article at: http:\/\/www.eng.cam.ac.uk\/news\/stories\/2011\/Antony_Gormley\/\u000d\u000a    [5.5] A description of Metail,\u000d\u000a      including their connection to Professor Cipolla. Archived at http:\/\/www.webcitation.org\/69Xy7Vz2H\u000d\u000a      ).\u000d\u000a    [5.6] The announcement of the opening of the F&amp;F online fitting room\u000d\u000a      based on Metail software can be found at http:\/\/internetretailing.net\/2012\/03\/tesco-launches-virtual-ff-fitting-room\u000d\u000a      and a \"You-Tube\" video showing the operation of the online fitting room\u000d\u000a      can be viewed at http:\/\/www.youtube.com\/watch?v=UhYlvCROhck\u000d\u000a    [5.7] The CEO of Metail can be contacted to corroborate company\u000d\u000a      performance and impact of the software on the formation and development of\u000d\u000a      the business. \u000d\u000a    ","Title":"\u000d\u000a    Applications of Singularity Theory and 3D Modelling in Arts and Retail\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2653941","Name":"Cambridge"},{"GeoNamesId":"2643743","Name":"London"},{"GeoNamesId":"2648773","Name":"Gateshead"},{"GeoNamesId":"2644210","Name":"Liverpool"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The research underpinning this impact was undertaken by Professor Peter\u000d\u000a      Giblin, working in the Singularity Theory Group at the Department of Pure\u000d\u000a      Mathematics, now part of the Department of Mathematical Sciences.\u000d\u000a      Professor Giblin worked on the mathematical foundations of Computer\u000d\u000a        Vision, that is, the recovery of 3-dimensional information about the\u000d\u000a      world from 2-dimensional images, perhaps from several viewpoints or from a\u000d\u000a      moving (calibrated) camera whose positions in space may not be known.\u000d\u000a      Specifically, the impact is underpinned by research undertaken from\u000d\u000a      1995-2000, mainly in collaboration with Cipolla (Cambridge) and &#197;str&#246;m\u000d\u000a      (Lund, Sweden), on using 2-d images to recover an arbitrary camera motion\u000d\u000a      and the geometry of a surface.\u000d\u000a    One of the key ingredients of recovery, or reconstruction, is the use of\u000d\u000a      `apparent contours', also called `profiles', which are similar to\u000d\u000a      silhouettes of surfaces. There is a corresponding idea for the apparent\u000d\u000a      contour from a point of 3-space, which more precisely models views of an\u000d\u000a      object from a stationary, or moving camera. Prior to the research\u000d\u000a      described here, contours were discussed by Giblin and Weiss, and by A.\u000d\u000a      Blake and R. Cipolla, in the context of reconstructing a surface given the\u000d\u000a      camera motion.\u000d\u000a    An important development came through adding the ingredient of `frontier\u000d\u000a      points', first investigated theoretically in the PhD thesis of Giblin's\u000d\u000a      student Jeanette Rycroft (Liverpool 1992). A special case of circular\u000d\u000a      motion was worked out in detail [3.1] by Giblin, Rycroft and F.Pollick\u000d\u000a      (the latter working on the psychology of vision). An even more remarkable\u000d\u000a      idea was that by using suitable optimizing techniques an arbitrary camera\u000d\u000a      motion and the geometry of the surface, could be recovered from\u000d\u000a      measurements of apparent contours and frontier points. This was\u000d\u000a      investigated in research (1995- 1999) between Giblin, R.Cipolla (by then\u000d\u000a      in the Cambridge Engineering Department) and K. &#197;str&#246;m (Lund, Sweden), the\u000d\u000a      main article being published in 1999 [3.2]. Much of the theory was also\u000d\u000a      described, with extensive background material, in the book by Cipolla and\u000d\u000a      Giblin [3.3], which has 204 citations according to Google Scholar.\u000d\u000a    As is often the case in mathematics, the impact of this research was not\u000d\u000a      direct, but rather filtered through further work in software engineering.\u000d\u000a      The ideas introduced were extended and made highly practical by several of\u000d\u000a      Cipolla's PhD students and postdoctoral workers. (Giblin continued to act\u000d\u000a      as external examiner on these topics in Cambridge and in Lund, but his\u000d\u000a      work has now turned elsewhere.) Publications from Cipolla's group continue\u000d\u000a      to refer to the fundamental mathematical work (especially Ref.[3.3]).\u000d\u000a    "},{"CaseStudyId":"5819","Continent":[],"Country":[],"Funders":["Wellcome Trust","Research Councils UK"],"ImpactDetails":"\u000a    In March 2012, McLaughlin presented his research to a general audience as\u000a      part of the Newcastle Science Festival. He interviewed a subset of the\u000a      audience after his presentation, and realised that such interviews,\u000a      evaluation data and user feedback could be used to measure changes in\u000a      knowledge and behaviour. He recognised that a one-off presentation might\u000a      influence audience members on the immediate timescale, but a sustained\u000a      programme with the same group was needed to embed a lasting benefit.\u000a      Consequently, he devised a multiple visit, ongoing outreach programme\u000a      where he interacted with the same group multiple times in a\u000a      structured set of exercises arising from his recent research. He worked\u000a      with two sixth-form\/Further Education colleges: (a) Newcastle\u000a        College and (b) Gosforth\u000a        Academy in a collaborative outreach programme. The aim was to\u000a      investigate the positive effects and benefits of public engagement at\u000a      these sixth-form colleges using outreach materials underpinned by\u000a      McLaughlin's research into Solar Physics and MHD since 2010.\u000a    During October 2012 to June 2013 (2012\/13 school year), McLaughlin\u000a      visited these sixth-form colleges on multiple, independent occasions and\u000a      delivered five individual presentations (of increasing complexity) on the\u000a      following topics: (i) Introduction to the Sun; (ii) The Sun and its effect\u000a      on the Earth and Space Travel; (iii) Electromagnetism and MHD; (iv)\u000a      Special Relativity; and (v) Mars. Each presentation session lasted for one\u000a      hour, and was divided into a 20 minute oral presentation followed by 5-10\u000a      minutes of questions, followed by a second 20 minute session, again\u000a      followed by 5-10 minutes of questions. This format was judged most\u000a      appropriate for the audience.\u000a    Evidence gathering - Evaluation questionnaires were\u000a      completed by the participants at the end of the fifth presentation and an\u000a      analysis of the responses was performed. The questionnaires (available on\u000a      request) were constructed using the Likert scale method (with option for\u000a      free text and open questions) and were developed in collaboration with a\u000a      developmental psychologist. Registers were taken at each of the five\u000a      presentations to ensure that students completing the questionnaires had\u000a      indeed attended the whole outreach programme. Hence, the 50 beneficiaries\u000a      represent a subset of the total number of students studying AS-level\u000a      Physics across both colleges. Data from the 50 evaluation questionnaires\u000a      was used to evidence the impact on the change in participants' interest,\u000a      knowledge, engagement and motivation. There was 100% response rate from\u000a      all 50 participants who individually each answered 100% of the questions.\u000a    Impact (on society, culture and creativity) - The\u000a      public engagement programme resulted in multiple impacts as defined under\u000a      \"Impacts on society, culture and creativity\":\u000a    \u000a      the beneficiaries' interest in science was stimulated;\u000a      the beneficiaries' engagement with science was improved;\u000a      their science-related education was enhanced;\u000a      the outreach programme made the participants excited about the science\u000a        topics covered;\u000a      the awareness and understanding of the beneficiaries was improved by\u000a        engaging them with the research;\u000a      evidence of an improvement in both AS-level grades, and student\u000a        retention.\u000a    \u000a    This was evidenced by questionnaire responses (user feedback),\u000a      participants' quotes, AS-results and factual statements. A summary of\u000a      responses is given below (e.g. 92% rated overall programme as good or very\u000a      good) with full survey feedback available on request.\u000a    Specific impacts and specific evidence\u000a    The impact, the beneficiaries' interest in science was stimulated,\u000a      was evidenced by the results of two feedback questions:\u000a    \u000a      \u000aAs a direct result of Dr McLaughlin's outreach programme, are you\u000a          now more interested in science as a subject than you were before?\u000a        54% of responses indicated that they were more (or much more)\u000a        interested.\u000a      \u000aPlease indicate to what extent you agree or disagree with the\u000a          following statement: \"My interest in science has been stimulated as a\u000a          direct result of Dr McLaughlin's outreach programme\". 64%\u000a        of responses agreed (or strongly agreed).\u000a    \u000a    The impact, the beneficiaries' engagement with science was improved,\u000a      was evidenced by:\u000a    \u000a      \u000aAs a direct result of Dr McLaughlin's outreach programme, are you\u000a          now more likely to talk to your teacher about science? 32%\u000a        of responses indicated that they were more (or much more) likely.\u000a      \u000aAs a direct result of Dr McLaughlin's outreach programme, are you\u000a          now more likely to consider studying science at university? 34%\u000a        of responses indicated that they were more (or much more) likely.\u000a    \u000a    The impact, their science-related education was enhanced, was\u000a      evidenced as follows:\u000a    \u000a      \u000aIndicate to what extent you agree or disagree with the following\u000a          statement: \"My science-based education has been enhanced as a direct\u000a          result of Dr McLaughlin's outreach programme\". 68% of\u000a        responses agreed (or strongly agreed).\u000a    \u000a    The impact, the outreach programme made the participants excited\u000a        about the topics covered, was evidenced by the results of the\u000a      following feedback question:\u000a    \u000a      \u000aDid the outreach programme make you excited about the science\u000a          topics covered? 80% of responses indicated yes (or yes to\u000a        a strong extent).\u000a    \u000a    The impact, the awareness and understanding of the beneficiaries was\u000a        improved by engaging them with the research, was evidenced by the\u000a      following illustrative feedback, i.e. participants' quotes from survey\u000a      (quotations may also evidence the other impacts):\u000a    \u000a      \"It inspired me to do a space based EPQ (about Mars)\".\u000a        [Extended Project Qualifications are part of level three of the National\u000a        Qualifications Framework].\u000a      \"I would like to take geology at university and am now\u000a            going to look at the courses to see if they include\u000a          geology on other planets\".\u000a      \"It has extended my knowledge of science and\u000a          gave me more motivation to do well in science.\u000a          I am much more enthusiastic about it now\".\u000a      \"It did make me more enthusiastic about\u000a          physics, discovering new things, and realising that there are so many\u000a          things that you don't know\".\u000a      \"It's given me more knowledge on the\u000a          subjects discussed. It's made me more interested in\u000a          learning physics in more detail, rather than just what is learnt in\u000a          lessons\".\u000a      \"I realise that I can research topics myself in\u000a          order to increase my knowledge\".\u000a      \"I feel more happy going into A2 with\u000a            a higher knowledge about space. A lot of the topics\u000a          discussed I wasn't confident on beforehand\".\u000a    \u000a    Additional Significance - This was a sustained and ongoing\u000a      engagement with the groups (sustainability as well as secondary\u000a        reach). Specifically, the same group of students was seen at five\u000a      individual points over the period October 2012 to June 2013 (i.e. 50\u000a      students had attended all five presentations). It was believed that the\u000a      information, knowledge and benefits of the outreach programme `take time\u000a      to really sink in', and so an outreach programme spread across a school\u000a      year seemed appropriate. Moreover, McLaughlin's research (which\u000a      underpinned the outreach material) is specialist material, and it took\u000a      time to build the knowledge and context for the audience to a mature level\u000a      in order to engage properly with and understand the underpinning research.\u000a    A factual statement from the Head of Physics at Newcastle College states:\u000a      \"The topics covered encouraged several students with a lower than\u000a        average ALIS (Advanced Level\u000a        Information System) predicted grade to attend the presentations.\u000a        Engaging some of these students was an achievement in itself and from\u000a        the discussions that followed it seemed to have a motivational effect.\u000a        Statistically speaking these students have a lower than average chance\u000a        of achieving the high grades required in order to gain a place at\u000a        university making engaging and motivating them to learn even more\u000a        important\". A comparison of the AS-level Physics predicted grades\u000a      versus actual grades across the Newcastle College students showed a clear\u000a      increase in grades, specifically:\u000a    \u000a      \u000aPredicted grades: A= 0%, B=18%, C=23%; D=14%; E=45%\u000a      \u000aActual grades: A=18%, B=27%, C=18%; D=9%; E=27%\u000a    \u000a    with an average increase of +0.86 grade per students (not uniform\u000a      increase). Thus, from the factual statement and grade comparison, there is\u000a      evidence that the programme contributed to an improvement in AS-level\u000a      grades, i.e. improved attainment.\u000a    A factual statement from Gosforth Academy states: \"This year 60% of\u000a        students have continued from AS level Physics to A2 level Physics\u000a        compared to an average of 50% over the last few years. I would not be\u000a        able to hold Dr McLaughlin completely responsible for this, but do\u000a        believe that his delivery of lectures through the year has indeed\u000a        partially contributed to this success\", i.e. there is some\u000a      (indirect) evidence of increased retention and progression.\u000a    Beneficiaries and Additional Reach - The target audience\u000a      was the first year of sixth form of AS- level Physics. Across the two\u000a      colleges, data was collected from 50 AS-level students (16-17 year olds),\u000a      consisting of 39 males and 11 females where, for STEM subjects, the NCCPE\u000a      (National Co- ordinating Centre for Public Engagement) defines female\u000a      students as a `hard to reach' audience.\u000a    This case study details the impact arising from public engagement\u000a      activity and follows the recommendations of the NCCPE and their guidance\u000a      for assessing REF impact arising from public engagement with research. The\u000a      NCCPE is part of the Beacons for Public Engagement project, funded by the\u000a      UK Funding Councils, Research Councils UK and the Wellcome Trust, and the\u000a      NCCPE is at the forefront of the national debate on public engagement. The\u000a      Director of the NCCPE has described McLaughlin's public engagement\u000a      programme as \"an impressive and thoughtful project\". In 2014, the\u000a      NCCPE will be drafting guidelines of best practice and useful tools to\u000a      apply to the next REF exercise. As a direct result of his public\u000a      engagement programme, the Director of the NCCPE has invited McLaughlin to\u000a      be part of this process.\u000a    ","ImpactSummary":"\u000a    This case study details the impact arising from a sustained public\u000a      engagement activity with sixth-form students (16 to 17 year-olds) across\u000a      two Further Education Colleges during 2012\/13. The activity was\u000a      underpinned by research carried out in the Unit (2010-2012). The programme\u000a      resulted in multiple impacts as defined under \"Impacts on society,\u000a        culture and creativity\". Specifically:\u000a    \u000a      the beneficiaries' interest in science was stimulated;\u000a      the beneficiaries' engagement in science was improved;\u000a      their science-related education was enhanced;\u000a      the outreach programme made the participants excited about the science\u000a        topics covered;\u000a      the beneficiaries' awareness and understanding was improved by\u000a        engaging them with the research;\u000a      evidence of an improvement in both AS-level grades and in student\u000a        retention.\u000a    \u000a    These impacts are evidenced by the user feedback collected from 50\u000a      questionnaires, factual statements from the teachers and individual\u000a      participants. This case study details the impact arising from public\u000a      engagement as described in the recommendations of the National\u000a      Co-ordinating Centre for Public Engagement (NCCPE).\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of Northumbria at Newcastle\u000a    ","Institutions":[{"AlternativeName":"Northumbria at Newcastle (University of)","InstitutionName":"Northumbria University Newcastle","PeerGroup":"D","Region":"North East","UKPRN":10001282}],"Panel":"B         ","PlaceName":[],"References":"\u000a    (* references which best indicate quality of underpinning research)\u000a    \u000a[1] McLaughlin, J.A., Hood, A.W. &amp; De Moortel, I. (2011)\u000a      Review Article: MHD Wave Propagation Near Coronal Null Points of Magnetic\u000a      Fields Space Science Reviews, 158, pp. 205-236. http:\/\/dx.doi.org\\10.1007\\s11214-010-9654-y\u000a    \u000a\u000a[2*] Morton, R.J., Verth, G., McLaughlin, J.A. &amp; Erd&#233;lyi, R. (2012)\u000a      Determination of sub-resolution structure of a jet by solar\u000a      magnetoseismology Astrophysical Journal, 744, 5-15. http:\/\/dx.doi.org\\10.1088\\0004-637X\\744\\1\\5\u000a    \u000a\u000a[3*] McLaughlin, J.A., Verth, G., Fedun, V. &amp; Erd&#233;lyi, R. (2012)\u000a      Generation of quasi-periodic waves &amp; flows in the solar atmosphere by\u000a      oscillatory reconnection\u000a      Astrophysical Journal, 749, 30-40. http:\/\/dx.doi.org\\10.1088\\0004-637X\\749\\1\\30\u000a    \u000a\u000a[4] McLaughlin, J.A., Thurgood, J.O. &amp; MacTaggart, D. (2012) On the\u000a      periodicity of oscillatory reconnection\u000a      Astronomy &amp; Astrophysics, 549, A98. http:\/\/dx.doi.org\\10.1051\\0004-6361\\201220234\u000a    \u000a\u000a[5*] Morton, R.J. &amp; McLaughlin, J.A. (2013) Hi-C and AIA observations\u000a      of transverse MHD waves in active regions Astronomy &amp; Astrophysics,\u000a      553, L10. http:\/\/dx.doi.org\\10.1051\\0004-6361\\201321465\u000a    \u000aRelevant Grants\u000a    &#8226; McLaughlin, J.A. (2012-2013), &#163;4,000, Northumbria University HEIF\u000a      funding: Engagement Event Funding.\u000a    &#8226; McLaughlin, J.A. (2013-2014), FA8655-13-1-3067, &#163;47.4k, \"The Hunt\u000a        for the Missing Modes: Revealing the True Nature of the Solar Wind\",\u000a      US Air Force Office for Scientific Research.\u000a    ","ResearchSubjectAreas":[{"Level1":"2","Level2":"2","Subject":"Atomic, Molecular, Nuclear, Particle and Plasma Physics"}],"Sources":"\u000a    \u000a      Summary\/breakdown of responses (corroborating all impacts).\u000a      Selected participants' quotations (corroborating all impacts).\u000a      Predicted &amp; actual grades of Newcastle College AS-level students (showing\u000a          grade increases).\u000a      Factual statement from Newcastle College (corroborating positive\u000a          impact on beneficiaries' interest, educational enhancement, and\u000a          engagement).\u000a      Factual statement from Gosforth Academy (corroborating impacts on\u000a          beneficiaries' interest and engagement, educational enhancement, and\u000a          evidence of improved retention and progression).\u000a      Invitation from the Director of the NCCPE.\u000a    \u000a    Copies of these documents are available on request. \u000a    ","Title":"\u000a    Sustained Public Engagement Underpinned by Magnetohydrodynamics and Solar\u000a      Physics Research: A Measured Increase In Learning Outcomes\u000a    ","UKLocation":[{"GeoNamesId":"2641673","Name":"Newcastle-upon-Tyne"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Dr McLaughlin (Senior Lecturer at Northumbria University since January\u000a      2010) is interested in problems in fluid dynamics and mathematical\u000a      modelling of solar and astrophysical processes. His research involves\u000a      solving nonlinear, three-dimensional, coupled systems of PDEs under\u000a      various physical assumptions. He approaches this research using both\u000a      analytical techniques and a variety of numerical methods, including the\u000a      use of parallel computing. In 2011, he published a detailed review article\u000a      on MagnetoHydroDynamic (MHD) wave behaviour within inhomogeneous magnetic\u000a      media [1]. In 2012, McLaughlin reported a novel application of\u000a      magnetoseismology to describe the properties of a previously unseen dark\u000a      thread accompanying a solar jet [2]. Magnetoseismology is the application\u000a      of MHD wave theory to magnetic wave observations to probe the plasma. In\u000a      2012 he reported new results from oscillatory reconnection\u000a      (time-dependent, wave-generating reconnection) which demonstrated that\u000a      oscillatory reconnection driven by magnetic flux emergence provides a\u000a      natural explanation for generating the observed (transverse) solar jets\u000a      [3], and investigated the sensitivity of the reconnection mechanism to\u000a      various parameters [4]. In 2013, he analysed high-resolution observations\u000a      from state-of-the-art solar satellites [5].\u000a    In 2012, McLaughlin constructed a five-part outreach programme of\u000a      presentations: (i) Introduction to the Sun; (ii) The Sun and its effect on\u000a      the Earth and Space Travel; (iii) Electromagnetism and MHD; (iv) Special\u000a      Relativity; and (v) Mars. Key aspects of the presentations were based on\u000a      the specific research carried out within the Unit. E.g.:\u000a    \u000a      Outreach materials covering magnetic flux emergence and magnetic\u000a        reconnection were underpinned by McLaughlin's research detailed in\u000a        outputs [3] and [4], respectively.\u000a      Outreach materials explaining solar observations were underpinned by\u000a        research reported in [5].\u000a      A detailed mathematical model of the solar wind was presented as part\u000a        of Presentations (ii) and (v), and this model was developed as part of\u000a        US Air Force grant FA8655-13-1-3067.\u000a      Presentation (iii) contained a discussion of MHD waves underpinned by\u000a        research from [1] &amp; [4].\u000a    \u000a    The rest of the outreach materials were created from McLaughlin's body of\u000a      research carried out within the Unit. The cited outputs above are specific\u000a      examples of key research outputs produced within the Unit. These outputs\u000a      detail the original research, whereas the outreach programme acted as the\u000a      vehicle to engage the audience with the research.\u000a    "},{"CaseStudyId":"5820","Continent":[{"GeoNamesId":"6255146","Name":"Africa"},{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"953987","Name":"South Africa"},{"GeoNamesId":"732800","Name":"Bulgaria"},{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Research Councils UK","Engineering and Physical Sciences Research Council","Medical Research Council"],"ImpactDetails":"\u000a    In December 2011, TPP was applied by Angelova and collaborators from the\u000a      MATSIQEL team to high-dimension data from Valley Care. The data were\u000a      subjected to substantial data cleaning and preparation between December\u000a      2011 and April 2012. Valley Care's primary aim is to assist people to live\u000a      independently in their own homes. Therefore the data describes the usage\u000a      of equipment and facilities required for ambient living for people (mostly\u000a      ageing or in a need of assistance). The Telecare equipment mainly consists\u000a      of: (i) a base unit for making and\/or receiving calls from the operator,\u000a      which is available to each client; and (ii) a range of mobile or fixed\u000a      units, such as personal radio triggers, medication dispenser, smoke, gas\u000a      and CO detectors, fall detector, temperature-extremes sensor, property\u000a      exit sensor, bogus caller, bed\/chair sensor, flood detector, epilepsy\u000a      sensor and GPS trackers. These devices monitor the life of people with\u000a      limited abilities, enabling them to look after themselves and live in\u000a      their own homes, without the need of a carer or transfer to care homes.\u000a      Valley Care provided data for 500 (consent given) clients aged between 45\u000a      and 90 collected from August 2007 to December 2011. The objectives of\u000a      applying the TPP method were to evaluate the usage of the equipment, to\u000a      identify possible risk factors and to establish usage patterns of Valley\u000a      Care services. To the best of our knowledge, this is the first time that\u000a      the mathematical and statistical analysis described has been carried out\u000a      on a data set of this type in UK and Europe.\u000a    Our study identified a number of users at a higher risk of falls,\u000a      seasonal peaks in the use of the service (predominantly in the weekends\u000a      and bank holidays) and the frequency and type of calls. Our research\u000a      enabled Valley Care managers to establish the volume and frequency of\u000a      calls, to identify users at high risk and inform the manufacturers of the\u000a      equipment how to write database software to enable Valley Care to plan\u000a      efficiently the workloads of call operators and social care workers. The\u000a      Team Manager of Valley Care stated that the research findings have enabled\u000a      Valley Care: \"to improve the quality and efficiency of the services\u000a        whilst operating in the same budget.\"\u000a    As a direct outcome of applying our research, in 2012 Valley Care\u000a      transformed its system for the Call Centre operators, specifically:\u000a    \u000a       Providing more efficient workload planning for call centre\u000a          operators: This is due to a better understanding of the volume,\u000a        type and frequency of calls.\u000a       Providing more efficient allocation of warden visits: Wardens\u000a        respond to alarm calls day and night. The mobile warden service visits\u000a        clients in their own homes and such visits form one of the most\u000a        expensive parts of the Telecare service.\u000a       Prioritising calls to ambulance services and relatives: Such\u000a        prioritisation could not be done quantitatively nor automatically under\u000a        the old system (a key limitation), and thus our research has transformed\u000a        Valley Care's Call Centre service.\u000a       Eliminating false alarms: Eliminating false alarms\u000a        contributes to an overall quicker response from call centre operators to\u000a        real emergencies.\u000a    \u000a    The research also provided knowledge about the usage patterns of the\u000a      technology and valuably identified clients at high risk of falls.\u000a      Monitoring and allocating special attention to clients with a high risk of\u000a      falls allows them to live independently at home for longer and thus not go\u000a      into residential care. Laing &amp; Buisson's Care of Elderly People\u000a        Report 2012\/13 estimates that a person can expect to pay more than\u000a      &#163;27,200 per year in residential care costs, rising to over &#163;37,500 if\u000a      nursing care is necessary. Laing &amp; Buisson also report that the UK's\u000a      elderly care market is now a &#163;24 billion sector of the UK's service\u000a      economy:\u000a    http:\/\/www.laingbuisson.co.uk\/MarketReports\/MarketReportsHome\/tabid\/570\/ProductID\/548\/Default.aspx.\u000a    Valley Care is a preventative service, and thus allowing clients to avoid\u000a      going into residential care represents a significant saving.\u000a    Valley Care provides a Telecare service to over 5,000 customers as part\u000a      of the Northumbria Healthcare NHS Foundation Trust in the North East of\u000a      England (we analysed a subset of 500 consenting clients). The Team Manager\u000a      of Valley Care states that: \"the service informs a wider UK ageing\u000a        community as part of the NHS Foundation Trust\". Valley Care's Social\u000a      Care Call Centre provides a core service for vulnerable and elderly people\u000a      to remain in their homes for longer periods, and receives an estimated\u000a      129,000 calls per annum (as reported in the Northumberland County Council\u000a      minutes from the middle of the evaluation period &#8212;\u000a      http:\/\/www3.northumberland.gov.uk\/Councillor\/Upload\/CDocs\/4380_M516.doc).\u000a      As a consequence of our research, Valley Care has been able to transform\u000a      the quality and efficiency of their service while operating within the\u000a      same budget.\u000a    The positive benefits of this research also directly changed Valley\u000a      Care's policy for recording call information, i.e. implementing categories\u000a      and attributes proposed by our research. By extension, Valley Care is\u000a      currently in discussion with the manufacturers of its own Call Centre's\u000a      software and equipment (Tunstall Healthcare, the world's leading provider\u000a      of Telehealthcare solutions) for amendments to their database software\u000a      (using the categories\/attributes of our research).\u000a    Building on our success of applying the TPP method to NHS data, we\u000a      foresee significant future impact working with global health\u000a      practitioners. Further interest was generated when we presented details of\u000a      this impact case study at the LMS Regional Meeting and Workshop \"Mathematics\u000a        of Human Biology\" (July 2012) and through the MATSIQEL FP7\u000a      project, and we have received requests for implementation of the TPP\u000a      methodology to telecare data from Bulgarian and South African social care\u000a      centres; namely from CITT-Global\u000a      (the Center for Innovation and Technology Transfer-Global Ltd; a\u000a      European-wide consultancy company based in Bulgaria) and the Sout\u000a        hAfrican Medical Research Council.\u000a    Evidence for all the impacts described above can be found in a factual\u000a      statement from the Team Manager of Valley Care, Northumbria Healthcare NHS\u000a      Foundation Trust. Statements of support from the Managing Director of\u000a      CITT-Global and from the South African Medical Research Council also\u000a      corroborate our impact claims (see section 5).\u000a    ","ImpactSummary":"\u000a    Targeted Projection Pursuit (TPP) &#8212; developed at Northumbria University &#8212;\u000a      is a novel method for interactive exploration of high-dimension data sets\u000a      without loss of information. The TPP method performs better than current\u000a      dimension-reduction methods since it finds projections that best\u000a      approximate a target view enhanced by certain prior knowledge about the\u000a      data. \"Valley Care\" provides a Telecare service to over 5,000 customers as\u000a      part of Northumbria Healthcare NHS Foundation Trust, and delivers a core\u000a      service for vulnerable and elderly people (receiving an estimated 129,000\u000a      calls per annum) that allows them to live independently and remain in\u000a      their homes longer. The service informs a wider UK ageing community as\u000a      part of the NHS Foundation Trust.\u000a    Applying our research enabled the managers of Valley Care to establish\u000a      the volume, type and frequency of calls, identify users at high risk, and\u000a      to inform the manufacturers of the equipment how to update the database\u000a      software. This enabled Valley Care managers and staff to analyse the\u000a      information quickly in order to plan efficiently the work of call\u000a      operators and social care workers. Our study also provided knowledge about\u000a      usage patterns of the technology and valuably identified clients at high\u000a      risk of falls. This is the first time that mathematical and statistical\u000a      analysis of data sets of this type has been done in the UK and Europe.\u000a    As a result of applying the TPP method to its Call Centre multivariate\u000a      data, Valley Care has been able to transform the quality and efficiency of\u000a      its service, while operating within the same budget.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Northumbria at Newcastle\u000a    ","Institutions":[{"AlternativeName":"Northumbria at Newcastle (University of)","InstitutionName":"Northumbria University Newcastle","PeerGroup":"D","Region":"North East","UKPRN":10001282}],"Panel":"B         ","PlaceName":[],"References":"\u000a    (* references which best indicate quality of underpinning research)\u000a    \u000a[1*] Faith J., Mintram R. &amp; Angelova M. (2006)\u000a      Targeted Projection Pursuit for Gene Expression Data Classification and\u000a      Visualisation\u000a      Bioinformatics, vol. (22), no. (21), pp. 2667-2673\u000a      http:\/\/dx.doi.org\/10.1093\/bioinformatics\/btl463\u000a    \u000a\u000a[2] Faith J. &amp; Brockway M. (2006)\u000a      Targeted Projection Pursuit Tool for Gene Expression Visualisation\u000a      Journal of Integrative Bioinformatics, vol. (3), no. (2)\u000a      http:\/\/dx.doi.org\/10.2390\/biecoll-jib-2006-43\u000a      For a further demonstration of how the tool works see http:\/\/vimeo.com\/17527997\u000a    \u000a\u000a[3*] Faith J. (2007)\u000a      Targeted Projection Pursuit for Interactive Exploration of\u000a      High-dimensional Data Sets\u000a      Book Series: IEEE International Conference on Information Visualization\u000a      Book Editor(s): Banissi E., Burkhard RA., Grinstein G.; et al., pp.\u000a      286-292\u000a      http:\/\/dx.doi.org\/10.1109\/IV.2007.107\u000a    \u000a\u000a[4*] Angelova M., Myers C. &amp; Faith J. (2008)\u000a      Classification of Genes Based on Gene Expression Analysis\u000a      Physics of Atomic Nuclei, vol. (71), no. (5), pp. 780-787,\u000a      http:\/\/dx.doi.org\/10.1134\/S1063778808050025\u000a    \u000a\u000a[5] Enshaei Amir (2012)\u000a      Development of Artificial Intelligence systems as a prediction tool in\u000a      ovarian cancer\u000a      PhD Thesis (Newcastle University)\u000a      http:\/\/hdl.handle.net\/10443\/1552\u000a    \u000aRelevant Grants\u000a    &#8226; Clarke C. et al. (2009-2010), Ref: G0900012,\u000a      &#163;50,000\u000a      \"Enabling Environment: Modelling Wellbeing in Ageing\"\u000a      Funded by the Lifelong Health and Wellbeing Cross-Council Programme led by\u000a      MRC\u000a    &#8226; Angelova M., et al. (2011-2014), FP7-PEOPLE-2009-IRSES 247541,\u000a      &#8364;189,000\u000a      \"MATSIQEL: Models of Ageing and Technological Solutions for Improving\u000a        and Enhancing the Quality of Life\"\u000a      FP7 Marie Curie Actions: International Research Staff Exchange Scheme\u000a    &#8226; Angelova et al. (2012), London Mathematical Society, &#163;7,500\u000a      \"Mathematics of Human Biology\": LMS Regional Meeting and Workshop\u000a      (results of work on the Valley Care Call Centre data were reported at this\u000a      meeting)\u000a    &#8226; Gibson H., Faith J., Dobree J. &amp; MacManus L. (2011), &#163;15,000\u000a      \"Modelling Propensity to Buy for UK Business\"\u000a      Part of the KTN's Industrial Mathematics KTP Programme, co-funded by EPSRC\u000a      https:\/\/connect.innovateuk.org\/web\/partnership-programmes\/articles\/-\/blogs\/modelling-propensity-to-buy-for-uk-business\u000a      and http:\/\/www.mathscareers.org.uk\/_db\/_documents\/IP11-001-LevelBusiness_Northumbria_CaseStudy.pdf\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"},{"Level1":"8","Level2":"6","Subject":"Information Systems"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    \u000a       Factual statement from Team Manager of Valley Care\u000a        (Northumbria Healthcare NHS Foundation Trust) corroborating that, as a\u000a        direct result of applying the TPP method:\u000a\u0009\u0009\u000a      (i) a more efficient system for workload planning has been established;\u000a      (ii) a more efficient allocation of warden visits has been implemented;\u000a      (iii) calls to ambulance services and relatives have been prioritised;\u000a      (iv) quality and efficiency of the services improved while operating\u000a        within same budget;\u000a      (v) customers at higher risk of falls have been identified; and\u000a      (vi) false alarms have been eliminated.\u000a      \u000a\u0009   Statement of Interest from Managing Director of CITT-Global,\u000a        reporting aspiration to implement TPP method in Telecare\/Telehealth\u000a        practice for improving the effectiveness of social care centres in\u000a        Bulgaria.\u000a       Statement of Support from South African Medical Research Council,\u000a        reporting desire to implement TPP model to South African Telehealth and\u000a        Telecare data.\u000a    \u000a    Copies of these documents are available on request. \u000a    ","Title":"\u000a    Improving Social Care Call Centre Operational Effectiveness\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Targeted Projection Pursuit (TPP) is a novel data-mining method for\u000a      interactive exploration of high- dimension data sets without loss of\u000a      information. The method was developed by Dr Joe Faith, Dr Robert Mintram\u000a      (both Senior Lecturers at the time of the research) and Professor Maia\u000a      Angelova at Northumbria University [1] and was first published in\u000a      September 2006. Mintram subsequently moved to Bournemouth University in\u000a      2007 and Faith moved to Google\u000a        Inc. in July 2011. The method proceeds by finding projections of the\u000a      data that best approximate a target view. Two versions of the TPP method\u000a      were introduced based on: (i) Procrustes analysis and (ii) a single layer\u000a      perceptron. These are capable of finding orthogonal or non-orthogonal\u000a      projections, respectively. The method was quantitatively and qualitatively\u000a      compared with other dimension reduction techniques. It was shown to find\u000a      two-dimensional views that display the classification of cancers from gene\u000a      expression data with a visual separation equal to, or better than,\u000a      existing dimension reduction techniques developed for this purpose.\u000a    TPP allows classification and visualisation of large data sets and, if\u000a      required, the implementation of certain prior knowledge about the data\u000a      (known as supervised data mining) to enhance learning from data by finding\u000a      patterns and visualising high-dimensional data without loss of\u000a      information. The method was originally applied to gene expression data for\u000a      classification of leukemic cancers based on microarray data for several\u000a      types of leukemic cancers [1]. TPP was further implemented into a\u000a      web-based tool [2], which can be used with any type of multivariate\u000a      clustered data, and detailed in a book chapter [3]. In 2008, the method\u000a      was used for classification and visualisation of gene expression to\u000a      investigate the effect of knock-out of the metJ gene in the E.coli\u000a      genome [4]. In 2011, post-graduate research student Helen Gibson\u000a      (Northumbria University) applied the TPP method to Companies House data as\u000a      part of an Industrial\u000a        Mathematics KTP Programme entitled \"Modelling\u000apropensity\u000a          to buy for UK business\" (in partnership with Level\u000a        Business Limited). This revealed a number of key insights into the\u000a      data, including visualising relationships between companies and Local\u000a      Authority departments, and implementing an algorithm to 'rate' company\u000a      directors. In 2012, the TPP method was used in the development of an\u000a      artificial intelligence system applied to the study of ovarian cancer [5].\u000a    In 2009-2010, Angelova was co-investigator in a Northumbria-led,\u000a      multi-disciplinary research grant, \"Enabling environment: modelling\u000a        wellbeing in ageing\", funded by the Lifelong Health and Wellbeing\u000a      Cross-Council Programme led by MRC (Ref: G0900012, Grant ID 90535,\u000a      &#163;50,000) in collaboration with Newcastle, Manchester and Sheffield\u000a      Universities. As part of this project, Angelova became aware of the\u000a      Telecare service provided by Valley Care, and during these discussions\u000a      realised that TPP could be used to better understand Valley Care's\u000a      multivariate data, and so potentially improve the operational efficiency\u000a      of the Telecare service. These discussions led to the application of TPP\u000a      research to Valley Care data as part of Work Packages 1, 2 and 5 in a\u000a      European Framework 7 Project entitled \"MATSIQEL: Models for ageing and\u000a        technological solutions for improving and enhancing the quality of life\"\u000a      (FP7-PEOPLE-2009-IRSES 247541, 2011-2014) for which Angelova was PI and\u000a      international coordinator.\u000a    "},{"CaseStudyId":"6244","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"2802361","Name":"Belgium"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"1835841","Name":"South Korea"},{"GeoNamesId":"6251999","Name":"Canada"},{"GeoNamesId":"3175395","Name":"Italy"},{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"3017382","Name":"France"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Researchers, both academic and industrial, from all over the world\u000a      (Netherlands, Belgium, Italy, France, USA, Canada, India, Korea) made use\u000a      of our work as evidenced from the citations made to our papers listed\u000a      above. One paper that cited our work is by a group in the Vancouver-based\u000a      company LB Foster Friction Management.\u000a    Eadie D.T., Santoro M. and Kalousek J. (2005) Railway noise and the\u000a      effect of top of rail liquid friction modifiers: changes in sound and\u000a      vibration spectral distributions in curves. Wear 258, 1148-1155.\u000a    LB Foster used our work to gain crucial insight into the fundamental\u000a      mechanisms of wheel squeal, in particular the role of stick slip and the\u000a      shape of the traction-creepage curve. Armed with this insight, they\u000a      developed a commercial product, the KELTRACK &#174; Top of Rail Friction\u000a      Modifier, which has revolutionised the wheel\/rail interface dynamics and\u000a      has achieved unsurpassed reduction of squeal noise. http:\/\/www.lbfoster.co.uk\/_pdf\/Portec-Keltrack.pdf\u000a    KELTRACK is a water-based suspension of proprietary solids, which is\u000a      pumped to the top of the rail by a trackside delivery system, prior to a\u000a      curve where squeal is of particular concern. The water component\u000a      evaporates and the material remains as a thin dry film. This film alters\u000a      the coefficient of friction to a value between that of dry friction and\u000a      that of a lubricated contact, and hence changes the lateral friction force\u000a      between wheel and rail. In this way, the stick\/slip feedback between wheel\u000a      oscillations and friction force is disrupted, i.e. the mechanism that\u000a      generated the squeal noise has been deactivated. This technique allows\u000a      KELTRACK to dramatically reduce squeal noise as trains negotiate a curve.\u000a      The noise abatement levels (up to 27 dB) are almost immediate. Other\u000a      benefits (not found in traditional lubricants such as oil or grease) of\u000a      this system include reduced lateral forces, reduced wear, decreased\u000a      potential for derailment, no effect on traction and breaking,\u000a      environmentally safe (non-contaminating).\u000a    The Top of Rail Friction Modifier is now used on railways around the\u000a      world, for example on Tokyo Metro, Beijing Metro, New York City Transit\u000a      Authority, Madrid Metro and numerous others which transport billions of\u000a      passengers daily. Within the UK, KELTRACK is applied on London\u000a      Underground, and on sites on Network Rail. For example there are a number\u000a      of application systems controlling curve squeal around Euston Station in\u000a      London. (see the e-mail by Dr Don Eadie, Vice President, Technology and\u000a      Innovation, LB Foster Friction Management dated 6 June 2012). The impact\u000a      world-wide has been and continues to be massive, benefitting millions of\u000a      commuters and city-dwellers.\u000a    Another paper that cited our work was produced by a European research\u000a      consortium focussing on active squeal attenuation.\u000a    Cigada, A. Fehren, H., Manzoni, S., Redaelli, M., Schiedewitz, M. and\u000a      Siebald, H. (2008) Investigations on the attenuation of squeal noise from\u000a      a resilient railway wheel by means of piezo-actuators. Proceedings of ISMA\u000a      2008, 2709-2723.\u000a    Cigada et al applied our work within the FP6 &#8212; funded project InMAR\u000a      (Intelligent Materials for Active Noise Reduction, see http:\/\/www.inmar.info\/start.htm\u000a      ). In an e-mail dated 29\/6\/2012, they write \"we have studied your research\u000a      in order to fully comprehend the squeal phenomenon and its generation\u000a      mechanism and to find suggestions on the way to actively control it.\u000a      Finally we have been able to control squeal\". One of the partners of the\u000a      InMAR consortium was the company ERAS GmbH (G&#246;ttingen, Germany), which has\u000a      a commercial interest in controlling squeal noise (and other unwanted\u000a      vibrations) by active methods.\u000a    ","ImpactSummary":"\u000a    Rail transport is the greenest form of transport in that it produces the\u000a      least pollution of the environment. However, the noise from squealing\u000a      trains has been a major factor preventing the wider use of rail transport\u000a      in populated areas, especially in cities, where trains have to traverse\u000a      tight curves in built-up areas. Research carried out at Keele University\u000a      on curve squeal gave crucial input to developing an effective control\u000a      method (KELTRACK friction modifier, developed by the company LB Foster\u000a      Friction Management). This is a device by which a thin film is applied at\u000a      the wheel-rail interface, which in turn destroys the generation mechanism\u000a      of curve squeal. The KELTRACK friction modifier is now used in transport\u000a      systems all over the world, especially in underground systems, such as the\u000a      metros of Tokyo, Beijing and Madrid.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Keele University\u000a    ","Institutions":[{"AlternativeName":"Keele University","InstitutionName":"Keele University","PeerGroup":"B","Region":"West Midlands","UKPRN":10007767}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"3117735","Name":"Madrid"},{"GeoNamesId":"2918632","Name":"Göttingen"}],"References":"\u000a    \u000aHeckl, Maria A. &amp; Abrahams, I.D. (2000) Curve squeal of train wheels,\u000a      Part 1: Mathematical model for its generation. Journal of Sound and\u000a        Vibration 229, 669-693. DOI: 10.1006\/jsvi.1999.2510\u000a    \u000a\u000aHeckl, Maria A. (2000) Curve squeal of train wheels, Part 2: Which wheel\u000a      modes are prone to squeal? Journal of Sound and Vibration 229,\u000a      695-707. DOI: 10.1006\/jsvi.1999.2511\u000a    \u000a\u000aHeckl, Maria A. &amp; Huang, X.Y. (2000) Curve squeal of train wheels,\u000a      Part 3: Active control. Journal of Sound and Vibration 229,\u000a      709-735. DOI: 10.1006\/jsvi.1999.2512\u000a    \u000aThe Journal of Sound and Vibration (JSV) is an established international\u000a      journal with an impact factor of 1.588, publishing 50 issues per year.\u000a    Grants:\u000a      &#163; 94,726 from EPSRC for research on 'Development of active control of\u000a      wheel squeal from a nonlinear dynamical systems analysis'. Principal\u000a      investigator: Maria A. Heckl, co-investigator: I. David Abrahams.\u000a      1993-1996.\u000a    &#8364; 876,858 from the European Commission (FP7) for the Marie Curie Initial\u000a      Training Network LIMOUSINE (Limit cycles of thermo-acoustic oscillations\u000a      in gas turbine combustors). Deputy Coordinator: Maria Heckl. The total\u000a      award for all partners in the network was &#8364; 4.1 million. 2008-2012.\u000a    &#8364; 3.73 million from the European Commission (FP7) for the Marie Curie\u000a      Initial Training Network TANGO (Thermo-acoustic and aero-acoustic\u000a      nonlinearities in green combustors with orifice structures). Coordinator:\u000a      Maria Heckl. The award for the activities at Keele is &#8364; 1.13 Million.\u000a      2012-2016\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"9","Level2":"13","Subject":"Mechanical Engineering"}],"Sources":"\u000a    The Vice President, Technology and Innovation at LB Foster Friction\u000a      Management, Canada, can corroborate the claim that LB Foster have gained\u000a      valuable insight through our work and that this was crucial for the\u000a      development of their KELTRACK &#174; Top of Rail Friction Modifier, which now\u000a      controls curve squeal in public transport systems around the world.\u000a    An Associate Professor at Politecnico di Milano can corroborate the claim\u000a      that my work was used in the FP6 &#8212; funded project InMAR to\u000a      develop an active squeal control method. \u000a    ","Title":"\u000a    Modelling and control of curve squeal\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The research that provided the basis for this impact was carried out at\u000a      Keele University from 1993 to 1999. It was initially funded by the EPSRC\u000a      (&#163; 94,726 on 'Development of active control of wheel squeal from a\u000a      nonlinear dynamical systems analysis', grant number GR\/H73776). The key\u000a      researchers were: Dr Maria A. Heckl (principal investigator, at Keele\u000a      since 1992), Professor I. David Abrahams (co-investigator, at Keele until\u000a      January 1998), Dr Xiao Yang Huang (researcher from Singapore, visiting\u000a      Keele for 4 weeks in May\/June 1993).\u000a    Curve squeal is the noise produced by train wheels when they traverse a\u000a      tight curve, giving rise to a lateral friction force in the wheel-rail\u000a      contact point. This friction force excites bending oscillations of the\u000a      wheels, which in turn affect the friction force, and this feedback is\u000a      responsible for high-amplitude wheel oscillations that are radiated into\u000a      the environment as high-amplitude sound waves. We developed a\u000a      sophisticated mathematical model to understand in detail the physical\u000a      mechanisms that play a key role in the generation of squeal. Our work was\u000a      published in a series of three papers in 2000, and their content is\u000a      summarized below.\u000a    Our mathematical model is described in Heckl and Abrahams (2000). The\u000a      train wheel is modelled as an annular disc (Kirchhoff plate) with a hub in\u000a      the centre; the friction force between rail and wheel is modelled by a\u000a      stick\/slip friction characteristic, which is nonlinear. We developed a\u000a      Green's function approach, which can deal with this nonlinearity and\u000a      predict the time-history of the velocity of an individual wheel. This\u000a      allows one to identify the stability behaviour, transient oscillations,\u000a      limit cycles and other nonlinear features that lead to wheel squeal. In\u000a      Heckl (2000), we use this model to make predictions about the dynamics of\u000a      individual wheel modes. Typically only one wheel mode is unstable, and we\u000a      determined which wheel properties make that mode unstable. In Heckl and\u000a      Huang (2000), we extended our mathematical model to incorporate an active\u000a      control system. The effect of key control parameters was predicted, and\u000a      the parameter range was identified, where an unstable mode could be\u000a      successfully controlled. We verified our findings with a laboratory test\u000a      rig.\u000a    The idea of modelling a feedback instability by a Green's function\u000a      approach works not just for a friction-driven instability such as squeal.\u000a      The underpinning research in train wheel squeal has led to its application\u000a      in thermo-acoustic instabilities, which occur in certain combustion\u000a      systems, in particular gas turbine engines. Such instabilities can lead to\u000a      catastrophic engine failure and are a serious problem in the development\u000a      of clean combustion technologies. Building on our experience gained in the\u000a      squeal project, we have managed to secure two major European grants\u000a      (LIMOUSINE and TANGO) for collaborative research in this area.\u000a    "},{"CaseStudyId":"6270","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000d\u000a    The impact of this work has taken place via Rolls-Royce, the second\u000d\u000a      largest maker of aircraft engines in the world, and a world leader in gas\u000d\u000a      turbine technology, and via the MoD and Thales Underwater Systems. A large\u000d\u000a      number of research contracts between Rolls-Royce and university research\u000d\u000a      groups have exploited Chapman's ray theory of aeroengine noise; this\u000d\u000a      research has concerned the design of new aeroengines, and has greatly\u000d\u000a      reduced the risk that certain types of aircraft might be banned. The work\u000d\u000a      has also been exploited in EU projects arising from EU directives. The\u000d\u000a      exploitation has taken place outside of Keele, most notably through the\u000d\u000a      long-standing connections of Rolls-Royce with the Department of Applied\u000d\u000a      Mathematics and Theoretical Physics, Cambridge University, and the\u000d\u000a      Institute of Sound and Vibration, Southampton University. The exploitation\u000d\u000a      in underwater acoustics took place via consultancy for the Successor class\u000d\u000a      nuclear submarine project, initiated by DSTL, and led to the report\u000d\u000a      Broadband Hydroacoustic Research (ref. 9).\u000d\u000a    Two recent articles co-authored by senior Rolls-Royce engineers are\u000d\u000a      explicit about the impact of Chapman's work on noise reduction. The first,\u000d\u000a      co-authored by Dr A. B. Parry, is the survey article `Modern Challenges\u000d\u000a      Facing Turbomachinery Aeroacoustics', published in 2012 in the Annual\u000d\u000a      Review of Fluid Mechanics (ref. 1). The second article, co-authored by Dr\u000d\u000a      A. J. Kempton and published in 2010, is explicit about the way in which\u000d\u000a      Chapman's work is needed for prediction of the far-field directivity of\u000d\u000a      broadband noise using measurements made in the duct (ref. 2).\u000d\u000a    A world-class group which has exploited Chapman's ray theory is that led\u000d\u000a      at Cambridge University by Professor N. Peake, working jointly with\u000d\u000a      Rolls-Royce over a period of many years. The impact trail from Chapman's\u000d\u000a      work to industrial practice in Rolls-Royce lies in a series of EPSRC CASE\u000d\u000a      awards between Rolls-Royce and Cambridge University in which Chapman's ray\u000d\u000a      theory explicitly plays an underpinning role. A long-lasting contact at\u000d\u000a      Rolls-Royce is Dr A. B. Parry, an engineering specialist in aerothermal\u000d\u000a      methods, aeroacoustics, and aerodynamics.\u000d\u000a    Full details of four of the above-mentioned research awards are given in\u000d\u000a      refs. 2-6, in which the published papers noted refer to the collaboration\u000d\u000a      with Rolls-Royce and explain the relation of the results to Chapman's\u000d\u000a      work. These details provide the impact trail which leads from Chapman's\u000d\u000a      work to Rolls-Royce. The young researchers who worked with N. Peake\u000d\u000a      include E. J. Brambley, A. J. Cooper, C. J. Heaton, G. M. Keith, and B.\u000d\u000a      Veitch. The areas investigated in these research projects were\u000d\u000a    \u000d\u000a      Resonant phenomena in gas turbines,\u000d\u000a      Aeroacoustic models of fan noise,\u000d\u000a      Wave propagation and resonance in aeroengines,\u000d\u000a      Turbomachinery broadband noise.\u000d\u000a    \u000d\u000a    The research awards cover the period 1998-2009. Given that Chapman's ray\u000d\u000a      theory provides basic underpinning science, and aeroengine development is\u000d\u000a      a long-term process, this continuity of use over an extended period is an\u000d\u000a      essential part of the impact which the work has to the present day. In\u000d\u000a      detail, the above research awards have led, via Chapman's ray theory, to\u000d\u000a      advances at Rolls-Royce relating to the acoustic effects of\u000d\u000a    \u000d\u000a      the rotor, stator, and guide vanes in the aeroengine duct;\u000d\u000a      the precise shape of the duct, including for example the non-circular\u000d\u000a        cross-section, the curvature of the centre-line, the variation in\u000d\u000a        duct-liner properties, and angling of the front face of the duct; and\u000d\u000a      the interaction of the effects in (1) and (2), which occurs because of\u000d\u000a        scattering and diffraction.\u000d\u000a    \u000d\u000a    Another world-class group which has disseminated Chapman's work to\u000d\u000a      Rolls-Royce and the worldwide aircraft industry is the Rolls-Royce\u000d\u000a      University Technology Centre in Gas Turbine Noise at Southampton\u000d\u000a      University. This is housed in the Institute of Sound and Vibration\u000d\u000a      Research (ISVR). The research workers there have made frequent use of\u000d\u000a      Chapman's ray theory, often making use of Hocter's papers noted in Section\u000d\u000a      3, on sound radiation and reflection from the front face of the aeroengine\u000d\u000a      duct. This contribution to Rolls-Royce's research is indicated in ref. 2.\u000d\u000a    A further dissemination route of Chapman's work has been directly to the\u000d\u000a      European aircraft industry, via EU projects in Frameworks 5 and 6. The EU\u000d\u000a      directives 2002\/30\/EC and 2002\/49\/EC were to reduce aircraft noise, and\u000d\u000a      included the setting of noise standards. Two such projects were\u000d\u000a      SILENCE(R), referring to Significantly Lower Community Exposure to\u000d\u000a      Aircraft noise, and MESSIAEN, referring to Methods for Efficient\u000d\u000a      Simulation of Aircraft Engine Noise. Refs. 7-8 give published papers\u000d\u000a      arising from these projects which refer to Chapman's ray theory and\u000d\u000a      provide a link to the European aircraft industry.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    In response to many EU directives (e.g. 89\/629\/EEC, 2002\/30\/EC), and to\u000d\u000a      the threat of financial penalties, the aircraft industry has long\u000d\u000a      considered it a matter of the utmost importance to develop tools for the\u000d\u000a      reduction of aircraft noise. Chapman's ray theory of aeroengine noise,\u000d\u000a      created and developed in 1994-2000, provided such a tool. The impact of\u000d\u000a      this work has extended through aircraft industry giants such as\u000d\u000a      Rolls-Royce to consumers and the general public worldwide, because of its\u000d\u000a      influence on the design of quieter aircraft.\u000d\u000a    Following application of the same theory to broadband underwater\u000d\u000a      acoustics, the impact now extends to the government's plans for the next\u000d\u000a      generation of nuclear submarines. This is a &#163;25 billion project to design\u000d\u000a      and build the Successor class, to replace the Vanguard class of Trident\u000d\u000a      submarines. Chapman's ray theory has been used in the current Assessment\u000d\u000a      Phase leading to Main Gate in 2016, when the Government will decide on\u000d\u000a      production.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    Keele University\u000d\u000a    ","Institutions":[{"AlternativeName":"Keele University","InstitutionName":"Keele University","PeerGroup":"B","Region":"West Midlands","UKPRN":10007767}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000aJFM: Journal of Fluid Mechanics; JSV: Journal of Sound and Vibration;\u000d\u000a        PRSLA: Proceedings of the Royal Society of London A.\u000d\u000a    \u000a\u000aC. J. Chapman 1994 Sound radiation from a cylindrical duct. Part\u000d\u000a      I. Ray structure of the duct modes and of the external field. JFM\u000d\u000a      281, 293-311.\u000d\u000a    \u000a\u000aC. J. Chapman 1996 Sound radiation from a cylindrical duct. Part\u000d\u000a      II. Source modelling, nil-shielding directions, and the open-to-ducted\u000d\u000a      transfer function. JFM 313, 367-380.\u000d\u000a    \u000a\u000aC. J. Chapman 1999 Caustics in cylindrical ducts. PRSLA 455,\u000d\u000a        2529-2548.\u000d\u000a    \u000a\u000aS. T. Hocter 1999 Sound radiation from a cylindrical duct. PhD\u000d\u000a        thesis, University of Keele.\u000d\u000a    \u000a\u000aS. T. Hocter 1999 Exact and approximate directivity patterns of\u000d\u000a      the sound radiated from a cylindrical duct. JSV 227,\u000d\u000a        397-407.\u000d\u000a    \u000a\u000aC. J. Chapman 2000 Similarity variables for sound radiation in a\u000d\u000a      uniform flow. JSV 233, 157-164.\u000d\u000a    \u000a\u000aS. T. Hocter 2000 Sound radiated from a cylindrical duct with\u000d\u000a      Keller's geometrical theory. JSV 231, 1243-1256.\u000d\u000a    \u000a\u000aS. T. Hocter 2000 Sound reflection into a cylindrical duct. PRSLA\u000d\u000a      456, 2707-2716.\u000d\u000a    \u000a\u000aC. J. Powles 2002 Energy paths in sound fields. PRSLA 458,\u000d\u000a        841-855.\u000d\u000a    \u000a\u000aC. J. Powles 2004 Supersonic leading-edge noise. PhD thesis,\u000d\u000a        University of Keele.\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"9","Level2":"13","Subject":"Mechanical Engineering"},{"Level1":"2","Level2":"1","Subject":"Astronomical and Space Sciences"}],"Sources":"\u000d\u000a    ARFM: Annual Review of Fluid Mechanics; JFM: Journal of Fluid\u000d\u000a        Mechanics; JSV: Journal of Sound and Vibration\u000d\u000a    \u000d\u000a      \u000aN. Peake &amp; A. B. Parry 2012 Modern challenges facing\u000d\u000a        turbomachinery aeroacoustics. ARFM 44, 227-248.\u000a\u000d\u000a      \u000aC. R. Lowis, P. F. Joseph &amp; A. J. Kempton 2010 Estimation\u000d\u000a        of far-field directivity of broadband aeroengine fan noise using an\u000d\u000a        in-duct axial microphone array. JSV 329, 3940-3957.\u000a\u000d\u000a      Rolls-Royce\/EPSRC CASE award GR\/L80317, 1998-2001. A mathematical\u000d\u000a        investigation of resonant phenomena in gas turbines. A. J. Cooper\u000d\u000a          &amp; N. Peake 2001, 2005 JFM 445, 207-234, 523,\u000d\u000a          219-250; N. Peake &amp; A. J. Cooper 2001 JSV 243,\u000d\u000a          381-401.\u000a\u000d\u000a      Rolls-Royce\/EPSRC CASE award GR\/M21638\/01, 1999-2002. Development and\u000d\u000a        validation of aeroacoustic models for fan noise. G. M. Keith &amp;\u000d\u000a          N. Peake 2002 JSV 255, 129-146, 147-160; C. J. Heaton\u000d\u000a          &amp; N. Peake 2005 JFM 540, 189-220.\u000a\u000d\u000a      Rolls-Royce\/EPSRC CASE award RG\/42494, University Gas Turbine Research\u000d\u000a        Partnership Programme, 2004-2007. Wave propagation and resonance in\u000d\u000a        aeroengines. E. J. Brambley &amp; N. Peake 2008 JFM 596,\u000d\u000a          387-412.\u000a\u000d\u000a      Rolls-Royce\/EPSRC CASE award EP\/D035031\/1, 2006-2009. Mathematical\u000d\u000a        modelling and computational engineering prediction of turbomachinery\u000d\u000a        broadband noise. B. Veitch &amp; N. Peake 2008 JFM 613,\u000d\u000a          275-307.\u000a\u000d\u000a      Rolls-Royce\/EU-FP5 SILENCE(R) project, 2001-2005. Significantly lower\u000d\u000a        community exposure to aircraft noise. A. McAlpine, R. J. Astley, A.\u000d\u000a          J. Kempton et. al. 2006 JSV 294, 780-806.\u000a\u000d\u000a      Rolls-Royce\/EU-FP6 MESSIAEN project 502938, 2003-2007. Methods for\u000d\u000a        efficient simulation of aircraft engine noise. G. G. Vilenskii &amp;\u000d\u000a          S. W. Rienstra 2007 JFM 583, 45-70.\u000a\u000d\u000a      Thales Underwater Systems, project DSTlx-10062650\/BHAR, 2010-2012.\u000d\u000a        Broadband Hydroacoustic Research (Phase II). D. Yumashev, I. D.\u000d\u000a          Abrahams, C. J. Chapman et al. 2012, 1-47.\u000a\u000d\u000a    \u000d\u000a    Source to corroborate the impact of the work on Rolls-Royce and the\u000d\u000a      aircraft industry: Engineering Specialist &#8212; Aerodynamics, Rolls-Royce plc.\u000d\u000a    Source to corroborate the spin-off impact on the submarine industry\u000d\u000a      (defence work): Naval Systems Department, DSTL.\u000d\u000a    Source to corroborate the impact of the work on the European aircraft\u000d\u000a      industry: Institute of Sound and Vibration Research, University of\u000d\u000a      Southampton.\u000d\u000a    Source to corroborate the impact of the work on the aircraft industry:\u000d\u000a      Centre for Mathematical Sciences, Cambridge University. \u000d\u000a    ","Title":"\u000d\u000a    The reduction of sound from aircraft engines\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2637487","Name":"Southampton"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The impetus to create a ray theory of aeroengine noise arose from two key\u000d\u000a      aspects of the state of aircraft noise research in the early 1990's:\u000d\u000a    \u000d\u000a      The prevailing theory of fan-generated sound in aeroengine ducts\u000d\u000a        contained a fundamental error, in that sound rays were mistakenly\u000d\u000a        believed to lie on smooth helices, rather like the stripes on a barber's\u000d\u000a        pole, winding around a cylinder of fixed radius. In fact, the sound\u000d\u000a        field has a very different structure.\u000d\u000a      Advances in theoretical understanding were urgently needed, for\u000d\u000a        development and certification of new engines, to account for the\u000d\u000a        high-frequency, short wavelength sound field produced by a rapidly\u000d\u000a        rotating fan with many blades. Such fans were by then the norm in large\u000d\u000a        high-bypass aeroengines, as used in all large commercial aircraft\u000d\u000a        throughout the world.\u000d\u000a    \u000d\u000a    The error (1) was catastrophic for prediction methods, because `barber's\u000d\u000a      pole' rays (if they really existed) would send net energy in the duct\u000d\u000a      direction only, with no provision for the enormous sideways propagation of\u000d\u000a      sound energy which actually occurs from the front face of an aeroengine.\u000d\u000a      Although barber's-pole type rays do not exist, this did not stop sketches\u000d\u000a      of them appearing in some research papers.\u000d\u000a    No real progress with research program (2) was possible until Chapman\u000d\u000a      demonstrated that the sound rays are not smooth helices at all, but are\u000d\u000a      piecewise-linear helices, consisting of a sequence of straight-line\u000d\u000a      segments joined up at sharp corners. In consequence, the rays all have a\u000d\u000a      sideways, i.e. radial, component in their direction. This fact has an\u000d\u000a      enormous impact on the sound produced by an aircraft engine, because it\u000d\u000a      implies that in an aeroengine duct the sound rays bounce repeatedly from\u000d\u000a      the duct wall, to emerge from the front face of the duct at definite\u000d\u000a      sideways angles, which Chapman calculated explicitly as a function of the\u000d\u000a      parameters specifying the modes in the duct.\u000d\u000a    An immediate development was that within two years Chapman obtained a\u000d\u000a      complete theory of aeroengine fan noise, which not only modelled the\u000d\u000a      source of the noise on the fan, but also tracked the energy flow all the\u000d\u000a      way through the duct and out into the far field, where the energy is\u000d\u000a      perceived as noise. Within a few more years, Chapman elucidated various\u000d\u000a      intricate patterns of focusing in the sound field, and his research\u000d\u000a      student Hocter calculated many complete directivity patterns of aeroengine\u000d\u000a      sound fields, hence determining the sound received in all directions from\u000d\u000a      an aircraft.\u000d\u000a    The programme of research on aircraft engine noise occupied a major part\u000d\u000a      of Chapman's research time as a Lecturer and then Reader at Keele\u000d\u000a      University in the period 1994-2000, and resulted in his promotion to a\u000d\u000a      Professorship. As part of this programme, Chapman supervised a PhD\u000d\u000a      student, S. T. Hocter from 1996-1999, with funding provided by an EPSRC\u000d\u000a      Doctoral Grant, to work on the sound radiation from the front face of the\u000d\u000a      aeroengine duct. Also as part of the programme, Chapman supervised C. J.\u000d\u000a      Powles, who obtained a Nuffield bursary in 2000 to work on energy paths\u000d\u000a      within the duct. The following year, Powles began an EPSRC-funded PhD at\u000d\u000a      Keele under Chapman's supervision, on the generation of sound rays by the\u000d\u000a      leading edges of the fan blades in the duct, completing his PhD in 2004.\u000d\u000a    In the period 2010-2012, Chapman extended the theory to broadband noise\u000d\u000a      in an MoD research project, with Thales Underwater Systems Ltd and other\u000d\u000a      universities, on the next generation of nuclear submarines, the Successor\u000d\u000a      class. The theory determines the ray directions of underwater sound from\u000d\u000a      the propulsor.\u000d\u000a    "},{"CaseStudyId":"6333","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"99237","Name":"Iraq"}],"Funders":[],"ImpactDetails":"\u000a    Provenance of corroboration\u000a    The Leeds MoD group conducted its research on a commercial-in-confidence\u000a      basis, funded [5,6] by the MoD (formerly as RARDE, DRA and DERA; now as\u000a      the private company QinetiQ). The Leeds research underpinned the MoD's\u000a      ballistic-simulation computational code &#8212; known as \"GRIM\" &#8212; that was\u000a      developed by QinetiQ for its main UK customer, Dstl, both of whom are in\u000a      the rare position of being able to provide corroborative evidence of\u000a      impact. QinetiQ's Senior Research Fellow, the current President of the International\u000a        Ballistics Society [E], has accordingly provided two supporting\u000a      statements, namely: [B] a generic summary of the end-user benefits\u000a      accrued, and; [C] a review of specific impacts, in diverse areas, together\u000a      with dates. Both the content of [B,C] and the claims made in this case are\u000a      \"fully endorsed\" [D] by the current Programme Weapons Leader at\u000a      Dstl (the main end-user of the GRIM codes underpinned by the Leeds'\u000a      research), who states \" I have been so impressed by the\u000a        capability that it [GRIM] provides to [the] MOD, that I have continued\u000a        to fund a number of developments ... the numerical simulation capability\u000a        has allowed the MOD to maintain an international credibility in the face\u000a        of a changing research focus and financial pressures\" [D].\u000a    The following list of areas in which GRIM has been applied at both\u000a      national and international levels bears testimony to both the reach and\u000a      significance of the impact generated by the underpinning Leeds research.\u000a      Unless otherwise stated, all claims and quotes in (a)-(b)\u000a      and (c)-(f) refer to [B] and [C] respectively.\u000a    (a) R &amp; D cost benefits for the MoD\u000a    QinetiQ states that the Leeds research was \"seminal to the\u000a        development of [its] current capability\" for several reasons,\u000a      including enabling the MoD to focus on a development path that saved the\u000a      company \"2 years in development work and &#163;1.5m-&#163;2m in costs and\u000a        guaranteed a method that was robust and worked.\" QinetiQ add that \"Leeds\u000a        research has continued, up to the present day, to underpin the annual\u000a        income (&#163;4m) of the Warheads Technology Group at Fort Halstead\".\u000a    The Leeds research underpinned the validity of QinetiQ's numerics and\u000a      demonstrated the benefits of adaption as an effective approach towards\u000a      predictive numerical simulation, as a result of which it enabled QinetiQ\u000a      to develop what it describes as an \"integrated numerical\u000a        simulation-experiment methodology\" that it is now deploying \"over\u000a        a widening number of areas with significant cost savings.\"\u000a    QinetiQ uses the adaptive numerical simulations to \"routinely develop\u000a        concepts without having to resort to large numbers of experimental\u000a        trials with significant cost savings\". As an example, the company\u000a      has described how it recently used simulations to develop an explosively\u000a      formed projectile verified at one-third-scale using three experiments, so\u000a      that only one experiment was needed to validate the full-scale prediction.\u000a      QinetiQ asserts: \"Previously this would have required up to 10-20\u000a        experiments\", and; \"This has enabled the MoD research programme,\u000a        under severe budgetary pressures, to continue to deliver advanced\u000a        research output cost effectively.\"\u000a    (b) International recognition for the UK\u000a    The Leeds research underpinning QinetiQ's numerical simulations has given\u000a      the company a position of international expertise in the field: \"In\u000a        this regard we now have a world leading capability in our approach,\u000a        which is unique to the UK and Europe and generally in the USA,\" the\u000a      company states. The ability to advise others and share the research has\u000a      also given QinetiQ credibility and enabled collaboration on cutting-edge\u000a      research with international allies: \"By being able to share this\u000a        research with our collaborators in the US National Laboratories we were\u000a        able to establish the UK as being credible, providing us access to\u000a        related US research with significant benefits to the UK.\"\u000a    In more general terms, QinetiQ states: \"The Leeds research helped the\u000a        UK maintain a leading internationally recognised capability in numerical\u000a        research, its ability to write 3rd generation software and its\u000a        application in advanced and complex problems of interest to the defence\u000a        and security industries.\"\u000a    QinetiQ continues to benefit from the Leeds research as it builds on its\u000a      expertise in the area of numerical simulation. The company states: \"We\u000a        are currently [2011] developing a new code with a completely different\u000a        strength treatment, which would not have been possible without the\u000a        underpinning provided by the Leeds research.\" [A]\u000a    (c) Protecting the lives of the armed forces\u000a    The advanced numerics of GRIM, developed with support of the Leeds\u000a      research, have been used by QinetiQ to rapidly assess new threats to the\u000a      British Armed Forces in Afghanistan and, latterly, Iraq. Between 2003\u000a        and the present day, GRIM has been used extensively in the battle\u000a      against improvised explosive devices by allowing Dstl to develop new\u000a      countermeasures for deployment on vehicles and body armour, and in the\u000a      protection of military bases, all of which has led to a \"resultant\u000a        reduction in front-line casualties\". QinetiQ further state that \"Without\u000a        the Leeds work we would not have been able to develop our physically\u000a        based material algorithms in a predictive capability\u000a        to be able to respond to these urgent operational requirements.\"\u000a    (d) Protecting the public\u000a    The simulation system has also found civilian applications. Between 1995\u000a        and the present day, QinetiQ has supported other government agencies\u000a      to assess the effects of different threats to transport and\u000a      public-building infrastructure, including planning aspects for the 2012\u000a      Olympic Games. \"As a result new strategies have been evolved and the\u000a        potential of new materials in construction identified\", states\u000a      QinetiQ.\u000a    (e) Increasing productivity in the oil industry\u000a    Between 2008 and the present day, QinetiQ has been working with\u000a      oil-extraction companies to improve yields from oil wells. QinetiQ notes\u000a      that \"In many oil wells only 50% of the available oil is actually\u000a        extracted, particularly in wells that exhibit low permeability. The main\u000a        reason is because it is uneconomical and impractical to use standard\u000a        perforation technologies, which are based on explosive shaped charges.\"\u000a      QinetiQ has used GRIM and its fundamental understanding of materials to\u000a      develop a new perforator that increases the energy imparted to the rock\u000a      above that offered by conventional perforator shaped charges; this has\u000a      been achieved through the use of materials that react under shock loading.\u000a      The higher energy results in significantly larger bore-hole volumes which,\u000a      in tests by the company GEODynamics [F], have \"increased oil flow by\u000a        30-40%\". QinetiQ add that \"The fundamental understanding of the\u000a        behaviour of these reactive materials under shock loading and\u000a        interpretation of experiments was only possible through the use of GRIM.\u000a        This significantly shortened the R&amp;D process, by approximately one\u000a        half, the time to market and cost of the programme\". Additionally,\u000a      QinetiQ \"has been able to licence the new perforator design [G],\u000a        which is now being extensively used by the industry, throughout the\u000a        world, earning royalty fees and benefiting UK oil companies and the\u000a        economy\". These royalties comprise a \"significant fraction\"\u000a      of the annual &#163;4m income to QinetiQ's Warhead Technology Group\u000a      (see quote in &#167;4a); the exact fraction is commercially sensitive\u000a      and has not been disclosed by QinetiQ.\u000a    (f) Informing expert witnesses\u000a    QinetiQ have used GRIM to inform expert witnesses in several high-profile\u000a      inquiries, including the Lockerbie disaster, the Oklahoma bomb and the\u000a      fatal Larkhall gas explosion. Although these predate the REF period,\u000a      QinetiQ state that the GRIM study into the first of these \"resulted in\u000a        the development of appropriate security strategies and the development\u000a        of a `bomb proof' liner for the luggage holds of short haul aircraft\":\u000a      it is reasonable to assert that the impact of these two developments is\u000a      both far reaching and, more importantly, has not only continued into the\u000a      REF period, but will continue to do so beyond it.\u000a    ","ImpactSummary":"\u000a    The Leeds unit's MoD-funded research programme in hypervelocity impact\u000a      dynamics has: saved the MoD two years in ballistic development and\u000a      &#163;1.5m-&#163;2m in costs; guaranteed robustness and reliability of MoD\u000a      computations; enabled the MoD to deliver advanced research output cost-\u000a      effectively under severe budgetary pressures; continued to underpin a &#163;4m\u000a      annual income for the MoD's War Technology consultants QinetiQ;\u000a      provided the MoD with a world-leading explosion- simulation capability.\u000a      MoD codes underpinned by the Leeds research have, during the REF period,\u000a      led to a reduction in front-line casualties of British Forces in\u000a      Afghanistan and Iraq, and enabled government agencies to make quantifiable\u000a      assessments of threats to transport and public-building infrastructure,\u000a      e.g. in the planning of the 2012 Olympic Games. QinetiQ have used the\u000a      codes with industry to develop a new explosive perforator for oil\u000a      extraction that has: \"halved the R&amp;D process, time-to-market and\u000a        cost of oil-well exploitation\"; improved oil flows by 30-40% in\u000a      tests undertaken by oil companies, and; yielded substantial (but\u000a      confidential, see &#167;4e) recurrent licensing royalties.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Leeds\u000a    ","Institutions":[{"AlternativeName":"Leeds (University of)","InstitutionName":"University of Leeds","PeerGroup":"A","Region":"Yorkshire And Humberside","UKPRN":10007795}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"4544379","Name":"Oklahoma"}],"References":"\u000a    In the following list, references best indicating the quality of the\u000a      research are starred, and all Leeds staff appear in bold font. All\u000a      documents are available on request.\u000a    \u000a[1] Kelmanson, M.A., Error analysis of a 2-D Eulerian impact\u000a      code, Defence Research Agency Working Paper\u000a      D\/ER1\/9\/4\/2062\/142\/RARDE\/6, 65pp., 1993.\u000a    \u000a\u000a[2*] Kelmanson, M.A., Unstructured rectangular\u000a      adaptive-mesh generation for 2-D conservative schemes, Appl. Math.\u000a        Lett., 6(2), 17-21, 1993.\u000a      (http:\/\/dx.doi.org\/10.1016\/0893-9659(93)90005-8)\u000a    \u000a\u000a[3*] Kelmanson, M.A., Truncation errors in a 2-D\u000a      hyperbolic PDE integration scheme, Math. Engng Ind., 6(3),\u000a      171-183, 1997.\u000a    \u000a\u000a[4*] Kelmanson, M.A. and Maunder, S.B., Modelling\u000a      high-velocity impact phenomena with unstructured dynamically-adaptive\u000a      Eulerian meshes, J. Mech. Phys. Solids, 47(4), 731-762,\u000a      1999. (http:\/\/dx.doi.org\/10.1016\/S0022-5096(98)00091-X)\u000a    \u000a[5] MoD grant, \"Adaptive Meshing for Impact Dynamics\", DRA WSFH\/U2122C\u000a      (grant holder Goldsworthy, F.A. &amp; Project Manager Kelmanson,\u000a        M.A., &#163;74k, 1995-97).\u000a    [6] MoD grant, \"Adaptive Meshing for Impact Dynamics\", DERA WSS\/U6884\u000a      (grant holder Goldsworthy, F.A. &amp; Project Manager Kelmanson,\u000a        M.A., &#163;43k + &#163;43k, 1998-99 + 1999-2000).\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000a    All sources are stored electronically and are available on request.\u000a    [A] (19th October, 2011) Email from \"Senior QinetiQ Fellow\"\u000a      attesting to underpinning nature of Leeds research on QinetiQ's current\u000a      capability.\u000a    [B] (11th June, 2013) Letter from \"Senior QinetiQ Fellow\"\u000a      outlining end-user benefits.\u000a    [C] (11th June, 2013) Letter from \"Senior QinetiQ Fellow\"\u000a      outlining and dating end-user impact.\u000a    [D] (14th June, 2013) Letter from (end-user) Dstl's \"Programme\u000a        Leader, Weapons Domain\" endorsing the claims made in both this\u000a      impact statement and the supporting statements [B,C].\u000a    [E] Corroboration of status of author of [A,B,C], who is the current\u000a      President of the International Ballistics Society; see 2013 board\u000a      of directors at http:\/\/www.ballistics.org\/board_of_directors.php\u000a    [F] (a) PDF of 2010-12 GEODynamics ConneX&#174; (main site\u000a      http:\/\/www.perf.com\/connex\/) brochure, with proof of increased production\u000a      revenue shown on page 8, which implies flow-rate increases of 50%, i.e.\u000a      even better than those cited in [C]. (b) PDF of 2008 version of item [F]\u000a      (a).\u000a    [G] (a) PDF of information on ConneX&#174; reactive perforating shaped charge\u000a      used by Weatherford International (main site http:\/\/www.weatherford.com)\u000a      for oil extraction. (b) Video demonstrating benefits of new technology at\u000a      http:\/\/www.youtube.com\/watch?v=nrM4rrKhopY \u000a    ","Title":"\u000a    Case Study 6: Cost-effective simulation and prediction of explosions for\u000a      military and public safety, and for improved oil extraction\u000a    ","UKLocation":[{"GeoNamesId":"2644853","Name":"Larkhall"},{"GeoNamesId":"2644688","Name":"Leeds"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"},{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Background\u000a    Between the mid-1980s and 2000, MoD Fort Halstead - formerly RARDE, DRA\u000a      and DERA; now QinetiQ - funded a research programme [5,6] in Applied\u000a      Mathematics at Leeds on the simulation of hypervelocity impacts;\u000a      specifically, the pitting of supersonic-aircraft windscreens by airborne\u000a      water droplets, and the penetration of vehicular armour by ballistic\u000a      weapons. The research improved the MoD's existing simulations, which\u000a      suffered \"significant limitations\" [B], on two fronts: (a)\u000a      deficient modelling of the underlying physics, and; (b) restricted\u000a      computational capability due to inefficient algorithms and insufficient\u000a      memory. While the MoD addressed \"(a)\" in-house through improved material\u000a      models, \"(b)\" was addressed by the Leeds research group through the\u000a      development of innovative computational strategies that delivered improved\u000a      - that is, more efficient and stable - simulation capabilities using\u000a      reduced computational resources. These strategies were successfully\u000a      incorporated into the MoD's ballistic-simulation software-development\u000a      programme, and subsequently used by QinetiQ to generate impact through the\u000a      diverse applications (presented in &#167;4) in military, private and\u000a      industrial (oil-extraction) sectors.\u000a    Personnel ( all Leeds-based)\u000a    The MoD research group in Leeds' Department of Applied Mathematics\u000a      comprised (the late) Professor F A Goldsworthy (SRF, 1993-1996), Dr S B\u000a      Maunder (PDRF, 1993-1998) and Professor M A Kelmanson (permanent staff,\u000a      1993-present). From 1995-2000, Kelmanson was Project Manager, the MoD\u000a      equivalent of P.I., in which role he was sole supervisor of Dr U Mullane\u000a      (PDRA, 1994-1995) and Dr D Wilson (PDRA, 1996-1999) on MoD grants\u000a      totalling &#163;160k [5,6].\u000a    Publications and Research Outputs\u000a    Research outputs from the programme were almost exclusively in the format\u000a      of regular, contractually required, substantial MoD Working Papers\u000a      which, designated as \"Restricted\", were circulated within only MoD\u000a      establishments. Kelmanson wrote four such papers (and his PDRAs nine) of\u000a      which [1] underpins most directly the impact of the research. Submission\u000a      to peer-reviewed international journals was occasionally possible\u000a      following extensive MoD internal vetting; in this context, papers [2,3]\u000a      emanated from two of Kelmanson's Working Papers. The \"significant\u000a        journal paper\" [A] by Kelmanson and Maunder [4] (the primary paper\u000a      of this case) summarises the main research of the Leeds group between 1993\u000a      and 1999.\u000a    Optimization of Computational Resources\u000a    Needing to undertake \"significantly more complex and larger\u000a        simulations\" [B], the MoD funded the Leeds group to undertake\u000a      research into adaptive techniques, whose distinctive strength,\u000a      absent from standard approaches, is their inherent ability to resolve\u000a      phenomena occurring contemporaneously over widely disparate length\u000a      scales. That is, at a given time, adaptive techniques allow physical\u000a      activity to be simulated on a hierarchy of co-existing, unstructured,\u000a      \"coarse-to-fine\" computational grids. Adaptive techniques therefore\u000a      automatically concentrate\/divert computational resources into\/from regions\u000a      where there is physical activity\/quiescence (or geometrical\u000a      complexity\/simplicity); this optimizes the efficiency of simulations since\u000a      detailed computations occur only where necessary - as defined by\u000a      pre-specified adaption criteria. In addition, adaptive grids admit\u000a      automatic and dynamic grid refinement and coarsening in\u000a      response to the evolving physics, making the techniques versatile,\u000a      flexible and computationally cost-effective, since computer memory is\u000a      dynamically released back into the system when and where high resolution\u000a      is no longer required. The Leeds group's research and development\u000a      [1,2,3,4] culminated in the implementation and validation of a fully\u000a      automated dynamically adaptive technique that was embedded into the MoD's\u000a      simulation capability, known as \"GRIM\", for subsequent use in military,\u000a      civilian and industrial applications.\u000a    Validation of the Research\u000a    The computational efficiency of Leeds' adaptive techniques was quantified\u000a      in [4] through application to test ballistic-penetration problems,\u000a      supplied by the MoD, on: (i) the simulated penetration of protective\u000a      armour by a hypervelocity rod, and; (ii) the simulated implosion of a\u000a      spherical shell (for which there is an analytical solution). In [4], it is\u000a      demonstrated that Leeds' methods require respectively only 15% and 25% of\u000a      the memory and CPU of the equivalently resolved non-adaptive techniques\u000a      applied to the same computationally intensive test problems. Additionally,\u000a      in [3], a pioneering error analysis is performed of an adaptive form of\u000a      the difference equations that approximate the transient equations of\u000a      motion of solid mechanics. This analysis enables explicit determination of\u000a      the conditions under which the Leeds group's methods are computationally\u000a      stable, thereby providing the MoD with \"the confidence in the numerics\u000a        to be able to develop physically based material constitutive models and\u000a        equations of state\" [B] that enabled it to continue its research\u000a      programme. So ahead of its time was the Leeds group's organic approach in\u000a      [4] that it was not until 2011 that a competitive approach emerged, via an\u000a      entirely independent (genetic-algorithm) theoretical and computational\u000a      route (http:\/\/dx.doi.org\/10.1007\/s00500-010- 0684-x): contemporaneous\u000a      alternative approaches required either an a priori knowledge of\u000a      the location of the regions to be resolved, or were applicable only to\u000a      steady-state problems.\u000a    "},{"CaseStudyId":"11848","Continent":[],"Country":[],"Funders":["Natural Environment Research Council"],"ImpactDetails":"\u000a    Accurate knowledge of the spatial distribution of forest biomass and its\u000a      changes over time are\u000a      needed to improve present assessments and future projections of the\u000a      terrestrial carbon cycle, as\u000a      they integrate processes that release (e.g. through deforestation) and\u000a      absorb (e.g. through\u000a      reforestation) carbon dioxide. This knowledge is essential to\u000a      understanding the role of forests in\u000a      climate change and the carbon cycle.\u000a    However, it has hitherto been impossible to make accurate large-scale\u000a      measurements of biomass\u000a      in situ because of logistical issues. This is a major obstacle to\u000a      carbon trading schemes that finance\u000a      forest management in developing countries to reduce emissions. In\u000a      particular, the proposed United\u000a      Nations mechanism \"Reductions of Emissions from Deforestation and\u000a      Degradation\" (REDD+) may\u000a      never be ratified unless the difficulties of monitoring and verifying\u000a      reported values of biomass\u000a      change in tropical forest countries are resolved.\u000a    The lack of reliable biomass information also has severe consequences for\u000a      climate calculations.\u000a      Biomass loss from tropical deforestation gives rise to between 9 and 24%\u000a      of total anthropogenic\u000a      emissions (Intergovernmental Panel on Climate Change, 2007). This huge\u000a      uncertainty yields even\u000a      greater uncertainty in the uptake of carbon land surface, because this is\u000a      estimated as a residual\u000a      after accounting for total emissions, the growth of atmospheric carbon\u000a      dioxide and carbon flux into\u000a      the oceans. The size of land uptake, its location, and how much is stored\u000a      as forest biomass\u000a      therefore constitute major unknowns about the Earth's carbon cycle.\u000a    Quegan contributed some of the key ecological and technological research\u000a      underpinning the\u000a      BIOMASS mission concept, and was crucial in creating the convincing\u000a      scientific basis that led to\u000a      the concept being selected for implementation [S1-S5].\u000a    The BIOMASS mission aims to take measurements of forest biomass to assess\u000a      terrestrial carbon\u000a      stocks and fluxes. The mission employs a novel P-band synthetic aperture\u000a      polarimetric radar\u000a      operating at 435 MHz with a 6 MHz bandwidth. In addition to unprecedented\u000a      data on forest\u000a      biomass, the deployment of this wavelength for the first time in space\u000a      means that the mission will\u000a      also provide new information on ice-sheet thickness and internal\u000a      structures in cold regions,\u000a      subsurface geology and water resources in arid regions, as well as data on\u000a      soil moisture,\u000a      permafrost, the ionosphere and sea-surface salinity.\u000a    This is the seventh in the ESA's Earth Explorer series of missions, of\u000a      which three are currently in\u000a      orbit. The first was launched in 2009. Earth Explorer is one of ESA's\u000a      \"optional\" programmes,\u000a      meaning that funding from member states is voluntary, but the individual\u000a      member states have all\u000a      approved the spending on BIOMASS from their public budgets.\u000a    The project has had significant economic impact both in terms of the\u000a      money already been spent in\u000a      the European economy through the initial phases, and in the quantifiable\u000a      re-allocation of public\u000a      spending committed for the future mission.\u000a    Re-allocation of spending\u000a    ESA has committed &#8364;470M to the mission up to its launch in 2020. To put\u000a      this in context, the total\u000a      ESA budget for 2013 is &#8364;4,282M, of which 22.9% is earmarked for Earth\u000a      Observation missions\u000a      such as BIOMASS. Individual member states are expected to commit\u000a      additional funding to the\u000a      project. Of the overall &#8364;470M approved by ESA, &#8364;277M has been specifically\u000a      earmarked for spend\u000a      with industry, as well as a share of a further &#8364;32M. The mission has\u000a      already led to at least &#8364;5.6M of\u000a      ESA money being spent with two European industrial consortia led by\u000a      Astrium and Thales since\u000a      2009, with a further &#8364;1.3M spent on science and campaigns.\u000a    ","ImpactSummary":"\u000a    A &#8364;470 million earth observation mission (BIOMASS) based on research\u000a      carried out in the\u000a      University of Sheffield's Centre for Terrestrial Carbon Dynamics was\u000a      approved by the European\u000a      Space Agency Programme Board on 7 May 2013, for launch in 2020, to measure\u000a      the biomass and\u000a      height of the Earth's forests, globally, at a scale of ~200 m. The twenty\u000a      European member states\u000a      have committed contributions to fund the mission, representing a\u000a      significant reallocation of public\u000a      budgets. The mission was selected as the most scientifically convincing of\u000a      the six initially\u000a      shortlisted in 2005 (further down-selected to three in 2009), and is the\u000a      only one that will be funded.\u000a      The project has already led to two European industrial consortia receiving\u000a      some &#8364;5.6 million for\u000a      studies to demonstrate feasibility. A further &#8364;277 million of the &#8364;470\u000a      million approved funding has\u000a      been specifically earmarked for industrial spending to prepare for the\u000a      mission.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University of Sheffield\u000a    ","Institutions":[{"AlternativeName":"Sheffield (University of)","InstitutionName":"University of Sheffield","PeerGroup":"A","Region":"Yorkshire And Humberside","UKPRN":10007157}],"Panel":"B         ","PlaceName":[],"References":"\u000a    [*= References that best indicate the quality of the research]\u000a    \u000aR1* Quegan, S., Yu, J.J. (2001). Filtering of multichannel SAR\u000a      images, IEEE Transactions on\u000a        Geoscience and Remote Sensing, 39, 11, 2373-9.\u000a    \u000a\u000aR2* Le Toan, T., Quegan, S., Woodward, F.I., Lomas, M.R., Delbart,\u000a      N. (2005) Relating radar\u000a      remote sensing of biomass to modelling of forest carbon budgets. Climatic\u000a        Change, 67, 379-\u000a      402.\u000a    \u000a\u000aR3 Chen, J., Quegan, S. (2010) Improved estimators of Faraday\u000a      rotation in spaceborne\u000a      polarimetric SAR data. IEEE Transactions on Geoscience and Remote\u000a        Sensing Letters, 7, 4,\u000a      846-50.\u000a    \u000a\u000aR4* Le Toan, T., Quegan, S., Davidson, M., Balzter, H., Paillou,\u000a      P., Papathanassiou, K.,\u000a      Plummer, S., Rocca, F., Saatchi, S., Shugart, H., Ulander, L. (2011). The\u000a      BIOMASS Mission:\u000a      Mapping global forest biomass to better understand the terrestrial carbon\u000a      cycle. Remote\u000a        Sensing of Environment, 115, 2850-60.\u000a    \u000a\u000aR5 Chen, J., Quegan, S, Yin, X.J. (2011) Calibration of spaceborne\u000a      linearly polarized low\u000a      frequency SAR using polarimetric selective radar calibrators. Progress\u000a        in Electromagnetic\u000a        Research, 114, 89-111.\u000a    \u000a\u000aR5 Rogers, N.C., Quegan, S., Kim, J.S., Papathanassiou, K.P.\u000a      (2013) Impacts of ionospheric\u000a      scintillation on the BIOMASS P-band satellite SAR. IEEE Transactions\u000a        on Geoscience and\u000a        Remote Sensing doi: 10.1109\/TGRS.2013.2255880.\u000a    \u000aFunding\u000a    G1 NERC: Ionospheric effects on P-band satellite radar, 2007-08,\u000a      &#163;75,000\u000a    G2 ESA: Simulation of Ionospheric Disturbances and Impact\u000a      Assessment on BIOMASS\u000a      Product Quality, 2007-08, &#163;77,949\u000a    G3 ESA: Ionospheric mitigation schemes and their consequences for\u000a      BIOMASS product quality,\u000a      2009-11, &#163;150,760\u000a    G4 NERC: IonoSAR, 2010-12, &#163;65,339\u000a    G5 ESA: End-to-End BIOMASS Simulator, 01\/2011-06\/2012, &#8364;30,000\u000a    Preparation for the BIOMASS mission has also led to ESA funding for\u000a      science studies in several\u000a      international research institutes and contracts to industry. The\u000a      approximate total values of these\u000a      contracts (exact values are not available) are:\u000a    &#8364;5.6M for industry (split between Astrium and Thales)\u000a      &#8364;1.3M for science studies and airborne campaigns\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"9","Subject":"Geomatic Engineering"},{"Level1":"2","Level2":"1","Subject":"Astronomical and Space Sciences"}],"Sources":"\u000a    S1 European Space Agency (2008). BIOMASS Report for Assessment,\u000a      European Space\u000a      Agency, ESA SP 1313\/2, page 3 corroborates Sheffield contribution to the\u000a      underpinning\u000a      science.\u000a    S2 European Space Agency (2012). BIOMASS Report for Selection:\u000a        an Earth Explorer to\u000a        measure forest biomass, European Space Agency, ESA SP 1324\/1. Page\u000a      ii confirms\u000a      Sheffield membership of Biomass Mission Advisory Group; page 185 confirms\u000a      Quegan's\u000a      research outputs cited.\u000a    S3 European Space Agency (2013). Earth Explorer 7 Candidate\u000a        Mission Biomass: Addendum\u000a        to the Report for Mission Selection, EOP-SM\/2458\/MD-md.\u000a    S4 Further confirmation of the essential contribution of Quegan's\u000a      research to the Biomass\u000a      mission being selected can be obtained from the ESA BIOMASS Mission\u000a      Scientist, Mission\u000a      Science Division, ESA-ESTEC (the European Space Agency's space research\u000a      and\u000a      technology centre, where all ESA space missions are developed and\u000a      managed).\u000a    S5 Further confirmation of the essential contribution of Quegan's\u000a      research to the Biomass\u000a      mission being selected can be obtained from the Business Development\u000a      Manager, Earth\u000a      Observation, EADS Astrium. Astrium is the European leader, and second in\u000a      the world, in\u000a      space transportation, satellite systems and services, with 18,000\u000a      employees.\u000a    \u000a    ","Title":"\u000a     BIOMASS: measuring global forest biomass from space\u000a    ","UKLocation":[{"GeoNamesId":"2638077","Name":"Sheffield"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Professor Quegan's and the University of Sheffield's leading role in the\u000a      BIOMASS mission arises\u000a      from the breadth of Quegan's research, including carbon-cycle studies\u000a      using satellite and ground\u000a      data with models; estimation of forest properties from satellite data;\u000a      ionospheric modelling and\u000a      assessment and correction of its effects on spaceborne systems;\u000a      measurement properties,\u000a      information extraction and signal processing for radar remote sensing\u000a      systems; and the theory of\u000a      radar scattering from land surfaces and vegetation. Knowledge in all of\u000a      these is necessary to\u000a      develop the science underlying the BIOMASS mission. The gestation period\u000a      for BIOMASS has\u000a      been long: the basic concept goes back to the early 1990s, but interest\u000a      waned once it became\u000a      clear that international regulations would not allow spaceborne use of the\u000a      key wavelength. Only in\u000a      2004 did this change, as a result of evidence from earlier airborne\u000a      studies. Correction of\u000a      ionospheric effects exploits research carried out in the 1980s,\u000a      reincarnated for BIOMASS.\u000a    The BIOMASS concept as it was understood in 2008 is given in [R4],\u000a      but further science\u000a      development led to considerable changes, as described in the key ESA\u000a      reports [S1-S3] giving the\u000a      science case for the mission, referenced in Section 5. Quegan wrote large\u000a      parts of these reports,\u000a      and edited them all, in his capacity as chairman of the BIOMASS Mission\u000a      Advisory Group.\u000a    The use of BIOMASS-type data to test and improve carbon flux estimates\u000a      from ecosystem models\u000a      is demonstrated in [R2], based on early crude measurements of the\u000a      biomass of Siberian forests\u000a      from space in the EU SIBERIA-1 project, in which Quegan was the Technical\u000a      Coordinator.\u000a    The value of long-wavelength radar for monitoring tropical deforestation\u000a      is demonstrated in Whittle,\u000a      Quegan et al. (Remote Sensing of Environment, 2012), which\u000a      also illustrates the limitations of\u000a      sensors with shorter wavelengths than BIOMASS.\u000a    Optimal methods to combine data at different polarisations and from\u000a      different times to make\u000a      precise measurements at the spatial scales needed by BIOMASS are derived\u000a      in [R1]. This is\u000a      unique in deriving an analytical expression for the measurement precision,\u000a      which is a critical\u000a      parameter in designing the biomass inversion procedure.\u000a    Ionospheric Faraday rotation will badly corrupt the multiple\u000a      polarisations used by BIOMASS, and\u000a      ionospheric scintillations can degrade image contrast and distort\u000a      intensity measurements, so both\u000a      effects must be removed from BIOMASS data. Characterisation of Faraday\u000a      rotation and methods\u000a      to correct it are given in [R3] (also in Wright, Quegan et al.\u000a      IEEE Transactions on Geoscience and\u000a        Remote Sensing, 2003). Analysis of scintillation [R6] led to\u000a      the selection of a dawn-dusk orbit for\u000a      BIOMASS; this removes any significant scintillation impact on biomass\u000a      measurements (but not on\u000a      secondary ice measurements).\u000a    A related key problem, solved in [R5], is to derive estimates of\u000a      system errors in the presence of\u000a      Faraday rotation, and to use them to calibrate the measurements. This\u000a      process also allows\u000a      ionospheric structure and dynamics to be measured, leading to the adoption\u000a      of space weather as a\u000a      secondary mission objective.\u000a    "},{"CaseStudyId":"11849","Continent":[{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Natural Environment Research Council"],"ImpactDetails":"\u000a    An informal survey of radiocarbon dating laboratories worldwide suggests\u000a      that over 50,000\u000a      radiocarbon determinations are undertaken annually, at a cost of around\u000a      &#163;350 each, i.e. more than\u000a      &#163;17.5m pa. Most users interpret radiocarbon determinations with\u000a      freely-available radiocarbon\u000a      calibration software such as CALIB (http:\/\/calib.qub.ac.uk\/calib\/),\u000a      OxCal\u000a      (http:\/\/c14.arch.ox.ac.uk\/oxcal.html)\u000a      or BCal (http:\/\/bcal.sheffield.ac.uk).\u000a      Due to improvements in\u000a      accuracy, and for reasons of consistency, the providers of such software\u000a      automatically migrate\u000a      their packages to the new curves as they are published. Consequently,\u000a      curves estimated at\u000a      Sheffield have been at the core of all of these software packages since\u000a      2004 until the present time.\u000a      One of the three packages, BCal, is an on-line service based at Sheffield,\u000a      which is offered free-of-charge\u000a      to the international community by Buck and a team of volunteers.\u000a    Users in the public and private sectors since 2008 include:\u000a    \u000a      commercial radiocarbon dating laboratories (e.g. Beta Analytic)\u000a      commercial archaeology units (e.g. University of Leicester\u000a        Archaeological Services and\u000a        Trent &amp; Peak Archaeology)\u000a      palaeoenvironmental scientists in governmental and intergovernmental\u000a        agencies (e.g. the\u000a        British Antarctic Survey and the Intergovernmental Panel on Climate\u000a        Change)\u000a      private and public sector staff charged with the care of ancient\u000a        buildings and environments\u000a        (e.g. English Heritage)\u000a      freelance experts who undertake radiocarbon dating to advise private\u000a        customers, public\u000a        sector companies and government agencies.\u000a    \u000a    Economic impact\u000a    The need to reduce the costs of archaeological work has made the improved\u000a      accuracy offered by\u000a      the Sheffield curves increasingly important. The Head of Research at Trent\u000a      &amp; Peak Archaeology,\u000a      UK says: \"As a commercial archaeological unit ... we are of course\u000a        reliant upon accurate\u000a        calibration methodologies, and hence support strongly the IntCal team's\u000a        work on refining\u000a        calibration curves. Refinements in calibration have increased confidence\u000a        in the accuracy of this\u000a        dating technique and in our area of activity have spurred its wider use\u000a        ... This demonstrable\u000a        increase in accuracy is especially crucial in view of the ever greater\u000a        pressure to reduce the costs of\u000a        archaeological work in advance of development, and hence the need to\u000a        defend more fiercely\u000a        investments in appropriate programmes of radiocarbon dating\" [S1].\u000a    Impact on national and international policy and services\u000a    Consistency is particularly important to those working on international\u000a      policy since it allows them to\u000a      compare and combine results from different projects. Eric Wolff FRS,\u000a      Science Leader (Chemistry\u000a      and Past Climate), British Antarctic Survey says that \"Correct and\u000a        consistent dating of past events\u000a        in the palaeorecord is crucial to our ability to understand processes in\u000a        the Earth system, and\u000a        therefore to verify and improve models of its future behaviour. In this\u000a        sense the work of IntCal is\u000a        one of the cornerstones that allows us to use the past to provide\u000a        insights that can inform policy\"\u000a      [S2]. One of the lead authors, Jonathan Overpeck, of the\u000a      Palaeoclimate chapter in the\u000a      Intergovernmental Panel on Climate Change (IPCC) Fourth Assessment Report\u000a      [S3] - the most\u000a      authoritative document influencing international climate change policy\u000a      during 2007-13 - says that\u000a      the work undertaken by the IntCal group was \"a critical underpinning of\u000a        much of the chapter that\u000a        relied on radiocarbon chronologies. More specifically, a number of\u000a        papers assessed in the chapter\u000a        produced timescales based on calibration of radiocarbon ages using\u000a        IntCal04\" [S4].\u000a    Impact on practitioners\u000a    Practitioners confirm that the Sheffield curves have offered substantial\u000a      improvements, not only\u000a      providing a common standard and improved method, but extending the range\u000a      of calibration. The\u000a      President of Beta Analytic (the world's largest commercial radiocarbon\u000a      dating laboratory) says that\u000a      he supports IntCal as \"the primary (and preferably only) database used\u000a        in radiocarbon dating\u000a        calibrations\" [S5]. The President of archaeology consultancy,\u000a      T. S. Dye &amp; Colleagues,\u000a      Archaeologists, Inc., Honolulu, states that incremental improvements to\u000a      the calibration curve \"have\u000a        become increasingly important as Hawaiian archaeologists solve the\u000a        substantial problems of\u000a        chronologically ordering and dating events important to the short\u000a        prehistoric sequence in the\u000a        islands\" [S6].\u000a    The Scientific Dating Coordinator, English Heritage defines the IntCal\u000a      programme as `industry\u000a        standard' in both economic and professional practice terms. He says\u000a      that the new curves are\u000a      adopted \"not only because they form a common standard, but also because\u000a        of the enhancements\u000a        which the new data and compilations provide. ... Key benefits ...\u000a        include improved methods for the\u000a        modelling of the tree-ring data (Buck &amp; Blackwell 2004 [R1]),\u000a        which enable more robust wiggle-matching\u000a        procedures to be adopted. This is critical for the development of this\u000a        technique to allow\u000a        the dating of historic buildings (e.g. Tyers et al. 2009 [S7])\u000a        to an equivalent level of precision and\u000a        accuracy as routinely provided by dendrochronology. Both this curve and\u000a        the subsequent 2009\u000a        release have extended the range of radiocarbon calibration to the limit\u000a        of the technique\" [S8].\u000a    Impact on public engagement in science\u000a    The dating aspects of high-profile projects also provide archaeologists\u000a      with effective ways to\u000a      engage the general public in the complexities of modern science-based\u000a      archaeology. The lead\u000a      archaeologist on the \"Searching for Richard III\" project, says of IntCal09\u000a      that \"Without such a\u000a        calibration curve it would not have been possible to place any of the\u000a        remains on a real timescale\u000a        and thus to determine their validity as belonging to the era of Richard\u000a        III. The IntCal calibration\u000a        curve has thus played an important role in helping the project to\u000a        capture the public imagination\".\u000a      He also says that there have been \"many thousands of hits\" on the\u000a      web page\u000a      http:\/\/www.le.ac.uk\/richardiii\/science\/carbondating.html\u000a      on which his team documented the\u000a      radiocarbon dating parts of their work, that \"5 million people watched\u000a        the Channel 4 programme on\u000a        which we discussed the interpretation of the radiocarbon dating evidence\"\u000a      and that \"the exhibition\u000a        at the Guildhall in Leicester in which details of the dating are\u000a        presented has had over 60,000\u000a        visitors\" [S9].\u000a    ","ImpactSummary":"\u000a    Statistical research undertaken at Sheffield has resulted in the\u000a      provision of internationally-agreed\u000a      calibration curves for radiocarbon dating that offer greater accuracy and\u000a      higher resolution, and\u000a      which (for the first time) span the full range of timelines over which\u000a      radiocarbon dating is feasible.\u000a      Since the amount of radioactive carbon in the Earth's atmosphere has not\u000a      remained constant over\u000a      time, anyone seeking to interpret a radiocarbon determination now\u000a      calibrates it using one of these\u000a      curves, which results in up to 50% reduction in calibrated date intervals\u000a      over those previously\u000a      obtainable. Non-academic users of these curves include staff in commercial\u000a      radiocarbon\u000a      laboratories, those working in commercial archaeology units, freelance\u000a      archaeological consultants,\u000a      palaeoenvironmental scientists working in governmental and\u000a      intergovernmental bodies, private and\u000a      public sector staff charged with the care of ancient buildings and\u000a      environments, and freelance\u000a      consultants who undertake radiocarbon dating in order to advise private\u000a      customers, public sector\u000a      companies and government agencies.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Sheffield\u000a    ","Institutions":[{"AlternativeName":"Sheffield (University of)","InstitutionName":"University of Sheffield","PeerGroup":"A","Region":"Yorkshire And Humberside","UKPRN":10007157}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"5855797","Name":"Hawaii"},{"GeoNamesId":"5856195","Name":"Honolulu"}],"References":"\u000a    [* = References that best indicate the quality of the research]\u000a    \u000aR1* Buck, C.E., Blackwell, P.G. (2004). Formal statistical models\u000a      for estimating radiocarbon\u000a      calibration curves. Radiocarbon, 46, 1093-02.\u000a    \u000a\u000aR2 Buck, C.E., G&#243;mez Portugal Aguilar, D., Litton, C.D., O'Hagan,\u000a      A (2006). Bayesian\u000a      nonparametric estimation of the radiocarbon calibration curve. Bayesian\u000a        Analysis, 1, 265-88.\u000a      doi: 10.1214\/06-BA109\u000a    \u000a\u000aR3 Blackwell, P.G., Buck, C.E. (2008). Estimating radiocarbon\u000a      calibration curves (with\u000a      discussion). Bayesian Analysis, 3, 225-68. doi: 10.1214\/08-BA309\u000a    \u000a\u000aR4* Reimer, P.J., Baillie, M.G.L., Bard, E., Bayliss, A., Beck,\u000a      J.W., Bertrand, C.J.H., Blackwell,\u000a      P.G., Buck, C.E., Burr, G.S., Cutler, K.B., Damon, P.E., Edwards, R.L.,\u000a      Fairbanks, R.G.,\u000a      Friedrich, M., Guilderson, T.P., Hogg, A.G., Hughen, K.A., Kromer, B.G.M.,\u000a      Manning, S.,\u000a      Ramsey, C.B., Reimer, R.W., Remmele, S., Southon, J.R., Stuiver, M.,\u000a      Talamo, S., Taylor,\u000a      F.W.;, van der Plicht, J., Weyhenmeyer, C.E. (2004). IntCal04&#8212;terrestrial\u000a      radiocarbon age\u000a      calibration, 0-26 cal kyr BP. Radiocarbon, 46, 1029-58. [2,437\u000a      citations (ISI)]\u000a    \u000a\u000aR5 Heaton, T.J., Blackwell, P.G., Buck, C.E. (2009). A Bayesian\u000a      approach to the estimation of\u000a      radiocarbon calibration curves: the IntCal09 methodology. Radiocarbon,\u000a      51, 1151-64.\u000a    \u000a\u000aR6* Reimer, P.J., Baillie, M.G.L., Bard, E., Bayliss, A., Beck,\u000a      J.W., Blackwell, P.G., Bronk\u000a      Ramsey, C., Buck, C.E., Burr, G.S., Edwards, R.L., Friedrich, M., Grootes,\u000a      P.M., Guilderson,\u000a      T.P., Hajdas, I., Heaton, T.J., Hogg, A.G., Hughen, K.A., Kaiser, K.F.,\u000a      Kromer, B.,\u000a      McCormac, F.G., Manning, S.W., Reimer, R.W., Richards, D.A., Southon,\u000a      J.R., Talamo, S.,\u000a      Turney, C.S.M., van der Plicht, J., Weyhenmeyer, C.E. (2009). IntCal09 and\u000a      Marine09\u000a      Radiocarbon Age Calibration Curves, 0-50,000 Years cal BP. Radiocarbon,\u000a      51, 1111-50.\u000a      [1,690 citations (ISI)]\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"4","Level2":"3","Subject":"Geology"},{"Level1":"21","Level2":"1","Subject":"Archaeology"}],"Sources":"\u000a    S1 Letter from the Head of Research at Trent &amp; Peak\u000a      Archaeology corroborates economic\u000a      impact of research.\u000a    S2 Science Leader (Chemistry and Past Climate) at the British\u000a      Antarctic Survey, Cambridge, UK\u000a      corroborates impact on national\/international policy and services.\u000a    S3 Jansen, E. J. et al. (2007). Palaeoclimate. In S. Solomon, et\u000a      al. (eds). Climate Change 2007:\u000a        The Physical Science Basis. Contribution of Working Group I to the\u000a        Fourth Assessment\u000a        Report of the Intergovernmental Panel on Climate Change. Cambridge\u000a      University Press, pp.\u000a      433-97.\u000a    S4 Co-director, Institute of the Environment, and Professor of\u000a      Geosciences and Atmospheric\u000a      Sciences, University of Arizona, USA, lead author on Palaeoclimate chapter\u000a      in Jansen et al.\u000a      (S3) corroborates underpinning of IPCC report by Sheffield research.\u000a    S5 President of Beta Analytic, USA, corroborates impact on\u000a      practitioners.\u000a    S6 President of T.S. Dye &amp; Colleagues, Archaeologists, Inc.,\u000a      Honolulu, corroborates impact on\u000a      practitioners.\u000a    S7 Tyers, C., Sidell, J., van der Plicht, J., Marshall, P., Cook,\u000a      G., Bronk Ramsey, C., Bayliss, A.\u000a      (2009) Wiggle-matching using known-age pine from Jermyn Street, London,\u000a      UK.\u000a      Radiocarbon, 51, 385-96.\u000a    S8 Scientific Dating Coordinator, English Heritage, UK,\u000a      corroborates impact on practitioners\u000a    S9 Director of University of Leicester Archaeological Services,\u000a      UK, corroborates impact on\u000a      public engagement in science (http:\/\/www.le.ac.uk\/ulas\/about\/index.html).\u000a    \u000a    ","Title":"\u000a    Improving Radiocarbon Calibration\u000a    ","UKLocation":[{"GeoNamesId":"2638077","Name":"Sheffield"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Radiocarbon dating is crucial to the establishment of archaeological\u000a      chronologies and of timelines\u000a      for many Holocene and late Pleistocene palaeoclimate studies, and\u000a      palaeoenvironmental\u000a      reconstructions. In order to provide useful dating evidence, all\u000a      radiocarbon determinations must be\u000a      calibrated because the proportion of radioactive carbon (14C)\u000a      in the earth's atmosphere has varied\u000a      over time. Our knowledge about the scale and timing of variations in 14C\u000a      levels comes from\u000a      radiocarbon determinations for known-age samples. The data are used to\u000a      derive calibration curves\u000a      that map radiocarbon to calendar ages. Until the late 1990s, these curves\u000a      were based on\u000a      measurements on tree rings, for which the calendar ages are well\u000a      established, and were estimated\u000a      using rather ad hoc statistical methods.\u000a    By 2000, however, it had become clear that significant other potential\u000a      sources of calibration data\u000a      existed and that, if they could be utilised, it should be possible to both\u000a      extend the length of the\u000a      calibration and improve its resolution. Such data derive from long\u000a      environmental archives (such as\u000a      marine sediments, stalactites and stalagmites) and are considerably more\u000a      difficult to utilise than\u000a      those from tree rings because of the complex nature and scale of the\u000a      uncertainties in the calendar\u000a      age estimates associated with them. These age estimates derive from\u000a      methods such as uranium-thorium\u000a      dating, counting of annual laminations and age-depth modelling, each of\u000a      which leads to a\u000a      different error structure, some of which produce calendar age estimates\u000a      that are highly correlated\u000a      with those of other samples in the database [R1-R6].\u000a    The key research that gave rise to the impact was undertaken in the\u000a      School of Mathematics and\u000a      Statistics at Sheffield, between 2001 and 2009, by Professor Caitlin Buck,\u000a      Professor Paul\u000a      Blackwell and Dr Tim Heaton. It involves the development of a fully\u000a      probabilistic, Wiener process-based\u000a      modelling framework for the estimation of radiocarbon calibration curves [R1-R3]\u000a      and\u000a      Bayesian implementation of those methods in such a way that they can be\u000a      extended as new data\u000a      structures become available [R5]. Users typically appreciate that\u000a      the internationally-agreed\u000a      calibration curves are at the heart of the calibration software that they\u000a      use and most cite both the\u000a      software and the paper that launched the curve they utilised in their\u000a      work. Given that academic,\u000a      commercial and public sector users all publish work in the academic\u000a      literature, it is not practical to\u000a      distinguish the citations from authors working in different sectors but,\u000a      as of October 2013, there\u000a      have been more than 4,000 citations of [R4] and [R6].\u000a    The IntCal Working Group (IWG), coordinated by Paula Reimer at Queen's\u000a      University Belfast\u000a      (QUB), is responsible for providing the internationally-agreed estimates\u000a      of the radiocarbon\u000a      calibration curve. Buck was invited to join the group in 2001, and other\u000a      Sheffield staff (Blackwell,\u000a      Heaton) joined between 2002 and 2008. Since 2001, the work has been funded\u000a      by the Leverhulme\u000a      Trust and NERC. The calibration database is managed at QUB, but all\u000a      statistical work was\u000a      undertaken at Sheffield [R1-R3, R5], and all\u000a      internationally-agreed curves have been constructed\u000a      at Sheffield since 2004 (notably [R4] and [R6]).\u000a    "},{"CaseStudyId":"11850","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The Sheffield research has changed the way Pratt &amp; Whitney designs\u000a      and manufactures aircraft engines, and the way the Food and Environment\u000a      Research Agency (Fera), part of the Department for Environment, Food and\u000a      Rural Affairs (Defra) conducts risk assessments.\u000a    Commercial impact\u000a    The Design for Variation initiative was led by Grant Reinman, a\u000a      statistician at Pratt &amp; Whitney. Pratt &amp; Whitney learned of\u000a      Kennedy and O'Hagan's work from a literature search. A key dissemination\u000a      route for Oakley &amp; O'Hagan (2004) was the software package GEM-SA,\u000a      which implements the methodology in this paper, written by Marc Kennedy\u000a      during his time at Sheffield. It was made available for free download, and\u000a      Pratt &amp; Whitney have used it in their design processes (though they\u000a      have now built on it to develop their own software). Kennedy is now a risk\u000a      analyst at Fera, and so disseminated the research within Fera directly.\u000a    Pratt &amp; Whitney's Design for Variation process has five steps: (i)\u000a      define probabilistic design criteria; (ii) use computer models and\u000a      physical experiments to identify causes of performance variation and\u000a      uncertainty; (iii) find the optimum design to satisfy the design criteria;\u000a      continue data collection to (iv) validate the models; and (v) ensure the\u000a      models remain consistent with the real world. Methods in Oakley &amp;\u000a      O'Hagan (2004) and Kennedy &amp; O'Hagan (2001) play an essential role in\u000a      steps (ii) and (iii), and hence have contributed to what Al Brockett, a\u000a      former vice president of engineering module centres at Pratt &amp;\u000a      Whitney, describes as a \"paradigm shift\" and a \"high-visibility\u000a        strategic priority\" in the way they design and manufacture aircraft\u000a      engines [S1].\u000a    An illustration is given in Reinman et al. [S2]. In the design of\u000a      a jet engine turbine airfoil, a computer model predicted the life\u000a      expectancy of the airfoil, given its design. There was variability in\u000a      airfoil life expectancy due to part-to-part variation, engine-to-engine\u000a      variation, and environmental variation, and the designers wanted to know\u000a      how to reduce variation in life expectancy. A variance-based sensitivity\u000a      analysis was used: the analysis told them how much of the output variance\u000a      was caused by each source of input variation. As the model was\u000a      computationally expensive, the analysis could not have been done without\u000a      Oakley &amp; O'Hagan (2004). The designers used the results to assess the\u000a      most cost-effective way of reducing variability in life expectancy, by\u000a      targeting the most important sources of input variation (and not wasting\u000a      resources by reducing unimportant input variation).\u000a    Pratt &amp; Whitney calibrate their computer models to data using Kennedy\u000a      &amp; O'Hagan (2001). This method allows them to account for all sources\u000a      of uncertainty in their model predictions &#8212; in particular, uncertainty due\u000a      to a model not representing reality perfectly. Reinman et al. explain the\u000a      benefits: \"Significant insight can be gained from the calibration\u000a        results. In a recent study, assumptions typically made about boundary\u000a        conditions near the airfoil surfaces were found to be over 20% higher\u000a        than what the calibration process revealed them to be. Part temperatures\u000a        were being over-estimated, and correspondingly airfoil life was being\u000a        under-estimated\" [S1].\u000a    To quantify the financial benefits of DFV, Pratt &amp; Whitney did a\u000a      Business Case Study to assess the value of quantifying and managing\u000a      uncertainty over the entire life cycle of an engine (from design through\u000a      to service), using sensitivity analysis and calibration methods within\u000a      their DFV process. The published saving in sustainment costs from doing\u000a      this, for a large fleet of military aircraft, was approximately [text\u000a      removed for publication] [S3]. The company also estimates that its\u000a      component-level DFV initiatives \"have yielded a 64% to 88% return on\u000a        investment by reducing design iterations, improving manufacturability,\u000a        increasing reliability, improving on-time deliveries, and providing\u000a        other performance benefits\" [S1].\u000a    Change to professional practice in environmental management\u000a    The Sheffield research has also changed the way the Food and\u000a      Environmental Research Agency (Fera) conduct probabilistic risk\u000a      assessments. Kennedy et al. [S4] report an analysis funded by the\u000a      UK Health and Safety Executive's Chemicals Regulation Directorate (Defra\u000a      project no. PS2005), investigating risks of exposure to pesticide from the\u000a      spray drift of an agricultural boom sprayer. A computer model predicted\u000a      the level of exposure to bystanders and residents after a crop-spraying\u000a      event. The model had uncertain and variable inputs, such as the height of\u000a      the boom, distance of a bystander from the source, wind speed, etc. Using\u000a      the sensitivity analysis method of Oakley &amp; O'Hagan (2004),\u000a      implemented in GEM-SA, they quantified the contribution of each\u000a      uncertain\/variable input to the output uncertainty, to give risk managers\u000a      information on how best to manage risks by reducing output uncertainty.\u000a      Due to the computational expense of the model, this would not have been\u000a      feasible without Oakley &amp; O'Hagan (2004). The analysis in this case\u000a      suggested reducing boom height and variation in boom height has the\u000a      potential to reduce exposure.\u000a    Other ongoing projects at Fera are using GEM-SA for contaminated land and\u000a      assessing the impact of recycling pesticide containers: Defra research\u000a      project PS1010 &#8212; Development of Category 4 Screening Levels for\u000a      Assessment of Land Affected by Contamination; and Defra research project\u000a      PS2808 &#8212; Recycling of Home and Garden Pesticide Containers.\u000a    ","ImpactSummary":"\u000a    Pratt &amp; Whitney (one of the world's largest makers of aircraft\u000a      engines) has developed a process, \"Design for Variation\" (DFV), that uses\u000a      Bayesian methods developed at Sheffield for analysing uncertainty in\u000a      computer model predictions within the design, manufacture and service of\u000a      aircraft engines. The DFV process significantly improves cost efficiency\u000a      by increasing the time an engine stays operational on the wing of an\u000a      aircraft, so reducing the time that the aircraft is unavailable due to\u000a      engine maintenance. DFV also saves costs by identifying design and process\u000a      features that have little impact on engine performance, but are expensive\u000a      to maintain. Pratt &amp; Whitney estimate the DFV process to generate\u000a      savings, for a large fleet of military aircraft, of [text removed for\u000a      publication].\u000a    The UK Food and Environment Research Agency (Fera) has used these methods\u000a      in their risk analyses, for example in assessing risks of exposure to\u000a      pesticides.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Sheffield\u000a    ","Institutions":[{"AlternativeName":"Sheffield (University of)","InstitutionName":"University of Sheffield","PeerGroup":"A","Region":"Yorkshire And Humberside","UKPRN":10007157}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Papers:\u000a    \u000aR1 Kennedy, M.C., O'Hagan, A. (2001). Bayesian calibration of\u000a      computer models (with discussion). Journal of the Royal Statistical\u000a        Society, Series B 63, 425-64. doi: 10.1111\/1467-9868.00294\u000a    \u000a\u000aR2 Oakley, J.E., O'Hagan, A. (2004). Probabilistic sensitivity\u000a      analysis of complex models: a Bayesian approach. Journal of the Royal\u000a        Statistical Society, Series B 66, 751-69. doi: 10.1111\/j.1467-9868.2004.05304.x\u000a    \u000aGrants:\u000a    G1 Engineering and Physical Sciences Research Council, &#163;105,685\u000a      (1995-98) Bayesian uncertainty analysis and computer model inadequacy,\u000a      with support from the National Radiological Protection Board. PI: Anthony\u000a      O'Hagan\u000a    G2 Engineering and Physical Sciences Research Council, &#163;86,940\u000a      (2000-02). Realising Our Potential Award: Bayesian elicitation of expert\u000a      opinion. PI: Anthony O'Hagan.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"}],"Sources":"\u000a    S1 ANSYS (2013). ANSYS Advantage, 7, 2, p. 18. Available at http:\/\/tinyurl.com\/q63h646\u000a    S2 Reinman, G. et al. (2012). Design for variation. Quality\u000a        Engineering, 24: 317-45 doi: 10.1080\/08982112.2012.651973\u000a    S3 Statistician,Pratt &amp; Whitney, letter on file corroborating\u000a      savings in sustainment costs.\u000a    S4 Kennedy M.C., Butler Ellis, M.C., Miller, P.C.H. (2012). BREAM:\u000a      A probabilistic bystander and resident exposure assessment model of spray\u000a      drift from an agricultural boom sprayer. Computers and Electronics in\u000a        Agriculture, 88: 63-71. \u000a    ","Title":"\u000a    Managing uncertainty in computer models: aircraft engine design and food\u000a      safety risk assessment\u000a    ","UKLocation":[{"GeoNamesId":"2638077","Name":"Sheffield"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The research is concerned with statistical methods for handling\u000a      uncertainty in computer models. By \"computer model\", we mean a\u000a      deterministic mathematical model of a physical system, implemented on a\u000a      computer. Uncertainty can arise from not knowing the true values for the\u000a      model inputs, and an imperfect model structure; we may not understand the\u000a      underlying physics perfectly, and\/or it may not be feasible to implement\u000a      our best description of the physical process on a computer. Such models\u000a      can take a long time to run. Reinman et al. [S1] describe\u000a      computationally expensive models used at Pratt &amp; Whitney: finite\u000a      element models, as used for heat transfer and mechanical stress modelling,\u000a      can take hours to run at one choice of input values, and computational\u000a      fluid dynamics models based on Navier-Stokes equations can take days to\u000a      run just once.\u000a    Kennedy and O'Hagan [R1] is concerned with calibrating computer\u000a      models to data. They consider a computer model with two types of inputs:\u000a      uncertain, fixed \"calibration\" inputs, and variable, known \"control\"\u000a      inputs. Physical experiments are conducted at different values of the\u000a      control inputs, and the aim is to find values of the calibration inputs so\u000a      that the computer model outputs match the physical data as closely as\u000a      possible. An important development is the inclusion of a \"model\u000a      discrepancy\" function: a mechanism for learning the error in the model\u000a      structure and correcting the model prediction at new input settings. The\u000a      authors use a Bayesian framework, and show how to quantify all sources of\u000a      uncertainty when predicting with the model.\u000a    This research was started at the University of Nottingham by O'Hagan as\u000a      Principal Investigator and Kennedy as Research Associate, funded by an\u000a      EPSRC grant with support from the National Radiological Protection Board [G1].\u000a      O'Hagan and Kennedy moved to the University of Sheffield in January 1999,\u000a      where they continued and developed their research, leading to their 2001\u000a      publication.\u000a    Oakley and O'Hagan [R2] is concerned with identifying the most\u000a      influential inputs in a computer model. They consider a computer model\u000a      with multiple inputs, the true values of which are uncertain. The aim is\u000a      to quantify how each uncertain input contributes to the uncertainty in the\u000a      model output. A variance-based approach was taken, which quantifies how\u000a      much the output variance can be reduced by learning the true value of an\u000a      input.\u000a    The key development is to use a Gaussian process emulator to speed up\u000a      computation of the variance-based measures of input influence. The\u000a      emulator is a statistical approximation of the computer model, which is\u000a      constructed from a relatively small number of computer model runs, and can\u000a      then be used as a fast surrogate. Previous computational methods required\u000a      large numbers of runs of the computer model, and were infeasible for\u000a      computationally expensive models. (Faster computers have not solved this\u000a      problem. As computing power increases, model users may choose to run their\u000a      models at higher resolution, improving accuracy, but requiring more\u000a      computational effort).\u000a    This research was conducted at the University of Sheffield, funded by an\u000a      EPSRC grant with O'Hagan as Principal Investigator and Oakley as Research\u000a      Associate (now Lecturer at the University of Sheffield) [G2].\u000a    "},{"CaseStudyId":"12701","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000a    The cost of IEP installation to the process owner is non-trivial, so\u000a      offering a transparent mechanism for comparing cost versus safety\u000a      trade-off is clearly valuable and fulfills a previously unmet user need in\u000a      the explosion protection installation industry. A typical IEP installation\u000a      on a process plant costs between &#163;40,000 and &#163;120,000, with some\u000a      installations costing as much as &#163;700,000. To pay any extra money for a\u000a      `better' (or safer) IEP system, the process plant owner needs to know what\u000a      `better' means in the context of improved safety. The Brunel-Kidde\u000a      algorithm, backed by peer reviewed academic research [1] and presented to\u000a      industrial researchers at multiple international conferences [2], is the\u000a      first such algorithm which can compare two IEP systems in terms of their\u000a      safety integrity levels.\u000a    Kidde, and later UTCFS, were sufficiently convinced of the value of this\u000a      research to give a contract worth &#163;75,000 to an external, UK-based\u000a      software developer (Optirisk Systems Limited, London) to develop a\u000a      commercial level version of the software (2009-2012). In addition, the\u000a      company made a very significant and sustained in-kind investment in\u000a      laboratory experiments to calculate the flame propagation probabilities in\u000a      typical process plant set-ups. After the handover of the software by\u000a      Optirisk Systems in 2012, UTCFS trained their entire worldwide sales force\u000a      of 31 sales operatives in its use. Training for this software tool is\u000a      mandatory for new sales staff as it is now a standard tool used in IEP\u000a      sales by UTCFS worldwide. For each candidate protection installation, the\u000a      software provides a transparent demonstration of the trade-off between the\u000a      cost paid by the process owner and the reduction in residual risk\u000a      achieved, and since 2012 it has played a key role in negotiating the sale\u000a      of explosion protection installations. The value of this work to\u000a      Kidde\/UTCFS can be judged from a letter to Brunel from their Principal\u000a      Research Scientist. A quote from his letter reads: \"We conduct this very\u000a      complex and challenging business in a competitive and a code compliant\u000a      environment. Every design ultimately causes us to address and review with\u000a      our clients the trade-offs between protection options. The envisioned\u000a      benefit from this work will be to make available at the point of sale a\u000a      systematic means to assist us and our clients, to make the best decisions\u000a      in specifying process explosion protection safety measures. Moreover the\u000a      quantification of the standing residual risk will allow the client to\u000a      elect a safety integrity level pertinent to their process application.\"\u000a    UTCFS believes that this algorithm is beneficial to the entire industrial\u000a      explosion protection industry (both for the process plant owners as well\u000a      as protection sellers). Thus the algorithm was put into the public domain\u000a      via publications [1]-[2]. In addition, the work has been disseminated\u000a      through three major international conferences on loss prevention in the\u000a      process industry which focussed on industry-relevant research. Company\u000a      users made presentations at two of these conferences, both of which were\u000a      in or after 2008 (see corroborating sources [S1] &amp; [S2]). The\u000a      algorithm and its commercial development has given the company a `first\u000a      user' advantage whilst, for the process plant owners, the algorithm\u000a      provides an improved method of risk assessment together with a way of\u000a      determining the cost of mitigation, which is now becoming widely accepted\u000a      by both academia and industrial peers.\u000a    As the safety installations sold using this algorithm become commonplace,\u000a      it is very likely that the computation of the residual risk for an IEP\u000a      system will become a standard practice in the process plant safety\u000a      industry. Further, it is expected that some form of quantification\u000a      of residual risk, similar to that achievable by Brunel-Kidde algorithm for\u000a      IEP in process plants, will eventually become mandatory in the practice of\u000a      buying and selling explosion protection installations in a variety of\u000a      other sectors (such as passenger aircrafts and offshore oil platforms).\u000a      Thus the impact of this work has a wide potential reach even beyond the\u000a      specific industry sector.\u000a    ","ImpactSummary":"\u000a    Researchers at Brunel developed a new algorithm for the computation of\u000a      residual risk in industrial explosion protection (IEP) installations in\u000a      collaboration with Kidde Plc, which later became a part of UTC Fire and\u000a      Security (UTCFS), a 57.7 billion USD company. This was the first algorithm\u000a      clearly quantifying the safety integrity level versus cost trade-off in\u000a      buying an IEP for the process plant owners. As the cost of such an\u000a      installation varies from &#163;40,000 to &#163;700,000, quantifying this trade-off\u000a      was a real unmet user need. A commercial implementation of this algorithm\u000a      by a UK-based software vendor Optirisk Systems is now being used by the 31\u000a      strong sales force of UTCFS worldwide, as their main tool for negotiating\u000a      the sales of IEP installations.\u000a    ","ImpactType":"Technological","Institution":"\u000a    BRUNEL UNIVERSITY (H0113)\u000a    ","Institutions":[{"AlternativeName":"Brunel University","InstitutionName":"Brunel University","PeerGroup":"C","Region":"London","UKPRN":10000961}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Peer reviewed journal paper\u000a    \u000a[1] P. Date, R. Lade, G. Mitra and P. Moore, Modelling the risk of\u000a        failure in explosion protection installations, Journal of Loss\u000a      Prevention in the Process Industries, 22 (4): 492-498, 2009 (2012 impact\u000a      factor: 1.150, 5 year impact factor: 1.464). http:\/\/dx.doi.org\/10.1016\/j.jlp.2009.03.007\u000a    \u000aPublications in conference proceedings (in major industry\u000a      conferences, for user dissemination)\u000a    \u000a[2] P. Date et al, 12th Symposium on Loss Prevention and Safety Promotion\u000a      in the Process Industries, Edinburgh, 2007 (three day symposium with\u000a      around 60 presentations; proceedings published by IChemE).\u000a    \u000aGrant from industry\u000a    [3] &#163;25k per annum for 3 years to Prof. Mitra and Dr Date, from Kidde Plc\u000a      (2004-2007); internal grant reference available from Management accounts.\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000a    Publications [1]-[2] mentioned in section 3 corroborate the impact; in\u000a      particular, the examples cited in publication [1] are also used for\u000a      training purposes in UTC Fire and Security.\u000a    Presentations by the users, Kidde and UTC Fire and Security:\u000a    [S1] R. Lade et al, Hazard XX: Process Safety and Environment Protection,\u000a      Manchester, 2008 (4 day symposium with 91 presentations; proc. published\u000a      by IChemE).\u000a    http:\/\/www.icheme.org\/communities\/subject_groups\/safety%20and%20loss%20prevention\/resources\/hazards%20archive\/~\/media\/Documents\/Subject%20Groups\/Safety_Loss_Prevention\/Hazards%20Archive\/XX\/XX-Paper-78.pdf\u000a    [S2] R. Lade et al, 11th Process Plant Safety Symposium, Tampa, 2009\u000a      (proc. published by AIChE). Abstract available at http:\/\/www3.aiche.org\/Proceedings\/Abstract.aspx?PaperID=145072\u000a    Additionally, the following person can be contacted:\u000a    Principal Research Scientist, Kidde UK, Thame Park Road, Thame,\u000a      Oxfordshire, OX9 3RT &#8212; a letter of appreciation detailing the value of\u000a      work to the company has been provided. \u000a    ","Title":"\u000a    Computation of residual risk in industrial explosion protection\u000a        installations\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The research was performed by Dr P. Date and Prof. G. Mitra, both from\u000a      the Department of Mathematical Sciences at Brunel University, between 2004\u000a      and 2012, with the actual impact taking place from 2009 onwards.\u000a    The interdisciplinary Centre for the Analysis of Risk and Optimisation\u000a      Modelling Applications (CARISMA) at Brunel has strong expertise in\u000a      measurement and optimisation of risk in a wide variety of applications.\u000a      British explosion protection firm Kidde Plc approached two members of\u000a      CARISMA, Prof. Mitra and Dr Date, in 2004 to explore ways of measuring\u000a      residual risk in explosion protection installations. They provided Brunel\u000a      with a grant of &#163;25,000 per annum for three years.\u000a    The research developed a novel and effective computational tool for\u000a      companies working in industrial explosion protection (IEP) installation\u000a      sector. Given a plant layout, a contracted protection installation company\u000a      recommends appropriate locations for explosion protection devices (such as\u000a      fire extinguishers, optical fire detectors and flame isolation valves) as\u000a      well as the specific choice of devices to the plant owner. Both the\u000a      locations and the choice of devices is dictated by many factors such as\u000a      the level of risk of unwarranted ignition in the process (e.g., due\u000a      to mechanical friction), the possible intensity of an explosion and the\u000a      predicted path of flame propagation. The purpose of an explosion\u000a      protection installation is to minimize the risk to property and personnel\u000a      in the event of any accident which results in a fire or an explosion. It\u000a      is always possible to install a `more expensive' system (e.g., with\u000a      higher specification extinguishers) which is `safer' (i.e. leaves a\u000a      smaller risk of an unmitigated explosion). While everybody understands\u000a      what `more expensive' means, there was no industry standard approach to\u000a      measuring what `safer' means in this context. In other words, there was no\u000a      standard way for measuring the `residual risk', i.e. the risk of\u000a      an unmitigated explosion even after installing a protection system, until\u000a      the Brunel-Kidde algorithm was put into practice for selling IEP\u000a      installations in 2012 .\u000a    Through sustained technical collaboration with researchers at Kidde from\u000a      2004-2009, academics at Brunel developed an algorithm for computation of\u000a      residual risk which was simple enough to explain to the process owners,\u000a      yet comprehensive enough to address the key issue of quantifying the\u000a      trade-off between the cost of protection installation and the safety in\u000a      terms of residual risk. The algorithm, which was published in a\u000a      peer-reviewed journal in 2009 [1], is based on a directed graph\u000a      representation of a typical process plant in which nodes represent\u000a      vessels, edges represent the possible flame paths and the weights on the\u000a      individual edges represent the flame propagation probabilities. The\u000a      residual probability of an unmitigated explosion in any one node of the\u000a      system, for a given explosion protection installation, is calculated by\u000a      enumerating and adding up probabilities along various flame propagation\u000a      paths. This algorithm reduces the residual risk to a single number between\u000a      0 and 1, with 0 representing a fully mitigated risk and 1 representing a\u000a      fully unmitigated risk. Then different IEP installations can be compared\u000a      in terms of this residual explosion probability as a proxy for safety\u000a      integrity level; the lower this number, the safer the system. As an\u000a      example, a process plant owner may or may not be willing to pay &#163;5,000 for\u000a      an extra fire extinguisher on a duct connecting two process vessels,\u000a      depending on whether it causes the residual risk to go down by a factor of\u000a      10 or only by a factor of 2. The key contribution of the algorithm is\u000a      making this trade-off between the increase in cost of the IEP installation\u000a      and the corresponding increase in safety integrity level very transparent.\u000a    Kidde Plc later became a part of UTC Fire and Security (UTCFS), which is\u000a      a global company with over 57 Billion USD net sales and over 218,000\u000a      employees as of 2013. The new parent company remained committed to the\u000a      project. The actual impact of research started in 2009, when UTCFS gave a\u000a      contract to an external software vendor (Optirisk Systems Limited, a\u000a      London-based software developer) for the development of commercial level\u000a      software implementing this algorithm. The input data needed for the\u000a      implementation of this algorithm was produced through extensive laboratory\u000a      work by a UTCFS subsidiary in Germany in consultation with the Brunel\u000a      team. The project was completed in 2012.\u000a    "},{"CaseStudyId":"14104","Continent":[{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"1814991","Name":"China"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Details of two projects undertaken by LORG members for hospitals in the\u000d\u000a      UK and China, each serving a catchment area of several hundred thousand\u000d\u000a      potential patients, but with different objectives and modes of operations,\u000d\u000a      will be given.\u000d\u000a    The first project involves the Queen Alexandra Hospital which is part of\u000d\u000a      Portsmouth Hospitals Trust and provides acute care provision for the\u000d\u000a      region of Portsmouth and South East Hampshire. The initial work involved\u000d\u000a      helping the clinicians of the Medical Assessment Unit (a buffer department\u000d\u000a      strategically placed between the Accident and Emergency Department and the\u000d\u000a      rest of the hospital) understand the nature of the patient flow and\u000d\u000a      resource capacity restrictions they faced. The project began when the\u000d\u000a      University of Portsmouth was approached by the Queen Alexandra Hospital as\u000d\u000a      the Medical Assessment Unit (MAU) was a new concept in UK patient flow\u000d\u000a      management at that time.\u000d\u000a    Their MAU had undergone a rapid expansion from 8 to 58 beds and this had\u000d\u000a      led to many questions regarding resource levels, optimal patient-flow\u000d\u000a      pathways, and lengths of stay, that were non-trivial in nature and\u000d\u000a      required the techniques developed by LORG members to answer. This was\u000d\u000a      achieved by a series of over ten face-to-face meetings during which goal\u000d\u000a      programming and simulation models were built and refined as the\u000d\u000a      preferences and goals of the practitioners were elicited. In order to gain\u000d\u000a      the depth of data required to build an accurate simulation model, a member\u000d\u000a      of LORG spent a period of three weeks collecting data and liaising with\u000d\u000a      medical staff at the MAU. The impact was due to the results of the models\u000d\u000a      built and the knowledge gained by the MAU managers, particularly Dr Paul\u000d\u000a      Schmidt who was responsible for the resource management of the MAU, which\u000d\u000a      were used to inform the policy and practice of the medical assessment unit\u000d\u000a      with respect to bed allocation policy, patient flow management, and\u000d\u000a      resource allocation. The impact of this project has taken place in the\u000d\u000a      period since the conclusion of the project in 2008 and has specifically\u000d\u000a      resulted in:\u000d\u000a    \u000d\u000a      An understanding of the optimal levels of nurses and doctors which has\u000d\u000a        assisted in the setting of appropriate resource levels in the MAU based\u000d\u000a        on the knowledge gained during the formulation and solution of the\u000d\u000a        model.\u000d\u000a      A quantification of the amount of time that nurses spend on patient\u000d\u000a        centred and non-patient centred tasks (shown by the project to be 50% on\u000d\u000a        each) which has led to better policies for nurse workload allocation.\u000d\u000a        Some non-patient centred tasks have hence been reduced and eliminated\u000d\u000a        whilst others have been mitigated against.\u000d\u000a      A successful application to the South Central Health Authority for a\u000d\u000a        &#163;160,000 grant allowing QA hospital to undertake a larger project\u000d\u000a        simulating their entire emergency pathways. The MAU project has thus\u000d\u000a        acted as a catalyst to allow the hospital management to engage with and\u000d\u000a        utilise the power of mixed modelling simulation techniques.\u000d\u000a    \u000d\u000a    The second case study involves the Zichan hospital, based in Zichan,\u000d\u000a      China, a state run but profit making non-emergency care hospital serving\u000d\u000a      the population of the Zichan region which approached the University of\u000d\u000a      Portsmouth as they had a set of beds split between their hospital\u000d\u000a      departments that were allocated in an arbitrary rather than a systematic\u000d\u000a      manner. The hospital was looking to optimise their bed allocation against\u000d\u000a      the two criteria of overall profit and probability of direct patient\u000d\u000a      admittance. The hospital manager was also concerned that the preferences\u000d\u000a      of all the department managers were fairly represented and modelled.\u000d\u000a    The preferences of the hospital and departmental managers where thus\u000d\u000a      elicited through the analytical hierarchy process (AHP) technique, and\u000d\u000a      multi-phase queuing theory was used to capture analytically the flow of\u000d\u000a      patients requiring different treatments to the hospital. A LORG member\u000d\u000a      visited the hospital in order to collect primary data to supplement the\u000d\u000a      existing data. A set of goal programming models was then developed, in\u000d\u000a      correspondence with the general manager of the hospital in order to\u000d\u000a      validate and verify the models built and their results. The models\u000d\u000a      recommended a different bed allocation between departments that improved\u000d\u000a      both the probability of direct patient admission and the overall profits\u000d\u000a      of the hospital. The impact occurred in the period from the conclusion of\u000d\u000a      the study in 2008 until the present and has consisted in enhanced\u000d\u000a      knowledge of the dynamics of the allocation of beds in the hospital. This\u000d\u000a      has allowed for more efficient policies with respect to bed allocation at\u000d\u000a      the hospital to be implemented which re-distributed the beds by allocating\u000d\u000a      more to departments shown in the study to have overly high utilisation\u000d\u000a      rates. This has led to the following improvements against the set\u000d\u000a      objectives of the study:\u000d\u000a    \u000d\u000a      An improvement of the percentage of directly admitted patients in the\u000d\u000a        period 2008-2013.\u000d\u000a      The doubling of overall profits in the period 2008-2013.\u000d\u000a      An efficient bed policy for the new expanded 450 bed (previously 280\u000d\u000a        bed) hospital to be implemented upon and since its opening in 2010. This\u000d\u000a        policy used the logic of the LORG model as its base.\u000d\u000a    \u000d\u000a    ","ImpactSummary":"\u000d\u000a    Managers of hospital units are required to allocate medical resources in\u000d\u000a      accordance with, sometimes conflicting, objectives and performance targets\u000d\u000a      and against continual variations in patient flow, staff and bed\u000d\u000a      availability. The Logistics and Operational Research Group (LORG) at the\u000d\u000a      University of Portsmouth has developed novel models, based on a\u000d\u000a      combination of discrete event simulation, multi-phase queuing theory, and\u000d\u000a      goal programming, that have improved the understanding of ward logistics\u000d\u000a      by hospital managers in the UK and China, enabling them to make changes\u000d\u000a      that have improved the efficiency of bed allocation, patient flow and\u000d\u000a      allocation of medical resources and improved outcomes for patients.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    University of Portsmouth\u000d\u000a    ","Institutions":[{"AlternativeName":"Portsmouth (University of)","InstitutionName":"University of Portsmouth","PeerGroup":"D","Region":"South East","UKPRN":10007155}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    The three references marked (*), R4, R5, and R6, best represent the\u000d\u000a        quality of the research.\u000d\u000a    \u000aR1: Jones, DF, Tamiz, M. (1995). Expanding the flexibility of goal\u000d\u000a      programming via preference modelling techniques, Omega, 23, 41-48. DOI: 10.1016\/0305-0483(94)00056-G\u000d\u000a    \u000a\u000aR2: Tamiz, M, Jones, DF and Romero, C. (1998). Goal programming for\u000d\u000a      decision making: An overview of the current state-of-the-art, European\u000d\u000a      Journal of Operational Research, 111, 569-581.\u000d\u000a      DOI: 10.1016\/S0377-2217(97)00317-2\u000d\u000a    \u000a\u000aR3: Jones, Dylan and Tamiz, M. (2010) Practical goal programming.\u000d\u000a      International Series in Operations Research and Management Science, 141\u000d\u000a      (141). Springer, New York. ISBN 9781441957702\u000d\u000a      http:\/\/www.springer.com\/business+%26+management\/operations+research\/book\/978-1-4419-5770-2\u000d\u000a    \u000a\u000aR4(*): J P Oddoye, M A Yaghoobi, M Tamiz, D F Jones and P Schmidt\u000d\u000a        (2007) A multi-objective model to determine efficient resource levels in\u000d\u000a        a medical assessment unit, Journal of the Operational\u000d\u000a          Research Society, 58, 1563-1573. DOI: 10.1057\/palgrave.jors.2602315\u000d\u000a    \u000a\u000aR5(*): Oddoye JP, Tamiz M, Jones DF, Schmidt P (2009), Combining\u000d\u000a        Simulation and Goal Programming for Healthcare Planning in a Medical\u000d\u000a        Assessment Unit, European Journal of Operational Research, 193, 250-261.\u000d\u000a      DOI: 10.1016\/j.ejor.2007.10.029\u000d\u000a      Ref2 output: 10-DJ-001\u000d\u000a    \u000a\u000aR6(*): Li X, Beullens P, Jones DF, and Tamiz M (2009) An integrated\u000d\u000a        queuing and multi-objective bed allocation model with application to a\u000d\u000a        hospital in China, Journal of the Operational Research Society, 59, 1-9.\u000d\u000a      DOI: 10.1057\/palgrave.jors.2602565\u000d\u000a      Ref2 output: 10-DJ-003\u000d\u000a    \u000aR1, R2, R4, R5, and R6 are papers in highly-ranked and well-respected\u000d\u000a      Operational Research journals: the journal Omega is ranked 3rd highest in\u000d\u000a      SJR rankings for Operations Research and Management Science and has a 2012\u000d\u000a      5-year Impact Factor of 3.474. The European Journal of Operational\u000d\u000a      Research is ranked 9th in SJR and has a 2012 5-year Impact Factor of\u000d\u000a      2.524.\u000d\u000a    Finally, the Journal of the Operational Research Society is ranked 23rd\u000d\u000a      in SJR &#8212; within the SJR top quartile &#8212; has a 2012 5-year Impact Factor of\u000d\u000a      1.282, and is the premium publication of the UK OR Society. R5 and R6 are\u000d\u000a      included in the research outputs of this submission. In addition, Jones\u000d\u000a      has been invited as key speaker on this work at conferences, including the\u000d\u000a      workshop of the Decision Analysis Special Interest Group (DASIG) of the\u000d\u000a      Operational Research Society (UK, 2011) and as a plenary speaker at the\u000d\u000a      international CMAC-Sudeste Conference on Applied and Computational\u000d\u000a      Mathematics (Brazil, 2013).\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"8","Level2":"6","Subject":"Information Systems"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    1) Factual Statement from the Medical Director, Trust Headquarters, Queen\u000d\u000a      Alexandra Hospital, Portsmouth Hospitals NHS trust. This confirms details\u000d\u000a      of the study undertaken and the benefits gained by the Queen Alexandra\u000d\u000a      Hospital.\u000d\u000a    2) Factual Statement from CEO, Zichan Hospital, Zichan, China. This\u000d\u000a      confirms details of improvements in the percentage of directly admitted\u000d\u000a      patients, doubling of hospital profits and a new bed policy with\u000d\u000a      efficiency improvements.\u000d\u000a    3) Conference Presentation Slides, IMA Quantitative Methods in Healthcare\u000d\u000a      conference, London, 2008. These give details of the healthcare projects\u000d\u000a      undertaken in both hospitals.\u000d\u000a    4) Poster presented at INFORMS conference, San Diego, USA, 2009. This\u000d\u000a      give details of the healthcare projects undertaken in both hospitals.\u000d\u000a    5) Workshop presentation slides, presented at DASIG workshop, Portsmouth,\u000d\u000a      UK. This give details of the healthcare projects undertaken in both\u000d\u000a      hospitals. \u000d\u000a    ","Title":"\u000d\u000a    The use of goal programming to optimise resource allocation in hospitals\u000d\u000a      in the UK and China.\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2639996","Name":"Portsmouth"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The underpinning research was conducted by members of the Logistics and\u000d\u000a      Operational Research Group (LORG), which has grown from the former\u000d\u000a      Logistics and Management Mathematics Group, Department of Mathematics,\u000d\u000a      University of Portsmouth, under the leadership of Professor Dylan Jones\u000d\u000a      (1997-present; Principal Lecturer at the time of the research). Key\u000d\u000a      academic co-investigators included: Dr Patrick Beullens, Principal\u000d\u000a      Lecturer, 2004-2010) and Professor M Tamiz, (Professor of Operational\u000d\u000a      Research, 1991-2008).\u000d\u000a    Goal programming is one of the most widely known and used techniques\u000d\u000a      within the field of Multi-Criteria Decision Making (MCDM) with decision\u000d\u000a      makers appreciating its relative simplicity and ease of use. Members of\u000d\u000a      the LORG in the Department of Mathematics at the University of Portsmouth\u000d\u000a      have been involved in advancing the theory and application of goal\u000d\u000a      programming since 1993, with their first publication on the topic of\u000d\u000a      modelling non-linear preference functions (R1) appearing in 1995. Goal\u000d\u000a      programming methodology was further developed by LORG members as a tool\u000d\u000a      that can effectively model a variety of utility functions of decision\u000d\u000a      makers, including the combination of balance and optimisation (R2), and\u000d\u000a      was combined with other Operational Research modelling techniques, such as\u000d\u000a      discrete-event simulation and queuing theory (R3), to provide a powerful\u000d\u000a      and flexible modelling paradigm to capture and optimise the flow of\u000d\u000a      entities through a system with multiple, conflicting criteria by which the\u000d\u000a      effectiveness of a solution could be measured.\u000d\u000a    Members of the LORG have been advancing the use of goal programming as\u000d\u000a      part of a mixed modelling methodology applied to healthcare resource\u000d\u000a      optimisation, a topic which exhibits multiple criteria and a stochastic\u000d\u000a      flow of patients in a complex and resource-constrained decision-making\u000d\u000a      environment. A set of integer goal programming models was developed and\u000d\u000a      used to predict resource levels in short-stay critical-care hospital units\u000d\u000a      considering the conflicting objectives of minimising patient delay and\u000d\u000a      achieving target levels for doctor, nurse, and bed numbers (R4). This work\u000d\u000a      was further expanded to develop a novel mixed modelling methodology in\u000d\u000a      which discrete event simulation and goal programming are combined (R5).\u000d\u000a      The discrete-event simulation model is used to capture flow through a\u000d\u000a      complex hospital unit, and the goal programming model to optimise resource\u000d\u000a      levels whilst minimising delays for the different resource types. A\u000d\u000a      different goal-programming methodology, suitable for non-critical care\u000d\u000a      hospitals, was also developed. This used a combination of multi-phase\u000d\u000a      queuing theory to model the arrivals of patients at a hospital and integer\u000d\u000a      goal programming with piecewise linearization to allow for the generation\u000d\u000a      of Pareto-optimal solutions against the conflicting criteria of\u000d\u000a      probability of patient admission and generation of sufficient revenue\u000d\u000a      (R6).\u000d\u000a    "},{"CaseStudyId":"14105","Continent":[{"GeoNamesId":"6255146","Name":"Africa"},{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"2464461","Name":"Tunisia"},{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"357994","Name":"Egypt"},{"GeoNamesId":"285570","Name":"Kuwait"},{"GeoNamesId":"2510769","Name":"Spain"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The impact relates to the use of goal programming models developed by\u000d\u000a      LORG members to aid in investment companies' strategic financial decision\u000d\u000a      making. Members of LORG were approached by an advisor to the Kuwait\u000d\u000a      Investment Authority (who are responsible for the Kuwaiti Sovereign Wealth\u000d\u000a      Fund) in 2008 as she had read the group's underpinning work on portfolio\u000d\u000a      selection and wished to see a set of models developed that pertained to\u000d\u000a      the type of multi-objective quantitative decisions she faced in the Kuwait\u000d\u000a      Sovereign Wealth Fund, as well as aiding her work as a freelance\u000d\u000a      investment advisor. The Kuwait Sovereign Wealth fund is the world's oldest\u000d\u000a      and most well established wealth fund, although the actual amount of its\u000d\u000a      investments is not made public.\u000d\u000a    A set of goal programmes for multi-objective portfolio optimisation was\u000d\u000a      thus developed by LORG members, applying the techniques developed in the\u000d\u000a      underpinning research (R1-R5) in accordance with the specifications of the\u000d\u000a      advisor (Dr Azmi). These models were capable of producing balanced finance\u000d\u000a      portfolios for deciding the level of investment in mutual funds that\u000d\u000a      included a range of goals specified by the decision including desired\u000d\u000a      levels of risk, return, and maturity of the funds being invested in. They\u000d\u000a      also took into account the GDP, inflation rate, and regional priorities of\u000d\u000a      the country when deciding on the investments to be made. The models were\u000d\u000a      trialled against a set of Egyptian mutual funds as a demonstration of\u000d\u000a      their potential. The advisor worked with LORG in the period 2008-2010 in\u000d\u000a      order to assist in ensuring that the models built upon the research of\u000d\u000a      LORG staff accurately reflected the objectives, priorities, goals, and\u000d\u000a      constraints that a real-world portfolio investment company is faced with.\u000d\u000a    The impact has occurred on two levels. The first level is through direct\u000d\u000a      collaboration between University of Portsmouth staff and professionals\u000d\u000a      with responsibility for portfolio management in financial companies to\u000d\u000a      apply the underpinning research to their specific problem domains. Prof\u000d\u000a      Tamiz has had direct contact with portfolio managers in investment\u000d\u000a      companies and institutions, having previously worked for the Nomura\u000d\u000a      investment bank in London in the 1990's. Dr Azmi has been working as an\u000d\u000a      economic advisor to the sovereign wealth fund of Kuwait (source 1) in the\u000d\u000a      period 2008-2013. She also has a range of business contacts due to her\u000d\u000a      supplemental position as a freelance trainer and consultant. In these\u000d\u000a      capacities she has advised and had various formal and informal meetings\u000d\u000a      with fund managers who acknowledged the use of a quantitative model in\u000d\u000a      their investment decision making, in a similar vein to the goal\u000d\u000a      programming models developed by LORG members in R1-R5 (Due to the nature\u000d\u000a      of the investment banking industry, fund managers are not prepared to\u000d\u000a      publicly state the nature of quantitative models they use or the results\u000d\u000a      that these models yield). LORG members have also been active in presenting\u000d\u000a      seminars and promoting the results of the models in a range of forums\u000d\u000a      frequented by investment fund managers in order to enhance the use of the\u000d\u000a      goal programming models developed by LORG members in the financial\u000d\u000a      investment sector (Source 1). In addition, our results have guided\u000d\u000a      financial decision making and investment advice given to the Kuwait\u000d\u000a      Sovereign Wealth Fund and other investment funds by the knowledge of the\u000d\u000a      quantitative skills and dynamics of the working of the goal programming\u000d\u000a      models gained during the liaison with LORG members (Source 1). Dr Azmi has\u000d\u000a      also been instrumental in promoting the concepts of achieving fairness by\u000d\u000a      the use of goal programming, as detailed in references R1 and R2, to the\u000d\u000a      issue of developmental planning at the United Nations, proposing goal\u000d\u000a      programming models be used to achieve better levels of gender equality\u000d\u000a      worldwide (Source 1).\u000d\u000a    The second source of impact is through dissemination of the results of\u000d\u000a      applying the research to specific domains in scientific conferences,\u000d\u000a      academic journals, and industry related publications. The work has been\u000d\u000a      placed in sources S2-S3; the specialised Arab Journal of Academic Sciences\u000d\u000a      (Source 2); and the Banking and Financial Systems eJournal (source 3) in\u000d\u000a      order to achieve good dissemination amongst the academic and practitioner\u000d\u000a      communities in the field. The results of the research have been presented\u000d\u000a      at the Multi-Attribute Portfolio Selection (Montreal, 2007 - containing\u000d\u000a      around 80% investment bankers and 20% academics) and MOPGP08 (Portsmouth,\u000d\u000a      2008), and MOPGP10 (Tunisia, 2010) conferences.\u000d\u000a    The research has been cited in case studies relating to the Iranian stock\u000d\u000a      market (source S4) and the Chinese stock market (source S5), the Kuwait\u000d\u000a      stock exchange (source S6) and in Spain relating to socially responsible\u000d\u000a      investment (source S7).\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This statement details the impact of research undertaken by members of\u000d\u000a      the Logistics and Operational Research Group (LORG) at the University of\u000d\u000a      Portsmouth in the area of strategic financial investment portfolio\u000d\u000a      selection. A set of goal programming models was developed, which for the\u000d\u000a      first time allowed the investment fund managers to consider a wider range\u000d\u000a      of objectives beyond the usual risk and return paradigm. As a result, the\u000d\u000a      decision making capabilities of key investment fund managers and advisors\u000d\u000a      including those working for the Kuwait Sovereign Wealth Fund were\u000d\u000a      enhanced, resulting in improved decision making capabilities.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    University of Portsmouth\u000d\u000a    ","Institutions":[{"AlternativeName":"Portsmouth (University of)","InstitutionName":"University of Portsmouth","PeerGroup":"D","Region":"South East","UKPRN":10007155}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"6077243","Name":"Montréal"}],"References":"\u000d\u000a    The three references marked (*), R1, R5, and R6, best represent the\u000d\u000a        quality of the research.\u000d\u000a    \u000aR1(*): Romero, C, Tamiz, M and Jones, DF. (1998) Goal programming,\u000d\u000a        compromise programming, and reference point method formulations:\u000d\u000a        linkages and utility interpretations, Journal of the Operational\u000d\u000a        Research Society,49, 986-991.s DOI:10.1057\/palgrave.jors.2600611\u000d\u000a    \u000a\u000aR2: Jones, Dylan and Tamiz, M. (2010) Practical goal programming.\u000d\u000a      International Series in Operations Research and Management Science, 141\u000d\u000a      (141). Springer, New York.\u000d\u000a      ISBN 9781441957702\u000d\u000a      http:\/\/www.springer.com\/business+%26+management\/operations+research\/book\/978-1-4419-5770-\u000a        2\u000d\u000a    \u000a\u000aR3: Tamiz, M, Hasham, R, Jones, DF, Hesni, B, Fargher, EK. (1996) A two\u000d\u000a      staged model for portfolio selection, Lecture notes in Economics and\u000d\u000a      Mathematical Systems, M.Tamiz(ed), Springer, 432, 286-299. DOI: 10.1007\/978-3-642-87561-8_19\u000d\u000a    \u000a\u000aR4: Tamiz, M., Treloar, R., (2006), \"Issues Concerning Block Trading and\u000d\u000a      Transaction Costs\", Journal of Information and Optimisation Sciences, 27,\u000d\u000a      127-144.\u000d\u000a      http:\/\/www.tarupublications.com\/journals\/jios\/full-text\/JIOS-27-1-2006\/jios130.pdf\u000d\u000a    \u000a\u000aR5(*): Perez Gladish B, Jones DF, Tamiz M, and Bilbao Terol A (2007)\u000d\u000a        An interactive three stage model for mutual fund portfolio selection,\u000d\u000a        Omega, 35, 75-88.\u000d\u000a        DOI: 10.1016\/j.omega.2005.04.003\u000d\u000a    \u000a\u000aR6(*) Tamiz M, Azmi R, Jones DF (2013) On Selecting Portfolio of\u000d\u000a        International Mutual Funds using Goal Programming with Extended Factors,\u000d\u000a        European Journal of Operational Research, 226, 560-576. DOI: 10.1016\/j.ejor.2012.11.004\u000d\u000a    \u000aR1, R4, R5, and R6 are papers in highly-ranked and well-respected\u000d\u000a      Operational Research journals: the journal Omega is ranked 3rd highest in\u000d\u000a      SJR rankings for Operations Research and Management Science and has a 2012\u000d\u000a      5-year Impact Factor of 3.474. The European Journal of Operational\u000d\u000a      Research is ranked 9th in SJR and has has a 2012 5-year Impact Factor of\u000d\u000a      2.524.\u000d\u000a    Finally, the Journal of the Operational Research Society is ranked 23rd\u000d\u000a      in SJR - within the SJR top quartile - has a 2012 5-year Impact Factor of\u000d\u000a      1.282, and is the premium publication of the UK OR Society.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"15","Level2":"2","Subject":"Banking, Finance and Investment"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    1) Factual Statement from Advisor, Strategy &amp; Planning Department,\u000d\u000a      Kuwait Investment Authority.\u000d\u000a    2) Article in Sovereign Wealth Quarterly. January 2011, Pages 20-23.\u000d\u000a    3) Azmi RA: Accepting the challenges of the ongoing financial crisis and\u000d\u000a      global market integration:\u000d\u000a      highlighting the role of cooperative finance for sustainability, Banking\u000d\u000a      and Financial Institutions eJournal, http:\/\/ssrn.com\/abstract=1583273\u000d\u000a    4) Amiri, Maghsoud; Ekhtiari, Mostafa; Yazdani, Mehdi. (2011) Nadir\u000d\u000a      compromise programming: A model for optimization of multi-objective\u000d\u000a      portfolio problem, Expert Systems With Applications, 38, 7222-7226. DOI: http:\/\/dx.doi.org\/10.1016\/j.eswa.2010.12.061\u000d\u000a    5) Li, Jun; Xu, Jiuping (2009) A novel portfolio selection model in a\u000d\u000a      hybrid uncertain environment, Omega-International Journal Of Management\u000d\u000a      Science, 37, 439-449.\u000d\u000a      DOI: http:\/\/dx.doi.org\/10.1016\/j.omega.2007.06.002\u000d\u000a    6) Al-Qaheri H and Hasan MK (2010) An End-User Decision Support System\u000d\u000a      for Portfolio Selection: A Goal Programming Approach with an Application\u000d\u000a      to Kuwait Stock Exchange (KSE), International Journal of Computer\u000d\u000a      Information Systems and Industrial Management Applications, 2.\u000d\u000a    7) Ballestero E, Bravo M, Perez-Gladish B, Arenas-Parra M, Pla-Santamaria\u000d\u000a      D (2012) Socially Responsible Investment: A multi-criteria approach to\u000d\u000a      portfolio selection combining ethical and financial objectives, European\u000d\u000a      Journal of Operational Research, 216, 487-494.\u000d\u000a      DOI: http:\/\/dx.doi.org\/10.1016\/j.ejor.2011.07.011\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Use of Goal Programming Models to Assist Strategic Financial Investment\u000d\u000a      Decision Making.\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2639996","Name":"Portsmouth"},{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The initial underpinning research was conducted in the period 1991-2008\u000d\u000a      by academic staff in the Logistics and Operational Research Group (LORG),\u000d\u000a      which has grown from the former Logistics and Management Mathematics\u000d\u000a      Group, Department of Mathematics, University of Portsmouth, under the\u000d\u000a      leadership of Professor Mehrdad Tamiz (Professor of Operational Research,\u000d\u000a      1991-2008). The key academic co-investigator was Professor Dylan Jones\u000d\u000a      (1997-present; Principal Lecturer during the research).\u000d\u000a    LORG members have been internationally leading in developing the theory\u000d\u000a      and applications of goal programming since 1995 until the present day.\u000d\u000a      Amongst the key developments have been joint work with Prof. Carlos Romero\u000d\u000a      (Polytechnic University of Madrid, Spain) on the connections between and\u000d\u000a      use of different goal programming variants which are used to model\u000d\u000a      different underlying philosophies such as optimisation, satisficing,\u000d\u000a      balance, fairness, and ordering. Prof. Romero provided the economic\u000d\u000a      expertise and Profs Tamiz and Jones the mathematical and computational\u000d\u000a      expertise that enabled this work. This research provided utility\u000d\u000a      interpretations of the major goal programming variants and demonstrated\u000d\u000a      their connectivity (R1). These goal programming variants were subsequently\u000d\u000a      expanded and combined with other Operational Research and Artificial\u000d\u000a      Intelligence techniques for symbiotic advantage (R2). Particularly novel\u000d\u000a      was the bridging of the gap between discrete and continuous multi-criteria\u000d\u000a      models with the mixed modelling concept of using a discrete method\u000d\u000a      (ELECTRE) to rank alternatives generated by a continuous method (goal\u000d\u000a      programming) (R5).\u000d\u000a    One important application area that LORG members have specialised in is\u000d\u000a      that of portfolio selection and dynamic re-optimisation. The initial work\u000d\u000a      in the application domain concentrated on the use of a dual phase goal\u000d\u000a      programming model that was novel as it used goal programming in a combined\u000d\u000a      descriptive manner (phase 1) to analyse past data and in a prescriptive\u000d\u000a      manner as a portfolio selection tool (phase 2). The model minimises the\u000d\u000a      risk against movements in a range of economic indices such as interest\u000d\u000a      rates, exchange rates, and the oil price under a number of scenarios and\u000d\u000a      was applied to a selection of a portfolio of shares from the British FTSE\u000d\u000a      100 index (R3). Between 1997 and 2000 research was undertaken into\u000d\u000a      incorporating transaction costs into a dynamic portfolio selection model\u000d\u000a      (R4). More recent research has concentrated on the selection of portfolios\u000d\u000a      comprised of mutual funds. A three-phase mixed modelling methodology used\u000d\u000a      statistical analysis, goal programming, and the ELECTRE method in order to\u000d\u000a      select a portfolio of Spanish mutual funds (R5). The underpinning research\u000d\u000a      continued, as demonstrated by (R6) in which Egyptian mutual funds were\u000d\u000a      also modelled using different goal programming variants.\u000d\u000a    This work also considers the use of goal programming to compose a minimal\u000d\u000a      portfolio of shares that is able to accurately track a market index such\u000d\u000a      as the British FTSE 100, and examines the three major variants of goal\u000d\u000a      programming: weighted, lexicographic, and Chebyshev for portfolio\u000d\u000a      selection, and provides guidance as to under which circumstances each\u000d\u000a      should be used. Thus, an effective framework for incorporating financial\u000d\u000a      decision maker and investors' goals, objectives, and priorities has been\u000d\u000a      built by LORG members.\u000d\u000a    "},{"CaseStudyId":"15376","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2658434","Name":"Switzerland"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Overall, the research team's work has promoted awareness of the benefits\u000d\u000a      of applying novel, `high-end'\u000d\u000a      techniques in computational modelling to help unravel the complexities and\u000d\u000a      causalities in the\u000d\u000a      cited areas of biomedicine.\u000d\u000a    The group's research described here has had a far-reaching effect, not\u000d\u000a      just on the academic\u000d\u000a      community, and not just in the field of mathematics, changing the way\u000d\u000a      different specialisms can\u000d\u000a      work together. Recognising that research papers do not necessarily attract\u000d\u000a      the attention of medical\u000d\u000a      practitioners, and the actions of medical practitioners are circumscribed\u000d\u000a      by protocols overseen by\u000d\u000a      regulators, our contacts with colleagues, described in more detail below\u000d\u000a      and cited in Section 5,\u000d\u000a      have been used to facilitate access to and interaction with clinicians. In\u000d\u000a      affecting the perceptions\u000d\u000a      and approaches of our collaborators we have had an impact on\u000d\u000a      practitioners, not least by\u000d\u000a      identifying potentially relevant programmes of testing.\u000d\u000a    Prof Dr Burkhard Ludewig (one of our co-authors and collaborators) is\u000d\u000a      Head of the Institute of\u000d\u000a      Immunology, which is part of the medical research centre and is an\u000d\u000a      independent research unit of\u000d\u000a      the Kantonal Hospital St. Gallen, Switzerland. Research activities are\u000d\u000a      focussed on\u000d\u000a      immunopathological processes in the cardiovascular system, the development\u000d\u000a      of new\u000d\u000a      immunotherapeutical approaches against cancer, and the biology of\u000d\u000a      coronaviruses. The Institute\u000d\u000a      also supports basic research projects of physicians from the clinical\u000d\u000a      departments.\u000d\u000a    Prof Dr Andreas Meyerhans (another collaborator and co-author with\u000d\u000a      Bocharov) is group leader of\u000d\u000a      the Infection Biology Group at UPF, Barcelona with research interests in\u000d\u000a      virus evolution;\u000d\u000a      lymphocyte responses in persistent human infections (HIV, HCV, CMV and\u000d\u000a      Mycobacterium\u000d\u000a      tuberculosis). His group's work is in two main areas:\u000d\u000a    \u000d\u000a      describing fundamental features in virus evolution i.e. the\u000d\u000a        characteristics of HIV quasispecies,\u000d\u000a        HIV and HBV hypermutation and HIV multi-infection of single cells in\u000d\u000a        vivo.\u000d\u000a      quantifying human T cell responses in viral (HIV, CMV, poliovirus),\u000d\u000a        bacterial (Mycobacterium\u000d\u000a        tuberculosis) and yeast (Candida albicans, etc.) infections directly\u000d\u000a        from the blood of patients. Part\u000d\u000a        of the work is funded by 2 international grants from the Bill and\u000d\u000a        Melinda Gates foundation and the\u000d\u000a        EU euco.net programme.\u000d\u000a    \u000d\u000a    Burkhard Ludewig (see Section 5) wrote of the ongoing joint work:\u000d\u000a    \"Changed perceptions arising from the work of Bocharov and his\u000d\u000a        co-workers [...] relate to issues of\u000d\u000a        cause-and-effect in the complex pathological states encountered in real\u000d\u000a        life illness. Ultimately,\u000d\u000a        though (because of the natural conservatism in applied medicine) not in\u000d\u000a        the short term this should\u000d\u000a        lead to increased understanding of the effects of clinical treatment\u000d\u000a        procedures and the discovery of\u000d\u000a        new or changing forms of medication in specific therapeutic\u000d\u000a        interventions.\u000d\u000a    The recent FDA approval of the prostate cancer drug PROVENGE\u000d\u000a      [approved in 2010] shows that\u000d\u000a        such developments are possible. A recent paper \"A model of dendritic\u000d\u000a        cell therapy for melanoma\",\u000d\u000a        by DePillis, Gallegos, and Radunskaya (2013) in Frontiers in Oncology\u000d\u000a        was based heavily on the\u000d\u000a        model that was introduced in the paper Ludewig et al. (2004) cited above\u000d\u000a      [in Section 3]; this will\u000d\u000a        further extend the impact of our work.\"\u000d\u000a    Likewise, Andreas Meyerhans (see Section 5) expresses the view that that\u000d\u000a      the Chester group's\u000d\u000a      work has contributed to changed perceptions.\u000d\u000a    The HIV-1 research (Bocharov et al., 2005) has had an impact on\u000d\u000a      biomedical researchers working\u000d\u000a      on the design of therapeutic interventions in HIV infections. Firstly, it\u000d\u000a      has directed attention towards\u000d\u000a      a closer examination of the role of multi-infection\/recombination in the\u000d\u000a      viral evolution in infected\u000d\u000a      individuals. Secondly, it has identified the scenario in which the\u000d\u000a      recombination may accelerate the\u000d\u000a      emergence of multi-drug resistance by 10-fold, and thirdly, it has led to\u000d\u000a      the establishment (since\u000d\u000a      2008) of a research initiative for further integrative analysis by a\u000d\u000a      multi-disciplinary consortium\u000d\u000a      based upon two teams in Barcelona. Both involve mathematicians and one\u000d\u000a      includes specialists in\u000d\u000a      clinical aspects of HIV infection.\u000d\u000a    Beyond direct interaction with clinicians and immunologists, further\u000d\u000a      tools for securing an impact of\u000d\u000a      the research on the community include: (a) a series of well-advertised\u000d\u000a      public lectures, which\u000d\u000a      induced interest in this cross-disciplinary activity at the University of\u000d\u000a      Chester, was delivered to an\u000d\u000a      audience including medical practitioners, (b) related research\u000d\u000a      publications (Section 3 details six\u000d\u000a      papers on this theme).\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research involving mathematical modelling is helping to unravel the\u000d\u000a      complexities of key areas of\u000d\u000a      biomedicine. Our study of the mammalian immune system focuses on two\u000d\u000a      areas: (1) genetic\u000d\u000a      evolution of HIV within the host during infection, and (2)\u000d\u000a      dendritic-cell-based immunotherapy. The\u000d\u000a      research has influenced understanding by biomedical practitioners of\u000d\u000a      control parameters, the\u000d\u000a      immune response and viral resistance to drugs. The involvement of\u000d\u000a      mathematicians has led to a\u000d\u000a      paradigm shift which has provided clear directions for investigation, and\u000d\u000a      insights into immunisation\u000d\u000a      programmes (an area of research which is still an emerging field).\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Chester\u000d\u000a    ","Institutions":[{"AlternativeName":"Chester (University of)","InstitutionName":"University of Chester","PeerGroup":"E","Region":"North West","UKPRN":10007848}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"3128760","Name":"Barcelona"}],"References":"\u000d\u000a    3 selected references are marked by *.\u000d\u000a    The following selection of outputs demonstrates the dissemination of the\u000d\u000a      outcomes of the research\u000d\u000a      to a receptive readership. Each jointly-authored research paper includes\u000d\u000a      at least two authors who\u000d\u000a      had Chester affiliations.\u000d\u000a    \u000a2007: SM Andrew, CTH Baker, GA Bocharov. Rival approaches to mathematical\u000d\u000a      modelling in\u000d\u000a      immunology. Journal of Computational and Applied Mathematics 205: 669-686.\u000d\u000a    \u000a\u000a*2007: Luzyanina T, Mrusek S, Edwards JT, Roose D, Ehl S, Bocharov G.\u000d\u000a      Computational analysis\u000d\u000a      of CFSE proliferation assay. Journal of Mathematical Biology. 54(1):\u000d\u000a      57-89.\u000d\u000a    \u000a\u000a*2005: Bocharov G, Ford NJ, Ludewig B. A mathematical approach for\u000d\u000a      optimizing dendritic cell-based\u000d\u000a      immunotherapy. Methods in Molecular Medicine. 109: 19-34.\u000d\u000a    \u000a\u000a2005: Bocharov G, Ford NJ, Edwards J, Breinig T, Wain-Hobson S, Meyerhans\u000d\u000a      A. A genetic-algorithm\u000d\u000a      approach to simulating human immunodeficiency virus evolution reveals the\u000d\u000a      strong\u000d\u000a      impact of multiply infected cells and recombination. Journal of General\u000d\u000a      Virology. 86 (Pt 11): 3109-18.\u000d\u000a    \u000a\u000a*2005: CTH Baker, GA Bocharov, JM Ford, PM Lumb, SJ Norton, CAH Paul, T\u000d\u000a      Junt, P Krebs, B\u000d\u000a      Ludewig. Computational approaches to parameter estimation and model\u000d\u000a      selection in immunology.\u000d\u000a      Journal of Computational and Applied Mathematics 84(1): 50-76.\u000d\u000a    \u000a\u000a2004: Ludewig B, Krebs P, Junt T, Metters H, Ford NJ, Anderson RM,\u000d\u000a      Bocharov G. Determining\u000d\u000a      control parameters for dendritic cell-cytotoxic T lymphocyte interaction.\u000d\u000a      European Journal of\u000d\u000a      Immunology. 34(9): 2407-18.\u000d\u000a    \u000aKey grants: Leverhulme Trust Visiting Professorship (Bocharov),\u000d\u000a      2002-2004, &#163;50,885.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"7","Subject":"Immunology"},{"Level1":"11","Level2":"8","Subject":"Medical Microbiology"}],"Sources":"\u000d\u000a    The University of Chester holds on file letters from (i) Andreas\u000d\u000a      Meyerhans (Department of\u000d\u000a      Experimental and Health Sciences, Universitat Pompeu Fabra), and from (ii)\u000d\u000a      Burkhard Ludewig (of\u000d\u000a      the Institute of Immuno-biology, Kantonsspital St. Gallen) that provide\u000d\u000a      corroboration of the claims\u000d\u000a      attributed to them in Section 4 of the case study.\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Mathematical modelling leads to advances in immunology\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2653228","Name":"Chester"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The Leverhulme Trust invested, over a period of two years in 2002-4, in\u000d\u000a      an initiative having the\u000d\u000a      specific objective of establishing research activity, in the areas\u000d\u000a      indicated, in the Chester group\u000d\u000a      through the employment of Bocharov as a Leverhulme international\u000d\u000a      professor. The team had two\u000d\u000a      targeted areas of research:\u000d\u000a    Area 1 - HIV-1. While HIV-1 infection poses a great challenge to\u000d\u000a      the development of strategies for\u000d\u000a      an effective cure, there has been major progress in controlling HIV\u000d\u000a      infection with highly active\u000d\u000a      antiretroviral therapy (HAART). The Type 1 virus is characterized by an\u000d\u000a      extreme variability\u000d\u000a      resulting from two major processes acting in parallel:\u000d\u000a    \u000d\u000a      changes in the viral genome (point mutations), and\u000d\u000a      the multi-infection of target cells in conjunction with the\u000d\u000a        recombination of viral genomes.\u000d\u000a    \u000d\u000a    These processes enable the virus to escape the immune response and\u000d\u000a      acquire resistance to drugs\u000d\u000a      by tuning its genomic sequence. The complexity of the virus dynamics in\u000d\u000a      vivo compounds the\u000d\u000a      difficulty of understanding intra-patient HIV evolution.\u000d\u000a    Many groups have analysed viral kinetics following highly active\u000d\u000a      antiretroviral therapy and have\u000d\u000a      made inferences about HIV dynamics. The major focus in previous studies of\u000d\u000a      drug-resistance was\u000d\u000a      on the effect of point mutations. By contrast, we are one of only a few to\u000d\u000a      have attempted to\u000d\u000a      simulate HIV sequence evolution and to examine recombination effects. In\u000d\u000a      seeking to appreciate\u000d\u000a      the impact on viral evolution of a high frequency of multi-infected cells,\u000d\u000a      with attendant\u000d\u000a      recombination, one of the contributions of the Team has been to develop an\u000d\u000a      in silico stochastic\u000d\u000a      model (Bocharov et al., J. of General Virology, 2005) to explore the\u000d\u000a      effects of major microscopic\u000d\u000a      parameters (e.g., the point-mutation and recombination rates, and the\u000d\u000a      proviral copy number per\u000d\u000a      cell), on the dynamics of macroscopic characteristics. This model showed\u000d\u000a      that the time to build up\u000d\u000a      n-point mutants is enhanced by multi-infection. Previous studies were of\u000d\u000a      infection with a single\u000d\u000a      provirus per cell rather than of multi-infection.\u000d\u000a    In `neutral' (random) evolution, where no selection occurs, mutants can\u000d\u000a      be temporarily fixed over\u000d\u000a      numerous rounds of replication before becoming extinct. This suggests that\u000d\u000a      the majority of\u000d\u000a      mutations observed in cross-sectional analyses do not arise from strong\u000d\u000a      selection in reaction to\u000d\u000a      drug therapy. Whenever a strong selection pressure is applied via the\u000d\u000a      administration of drugs to a\u000d\u000a      few sites, as under antiviral treatment, there is a rapid emergence of\u000d\u000a      variants encoding the\u000d\u000a      selected traits, showing partial or full resistance. Thus, under the\u000d\u000a      conditions of an initial\u000d\u000a      homogeneous infection, the selection of n-point mutants, which are\u000d\u000a      fundamental to the\u000d\u000a      development of multi-drug resistance, is generally accelerated by\u000d\u000a      multi-infection and\u000d\u000a      recombination, even though there is great variation in the kinetics of\u000d\u000a      fixation.\u000d\u000a    Area 2 - cancer vaccines. Growing knowledge of the molecular\u000d\u000a      identity of tumor-specific antigens\u000d\u000a      has opened new avenues for effective cancer vaccines.\u000d\u000a    Dendritic-cell- (DC)-based immunotherapeutic approaches appear\u000d\u000a      particularly promising, as\u000d\u000a      indicated by a series of preclinical experimental studies in mice which\u000d\u000a      demonstrated that anti-tumor\u000d\u000a      immunity can be induced using DC. Significantly, the FDA has recently\u000d\u000a      approved the first DC-based\u000d\u000a      cancer vaccine for prostate cancer.\u000d\u000a    The research reported by Ludewig et al. (2004) has developed a\u000d\u000a      mathematical model to determine\u000d\u000a      the major parameters controlling DC-Cytotoxic T-lymphocyte (CTL)\u000d\u000a      interaction. It found that T-cell\u000d\u000a      receptor avidity greatly affects the pattern of CTL dynamics in response\u000d\u000a      to single or multiple\u000d\u000a      immunisations with DC. For induction of high avidity CTL, the number of\u000d\u000a      adoptively transferred DC\u000d\u000a      was of minor importance once a minimal threshold of cells-per-spleen had\u000d\u000a      been reached.\u000d\u000a    The Unit's study indicated that as long as significant numbers of\u000d\u000a      activated CTL persist and ensure\u000d\u000a      rapid elimination of antigen-expressing DC, any further application of DC\u000d\u000a      has only a limited\u000d\u000a      `enhancement' effect. Nevertheless, such repeated DC application is\u000d\u000a      apparently necessary to\u000d\u000a      maintain high levels of activated CTL. These findings impinge in\u000d\u000a      particular on the use of DC in anti-tumor\u000d\u000a      therapy, where the availability of high-avidity CTL against the chosen\u000d\u000a      immune-therapeutical\u000d\u000a      target antigen should be carefully examined. The research suggests that\u000d\u000a      the translation of\u000d\u000a      successful preclinical studies is likely to be hampered by complexities\u000d\u000a      associated with the clinical\u000d\u000a      situation. The translation of successful preclinical studies is likely to\u000d\u000a      be delayed by complexities\u000d\u000a      associated with the clinical situations encountered by medical\u000d\u000a      practitioners.\u000d\u000a    In addition to Bocharov, members of the Chester team contributing to the\u000d\u000a      ongoing work (which has\u000d\u000a      continued from 2002 to the present) on mathematical immunology include\u000d\u000a      (relevant contract dates\u000d\u000a      in brackets) S Andrew (01\/09\/98 to present); CTH Baker (01\/08\/06 to\u000d\u000a      present); JT Edwards (1993\u000d\u000a      to 31\/08\/07); NJ Ford (1993 to present); P Lumb (01\/09\/99 to present); SJ\u000d\u000a      Norton (01\/09\/98 to\u000d\u000a      31\/08\/08).\u000d\u000a    "},{"CaseStudyId":"15514","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000a    Marie-Curie Individual Fellowships (IxF) are designed to add to research\u000a      excellence in both the public and private sectors in member states and\u000a      associated countries, through sharing and application of new knowledge\u000a      which is transferred and developed by highly qualified researchers. Career\u000a      Integration Grants (CIG) facilitate the transfer of knowledge that\u000a      researchers have already acquired and the development of lasting\u000a      co-operation with the scientific and\/or industrial environment between the\u000a      country from which they have moved and their new location. This action has\u000a      a particular emphasis on countering the European 'brain drain' to other\u000a      third countries (based on the FP7 2013 People Work Programme).\u000a    Funding allocations that follow from the relevant Marie-Curie actions\u000a      amount (in 2013, for example) to &#8364;134,000,000 (IEF), &#8364;44,500,000 (IOF),\u000a      &#8364;44,500,000 (IIF), and &#8364;44,000,000 (CIG). The programmes were the subject\u000a      of independent evaluation by the European Commission in 2010, and that\u000a      evaluation focused on the impact of the activities and of the funding\u000a      deployed following the advice provided by the expert evaluations of the\u000a      applications.\u000a    The impact of the funding decisions made is best summarised in the\u000a      following extracts from the European Commission's own publications. This\u000a      extract is taken from the Interim Evaluation of the Framework 7 Programme\u000a      (European Commission, November 2010, p30):\u000a    \"Training and mobility of professionals in science and technology is\u000a        crucial for the development of the European Research Area, and the Marie\u000a        Curie actions, now under the specific programme People, have been\u000a        important instruments to make Europe attractive to the best researchers\u000a        and to implement the Community's career development policy. People has\u000a        been implemented through a coherent set of actions that aim at\u000a        increasing the quality of human resources for research in Europe,\u000a        enhancing industry-academia cooperation, supporting research careers for\u000a        the young, for female researchers and for households with young\u000a        families, and spreading good practices in the recruitment and employment\u000a        of researchers. It is noteworthy that the Marie Curie actions, through a\u000a        bottom-up approach with no pre-defined themes, have promoted excellence\u000a        and contributed to internationalisation efforts in Europe. In strategic\u000a        terms, the Marie Curie actions are the most international initiatives in\u000a        FP7.\"\u000a    (and at p61):\u000a      \"... the selection criteria for funding are demanding and include a\u000a        focus on impacts, and ... FP7 attracts the best and most appropriate\u000a        researchers and research organisations. It is also instructive to note\u000a        that the stakeholder consultation highlights the diversity of ways in\u000a        which impact occurs and should, thus, be appraised. It arises, inter\u000a        alia, from networking and collaboration, through leverage effects and as\u000a        a result of raising the bar for research generally. Findings from a\u000a        study done for the UK government show that it is important in assessing\u000a        scientific outcomes to look beyond the direct scientific outputs of\u000a        projects. The study finds that `the FP has had a big impact on the\u000a        nature and extent of UK researchers' international relationships and\u000a        networks, as well as on their knowledge base and scientific\u000a        capabilities', and it is reasonable to infer that similar outcomes will\u000a        have occurred elsewhere. ... The interface between research outputs and\u000a        innovation is crucial. According to the Technopolis report for the UK\u000a        government, `the FP has yielded important commercial benefits. UK\u000a        business participants had made or gained access to new or significantly\u000a        improved tools or methodologies and other forms of intellectual\u000a        property. Participation had contributed to the development of new\u000a        products and processes and increased income and market share.'\"\u000a    It is important to note that the impact of the work extends well beyond\u000a      higher education: for example 43% of UK participation in FP7 activities\u000a      has been from outside HEIs. The case studies published by the European\u000a      Commission (see sources below) include evidence of the significance and\u000a      scope of the impacts from the Marie Curie actions.\u000a    ","ImpactSummary":"\u000a    This impact case study concerns the impact of FORD as an expert providing\u000a      advice to the European Commission and the Research Executive Agency (REA)\u000a      under the Framework 7 People programme contributing both as an\u000a      expert and vice-chair in making funding decisions for Marie- Curie\u000a      Fellowships (IxF (2007-2013) and CIG(2012-13)).\u000a    Marie-Curie Individual Fellowships (IxF) add to research excellence in\u000a      both the public and private sectors in Member States and associated\u000a      countries, due to the sharing and application of new knowledge transferred\u000a      and developed by highly qualified researchers embedded in the European\u000a      research effort while Career Integration Grants (CIG), which have a\u000a      particular emphasis on countering the European brain drain, allow the\u000a      transfer of knowledge that the researchers have already acquired as well\u000a      as to the development of lasting co-operation with the scientific and\/or\u000a      industrial environment of the country from which they have moved.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of Chester\u000a    ","Institutions":[{"AlternativeName":"Chester (University of)","InstitutionName":"University of Chester","PeerGroup":"E","Region":"North West","UKPRN":10007848}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Three key references to indicate the required standard are indicated by *\u000a    \u000a1 Ford, Neville J., Lumb, Patricia M., Ekaka-a, Enu, Mathematical\u000a      modelling of plant species interactions in a harsh climate, JOURNAL OF\u000a      COMPUTATIONAL AND APPLIED MATHEMATICS, 234, 2732-2744.\u000a      10.1016\/j.cam.2010.01.025, 2010\u000a    \u000a\u000a2* Diethelm, Kai, Ford, Neville J. VOLTERRA INTEGRAL EQUATIONS AND\u000a      FRACTIONAL CALCULUS: DO NEIGHBORING SOLUTIONS INTERSECT? JOURNAL OF\u000a      INTEGRAL EQUATIONS AND APPLICATIONS, 24, 25-37. 10.1216\/JIE-2012-24-1-25,\u000a      2012\u000a    \u000a\u000a3 Ludewig, B, Krebs, P, Junt, T, Metters, H, Ford, NJ, Anderson, RM,\u000a      Bocharov, G, Determining control parameters for dendritic cell-cytotoxic T\u000a      lymphocyte interaction, EUROPEAN JOURNAL OF IMMUNOLOGY, 34, 2407-2418.\u000a      10.1002\/eji.200425085, 2004\u000a    \u000a\u000a4* Diethelm, K, Ford, NJ, Freed, AD, Luchko, Y, Algorithms for the\u000a      fractional calculus: A selection of numerical methods, COMPUTER METHODS IN\u000a      APPLIED MECHANICS AND ENGINEERING, 194, 743-773.\u000a      10.1016\/j.cma.2004.06.006, 2005\u000a    \u000a\u000a5 Ford, Neville J., Manuela Rodrigues, M., Xiao, Jingyu, Yan, Yubin,\u000a      Numerical analysis of a two- parameter fractional telegraph equation,\u000a      JOURNAL OF COMPUTATIONAL AND APPLIED MATHEMATICS, 249, 95-106.\u000a      10.1016\/j.cam.2013.02.009, 2013\u000a    \u000a\u000a6* Ford, JM, Ford, NJ, Wheeler, J, Simulation of grain-boundary diffusion\u000a      creep: analysis of some new numerical techniques, PROCEEDINGS OF THE ROYAL\u000a      SOCIETY OF LONDON SERIES A- MATHEMATICAL PHYSICAL AND ENGINEERING\u000a      SCIENCES, 460, 2395-2413. 10.1098\/rspa.2004.1287, 2004\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"16","Level2":"5","Subject":"Policy and Administration"}],"Sources":"\u000a    Section 4, paragraphs 1 and 2\u000a      FP7 2013 People Work Programme\u000a      http:\/\/ec.europa.eu\/research\/participants\/portal\/ShowDoc\/Extensions+Repository\/General+Documentation\/All+work+programmes\/2013\/People\/m-wp-201301_en.pdf\u000a    Section 4, paragraphs 3, 4 and 5\u000a      November 2010, Interim Evaluation of the Seventh Framework Programme,\u000a      European Commission\u000a      http:\/\/ec.europa.eu\/research\/evaluations\/pdf\/archive\/other_reports_studies_and_documents\/fp7_interim_evaluation_expert_group_report.pdf\u000a    Section 4, paragraph 5\u000a      May 2010, The impact of the EU RTD Framework Programme on the UK,\u000a      Technopolis Group: http:\/\/ec.europa.eu\/research\/evaluations\/pdf\/archive\/fp7-evidence-base\/national_impact_studies\/impact_of_the_eu_rtd_framework_programme_on_the_uk.pdf\u000a    Section 4, paragraph 6\u000a      Case studies published by the European Commission\u000a      http:\/\/ec.europa.eu\/research\/mariecurieactions\/media-library\/success-stories\/index_en.htm\u000a    An email held on file by the University of Chester from the Rector of a\u000a      Belgian university, with wide experience as an FP7 Expert and Vice Chair,\u000a      confirms that the impact activity described here meets the definition in\u000a      paragraph 161d of the REF Guidance on Submissions.\u000a    ","Title":"\u000a    Expert Advice to EC\/REA: FP7\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Appointment as an Expert under FP7 is based upon the Experts' database\u000a      held on the European CORDIS system. Here, potential experts must provide\u000a      details of recent research outputs and previous relevant experience. On\u000a      the basis of the CORDIS record, potential experts are identified by REA\u000a      staff and placed on a reserve list in anticipation of the closure of a\u000a      particular call for proposals. When the call is closed, a careful matching\u000a      of expertise against specific applications received leads to the\u000a      confirmation of those experts who have the appropriate research expertise\u000a      to serve as experts for that evaluation. The evaluations are undertaken\u000a      using the expert's detailed judgements drawing specifically upon their\u000a      research results and experience to provide advice to the REA and to assist\u000a      in the final prioritisation of proposals. The specific work undertaken for\u000a      the REA is under a non-disclosure confidentiality agreement.\u000a    In the current case, specific areas of research expertise relevant to the\u000a      appointment are the mathematical modelling and simulation of problems from\u000a      the life sciences (see references 1 and 3), engineering (references 4 and\u000a      5), earth sciences (reference 6), immunology (reference 3) and ecology\u000a      (reference 1), numerical and analytical approaches to solution and\u000a      stability of differential and integral equations, and the analysis of\u000a      discrete systems (references 2, 4, and 5). These represent significant\u000a      areas of activity for FORD and the wider research group at Chester and the\u000a      evaluation reports draw directly on the Unit's research. FORD undertook\u000a      the underlying research between 1993 and 2013. FORD has met the conditions\u000a      of REF Category A at this institution from January 1986 to the present.\u000a    The evaluator's own research experience lies at the heart of the\u000a      evaluation process and forms the basis of all judgements made and advice\u000a      provided. The evaluation requires a detailed judgement of the scientific\u000a      and technological merit of each proposal, the appropriateness of the\u000a      research objectives, the relationship of the project to the current state\u000a      of the art and the quality and relevance of the research methodology. The\u000a      researcher's profile is evaluated in detail, including the quality and\u000a      impact of previous research results and the likelihood that working on\u000a      this particular fellowship will lead to new and useful results. The\u000a      detailed project work plan is assessed, as well as the dissemination and\u000a      impact strategy alongside arrangements for outreach activities to\u000a      stimulate wide public interest and engagement with current scientific\u000a      research. Specific attention is given to the potential impact on European\u000a      Science and European Competitiveness should the proposal be funded, since\u000a      the desire for impact on the economy, competitiveness and the supply of\u000a      well-qualified and excellent scientists is central to the funding\u000a      objectives and hence to the evaluations. Specific research results of the\u000a      evaluator may reinforce or undermine the scientific merit of the proposal;\u000a      experience in undertaking similar projects underpins the assessment of\u000a      methodology, feasibility and risk; knowledge gained from collaborative\u000a      research links with relevant academic and user communities provides the\u000a      foundation for assessment of the quality of the researcher, the benefit of\u000a      knowledge transfer, and the impact of the research on European excellence\u000a      and competitiveness.\u000a    "},{"CaseStudyId":"17232","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"2264397","Name":"Portugal"},{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The results of Linden's research undertaken during his time at Cambridge\u000d\u000a      have been used in two ways. First, through applying the principles of\u000d\u000a      natural ventilation in the design of specific buildings which were\u000d\u000a      modelled in the G.K. Batchelor Laboratory from 1993 - 1998 using the\u000d\u000a      principles established by Linden's research. This research revealed the\u000d\u000a      airflow pathways and ventilation rates that would occur in the building\u000d\u000a      and this allowed the designers to modify window opening sizes and\u000d\u000a      strategies. The research has a continuing impact in two ways: as case\u000d\u000a      studies of successful low-energy buildings that, with growing public\u000d\u000a      interest in global warming, have led to an increased influence on\u000d\u000a      designers and architects, and by their contribution to the UK's reduced\u000d\u000a      energy consumption and carbon emissions: naturally ventilated buildings in\u000d\u000a      the UK typically have carbon emissions around 25% of fully air conditioned\u000d\u000a      buildings (Steemers &amp; Manchanda, 2010, Build. and Environ, 45,\u000d\u000a      270-278). The following naturally-ventilated buildings fall into this\u000d\u000a      category:\u000d\u000a    Internal Revenue Building, Nottingham, UK 1994\u000d\u000a      Cable and Wireless Training Centre, Coventry, UK (Royal Fine Art\u000d\u000a      Commission\/Sunday Times Building of the Year Award 1994)\u000d\u000a      BRE Environmental building, Garston, UK 1996\u000d\u000a    The Centre for Mathematical Sciences at Cambridge (completed in 2004)\u000d\u000a      uses displacement ventilation, the mathematical modelling of which is\u000d\u000a      included in Linden's research papers.\u000d\u000a    Linden's mathematical algorithms for natural ventilation described in\u000d\u000a      Section 2 were adopted by and implemented into the US Department of Energy\u000d\u000a      whole-building simulation code EnergyPlus during the period 2003-2009,\u000d\u000a      thereby providing the code with the capability of modelling natural\u000d\u000a      ventilation for the first time. EnergyPlus is a public domain code capable\u000d\u000a      of calculating the energy performance and internal conditions within a\u000d\u000a      building and has been used continuously in the US for regulatory and\u000d\u000a      world-wide design purposes. The natural ventilation capabilities of\u000d\u000a      EnergyPlus are wholly the result of the research undertaken by Linden et\u000a        al and were an essential component in the design of the second\u000d\u000a      category of buildings, those designed with the use of EnergyPlus and the\u000d\u000a      specific modules based on Linden's research to optimize the energy\u000d\u000a      performance of the buildings.\u000d\u000a    The following buildings fall into this category:\u000d\u000a    San Francisco Federal Office Building, USA 2007\u000d\u000a      San Diego Children's Museum, USA (Winner 2008 Savings by Design Energy\u000d\u000a      Efficiency Integration Award)\u000d\u000a      San Diego Supercomputer Center, UCSD, USA 2008\u000d\u000a      New York Times HQ, New York, USA 2009\u000d\u000a    These buildings continue to provide energy savings and high-quality and\u000d\u000a      comfortable indoor air conditions, using design features predicated on the\u000d\u000a      code EnergyPlus, including modules specifically based on Linden's\u000d\u000a      research. As noted above naturally ventilated buildings use less energy\u000d\u000a      and have significantly less carbon emissions than a conventional air\u000d\u000a      conditioned building.\u000d\u000a    EnergyPlus is used world-wide to simulate the energy consumption and\u000d\u000a      internal conditions within buildings. First released in 2001 it currently\u000d\u000a      serves as a major regulatory calculation model in the USA for building\u000d\u000a      energy code purposes. It currently has over 3000 members in its user\u000d\u000a      group. Details of the code can be found at http:\/\/apps1.eere.energy.gov\/buildings\/energyplus\/.\u000d\u000a    The research carried out between 1993-1998 also made a significant\u000d\u000a      contribution to the establishment of the consultancy company NaturalWorks\u000d\u000a      (http:\/\/www.natural-works.com)\u000d\u000a      in 2002. The company is based in Portugal and currently employs 9 persons\u000d\u000a      and has a five year average annual turnover of 800k EURO. The company\u000d\u000a      provides design assistance in low-energy buildings in both Europe and the\u000d\u000a      US, using principles and methodologies developed to predict the behaviour\u000d\u000a      and performance of naturally ventilated buildings.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research undertaken at the University of Cambridge Department of Applied\u000d\u000a      Mathematics and Theoretical Physics (DAMTP) was the first to demonstrate\u000d\u000a      that low-energy systems could be modelled in the laboratory and that the\u000d\u000a      complex ventilation flows within buildings could be represented accurately\u000d\u000a      by simple algorithms. These algorithms were implemented as a series of\u000d\u000a      `low- energy' modules in the US Department of Energy whole-building\u000d\u000a      simulation code EnergyPlus. EnergyPlus is used worldwide for building\u000d\u000a      energy simulation and the user group currently has 3144 members. The use\u000d\u000a      of this code has led to optimised design of a number of buildings, such as\u000d\u000a      the New York Times HQ in Manhattan opened in 2009.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Cambridge\u000d\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"5125771","Name":"New York"},{"GeoNamesId":"5125771","Name":"Manhattan"},{"GeoNamesId":"5128638","Name":"New York"},{"GeoNamesId":"5391959","Name":"San Francisco"}],"References":"\u000d\u000a    \u000aCooper, P. &amp; Linden P.F. 1996 Natural ventilation of enclosures\u000d\u000a      containing two sources of buoyancy. J. Fluid Mech., 311, 155-176, DOI: 10.1017\/S0022112096002546\u000d\u000a    \u000a\u000aLinden, P.F. &amp; Cooper, P. 1996 Multiple sources of buoyancy in a\u000d\u000a      naturally ventilated enclosure. J. Fluid Mech., 311, 177-192, DOI: 10.1017\/S0022112096002558\u000d\u000a    \u000a\u000aHunt, G.R. &amp; Linden, P.F. 1999 The fluid mechanics of natural\u000d\u000a      ventilation &#8212; displacement ventilation by buoyancy-driven flows reinforced\u000d\u000a      by wind. Building and Environment, 34, 707-720, DOI:\u000d\u000a      10.1016\/S0360-1323(98)00053-5 (research conducted whilst Linden was at\u000d\u000a      Cambridge)\u000d\u000a    \u000a\u000aHunt, G.R. &amp; Linden, P.F. 2001 Steady-state flows in an enclosure\u000d\u000a      ventilated by buoyancy forces assisted by wind. J. Fluid Mech., 426,\u000d\u000a      355-386, DOI: 10.1017\/S0022112000002470 (research conducted whilst Linden\u000d\u000a      was at Cambridge)\u000d\u000a    \u000a\u000aHunt, G.R., Linden, P.F. &amp; Cooper, P. 2001 Thermal stratification\u000d\u000a      produced by jets and plumes in enclosed spaces. Building and Environment,\u000d\u000a      36, 871-882, DOI: 10.1016\/S0360-1323(01)00013-0 (research conducted whilst\u000d\u000a      Linden was at Cambridge)\u000d\u000a    \u000a\u000aHunt, G.R. &amp; Linden P.F. 2005 Displacement and mixing ventilation\u000d\u000a      driven by opposing wind and buoyancy. J. Fluid Mech., 527, 27-55, DOI:\u000d\u000a      10.1017\/S0022112004002575. (research conducted whilst Linden was at\u000d\u000a      Cambridge)\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"12","Level2":"99","Subject":"Other Built Environment and Design"},{"Level1":"12","Level2":"2","Subject":"Building"}],"Sources":"\u000d\u000a    Details of the user-group for EnergyPlus can be found at\u000d\u000a      http:\/\/tech.groups.yahoo.com\/group\/EnergyPlus_Support\/\u000d\u000a    References to specific use of the research in building design can be\u000d\u000a      found in:\u000d\u000a    Haves, P, Carrilho da Graca, G &amp; Linden, P.F. 2004 Use of simulation\u000d\u000a      in the design of a large naturally ventilated commercial office building.\u000d\u000a      Building Serv. Eng. Res. Technol., 25, 211-222.\u000d\u000a    Carrilho da Graca, G., Linden, P.F. &amp; Haves, P. 2004 Design and\u000d\u000a      testing of a control strategy for a large, naturally ventilated office\u000d\u000a      building. Building Serv. Eng. Res. Technol., 25, 223-240.\u000d\u000a    Kilicote, S., Piette, M. A., Watson, D.S. &amp; Hughes, G. 2006 Dynamic\u000d\u000a      controls for energy efficiency and demand response: framework concepts and\u000d\u000a      a new construction study case in New York. Proceedings of the 2006 ACEEE\u000d\u000a      Summer Study on Energy Efficiency in Buildings, Pacific Grove, CA, August\u000d\u000a      13-18, 2006\u000d\u000a    SDGE Progress by Design: Winter 2009 Edition. Article `The New Children's\u000d\u000a      Museum gives full play to energy performance'\u000d\u000a    Statement from Director, NaturalWorks \u000d\u000a    ","Title":"\u000d\u000a    Low-energy buildings\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2652221","Name":"Coventry"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    In the late 1980s Paul Linden, University of Cambridge Department of\u000d\u000a      Applied Mathematics and Theoretical Physics (DAMTP), began research into\u000d\u000a      the fluid mechanics of the ventilation of buildings. Until that time,\u000d\u000a      models of building ventilation were crude and assumed that every zone\u000d\u000a      within a building was at a single uniform temperature. Professor Linden\u000d\u000a      (in post until 1998, as Reader, and rejoined in 2010 as Professor) showed\u000d\u000a      that it is was possible to represent full-scale air flows in buildings\u000d\u000a      accurately at laboratory scale using water as the working fluid and salt\u000d\u000a      concentration as a proxy for temperature. This led to an experimental\u000d\u000a      programme that revolutionized our understanding of the dynamics of\u000d\u000a      low-energy ventilation, and provided the basis for the development of new\u000d\u000a      mathematical models that enable the performance of new low-energy\u000d\u000a      buildings to be evaluated for the first time. These models explicitly\u000d\u000a      account for the fact that significant temperature variations occur within\u000d\u000a      individual spaces within buildings as a result of internal and external\u000d\u000a      gains, and provide physically correct descriptions of the flow and\u000d\u000a      temperature fields.\u000d\u000a    Over the period from 1993 this research has examined natural ventilation,\u000d\u000a      both wind-driven and stack-driven, mechanical displacement ventilation and\u000d\u000a      underfloor air distribution. From 1993-1998 the research was carried out\u000d\u000a      at the University of Cambridge by Professor Linden and collaborators, in\u000d\u000a      particular Gary Hunt (Research Associate at DAMTP from 1994 to 2000, now\u000d\u000a      Dyson Professor in the Department of Engineering), and in collaboration\u000d\u000a      with Professor Paul Cooper of the University of Wollongong who visited\u000d\u000a      Cambridge for one year in 1995. Experiments in the G.K. Batchelor\u000d\u000a      Laboratory in DAMTP provided the basis for the research and were led by\u000d\u000a      Professor Linden who also developed the mathematical models of these\u000d\u000a      flows. The research during this period provided the mathematical\u000d\u000a      underpinnings and explicit algorithms for the prediction of the\u000d\u000a      ventilation produced by natural displacement ventilation, in particular\u000d\u000a      for the important case where both wind and buoyancy interact. The\u000d\u000a      inclusion of this interaction significantly increases the range of cases\u000d\u000a      and environmental conditions that can be modelled and calculated using\u000d\u000a      these methods.\u000d\u000a    "},{"CaseStudyId":"17234","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"6251999","Name":"Canada"},{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000a    There are two important contributions of the research of direct interest\u000a      to Unilever, a global\u000a      manufacturer of personal care products, including shampoos and\u000a      conditioners, which has clear\u000a      interests in understanding quantitatively the properties of hair.\u000a    Unilever Global Senior Vice President for Home and Personal Care R&amp;D\u000a      confirms these impacts in\u000a      a letter, saying:\u000a    \"The first is the differential equation now known as the `Ponytail Shape\u000a      Equation', whose solutions\u000a      describe the envelope of a hair bundle as a balance between elastic,\u000a      gravitational, and random\u000a      curvature effects. This has allowed Unilever to address a number of \"what\u000a      if\" questions about hair\u000a      in a very straightforward way. Since this is coded in Matlab, which is a\u000a      standard Unilever\u000a      application, it has been circulated widely and can be used by all\u000a      researchers in house.\"\u000a    The second important contribution is the suite of image analysis\u000a      protocols that Goldstein and\u000a      Warren developed to image both single hairs and bundles reliably and to\u000a      extract from stereoscopic\u000a      images their three-dimensional shapes. The strong verification that the\u000a      team did with these\u000a      protocols gives Unilever confidence in their accuracy.\u000a    Unilever Global Senior Vice President for Home and Personal Care R&amp;D,\u000a      adds \"In a Fast Moving\u000a      Consumer Goods business such as ours, we know that even just a three-month\u000a      reduction in time\u000a      to market for an innovation in one of leading billion Euro brands could\u000a      readily correspond to an\u000a      incremental turnover of the order of several hundred thousand Euro, if not\u000a      more.\"\u000a    In 2012 the team was awarded an Ig Nobel Prize for this research. The Ig\u000a      Nobel Prizes honour\u000a      achievements that first make people laugh, and then make them think. The\u000a      prizes are intended to\u000a      spur people's interest in science, medicine, and technology. The original\u000a      announcement of the\u000a      research, followed by the Ig Nobel Prize award, generated worldwide media\u000a      attention. The\u000a      research has featured on international radio programs (Canada, Germany,\u000a      UK), on worldwide\u000a      press, and has led to invited presentations in schools and universities\u000a      internationally.\u000a    ","ImpactSummary":"\u000a    Research conducted at the University of Cambridge yielded a theory for\u000a      the energy of hair arrays\u000a      and a differential equation for the shape of the envelope of a bundle\u000a      (\"Ponytail Shape Equation\").\u000a      It enabled Unilever to address quantitatively a number of \"what if\"\u000a      questions about how properties\u000a      of individual hairs are reflected in those of bundles, an important\u000a      component of product\u000a      development. Novel imaging techniques have been developed that allow for\u000a      quantitative studies of\u000a      hair properties. Finally, the story of this research generated worldwide\u000a      attention, in its original\u000a      announcement and also through the award of the 2012 Ig Nobel Prize in\u000a      physics to this team. It\u000a      has featured on international radio programs (Canada, Germany, UK), on\u000a      worldwide press, and\u000a      has led to invited presentations in schools and universities\u000a      internationally.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of Cambridge\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000aGoldstein, R.E., Warren, P.B. &amp; Ball, R.C. 2012 Shape of a ponytail\u000a      and the statistical physics\u000a      of hair fiber bundles. Phys. Rev. Lett., 108, 038103, DOI:\u000a      10.1103\/PhysRevLett.108.078101.\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    Letter from Unilever Global Senior Vice President for Home and Personal\u000a      Care R&amp;D\u000a    Both the original publication and the Ig Nobel award received extensive\u000a      media coverage: for\u000a      example\u000a    New York Times: 10th Feb 2012: Like How Your Hair Hangs? Praise the\u000a        Laws of Physics\u000a      http:\/\/www.nytimes.com\/2012\/02\/11\/science\/ponytail-shape-is-determined-by-complex-physics.html\u000a    BBC Web coverage 13th Feb 2012 Science behind ponytail\u000a        revealed\u000a      http:\/\/www.bbc.co.uk\/news\/science-environment-17012795\u000a    BBC Web coverage 21st Sept 2012 Ig Nobel honours ponytail\u000a        physics:\u000a      http:\/\/www.bbc.co.uk\/news\/science-environment-19667664\u000a\u000a    ","Title":"\u000a    Statistical Physics of Hair\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research on the statistical physics of hair was carried\u000a      out from 2009 to 2012 in\u000a      collaboration between Professor Raymond E. Goldstein, the Schlumberger\u000a      Professor of Complex\u000a      Physical Systems at the Department of Applied Mathematics and Theoretical\u000a      Physics, University of\u000a      Cambridge (DAMTP) throughout this time, and Dr Patrick B. Warren of\u000a      Unilever Research and\u000a      Development, Port Sunlight. The research involved combined experimental\u000a      and theoretical\u000a      investigation of the properties of hair fibre bundles. The two researchers\u000a      worked closely on all\u000a      aspects of the theoretical analysis, and Goldstein led the experimental\u000a      work. The key issues\u000a      addressed were the quantification of the random curvatures of real hairs\u000a      and the incorporation of\u000a      the effects of those curvatures into a continuum theory for the energy of\u000a      a bundle. This was\u000a      essential for a range of problems in hair physics, including the\u000a      elasticity of bundles, their dynamics,\u000a      and understanding the physics of hair tangling. The quantification problem\u000a      was solved by the\u000a      development of a stereoscopic imaging system combined with a statistical\u000a      reconstruction algorithm\u000a      to give highly reliable representations of the shapes of individual hair\u000a      fibres in three dimensions.\u000a      This allowed extraction of the locally varying intrinsic curvatures of the\u000a      filaments. Knowledge of the\u000a      statistical properties of these curvatures provided a way to distinguish\u000a      different hair types and to\u000a      assess the effects of various treatments on hair. The theoretical problem\u000a      was solved by\u000a      developing a density functional theory of hair arrays by analogy to the\u000a      theory of liquid crystals, and\u000a      using methods from fluid dynamics to simplify the mathematics to the point\u000a      that analytical progress\u000a      could be made. This resulted in the derivation of a fourth-order ordinary\u000a      differential equation for\u000a      the envelope of a ponytail, known now as the \"Ponytail Shape Equation\".\u000a      From this came a direct\u000a      way of determining the \"equation of state\" of hair from the shapes of\u000a      bundles hanging under\u000a      gravity. Tests of this method of analysis using real human hair bundles\u000a      showed that the amplitude\u000a      and characteristic length scale of the variations in bundle pressure could\u000a      be related directly to the\u000a      spectrum of random curvatures of the individual hairs.\u000a    "},{"CaseStudyId":"17235","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Royal Society"],"ImpactDetails":"\u000a    Communication and visualisation of risk: Professor\u000a      Spiegelhalter has worked with numerous\u000a      collaborators in establishing good practice in risk communication, in\u000a      particular the promotion of\u000a      visualisations such as icon arrays. These include:\u000a    Spiegelhalter and Pearson's animation research [1] was used to help Breakthrough\u000a        Breast Cancer,\u000a      the largest UK breast cancer charity, Senior Information Officer [6], said\u000a      \"Thank you so much for\u000a        the advice of yourself and Mike Pearson for the development of\u000a        Breakthrough Breast Cancer's\u000a        online guide to breast screening. The icon array animations were a new\u000a        venture for us and your\u000a        advice and guidance was extremely helpful and has improved the end\u000a        product. They are a pivotal\u000a        part of the tool and feedback so far from laypersons and colleagues\u000a        indicates that the animations\u000a        help people to understand the complex issue of the risk of over\u000a        diagnosis with breast screening\u000a        very well\".\u000a    In 2012 Professor Spiegelhalter was asked to lead the statistical\u000a      analysis for the Expert Advisory\u000a      Group on Breast Implants that was charged with setting government policy\u000a      following the Poly\u000a      Implant Prothese (PIP) breast implants scandal [7]. This complex analysis\u000a      required careful\u000a      communication of the deeper uncertainties due to limited evidence, and led\u000a      to the government\u000a      decision not to undertake removal of the implants unless clinically\u000a      justified.\u000a    The elicitation exercise on the effect of plain packaging of cigarettes\u000a      is a `highly accessed' paper\u000a      on Biomed Central (over 3000 in the first 3 months) [8]. The Department of\u000a      Health is using this\u000a      study [9] as a major part of the impact assessment for deciding a policy\u000a      on plain packaging. The\u000a      importance of this research is reflected in the tobacco industry\u000a      sponsoring full-page advertisements\u000a      in national newspapers criticising the study.\u000a    Professor Spiegelhalter has explained the techniques for communication\u000a      and visualization set out\u000a      in his research in numerous schools and public talks to around 40,000\u000a      people since 2008 including\u000a      around 16,000 school students. A teacher in a comprehensive school\u000a      [10]reported \"As a result of\u000a        David Spiegelhalter's visit, several of my year 13 students have openly\u000a        said they are very much\u000a        more interested in pursuing a career in Statistics as they could see how\u000a        useful and interesting it\u000a        could be.\"\u000a    Media presentation of risk and statistics: Based on\u000a      his research on the presentation of risk\u000a      stories in the news, good practice in media reporting has been promoted by\u000a      Professor\u000a      Spiegelhalter through, for example,\u000a    Working closely with the Science Media Centre from 2008, resulting in\u000a      numerous quotes provided\u000a      in leading news outlets concerning risk stories. This led to Professor\u000a      Spiegelhalter being awarded\u000a      the 2011 Science Communication Award for Established Researchers from the\u000a      Society of Biology.\u000a      In addition, he was a member of the group that produced guidance for\u000a      science reporting that had\u000a      been requested by the Leveson Inquiry [11]. The Chief Executive of the\u000a      Science Media Centre\u000a      [14], said: \"From mobile phones and cancer to vaccination and heart\u000a        disease, he has answered\u000a        countless calls from journalists to help them understand complex pieces\u000a        of scientific research.\u000a        Without his involvement, these public health stories would be prone to\u000a        inaccuracy. David's\u000a        patience and willingness to engage, as well as a flair for pithy\u000a        soundbites and clever analogies,\u000a        means the public are well informed about health and science through the\u000a        popular media.\"\u000a    Engagement with newspapers leading to the adoption of improved practices,\u000a      for example meetings\u000a      with sub-editors at The Times and groups of reporters from the Guardian,\u000a      Daily Mail, Daily\u000a      Telegraph and ITN, and lecturing to trainee journalists and press\u000a      officers. The Chief Science\u000a      Reporter, The Times [14], said: \"David makes the subjects of risk and\u000a        statistics not only easy to\u000a        understand, but also entertaining and intriguing. He's helped us at The\u000a        Times on all kinds of\u000a        stories. He's also come in to give our sub-editors a talk on the\u000a        importance of being rigorous in\u000a        reporting statistics, which certainly had an impact on the way they went\u000a        about their jobs.\".\u000a    Numerous public talks on media portrayal of statistics: including to over\u000a      500 at the Cambridge\u000a      Science Festival \"100% of people who filled in an evaluation card said\u000a        `How to spot a shabby\u000a        statistic' was good or very good (85% very good!)\"[13].\u000a    Professor Spiegelhalter comments on risk and statistics in the media on\u000a      Twitter as @undunc\u000a      (&gt;6000 followers, top 0.01% of Twitter users) and in 2012 was among\u000a      `top 6 scientists on Twitter'\u000a      in Observer magazine.\u000a    Professor Spiegelhalter's YouTube video on risk communication has had\u000a      over 85,000 views.\u000a    In 2012 Professor Spiegelhalter fronted a one-hour BBC4 documentary\u000a      `Tails You Win, the\u000a      Science of Chance' that featured visualisations based on the research of\u000a      Spiegelhalter and Mike\u000a      Pearson into icon arrays for `possible futures', and included\u000a      contributions from the Bank of England\u000a      on handling deeper uncertainties. The YouTube clip has received over\u000a      17,000 [15] views. Viewing\u000a      figures for the first showing were around 750,000 [12], extremely high for\u000a      a BBC4 science\u000a      programme. \"Understandably, much of human endeavour has amounted to an\u000a        effort to quantify,\u000a        regulate and eventually overcome chance. Spiegelhalter is an immensely\u000a        engaging guide to this\u000a        struggle, meeting seismologists, gamblers, statisticians and others who\u000a        study the throw of the dice,\u000a        metaphorical or literal\" [16]\u000a    Professor Spiegelhalter was the subject of an episode in the Radio 4\u000a      series `A Life Scientific' in\u000a      June 2013, discussing his work on Microlives and reaching an audience of\u000a      over 2 million.\u000a    ","ImpactSummary":"\u000a    This case study concerns the work of Professor David Spiegelhalter as\u000a      Winton Professor for the\u000a      Public Understanding of Risk at the University of Cambridge. Based on his\u000a      research on risk\u000a      communication, he has made numerous contributions to public service,\u000a      influencing the way health\u000a      screening information is given to the public, and public policy on breast\u000a      implants and plain\u000a      packaging of cigarettes. In addition, through lectures, Twitter, radio and\u000a      TV appearances he has\u000a      become a popular commentator on risk issues and reached a substantial\u000a      segment of the UK\u000a      public. He has had a continuing impact on the way that statistics, risk\u000a      and uncertainty are\u000a      discussed in the UK today.\u000a    ","ImpactType":"Political","Institution":"\u000a    University of Cambridge\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a*[1] DJ Spiegelhalter, I Short, M Pearson. Visualizing uncertainty about\u000a      the future. Science, 333:\u000a      1393-1400, 2011 DOI:\u000a        10.1126\/science.1191181\u000a    \u000a\u000a*[2] DJ Spiegelhalter, Using speed of ageing and \"microlives\" to\u000a      communicate the effects of lifetime\u000a      habits and environment. British Medical Journal, 345:e8223.\u000a      2012. DOI:\u000a        10.1136\/bmj.e8223\u000a    \u000a\u000a*[3] DJ Spiegelhalter, H Riesch. Don't know, can't know: embracing deeper\u000a      uncertainties when\u000a      analysing risks. Phil Trans Roy Soc A, 369 4730-4750,\u000a      2011. DOI: 10.1098\/rsta.2011.0163\u000a    \u000a\u000a[4] R Pechey, D Spiegelhalter and TM Marteau. (2013) Impact of plain\u000a      packaging of tobacco\u000a      products on smoking in adults and children: an elicitation of\u000a      international experts' estimates BMC\u000a        Public Health, 13:18. DOI: 10.1186\/1471-2458-13-18\u000a    \u000a\u000a[5] H Riesch DJ Spiegelhalter. 'Careless pork costs lives': risk stories\u000a      from science to press release\u000a      to media. Health, Risk &amp; Society , 13: 47-64, 2011 DOI:\u000a        10.1080\/13698575.2010.540645\u000a      \u000a(* papers best indicating the quality of the underpinning research)\u000a    ","ResearchSubjectAreas":[{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000a    [6] Screening Breakthrough Breast Cancer, Senior Information\u000a      Officer \u000a    [7] Breast Implants\u000a      https:\/\/www.gov.uk\/government\/publications\/poly-implant-prothese-pip-breast-implants-final-report-of-the-expert-group\u000a    [8] Email from Biomed Central confirming number of downloads\u000a    [9] Email from Department of Health confirming use of study in policy\u000a      assessment\u000a    [10] Email from Teacher at Meridian School, Royston\u000a    [11] Proposed guidelines to Leveson\u000a      http:\/\/www.levesoninquiry.org.uk\/wp-content\/uploads\/2012\/07\/Second-submission-to-Inquiry-from-Guidelines-for-science-and-health-reporting-31.05.12.pdf\u000a    [12] E-mail from BBC about viewing figures for `Tails you win, the\u000a      Science of chance'\u000a    [13] Public talks on media presentation - Evaluation Compendium and\u000a      Cambridge Science\u000a      Festival feed back\u000a    [14] Submission for Society of Biology Award:\u000a      http:\/\/www.jonathanpegg.com\/pages\/content\/index.asp?PageID=255\u000a    [15] Link to YouTube video Maximise your chances of living to 100 - Tails\u000a      You Win:\u000a      http:\/\/www.youtube.com\/watch?v=vApS8EkopTI\u000a    [16] The Guardian TV Listings and Previews 15th-19th\u000a      October:\u000a      http:\/\/www.theguardian.com\/tv-and-radio\/2012\/oct\/15\/tv-listings-previews-15-19-October\u000a    \u000a    ","Title":"\u000a    Communication of Risk and Uncertainty\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Following his appointment as Winton Professor of the Public Understanding\u000a      of Risk at the\u000a      Cambridge University Department of Pure Mathematics and Mathematical\u000a      Statistics (DPMMS) in\u000a      2007, Professor David Spiegelhalter has researched the communication of\u000a      risk and uncertainty\u000a      and engaged in extensive collaborations. These fall under three main\u000a      headings, although with\u000a      considerable overlap:\u000a    \u000a      \u000aCommunication and visualisation of risk: There is\u000a        increasing demand for attractive and\u000a        informative visualisations of quantified uncertainty, whether it\u000a        concerns health outcomes, future\u000a        weather, or financial forecasts, and this has become an active area of\u000a        research within medicine\u000a        and psychology. From 2007 with Mike Pearson (Computer Associate\u000a        throughout the period)\u000a        and Ian Short (Research Associate until his departure in 2010) in the\u000a        Millennium Mathematics\u000a        project in the Department of Applied Mathematics and Theoretical Physics\u000a        (DAMTP) in\u000a        Cambridge, Professor Spiegelhalter experimented with a variety of\u000a        different representations,\u000a        specialising in both static and animated icon arrays, as described in\u000a        Spiegelhalter, Short and\u000a        Pearson (2011) [1]. This work focussed explicitly on the need to\u000a        communicate the magnitudes\u000a        of risk and benefits to those with low numeracy, in which research has\u000a        shown that multiple\u000a        representations, and in particular icon arrays, can be of benefit in\u000a        aiding comprehension and\u000a        developing a degree of `immunity to misleading anecdotes'. He also\u000a        introduced the concept of\u000a        `microlives' - 30 minutes change in life-expectancy associated with a\u000a        daily habit [2].\u000a      \u000aCommunication of uncertainty: Not all uncertainties can\u000a        be quantified by a probability\u000a        distribution based on data. From 2009, in collaboration with Hauke\u000a        Riesch (Research\u000a        Associate on the Winton programme in DPMMS from 2007 to 2009, then\u000a        Research Associate\u000a        in the Judge Business School, Cambridge until 2011) Professor\u000a        Spiegelhalter analysed the\u000a        way that deeper scientific uncertainties were communicated, in\u000a        particular the limitations in\u000a        quantified risk analysis. A basic scale was developed while Riesch was\u000a        in DPMMS and\u000a        published in Spiegelhalter and Riesch (2011) [3], and the argument made\u000a        for the necessity of\u000a        explicit judgment in handling non-modelled uncertainty, and the need to\u000a        clearly identify the\u000a        level of scientific confidence in formal analyses. These ideas are\u000a        particularly relevant when\u000a        communicating public policy decisions in which there is a substantial\u000a        degree of scientific\u000a        uncertainty.\u000a      Professor Spiegelhalter is a Principal Investigator in the University\u000a        of Cambridge Behaviour\u000a        and Health Research Unit (BHRU). Using his research on judgement about\u000a        uncertainty, with\u000a        Professor Theresa Marteau (Director, BHRU) and Rachel Pechey (Research\u000a        Associate,\u000a        BHRU) an elicitation exercise was carried out in 2012 on expert opinion\u000a        about the possible\u000a        impact of plain packaging of cigarettes [4].\u000a      \u000aMedia presentation of risk and statistics: There is\u000a        considerable criticism of the way that the\u000a        popular media deal with stories concerning risk and statistics. Research\u000a        with Riesch from 2008\u000a        [5] analysed the process by which a scientific study is translated into\u000a        media stories, and\u000a        showed the vital importance of the press release in the final\u000a        presentation by journalists. Based\u000a        on this work, Professor Spiegelhalter has been active in promoting good\u000a        practice in media\u000a        reporting to both journalists and press officers.\u000a    \u000a    "},{"CaseStudyId":"17236","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Medical Research Council"],"ImpactDetails":"\u000d\u000a    Spiegelhalter's funnel plot methodology has been adopted by numerous\u000d\u000a      organisations charged with communicating medical outcomes to the public,\u000d\u000a      and has become increasingly influential in recent years with growing\u000d\u000a      public concern resulting from the release of evidence of certain NHS\u000d\u000a      performance outcomes.\u000d\u000a    A major application area has been in child heart surgery. The National\u000d\u000a      Institute for Cardiovascular Outcomes Research (NICOR) uses funnel plots\u000d\u000a      to communicate risk of surgery for congenital heart disease to the public\u000d\u000a      [6]. In addition they have strongly featured in Inquiries into possible\u000d\u000a      performance failures in UK hospitals: based on his research in this area,\u000d\u000a      Professor Spiegelhalter was a member of the 2010 Inquiry into child heart\u000d\u000a      deaths at the Oxford Radcliffe Infirmary, which resulted in the ceasing of\u000d\u000a      surgery in Oxford. In 2012 he also contributed funnel-plot analysis [7] to\u000d\u000a      the controversial Safe and Sustainable [8] programme that\u000d\u000a      recommended closure of centres for paediatric heart surgery. In April\u000d\u000a      2013, at the height of the controversy surrounding surgery at Leeds\u000d\u000a      General Infirmary, Professor Spiegelhalter was part of the group analysing\u000d\u000a      the revised data and he produced funnel plots to communicate the findings\u000d\u000a      &#8212; this analysis contributed to the decision to restart surgery at Leeds.\u000d\u000a      [9]\u000d\u000a    As a result of Spiegelhalter's research, funnel plots have become a\u000d\u000a      standard method used for comparing outcomes within the National Health\u000d\u000a      Service: Department of Health guidance on `Detection and management of\u000d\u000a      outliers' [10] is almost entirely based on Spiegelhalter's work. The\u000d\u000a      National Joint Registry [11] uses them to identify centres with poor rates\u000d\u000a      of knee-replacement problems, while Organ Donation compares kidney\u000d\u000a      transplant success rates between centres. The initiative in 2013 to\u000d\u000a      publish surgeon-specific outcome data makes extensive use of funnel plots,\u000d\u000a      which appeared in news coverage [12]. The NHS Information Centre uses\u000d\u000a      funnels as part of their reports on \"Summary Hospital-level Mortality\u000d\u000a      Indicator (SHMI) &#8212; Deaths associated with hospitalisation, England\" [13]\u000d\u000a      on an annual basis, including allowance for over-dispersion. Following the\u000d\u000a      Mid-Staffordshire Inquiry (at which Professor Spiegelhalter was a\u000d\u000a      witness), this information is used to select hospitals for further\u000d\u000a      investigation.\u000d\u000a    Funnel plots have been included in software distributed by the Eastern\u000d\u000a      Region Public Health Observatory (now part of Public Health England): a\u000d\u000a      training video has been produced and 6500 downloads have been reported.\u000d\u000a      [14]\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This case study concerns the research of Professor David Spiegelhalter on\u000d\u000a      `funnel plot' methodology for comparing institutions. This system has now\u000d\u000a      become the standard method within the National Health Service for\u000d\u000a      comparing clinical outcomes, including hospital Trusts with apparently\u000d\u000a      `outlying' mortality rates. In particular, mortality following children's\u000d\u000a      heart surgery is analysed and presented using funnel plots, and Professor\u000d\u000a      Spiegelhalter's work has been instrumental in handling high-profile cases\u000d\u000a      such as surgery at Oxford Radcliffe Infirmary and Leeds General Infirmary.\u000d\u000a    ","ImpactType":"Political","Institution":"\u000d\u000a    University of Cambridge\u000d\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a* [1] D J Spiegelhalter. Funnel plots for comparing institutional\u000d\u000a      performance. Statistics in Medicine, 24:1185-1202, 2005,\u000d\u000a      DOI: 10.1002\/sim.1970.\u000d\u000a    \u000a\u000a[2] D J Spiegelhalter. Handling over-dispersion of performance\u000d\u000a      indicators. Quality Safety Health Care, 14:347-351, 2005,\u000d\u000a      DOI: 10.1136\/qshc.2005.013755.\u000d\u000a    \u000a\u000a* [3] HE Jones, DI Ohlssen, and DJ Spiegelhalter. Use of the false\u000d\u000a      discovery rate when comparing multiple health care providers. J Clin\u000d\u000a        Epidemiol, 61:232-240, 2008, 10.1016\/j.jclinepi.2007.04.017.\u000d\u000a    \u000a\u000a[4] Jones HE and Spiegelhalter DJ. The identification of `unusual'\u000d\u000a      health-care providers from a hierarchical model. American Statistician, 65\u000d\u000a      : 154-163, 2011, DOI: 10.1198\/tast.2011.10190\u000d\u000a    \u000a\u000a* [5] Spiegelhalter DJ, Sherlaw-Johnson C, Bardsley M, Blunt I, Wood C\u000d\u000a      and Grigg O. Statistical methods for healthcare regulation: rating,\u000d\u000a      screening and surveillance (with discussion). J Roy Statist Soc Series\u000d\u000a        A, 175, 1-47, 2012, DOI: 10.1111\/j.1467-985X.2011.01010.x\u000d\u000a    \u000a*References which best represent the quality of the underpinning research\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    Use of funnels by NICOR\u000d\u000a    [6] https:\/\/nicor4.nicor.org.uk\/CHD\/an_paeds.nsf\/WBenchmarksYears?openview&amp;RestrictToCategory\u000a        =2010&amp;start=1&amp;count=500\u000d\u000a    Response to analysis of Mortality data NHS Trust in England providing\u000d\u000a      paediatric cardiac surgery 2000-2009\u000d\u000a    [7] http:\/\/www.specialisedservices.nhs.uk\/library\/30\/Appendix_F___Response_to_the_analysis_of_Mortality_Data_of_NHS_Trust_in_England_Providing_Paediatric_Cardiac_Surgery_2000___2009_including_Terms_of_Reference.pdf\u000d\u000a    \"Safe and Sustainable\" documents reporting analysis based on funnel\u000d\u000a      plots:\u000d\u000a    [8]\u000d\u000a      http:\/\/www.specialisedservices.nhs.uk\/news\/view\/response-to-south-central-sha-analysis-outcome-data\u000d\u000a    Report on children's heart surgery in Leeds\u000d\u000a    [9] http:\/\/www.england.nhs.uk\/2013\/04\/12\/reports-chs-leeds\/\u000d\u000a\u0009Department of Health Guidance on handling outliers\u000d\u000a    [10]\u000d\u000a      https:\/\/www.gov.uk\/government\/publications\/detection-and-management-of-outliers-guidance-prepared-by-national-clinical-audit-advisory-group\u000d\u000a    National Joint Registry and 2012 Annual Report\u000d\u000a    [11] http:\/\/www.njrcentre.org.uk\/njrcentre\/default.aspx\u000d\u000a    BBC News web-page showing funnel plot to display mortality rates for\u000d\u000a      vascular surgeons\u000d\u000a    [12] http:\/\/www.bbc.co.uk\/news\/uk-politics-22489062\u000d\u000a    Details of methodology used by NHS\u000d\u000a    [13] http:\/\/www.ic.nhs.uk\/CHttpHandler.ashx?id=11151&amp;p=0\u000d\u000a    [14] Download information: email from Director of Knowledge and\u000d\u000a      Intelligence Knowledge and Intelligence Team (East), Public Health England\u000d\u000a    ","Title":"\u000d\u000a    Methods for Comparing Clinical Outcomes across Institutions\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2644688","Name":"Leeds"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Professor Spiegelhalter joined the Medical Research Council (MRC)\u000d\u000a      Biostatistics Unit at Cambridge in 1981, was returned as Category C in\u000d\u000a      subsequent RAEs was appointed Professor for the Public Understanding of\u000d\u000a      Risk in the Department of Pure Mathematics and Mathematical Statistics at\u000d\u000a      the University in 2007. Since 2003 he researched appropriate graphical\u000d\u000a      methods for comparing institutions, in particular the funnel plot, which\u000d\u000a      presents a set of performance measures versus their precisions, with added\u000d\u000a      control limits around a target value. Spiegelhalter's highly-cited 2005\u000d\u000a      paper has become the definitive text on this topic [1]. The control\u000d\u000a      limits, generally set at 2 and 3 standard deviations (95% and 99.8%\u000d\u000a      intervals) create a `funnel', visually emphasising that we can expect more\u000d\u000a      variability in smaller institutions. Dating back to early work on control\u000d\u000a      charts by Shewhart in the 1930s, traditionally a 3 standard-deviation\u000d\u000a      funnel has been used to identify `special-cause' variation (although the\u000d\u000a      NHS Information Centre currently display 2 standard-deviation limits).\u000d\u000a    Basic funnels can be based on simple outcome rates assuming a Binomial\u000d\u000a      distribution. More sophisticated versions incorporate allowance for\u000d\u000a      different case-mix by producing a risk-adjusted Expected mortality rate:\u000d\u000a      the funnel is then based on the standardised mortality rate\u000d\u000a      (Observed\/Expected), with limits based on a Poisson model. An additional\u000d\u000a      refinement, unique to Spiegelhalter's model, is the allowance for\u000d\u000a      `over-dispersion' &#8212; that is a degree of permissible variability in\u000d\u000a      underlying risk that is intended to take into account the inevitable\u000d\u000a      limitation in risk adjustment [2]. This has the effect of producing a\u000d\u000a      funnel that does not narrow indefinitely as the volume increases, but\u000d\u000a      tends to parallel control limits.\u000d\u000a    Example Funnel Plot, taken from [Source 4]:\u000d\u000a    \u000d\u000a    \u000d\u000a   \u000d\u000a    In collaboration with members of the MRC Biostatistics Unit,\u000d\u000a      Spiegelhalter determined how to deal with multiple comparisons, and\u000d\u000a      critiqued a technique now often used in US health monitoring [3-4]. This\u000d\u000a      research, and its application to comparing NHS Trusts, was summarised in a\u000d\u000a      Royal Statistical Society discussion paper [5], with collaborators from\u000d\u000a      the Care Quality Commission and other institutions.\u000d\u000a    "},{"CaseStudyId":"17238","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"3175395","Name":"Italy"}],"Funders":["Royal Society"],"ImpactDetails":"\u000a    The impact has been the communication of new discoveries and theories\u000a      about the structure of the universe to the public and young people beyond\u000a      the academic world. These include (i) the discovery of the acceleration of\u000a      the universe and Barrow's research to explain it, and (ii) Barrow's work\u000a      on the evidence for very slow variations in the fine structure `constant'\u000a      of physics over billions of years and his research to describe this. The\u000a      impacts listed are entirely the results of Professor Barrow's research and\u000a      its dissemination.\u000a    Publication of this work on the cosmological constant was the main\u000a      scientific news story of the week in Nature, online publication 23\/2\/11\u000a      http:\/\/www.nature.com\/news\/2011\/110223\/full\/news.2011.105.html.\u000a    This quest to understand the expansion of the universe, the cosmological\u000a      constant, and the evidence changing constants of Nature has been widely\u000a      presented from 1999-2013: in talks, radio broadcasts (BBC, RAI),\u000a      interviews, newspapers (Daily Telegraph, La Repubblica) and magazines\u000a      (Standpoint, Scientific American) and books (Cosmic Imagery, 2008\u000a      and The Book of Universes, 2011). These included 24 public\u000a      lectures in London 2008-11 during Professor Barrow's appointment as\u000a      Gresham Professor of Geometry, and his lectures at the British Science\u000a      Festival as President of the Physics-Astronomy section (2009) and of the\u000a      Mathematics section (2012). Both Presidential Lectures (The Origin and\u000a        Evolution of the Universe and Expanding Minds and Expanding\u000a        Universes) covered Barrow's cosmological research. Expanding\u000a        Minds and Expanding Universes was rated `excellent' by all audience\u000a      survey respondents and highlighted as `informative and entertaining' [1].\u000a      The audiences included school students, general public, teachers,\u000a      journalists, and amateur astronomers.\u000a    Barrow talked about his research at many events around the world for\u000a      paying audiences, including Hay (2011 capacity audience 500+), Frome (2012\u000a      capacity audience), Lichfield (2012), Ilkley (2011 capacity audience),\u000a      World Science Festival, New York (2009 capacity audience + online\u000a      transmission), Edinburgh (2008 capacity audience), Cambridge (2011), Genoa\u000a      (2012 capacity audience 600+), the Royal Institution London (2011 x 2),\u000a      Bath Scientific and Literary Society (2013).\u000a    Professor Barrow's BBC radio 4 appearance on In Our Time\u000a      `Mathematics and the Universe' on 11\/2\/10 reached 2.2 million listeners\u000a      and 30,000 online listeners [2] (it is still listened to a few hundred\u000a      times per week). Downloads from the BBC Podcast archive occur at 24,000\u000a      per annum for this programme.\u000a    Professor Barrow is the only scientist awarded the Faraday Medal of the\u000a      Royal Society (2008), the Kelvin Medal of the IOP (2009) and the\u000a      Christopher Zeeman Medal of the LMS and the IMA (2011) for the public\u000a      communication of science, physics and mathematics. He is the only person\u000a      since 1652 to have held two Gresham professorships in different subjects\u000a      (`Astronomy', 2003-7, and `Geometry', 2008-12).\u000a    The Italian edition of Barrow's book Cosmic Imagery received the\u000a      Merck-Serono Prize, the principal Italian literary prize for non-fiction,\u000a      July 2011, and The Book of Universes received the 2012 Antica\u000a      Pignolo Literary Prize at La Fenice, Venice, Italy in November 2012.\u000a    On 6\/3\/12 Barrow made a joint presentation on La Musica del Vuoto,\u000a      talk plus performance about the vacuum in cosmology and physics, its role\u000a      in explaining the acceleration of the universe and the musical analogues\u000a      of silence and timing, with the leading Italian contemporary pianist,\u000a      Ludovico Einaudi, at the Auditorium Parco Musica, Rome on 6\/3\/12 selling\u000a      over 1000 tickets, and over 150,000 views on the dedicated webpage [3].\u000a    Articles by Barrow and article-interviews about this research have\u000a      appeared in the online e-publication PLUS (www.plus.maths.org)\u000a      for the general public, media, teachers and age 16+ school students: A\u000a        Matter of Gravity (1\/3\/06 - 6647 page views) What is dark\u000a        energy? (24\/08\/09 - 5287 page views), Are the constants of\u000a        Nature really constant? (31\/5\/09 - 4252 page views), What\u000a        happened before the big bang? (23\/3\/09 - 29,221 page views), Does\u000a        infinity exist? (2\/7\/12 - 21,035 page views) [4].\u000a    The Book of Universes (2011) is translated into Hungarian,\u000a      Turkish, Italian, German, French, Chinese, Czech, Polish, US, and Japanese\u000a      and Cosmic Imagery (2008) into Spanish, Italian, Chinese, Russian,\u000a      Czech, Japanese, Korean, and US [5].\u000a    Barrow's Gresham public 24-lecture series of which four were in period,\u000a      attracted capacity audiences of 240 at the Museum of London plus 11,149\u000a      online views [6].\u000a    Barrow co-authored an invited Scientific American article Inconstant\u000a        Constants about the work on varying constants in June 2005. This was\u000a      also selected for inclusion, in extended form to take into account new\u000a      work, in a special Scientific American issue entitled `A Matter of Time',\u000a      Spring 2012 [7].\u000a    On 27\/5\/13 Barrow gave the two opening talks on the Cosmological\u000a        Constant at the Stockholm International Workshop for Science Writers\u000a      organised by George Musser (Senior Editor, Scientific American magazine)\u000a      to inform science writers about cosmology [8].\u000a    ","ImpactSummary":"\u000a    This case describes the impact of Professor John D. Barrow's research\u000a      into the value of the cosmological constant and the constants of physics.\u000a      The impact occurred through extensive engagement with the public via talks\u000a      and public lectures, online talks and articles, newspaper and magazine\u000a      articles, books and radio broadcasts. The engagement led to widespread\u000a      public interest and increased public discourse in the UK and beyond, the\u000a      impact being especially strong in Italy. The research has inspired young\u000a      people, teachers and the general public internationally to follow\u000a      cosmological developments.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of Cambridge\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2673730","Name":"Stockholm"},{"GeoNamesId":"3169070","Name":"Rome"}],"References":"\u000a    \u000aJ.D. Barrow 2007 A Strong Constraint on Ever-Present Lambda, Phys. Rev. D\u000a      75, 067301, DOI: 10.1103\/PhysRevD.75.067301\u000a    \u000a\u000a*D.J. Shaw &amp; J.D. Barrow 2010 A Testable Solution of the Cosmological\u000a      Constant and Coincidence Problems, Phys. Rev. D 83, 04351, DOI:\u000a      10.1103\/PhysRevD.83.043518\u000a    \u000a\u000a*J.D. Barrow &amp; D.J. Shaw 2011 New solution of the cosmological\u000a      constant problems, Phys. Rev. Lett. 106, 101302, DOI:\u000a      10.1103\/PhysRevLett.106.101302\u000a    \u000a\u000a*H. Sandvik, J.D. Barrow, and J. Magueijo, 2002 A simple varying-alpha\u000a      cosmology, Phys. Rev. Lett. 88, 031302, DOI :\u000a      10.1103\/PhysRevLett.88.031302\u000a    \u000a\u000aJ.D. Barrow, H. Sandvik and J. Magueijo, 2002. The Behaviour of\u000a      varying-alpha cosmologies, Phys. Rev. D 65, 063504, DOI:\u000a      10.1103\/PhysRevD.65.063504\u000a    \u000a* References which best represent the quality of the underpinning\u000a      research.\u000a    ","ResearchSubjectAreas":[{"Level1":"2","Level2":"1","Subject":"Astronomical and Space Sciences"},{"Level1":"2","Level2":"2","Subject":"Atomic, Molecular, Nuclear, Particle and Plasma Physics"},{"Level1":"11","Level2":"9","Subject":"Neurosciences"}],"Sources":"\u000a    1) Audience feedback from `Expanding Minds and Expanding Universes', the\u000a      British Science Association Mathematics section Presidential Lecture at\u000a      Aberdeen provided by the section recorder\u000a    2) Listening figures for BBC Radio 4 In Our Time provided by Producer\u000a    3) Information on Professor Barrow's event `La Musica del Vuoto'\u000a      provided by Director of Etcaetera Consulting\u000a    4) Data on articles and article-interviews in the online e-publication\u000a      PLUS\u000a      (www.plus.maths.org)\u000a    5) Information on book translations provided by Foreign Rights Manager,\u000a      Random House\u000a    6) Gresham website downloads information provided by Gresham College IT\u000a      support\u000a    7) `A Matter of Time', Scientific American (volume 21, no.1 pp.\u000a      70-77), Spring 2012\u000a    8) 27\/5\/13 two opening talks on the Cosmological Constant at the\u000a      Stockholm International Workshop for Science Writers: http:\/\/prime-spot.de\/ww13\/index.html.\u000a    \u000a    ","Title":"\u000a    Cosmological Constants\u000a    ","UKLocation":[{"GeoNamesId":"2656241","Name":"Barrow in Furness"},{"GeoNamesId":"2644531","Name":"Lichfield"},{"GeoNamesId":"2643743","Name":"London"},{"GeoNamesId":"2649024","Name":"Frome"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The Nobel-prize winning discovery of the acceleration of the Universe in\u000a      1998, and its attribution to the presence of a non-zero cosmological\u000a      constant in the law of gravity, led to an effort by cosmologists to\u000a      understand its very unusual small value (10-121 in natural\u000a      `Planck' units). This is the biggest problem in fundamental physics. The\u000a      best effort before this to `explain' it was the anthropic bound, first\u000a      found by Professor Barrow (Professor of Mathematical Sciences, University\u000a      of Cambridge Department of Applied Mathematics and Theoretical Physics\u000a      since 1999) in 1986, long before the observation of the cosmic\u000a      acceleration, which showed that a value larger than about 10-120\u000a      would have prevented the formation of galaxies and stars in the past.\u000a      Barrow's work provided a new way to understand the unusual observed value\u000a      for the cosmological constant. More importantly, it also precisely\u000a      predicted the expected sign and value of the parameter that determines the\u000a      fraction of the energy density in the universe that contributes to the\u000a      curvature of space. This is a consequence of the prediction of the value\u000a      of the cosmological constant's value and takes into account the small\u000a      variations in the curvature of space created by the presence of galaxies\u000a      and clusters. This numerical prediction remains consistent with the\u000a      detailed Planck satellite mission data first announced in April 2013, with\u000a      further detail expected in the next 12 months.\u000a    This research was the first to make a definite prediction of the value of\u000a      the cosmological constant. It was able to do this by a small modification\u000a      to the process which derives Einstein's equations of general relativity by\u000a      a variational principle. It includes only variations that are causally\u000a      connected to us in space and time. This creates an extra constraint\u000a      equation, in addition to Einstein's classic equations. When evaluated in\u000a      our universe, allowing for the presence of galaxies, it leads to a\u000a      numerical deduction of the allowed value of the cosmological constant as\u000a      the reciprocal of the square of the age of the universe in Planck units\u000a      (10-121) and also predicts the curvature parameter of the\u000a      universe to -0.0055.\u000a    Barrow and collaborators have followed a long programme of observational\u000a      and theoretical work in the period from 1999 to the present which has used\u000a      observations of quasars to establish, first in 1998, the strongest limits\u000a      on any allowed time variation of certain constants of nature, and then in\u000a      1999 and 2001, to find the first evidence for a very slow variation of the\u000a      fine structure constant. This variation was too small to be detectable in\u000a      any laboratory experiment. This research has led to the development of a\u000a      whole field of astronomical study of varying constants that did not exist\u000a      beforehand. It showed how astronomical observations can give more precise\u000a      information about fundamental physics than laboratory experiments. This is\u000a      possible because of the enormous time that light has spent travelling from\u000a      distant quasars to our telescopes. In effect, we can observe what the laws\u000a      of physics were like more than ten billion years ago. There is an\u000a      important link between this work and the study of the cosmological\u000a      constant and the observed acceleration of the universe because it was\u000a      found in this work that there cannot be a time variation in the fine\u000a      structure constant after time when the expansion of the universe starts to\u000a      accelerate.\u000a    This research in 1999 devised a new way to compare the separations\u000a      between the wavelengths of particular pairs of spectral lines that appear\u000a      in the spectrum of light from distant quasars with the separations found\u000a      when the same spectrum is observed in the laboratory. This can be done\u000a      with unparalleled sensitivity using astronomical detectors and the shifts\u000a      in the separations for many pairs of lines can be compared with detailed\u000a      computations of what the separations will be if tiny differences in the\u000a      value of the fine structure constant exist between the time when the light\u000a      left the quasar and its reception on Earth. This work has consistently\u000a      found a relative shift of five parts in a million over about 13 billion\u000a      years from the observation of hundreds of quasars and more than 1000\u000a      spectral lines using different detectors and different telescopes.\u000a    "},{"CaseStudyId":"17506","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The major impact of this research has been its use by one of the\u000a      companies in the EPSRC\u000a      consortium. Xaar is the world-leading independent supplier of industrial\u000a      inkjet printheads and uses\u000a      a drop-on-demand mode. As a direct result of the research described above\u000a      the company was\u000a      provided with new understandings of the ways in which the inks they use\u000a      behave when they are\u000a      emitted from nozzles and impact on paper. It was also provided with new\u000a      validated numerical\u000a      codes that allow it to predict quantitatively the behaviour of the ink\u000a      depending on its viscoelastic\u000a      properties. Consequently, Xaar is able to make informed design decisions\u000a      based on validated\u000a      models of the ink behaviour, which have the capability to improve the\u000a      design and performance of\u000a      the printers.\u000a    Xaar has used the understanding of visco-elasticity to develop new\u000a      combinations of printer heads\u000a      and inks, and to identify rogue inks that underperform. While it is\u000a      difficult to place a quantitative\u000a      economic benefit to these developments, it is clear that the development\u000a      of new printer heads and\u000a      inks has both given the company a competitive advantage and provided a\u000a      better performance to its\u000a      customers. For example, Xaar provides an `ink optimisation process' to\u000a      design inks for specific\u000a      applications based on Professor Hinch's research. Former Technical\u000a      Director of Xaar and\u000a      consultant during the impact period writes:\u000a    \"these activities have been quite remarkably successful, and are\u000a        founded on the understanding\u000a        and development of rheological theory by Professor John Hinch who\u000a        supervised Jocelyn Etienne,\u000a        Oliver Harlen and Neil Morrison (the latter two now at Leeds University\u000a        - part of the consortium)\u000a        who have developed simulations of ink jetting for the consortium. As\u000a        well as this, Professor Hinch\u000a        has advised on all aspects of understanding of rheology and surface\u000a        tension effects during the\u000a        course of the two projects. Two instrumentation techniques have emerged\u000a        from this as well as the\u000a        successful simulation of jetting.\"\u000a      He further adds that\u000a    \"The Ink Jet Research Centre [of which Professor Hinch is a member] at\u000a        the Institute for\u000a        Manufacturing was set up to address these problems and has formed a\u000a        consortium of academic\u000a        institutions and commercial partners. The commercial partners together\u000a        represent more than &#163;1B\u000a        of annual business for the UK (much of it based in the Cambridge area).\"\u000a      [1]\u000a    Xaar's products sell worldwide and they currently have a turnover of 86M\u000a      and invest between 8% -\u000a      10% in R&amp;D. [2]\u000a    ","ImpactSummary":"\u000a    Research carried out by Professors Hinch and Rallison at the University\u000a      of Cambridge determined\u000a      how ink jet printer fluids behave when emitted from the printer head. The\u000a      research findings have\u000a      been used by industry to optimise the design of the printer. Xaar, the\u000a      world-leading independent\u000a      supplier of industrial inkjet printheads which uses a drop-on-demand mode,\u000a      has used the results of\u000a      this research to improve the design and operation of its ink-jet printers\u000a      to its own commercial\u000a      benefit and to the benefit of the users of its printers.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Cambridge\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a1*. Day, Richard F., Hinch, E. John &amp; Lister, John R. 1998\u000a      Self-similar capillary pinchoff of an\u000a      inviscid fluid. Phys. Rev. Lett. 80, 704-707, DOI:\u000a      10.1103\/PhysRevLett.80.704. (98 cites)\u000a    \u000a\u000a2*. Castrejon-Pita, J.R., Castrejon-Pita, A.A., Hinch, E.J., Lister, J.R.\u000a      and Hutchings, I.M. (2012)\u000a      Self-similar Breakup of Nearly-inviscid Fluids. Phys. Rev. E. 86,\u000a      015301, DOI:\u000a      10.1103\/PhysRevE.86.015301.\u000a    \u000a\u000a3*. Entov, V.M. &amp; Hinch, E.J. 1997 The effect of a spectrum of\u000a      relaxation times on the capillary\u000a      thinning of a filament of elastic liquid J. Non-Newtonian Fluid Mech. 72,\u000a      31-54, DOI:\u000a      10.1016\/S0377-0257(97)00022-0. (107 cites)\u000a    \u000a\u000a4. Harlen, O.G., Rallison, J.M. &amp; Szabo, P. (1995) A split\u000a      Lagrangain-Eulerian method for\u000a      simulating transient viscoeasltic flows. J. Non-Newtonian Fluid Mech. 60,\u000a      81-104, DOI:\u000a      10.1016\/0377-0257(95)01381-5.\u000a    \u000a\u000a5. Szabo, P., Rallison, J.M. &amp; Hinch, E.J. 1997 Start-up of flow of a\u000a      FENE-fluid through a $:1:4\u000a      constriction in a tube. J. Non-Newtonian Fluid Mech. 72, 73-86,\u000a      DOI: 10.1016\/S0377-\u000a      0257(97)00023-2.\u000a    \u000a\u000a6. Etienne, J., Hinch, E.J. &amp; Li, J. 2006 A Lagrangian-Eulerian\u000a      approach for the numerical\u000a      simulation of free-surface flow of a viscoelastic material. J.\u000a      Non-Newtonian Fluid Mech. 136, 157-\u000a      166, DOI: 10.1016\/j.jnnfm.2006.04.003.\u000a    \u000a*Research references which best reflect the quality of the underpinning\u000a      research.\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"}],"Sources":"\u000a    [1] Statement from former Technical Director of Xaar\u000a    [2] Xaar annual report 2012\u000a    \u000a    ","Title":"\u000a    Ink Jet Printing\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Professor E.J. Hinch (Lecturer 1975-1994, Reader 1994-1998, Professor\u000a      1998-date), and his\u000a      research group including Professor J.M. Rallison (Lecturer 1985-1998,\u000a      Reader 1998-2007,\u000a      Professor 2007-date), at the University of Cambridge Department of Applied\u000a      Mathematics and\u000a      Theoretical Physics (DAMTP) have conducted fundamental research on the\u000a      behaviour of visco-\u000a      elastic fluids; the microstructural origin of their rheology (20 papers),\u000a      the flow dynamics resulting\u000a      from that rheology (20 papers), and how to compute such flows (6 papers).\u000a      Hinch has also been\u000a      interested in how jets break up into drops (3 papers).\u000a    Polymers are added to printing inks to inhibit undesirable splattering\u000a      when a drop hits the paper.\u000a      Adding too much polymer, however, can stop a jet breaking up into drops.\u000a      The essential\u000a      mechanism is that the polymers alter the surface tension forces that\u000a      influence the way a\u000a      continuous jet breaks into drops. The way that polymers affect the\u000a      capillary forces squeezing a jet\u000a      was investigated by Hinch in [Ref 3] in 1997.\u000a    In order to provide useful results accurate computational models are\u000a      needed. Computing\u000a      viscoelastic flows has a long history of failures. The problem lies in a\u000a      proper mathematical\u000a      treatment of the hyperbolic equation governing the evolution of the\u000a      elastic part of the stress tensor.\u000a      From 1995 to 1997 Hinch and Rallison developed an appropriate method to\u000a      compute these flows\u000a      in a simplified axisymmetric geometry [Refs 4 and 5] and later extended\u000a      the axisymmetric code to\u000a      a fully three-dimensional code to account for variations associated with\u000a      nozzle geometries. That\u000a      research led to the discovery by numerical simulations of a drop-on-demand\u000a      printer of a very\u000a      curious way in which drops are formed as a jet breaks up [Ref 1]. The\u000a      predictions have been\u000a      recently confirmed by experiments performed [Ref 2].\u000a    The viscoelastic codes developed by Rallison were for applications where\u000a      inertia is negligible. The\u000a      ink jet printing application, however, has significant inertia. The\u000a      effects of inertia were added to\u000a      Rallison's code by O.G. Harlen and N. Morrison, former students of\u000a      Rallison and Hinch, working\u000a      together at the University of Leeds. This work was independently verified\u000a      by Hinch with J. Etienne\u000a      (Research Associate at DAMTP from 2005 to 2007) who added viscoelastic\u000a      effects to a pre-\u000a      existing inertial code [Ref 6].\u000a    This research was supported through a major collaboration with the\u000a      ink-jet printing industry, with 3\u000a      EPSRC grants involving 5 UK university departments and 8 UK companies. The\u000a      aim of the\u000a      collaboration was to understand scientific issues in ink-jet-printing and\u000a      to improve the design,\u000a      operation and performance of ink-jet printers.\u000a    "},{"CaseStudyId":"17508","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"2017370","Name":"Russia"},{"GeoNamesId":"3144096","Name":"Norway"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The research has had significant impacts through summer schools and\u000d\u000a      public engagement, and the most significant impact has been through the\u000d\u000a      development (by Dalziel Research Partners) of commercially-available,\u000d\u000a      user-friendly, flexible software tools.\u000d\u000a    The development of particle tracking velocimetry and dye attenuation\u000d\u000a      techniques drove the development (by Dalziel Research Partners) and\u000d\u000a      marketing (through Cambridge Environmental Research Consultants Ltd.) of\u000d\u000a      DigImage, a video-recorder based image processing system designed\u000d\u000a      specifically for analysing fluid mechanics laboratory experiments.\u000d\u000a      DigImage incorporated both specific techniques from Dalziel's published\u000d\u000a      research and solutions to many technological hurdles that Dalziel\u000d\u000a      overcame. Although development of DigImage ceased in 1999, its impact on\u000d\u000a      the community was such that it continues to be used in at least three\u000d\u000a      laboratories outside Cambridge and has had continued impact as new results\u000d\u000a      obtained using it continue to be published (e.g. Ansong &amp; Sutherland\u000d\u000a      2010; Corner et al. 2011; Read &amp; Risch 2011) and otherwise influence\u000d\u000a      science and technology (e.g. the incorporation of ideas originating in\u000d\u000a      DigImage in a recent patent; Dey et al. 2010).\u000d\u000a    To accommodate the development of synthetic schlieren along with the\u000d\u000a      transitions to digital video and 32-bit Windows, Dalziel replaced DigImage\u000d\u000a      with DigiFlow in 2005. This new system (now available directly from\u000d\u000a      Dalziel Research Partners with licences costing up to &#163;14,000) has been\u000d\u000a      widely adopted internationally, with in excess of 370 licences being\u000d\u000a      issued to research laboratories at 163 sites across 36 countries (around\u000d\u000a      160 of these over the period 2008 to June 2013). Based on the Times Higher\u000d\u000a      Education Rankings for in 2012\/13, DigiFlow is used in seven of the top\u000d\u000a      ten and over half of the top twenty universities (these figures apply\u000d\u000a      whether considering the rankings for Engineering, Physical Sciences or\u000d\u000a      Reputation). The installed base of DigiFlow users continues to grow with\u000d\u000a      major new installations during 2012 in China, Russia and two Baltic\u000d\u000a      States, in addition to the more traditional western economies.\u000d\u000a    The flexibility and capability of the tools developed through Dalziel's\u000d\u000a      research is also reflected in recent publications by DigiFlow users. These\u000d\u000a      span the range of fluid flows ranging from natural ventilation to oceanic\u000d\u000a      internal waves. They continue to exploit Dalziel's key underpinning\u000d\u000a      research of velocity measurement (Drazen et al. 2011; Park et\u000d\u000a        al. 2012), dye attenuation (e.g. Hunt &amp; Coffey 2010) and\u000d\u000a      synthetic schlieren (Peacock et al. 2008; Wang et al.\u000d\u000a      2012), although a complete list of the uses to which DigiFlow is put is\u000d\u000a      much broader.\u000d\u000a    The influence of Dalziel's research, through its commercialisation by\u000d\u000a      Dalziel Research partners, has had an enduring impact. As one academic\u000d\u000a      user (Professor of Fluid Dynamics, University of Dundee) says, \"I have\u000d\u000a        colleagues and collaborators at other institutions across the UK and all\u000d\u000a        over the world who have had a similar experience and come to utilise or\u000d\u000a        rely on DigiFlow for many of their laboratory diagnostics.... the use of\u000d\u000a        DigImage and DigiFlow has had a profound, continuing impact not only on\u000d\u000a        my own research, but the geophysical and environmental fluid dynamics\u000d\u000a        communities as a whole across the world\". This influence extends\u000d\u000a      beyond academia, with industrial use of DigiFlow ranging from the oil\u000d\u000a      industry to ship building. Here, the advanced algorithms and pioneering\u000d\u000a      techniques developed by Dalziel are particularly valued: \"We have\u000d\u000a        invested in very expensive equipment and software for Particle Image\u000d\u000a        Velocimetry, but have additionally chosen to invest in DigiFlow on the\u000d\u000a        grounds of a higher quality software with superior capabilities and\u000d\u000a        better access to support... We are applying this technique [synthetic\u000d\u000a        schlieren] to measure both surfactants and crystals... which are of\u000d\u000a        extremely high industrial importance in many process systems.\"\u000d\u000a      (Department Head, Process Technology and Fluid Flow, Institute for Energy\u000d\u000a      Technology, Norway). Indeed, even the first techniques developed by\u000d\u000a      Dalziel are finding new uses in industry as illustrated by\u000d\u000a      Schlumberger-Doll Research Center's recent purchase (July 2013) of\u000d\u000a      DigiFlow to undertake particle tracking measurements.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Mathematically-based image processing techniques developed at the\u000d\u000a      University of Cambridge have helped bring about a revolution in the\u000d\u000a      ability to extract quantitative measurements from laboratory experiments\u000d\u000a      in fluids. Techniques and software tools developed from this research and\u000d\u000a      incorporated into commercial software are now used in engineering, physics\u000d\u000a      and mathematics research laboratories around the world on projects ranging\u000d\u000a      from fundamental research to ones with strong industrial connections.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Cambridge\u000d\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000aParticle tracking velocimetry\u000d\u000a      *Dalziel, S.B. 1993 Rayleigh-Taylor instability: experiments with image\u000d\u000a      analysis; Dyn. Atmos. Oceans, 20 127-153, DOI:\u000d\u000a      10.1016\/0377-0265(93)90051-8.\u000d\u000a    \u000a\u000aDye attenuation\u000d\u000a      Hacker, J., Linden, P.F. &amp; Dalziel, S.B. 1996 Mixing in lock-release\u000d\u000a      gravity currents; Dyn. Atmos. Oceans 24, 183-195, DOI:\u000d\u000a      10.1016\/0377-0265(95)00443-2.\u000d\u000a    \u000a\u000aSynthetic schlieren\u000d\u000a      *Sutherland, B.R., Dalziel, S.B., Hughes, G.O. &amp; Linden, P.F. 1999\u000d\u000a      Visualisation and Measurement of internal waves by \"synthetic schlieren\".\u000d\u000a      Part 1: Vertically oscillating cylinder; J. Fluid Mech. 390,\u000d\u000a      93-126, DOI: 10.1017\/S0022112099005017.\u000d\u000a    \u000a\u000a*Dalziel, S.B., Hughes, G.O. &amp; Sutherland, B.R. 2000 Whole field\u000d\u000a      density measurements by `synthetic schlieren'; Exp. Fluids 28,\u000d\u000a      322-335, DOI: 10.1007\/s003480050391\u000d\u000a    \u000a\u000aDalziel, S.B., Carr, M., Sveen, K.J. &amp; Davies, P.A. 2007 Simultaneous\u000d\u000a      Synthetic Schlieren and PIV measurements for internal solitary waves. Meas.\u000d\u000a        Sci. Tech. 18, 533-547, DOI: 10.1088\/0957- 0233\/18\/3\/001.\u000d\u000a    \u000a\u000aHazewinkel, J., Maas, L.R.M. &amp; Dalziel, S.B. 2011 Tomographic\u000d\u000a      reconstruction of internal wave patterns in a paraboloid. Exp. Fluids\u000d\u000a      50, 247-258, DOI: 10.1007\/s00348-010-0909-x\u000d\u000a    \u000a*References which best represent the quality of the underpinning research\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"2","Level2":"99","Subject":"Other Physical Sciences"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"}],"Sources":"\u000d\u000a    DigImage\u000d\u000a      Recent references demonstrating continued use despite development\u000d\u000a        ceasing in 1999\u000d\u000a    Ansong, J. K., &amp; Sutherland, B. R. 2010 Internal gravity waves\u000d\u000a      generated by convective plumes. J. Fluid Mech. 648, 405.\u000d\u000a    Read, P. L., &amp; Risch, S. H. 2011 A laboratory study of global-scale\u000d\u000a      wave interactions in baroclinic flow with topography I: multiple flow\u000d\u000a      regimes. Geophysical &amp; Astrophysical Fluid Dynamics 105(2-\u000a      3), 128-160.\u000d\u000a    Dey, S., Wong, D., Wen, J., Takebuchi, Y. Arole, P. &amp; Panigrahi, D.\u000d\u000a      2010 Digital content buffering for adaptive streaming. United States\u000d\u000a        Patent US007743161B2\u000d\u000a    DigiFlow\u000d\u000a      Selection of recent publications where DigiFlow has provided the main\u000d\u000a        diagnostics\u000d\u000a    Hunt, G. R., &amp; Coffey, C. J. 2010 Emptying boxes-classifying\u000d\u000a      transient natural ventilation flows. J. Fluid Mech. 646,\u000d\u000a      137.\u000d\u000a    Drazen, D., Lichtsteiner, P., H&#228;fliger, P., Delbr&#252;ck, T., &amp; Jensen,\u000d\u000a      A. 2011 Toward real-time particle tracking using an event-based dynamic\u000d\u000a      vision sensor. Exp. Fluids 51, 1465.\u000d\u000a    Park, Y. S., Liu, P. L. F., &amp; Chan, I. C. 2012 Contact line dynamics\u000d\u000a      and boundary layer flow during reflection of a solitary wave. J. Fluid\u000d\u000a        Mech. 707, 307.\u000d\u000a    Peacock, T., Echeverri, P., &amp; Balmforth, N. J. 2008 An experimental\u000d\u000a      investigation of internal tide generation by two-dimensional topography. J.\u000d\u000a        Phys. Ocean., 38, 235.\u000d\u000a    Wang, T., Chen, X., &amp; Jiang, W. 2012 Laboratory experiments on the\u000d\u000a      generation of internal waves on two kinds of continental margin. Geophys.\u000d\u000a        Res. Let. 39, L04602.\u000d\u000a    User testimonials\u000d\u000a      Illustrating the value to both academic and industrial users of\u000d\u000a        DigiFlow.\u000d\u000a    Department Head, Process Technology and Fluid Flow, Institute for Energy\u000d\u000a      Technology (Norway)\u000d\u000a    Professor of Fluid Dynamics, Civil Engineering, University of Dundee (UK)\u000d\u000a    ","Title":"\u000d\u000a    Image Processing for Fluid Mechanics\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Fluid mechanics underpins many natural and man-made processes, and the\u000d\u000a      immense challenge of understanding fluid flows relies on theoretical\u000d\u000a      developments confirmed by experimental measurements. Dr Stuart Dalziel was\u000d\u000a      one of the first to recognise the potential for extracting non- intrusive\u000d\u000a      quantitative information from sequences of images for laboratory\u000d\u000a      experiments in this context. Dalziel joined the University of Cambridge\u000d\u000a      Department of Applied Mathematics and Theoretical Physics (DAMTP) in 1989\u000d\u000a      as a Postdoctoral Research Associate and has remained in the Department\u000d\u000a      throughout the period (Assistant Director of Research from 1994, Lecturer\u000d\u000a      from 2000, Senior Lecturer from 2001 and Reader from 2012 to present). In\u000d\u000a      1993, using ideas borrowed from operations research and graph theory,\u000d\u000a      Dalziel developed not only one of the first practical ways of determining\u000d\u000a      the velocity field through particle tracking velocimetry (PTV; Dalziel\u000d\u000a      1993), but also developed the first system that could be used readily by\u000d\u000a      others.\u000d\u000a    While most pioneers of image processing for fluid experiments focused\u000d\u000a      solely on velocity measurements, Dalziel recognised the importance of\u000d\u000a      measuring other fields with comparable fidelity. Consequently, his ideas\u000d\u000a      for the use of image processing as a laboratory tool expanded during the\u000d\u000a      mid-1990s to provide a viable method for measuring the spatial structure\u000d\u000a      and temporal evolution of density and\/or depth fields based on light\u000d\u000a      attenuation through dyes (Hacker et al. 1996). Although this\u000d\u000a      technique proved highly successful, a desire to measure much weaker\u000d\u000a      signals without the need to add any additional species led Dalziel to\u000d\u000a      invent `synthetic schlieren', with the first results published in 1998.\u000d\u000a      This technique (Dalziel et al. 2000), based on the quantification\u000d\u000a      of minute changes in the refractive index, is capable of measuring density\u000d\u000a      fields (Sutherland et al. 1999) with unprecedented accuracy and\u000d\u000a      spatial resolution. Dalziel has continued to develop synthetic schlieren\u000d\u000a      over the last ten years to allow simultaneous density and velocity\u000d\u000a      measurements (Dalziel et al. 2007), and to provide full\u000d\u000a      three-dimensional measurement of density fields (Hazewinkel et al.\u000d\u000a      2011).\u000d\u000a    "},{"CaseStudyId":"17512","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Science and Technology Facilities Council","Particle Physics and Astronomy Research Council"],"ImpactDetails":"\u000a    Since 2008, Dr Mason has given over 15 public lectures at teachers'\u000a      conferences (Institute of Physics, IoP, Association of Science Educators,\u000a      ASE), schools and astronomy societies, in which she discussed solar space\u000a      observations (SoHO, Hinode, SDO) and her research results, including those\u000a      on the Sun's magnetic field, solar wind, active regions, flares and CMEs,\u000a      and how CHIANTI has been used to analyse the UV observations.\u000a    It is clear from the positive feedback that Dr Mason's engagement\u000a      activity has informed and inspired teachers, students and the general\u000a      public. Sample feedback includes: 'Thanks again for a brilliant talk. I've\u000a      had some great feedback and you could tell by the questions how engaged\u000a      the audience was!' (teacher, Kesgrave High School, Ipswich, 2013, 50+)[7].\u000a      In July 2013, IoP Director of Science and Education wrote of Dr Mason,\u000a      \"Her talks have helped improve student and teacher understanding of solar\u000a      physics; and in particular the solar space projects, SoHO and Hinode.\u000a      Teachers were also given supporting materials and ideas to help them teach\u000a      this topic in the classroom. He also adds \"We believe that the future of\u000a      physics is very much in the hands of physics teachers and so are committed\u000a      to providing all the support we can. It is particularly pleasing when we\u000a      can do this in partnership with colleagues such as Dr Mason who has such a\u000a      wealth of experience in the field of solar space projects.\" [6]\u000a    Since 2008, Dr Mason has also given several high profile public lectures,\u000a      including a Friday Evening Discourse on `Our Dynamic Sun' at the Royal\u000a      Institution. This detailed her solar research with Hinode\/EIS, SDO and\u000a      CHIANTI (82% enjoyed the lecture, 73% learnt something new, 64% wanted to\u000a      find out more about her research, 2013, 250+)[1]. She also gave a\u000a      Rutherford Appleton Laboratory Talking Science lecture (65%\u000a      Strongly agreed that the talk was enjoyable and interesting 2013, 80+)\u000a      [2]. She has lectured twice at the INTECH Science Centre (2009, 120+;\u000a      2011, 170+; 'Dr Mason's talks are ideal for our Space Lecture series...\u000a      bringing in up-to-the-minute research and developments', INTECH Manager)\u000a      [3].\u000a    Dr Helen Mason launched Sun|trek, an educational website about the Sun\u000a      and its effect on the Earth's environment, in 2007. The site is directly\u000a      underpinned by the research carried out by Dr Mason and her group\u000a      described above. It features sections on solar space observatories (SoHO,\u000a      Hinode, SDO) and CHIANTI. Key research results which feature on Sun|trek\u000a      include: the heating of solar active regions and flares studied with SDO\u000a      and Hinode (e.g. iSun|trek `The Sun gets active with X-class Flares' and\u000a      the Sun|trek Hinode section). New Sun|trek Classroom projects released in\u000a      2012 use real solar space data (e.g. `Spectra: Solar fingerprints'\u000a      contains spectra from Hinode\/EIS, with CHIANTI spectral identifications).\u000a    The impact of the Sun|trek project has by 2012 been far reaching &#8212; around\u000a      300,000 individuals per annum. Below are sample stats from Google\u000a      Analytics for the Sun|trek website in 3 month groups from October 2010\u000a      until March 2012 (i.e. 10-12 2011 is Oct-Dec 2011 etc.). Visits denotes\u000a      all visits, unique visits are separate individuals.\u000a\u000a\u000a\u000a    The 2012 enhancement, iSun|trek (www.suntrek.org\/blog) links to\u000a      modern social communication networks (Facebook, YouTube, Twitter etc.).\u000a      The Sun|trek Facebook site, has topical information highlighting research\u000a      achievements (e.g. with SoHO, Hinode, SDO), reaching 2,500 individuals per\u000a      week by mid-2013. Since 2008, Sun|trek has been linked to many other\u000a      educational websites (e.g. STEM, RAS, STFC, ASE, IoP, International Year\u000a      of Astronomy (2009\/10), SunEarthPlan, Schoolscience, ESERO, BBC Science).\u000a    Dr Mason has presented her research themes through her work with the\u000a      media (BBC TV and radio). She featured in the BBC4 programme Seven\u000a        Ages of Starlight (with recent observations from SDO), which was\u000a      nominated for a BANFF Rockie award. This was broadcast in 2012 (reaching\u000a      795,000) and again in June 2013. The Audience Appreciation Index was 87,\u000a      significantly higher than the average score for a factual programme [4].\u000a      The Times wrote: 'Everyone who isn't an astronomer or a theoretical\u000a      physicist should watch Gaby Hornsby's incredible film... It is one of the\u000a      richest and most informative science programmes I have ever seen'. Dr\u000a      Mason also worked on Science Britannica (BBC2, 2013).\u000a    In 2010, Dr Mason was named as one of the UKRC's Women of Outstanding\u000a      Achievement for `her inspirational work in communication within SET and\u000a      her contribution to discovery in her field' [5]. The prize, a photo\u000a      portrait of Dr Mason, hangs at the IoP headquarters as an inspiration to\u000a      young females. She was awarded an STFC Science in Society Fellowship (Oct.\u000a      2011 &#8212; March 2012) in recognition of the high profile and impact of her\u000a      outreach work.\u000a    ","ImpactSummary":"\u000a    Research carried out by Dr Helen Mason, University of Cambridge, on solar\u000a      space projects such as SoHO, Hinode and the Solar Dynamics Observatory\u000a      (SDO), led to increased public interest in astronomy, space science,\u000a      physics and mathematics, and has inspired school students to study science\u000a      subjects, which should ultimately enhance the UK's technical and\u000a      scientific expertise. This impact was achieved via sustained engagement\u000a      activities including public lectures, work with the media and the Sun|trek\u000a      project. Sun|trek (www.suntrek.org)\u000a      is an educational website informed by Dr Mason's research targeted at UK\u000a      teachers and school students about the Sun and its effect on the Earth's\u000a      environment. Sun|trek also attracted a large user base in the USA,\u000a      Australia, India and worldwide.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of Cambridge\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000aDel Zanna, G.; Mitra-Kraev, U.; Mason, H.E. et al, 2011, The 22 May\u000a        2007 B-class flare: new insights from Hinode observations, A&amp;A,\u000a      526, 1, DOI: 10.1051\/0004-6361\/201014906\u000a    \u000a\u000aTripathi, D.; Mason, H.E.; Klimchuk, J.A., 2010, Evidence of\u000a        Impulsive Heating in Active Region Core Loops, ApJ, 723,\u000a      713, DOI: 10.1088\/0004-637X\/723\/1\/713\u000a    \u000a\u000a*O'Dwyer, B.; G. Del Zanna; H. E. Mason et al, 2010, SDO\/AIA response\u000a        to quiet Sun, coronal hole, active region and flare plasma, A&amp;A,\u000a      v. 521, A21, DOI: 10.1051\/0004-6361\/201014872 (44 citations)\u000a    \u000a\u000a*Tripathi, D., H. E. Mason et al 2009, Active Region Loops &#8212;\u000a        Hinode\/EIS Observations, ApJ, v.694, p1256, DOI:\u000a      10.1088\/0004-637X\/694\/2\/1256 (45 citations)\u000a    \u000a\u000a*Dere, K. P.; Landi, E.; Young, P. R.; Del Zanna, G.; Landini, M.; Mason,\u000a      H. E., 2009, CHIANTI &#8212; an atomic database for emission lines. IX.\u000a        Ionization rates, recombination rates, ionization equilibria for the\u000a        elements hydrogen through zinc and updated atomic data, A&amp;A, v.\u000a      498, p915, DOI: 10.1051\/0004-6361\/200911712 (578 citations)\u000a    \u000a\u000aChifor, C.; ; Mason, H.E.; Del Zanna, G. Et al, 2008, Magnetic flux\u000a        cancellation associated with a recurring solar jet observed with Hinode,\u000a        RHESSI, and STEREO\/EUVI, A&amp;A, 491, 279, DOI:\u000a      10.1051\/0004-6361:200810265\u000a    \u000a*References which best represent the quality of the underpinning\u000a      research.\u000a    This research has been supported by STFC (formerly PPARC):\u000a   2012-2014, STFC, Dr G. Oglvie (co-i's, Dr Mason, Dr Del Zanna), &#163;615,415;\u000a      2009-2012, STFC, Pi Prof. J. Papaloizou (co-i's Dr Mason, Dr Del Zanna),\u000a      &#163;1,713,063;\u000a      2006-2011, PPARC, Pi Prof. J. Papaloizou (co-i Dr Mason, PDRA &#8212; Dr\u000a      Tripathi), &#163;971,014. Prior to this Dr Mason held a series of individual\u000a      PPARC Standard Grant awards.\u000a    An international conference was held in 2010: `Solar Plasma\u000a        Spectroscopy &#8212; Achievements and Future Challenges: Celebrating the\u000a        Career of Dr Helen Mason', in recognition of Dr Mason's contribution\u000a      to solar research, in particular her work with SoHO, Hinode and CHIANTI.\u000a    Furthermore, in 2010, the CHIANTI team was awarded the RAS Group\u000a        Achievement Award. Dr Mason was a founder member of the CHIANTI\u000a      team, first released in 1997, and this project has been largely dependent\u000a      on her research contribution. Three out of five members of the Chianti\u000a      team are from Dr Mason's group. The CHIANTI papers have had almost 600\u000a      citations in the past 5 years.\u000a    ","ResearchSubjectAreas":[{"Level1":"2","Level2":"2","Subject":"Atomic, Molecular, Nuclear, Particle and Plasma Physics"},{"Level1":"2","Level2":"1","Subject":"Astronomical and Space Sciences"},{"Level1":"10","Level2":"5","Subject":"Communications Technologies"}],"Sources":"\u000a    [1] Statement from the Director of Science and Education, Royal\u000a      Institution regarding Friday Evening Discourse, April 2013\u000a    [2] Feedback Sheet, STFC, RAL Talking Science, May 2013\u000a    [3] Statement from INTECH Science Centre and Planetarium Manager\u000a      regarding Space Science lectures: 2009, 2011\u000a    [4] Seven Ages of Starlight, BBC4, producer\u000a    [5] RAS feature on Helen Mason, 2010, Women of Outstanding Achievement\u000a      Award: http:\/\/www.ras.org.uk\/news-and-press\/157-news2010\/1743-dr-helen-mason-named-as-one-of-the-uks-outstanding-women\u000a    [6] Statement from IoP Director of Science and Education, including\u000a      details of feedback on Dr Mason's talks\u000a    [7] Statement from Teacher, Kesgrave High School, Ipswich\u000a    ","Title":"\u000a    Solar Space Research and Sun|trek\u000a    ","UKLocation":[{"GeoNamesId":"2646057","Name":"Ipswich"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Dr Helen Mason, University of Cambridge's Department of Applied\u000a      Mathematics and Theoretical Physics (DAMTP), Assistant Director of\u000a      Research throughout the period, has been invited to become an active\u000a      researcher on many solar space projects, including Skylab, the Solar\u000a      Maximum Mission, the Solar and Heliospheric Observatory (SoHO), Hinode and\u000a      the Solar Dynamics Observatory (SDO). Using SoHO, Dr Mason studied many\u000a      solar phenomena and was responsible for identifying the origin of the fast\u000a      solar wind in 1999, which was one of SoHO's key objectives. Since 1998 the\u000a      focus of her research has been on the nature of the heating in active\u000a      regions and flares, together with studies of jets and micro-flares which\u000a      can trigger filament eruptions and Coronal Mass Ejections (CMEs). Dr Mason\u000a      has worked on Hinode, a joint UK\/Japanese\/NASA solar satellite from the\u000a      time of its launch in 2006. She and her group, including postdoctoral\u000a      researchers G. Del Zanna (STFC Advanced Fellow from 2008 to 2012,Senior\u000a      Research Associate from 2012 to present) and D. Tripathi (Research\u000a      Associate from 2008 to 2011), combined observations from Hinode\/EIS (EUV\u000a      Imaging Spectrometer) with those from SDO, launched in 2010, to\u000a      investigate some key questions in solar physics. As members of the\u000a      Hinode\/EIS Science Team, Dr Mason and her group have planned, carried out\u000a      and analysed many Hinode observations. This research work has been\u000a      internationally significant. For example, it has provided important\u000a      insights on the nature of heating in solar active regions and flares, by\u000a      comparing the analysis of observations with theoretical models (Del Zanna\u000a      et al, 2011, Tripathi et al, 2010, Tripathi et al, 2009). Their work has\u000a      favoured the impulsive (nano-flare) heating model, where they have\u000a      produced many high profile publications (see sample references below). Dr\u000a      Mason now co-leads a team of researchers at the International Space\u000a      Science Institute in a workshop series on the Heating of Coronal Loops.\u000a    Since 1998, Dr Mason and her group also studied the initiation (trigger)\u000a      for energetic events on the Sun, such as jets, solar flares and CMEs. They\u000a      found that the emergence and submergence of magnetic field plays a key\u000a      role (Chifor et al, 2008). This area of research was particularly\u000a      important for understanding and predicting space weather events, with\u000a      possible damaging effects on the terrestrial environment.\u000a    Drs Mason and Del Zanna have also developed (with colleagues in the USA)\u000a      a State-of-the-Art atomic database, called CHIANTI (Dere et al, 2009).\u000a      First launched in 1997, CHIANTI is now universally used for the analysis\u000a      of solar space observations. The latest release, v7, was in 2012. Drs\u000a      Mason and Del Zanna have provided new atomic data (wavelengths, radiative\u000a      data and electron collision data) for several ions, in particular those\u000a      observed in the spectrum from the solar corona, for example from\u000a      Hinode\/EIS. These data are a crucial component in CHIANTI.\u000a    "},{"CaseStudyId":"17636","Continent":[],"Country":[],"Funders":["Royal Society"],"ImpactDetails":"\u000d\u000a    Within the Internet control is end-to-end rather than hidden in the\u000d\u000a      network, and the algorithms concerned had been outstandingly successful,\u000d\u000a      as the Internet evolved from a small-scale research network to an\u000d\u000a      interconnection of tens of millions of endpoints and links. However, by\u000d\u000a      the turn of the millennium there were several developments that posed\u000d\u000a      challenges. The huge capacity of the links being deployed together with\u000d\u000a      the desire to carry delay-sensitive traffic was causing an evolution\u000d\u000a      towards a network with much smaller queueing delays, and it was unclear if\u000d\u000a      end-to-end congestion control would remain feasible in such a network. And\u000d\u000a      our demands for reliability and robustness were increasing as the Internet\u000d\u000a      came to be a critical part of the infrastructure of a modern society and\u000d\u000a      economy. The telephony network provided reliability and robustness through\u000d\u000a      the simultaneous use of parallel paths, but would such a development\u000d\u000a      compromise the stability of the Internet, much as \"route-flap\" had in the\u000d\u000a      Internet's early years?The research of Professor Kelly and his\u000d\u000a      collaborators provided a mathematical framework that allowed these\u000d\u000a      challenges to be addressed in systematic and principled manner. The\u000d\u000a      framework allowed congestion control and routing algorithms to be\u000d\u000a      interpreted as a distributed mechanism solving a global optimization\u000d\u000a      problem. The form of the optimization problem made explicit the\u000d\u000a      equilibrium resource allocation policy of the algorithms, which could\u000d\u000a      often be restated in terms of a fairness criterion. And the dynamics of\u000d\u000a      the models allowed the machinery of control theory to be used to study\u000d\u000a      stability, and to develop algorithms that scale to arbitrary capacities.\u000d\u000a    The impact on protocol development and on network architectures has been\u000d\u000a      significant. Detailed evidence is provided by use of the framework and\u000d\u000a      acknowledgements of the research papers in section 3 in standardization\u000d\u000a      documents for protocols (e.g. [7]), technical reports in companies, and\u000d\u000a      patent applications (a Google search for \"patent\" within the more than\u000d\u000a      four thousand citations of [2] gives over eighty results including over\u000d\u000a      twenty patents or patent applications). An indicative summary is given in\u000d\u000a      [8], and we quote from this paper:\u000d\u000a    \"After a decade of work by many researchers, there is now a substantial\u000d\u000a      set of theory, algorithms, applications and even commercialization based\u000d\u000a      on the NUM model of networks.\"\u000d\u000a    \"An analogy can be drawn with Shannon's seminal work in 1948. By turning\u000d\u000a      the focus from the design of finite-block length codes to the regime of\u000d\u000a      infinite-block length codes, thus enabling the Law of Large Numbers to\u000d\u000a      take effect, Shannon's work provided architectural principles (e.g.\u000d\u000a      source-channel-separation) and established fundamental limits (e.g.\u000d\u000a      channel capacity) in his mathematical theory of communication. .....\"A\u000d\u000a      similar development has happened in networking over the past decade. By\u000d\u000a      turning the focus from coupled queuing dynamics to an optimization problem\u000d\u000a      based on deterministic fluid models, thus enabling the views of\u000d\u000a      decomposition and distributed control, Kelly's work [1,2] leads to our\u000d\u000a      current study on architectural principles in networks and a mathematical\u000d\u000a      foundation for network protocol design.\"\u000d\u000a    The comparison with the work of Shannon is clearly an exaggeration, but\u000d\u000a      the quotation accurately reflects the fundamental change in conceptual\u000d\u000a      framework from a microscopic queueing view to a macroscopic optimization\u000d\u000a      view.\u000d\u000a    A former executive at multinational telecommunications equipment company\u000d\u000a      Lucent (and then Lucent-Alcatel) states that Kelly's research within the\u000d\u000a      period \"established the global stability and fairness of TCP\/IP, the basic\u000d\u000a      rate control mechanism of the Internet. It is fair to say that Prof.\u000d\u000a      Kelly's work established the framework in which almost all subsequent work\u000d\u000a      concerning these issues was done. The nature of the impact may be\u000d\u000a      characterized thus: &#8212; it rendered unnecessary other mechanisms, which\u000d\u000a      might have had a deleterious effect on the Internet, &#8212; it rendered\u000d\u000a      unnecessary other research programs, &#8212; it gave confidence to service\u000d\u000a      providers and users that the Internet would remain stable despite rapid\u000d\u000a      growth as long as certain conditions were met, and this was essential for\u000d\u000a      continued investment in the Internet infrastructure\". [13]\u000d\u000a    A senior Microsoft researcher writes \"MultiPath TCP is being standardised\u000d\u000a      by the IETF, and Kelly's work is one of the fundamental pieces of work\u000d\u000a      used in its architectural design, particularly with regard to the control\u000d\u000a      loop. This has been implemented in the Linux kernel, in FreeBSD and\u000d\u000a      recently saw its first large scale deployment on September 18th 2013 when\u000d\u000a      it was part of Apple's IoS7 release &#8212; hence it will be used on 10s of\u000d\u000a      millions of devices. For cloud computing, Data Center TCP (DCTCP)\u000d\u000a      developed by Stanford and Microsoft has been implemented in Linux,\u000d\u000a      prototyped by Microsoft, and also uses insight from Kelly and Voice [6] to\u000d\u000a      design the feedback controller.\" [14]\u000d\u000a    In 2008 Kelly was awarded the John von Neumann Theory Prize of INFORMS\u000d\u000a      for \"his profound contributions to the mathematical theory of stochastic\u000d\u000a      networks, and for applications of these theories to the understanding,\u000d\u000a      performance evaluation, and design of telecommunications networks\"[9]. For\u000d\u000a      other awards citing the impact of this research see [10]. In 2012\u000d\u000a      Professor Kelly was elected a Foreign Associate of the National Academy of\u000d\u000a      Engineering \"for contributions to the theory and optimization of\u000d\u000a      communication networks\"[11].\u000d\u000a    Several of the insights on large scale system behaviour developed in the\u000d\u000a      study of communication networks transfer to help understand cascading\u000d\u000a      failures in other large scale systems, such as transport infrastructures\u000d\u000a      [12].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This case study provides an account of work on a mathematical framework\u000d\u000a      for the design and optimization of communication networks, and some\u000d\u000a      examples of the framework's influence upon the development of the network\u000d\u000a      congestion control schemes that underlie modern communication networks,\u000d\u000a      notably the Internet.\u000d\u000a    The impact on protocol development and on network architectures has been\u000d\u000a      significant; in particular on the development of congestion control\u000d\u000a      algorithms and multipath routing algorithms that are stable and fair.\u000d\u000a      Several of the insights on large scale system behaviour have been\u000d\u000a      transferred to help understand cascading failures in other large scale\u000d\u000a      systems, including transport infrastructures.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Cambridge\u000d\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a[1] \"Charging and rate control for elastic traffic\", F.P. Kelly, European\u000d\u000a      Transactions on Telecommunications, volume 8 (1997) pages 33-37.\u000d\u000a    \u000a\u000a[2]* \"Rate control in communication networks: shadow prices, proportional\u000d\u000a      fairness and stability\", Frank Kelly, Aman Maulloo and David Tan, Journal\u000d\u000a      of the Operational Research Society 49 (1998) 237-252, DOI:\u000d\u000a      10.1057\/palgrave.jors.2600523.\u000d\u000a    \u000a\u000a[3] \"Resource pricing and the evolution of congestion control\", R.J.\u000d\u000a      Gibbens and F.P. Kelly, Automatica 35 (1999) 1969-1985, DOI:\u000d\u000a      10.1016\/S0005-1098(99)00135-1.\u000d\u000a    \u000a\u000a[4] *\"Models for a self-managed Internet\", F.P. Kelly, Philosophical\u000d\u000a      Transactions of the Royal Society A358 (2000) 2335-2348, DOI:\u000d\u000a      10.1098\/rsta.2000.0651.\u000d\u000a    \u000a\u000a[5] *\"Fairness and stability of end-to-end congestion control\", Frank\u000d\u000a      Kelly, European Journal of Control 9 (2003) 159-176, DOI:\u000d\u000a      10.3166\/ejc.9.159-176.\u000d\u000a    \u000a\u000a[6] \"Stability of end-to-end algorithms for joint routing and rate\u000d\u000a      control\", Frank Kelly and Thomas Voice, Computer Communication Review 35:2\u000d\u000a      (2005) 5-12.\u000d\u000a    \u000a*References which best reflect the quality of the underpinning research.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000d\u000a    [7] Evidence of impact on protocol development and standardization of\u000d\u000a      protocols:\u000d\u000a    Multipath TCP Resources: http:\/\/nrg.cs.ucl.ac.uk\/mptcp\/\u000d\u000a    Internet Research Task Force RFC 6077: http:\/\/tools.ietf.org\/html\/rfc6077\u000d\u000a    [8] Evidence of impact on architectural principles and network protocol\u000d\u000a      design:\u000d\u000a    \"Stochastic network utility maximisation - a tribute to Kelly's paper\u000d\u000a      published in this journal a decade ago\", Yung Yi and Mung Chiang, European\u000d\u000a      Transactions on Telecommunications. 2008; 19:421-442\u000d\u000a    [9] Evidence of impact of mathematical framework on the networking\u000d\u000a      community:\u000d\u000a      http:\/\/www.informs.org\/Recognize-Excellence\/Award-Recipients\/Frank-P.-Kelly\u000d\u000a    [10] Evidence of impact on computer and communication systems, and other\u000d\u000a      domains:\u000d\u000a    http:\/\/sigmetrics.org\/achievementaward-2009.shtml\u000d\u000a    http:\/\/www.theorsociety.com\/Pages\/Awards\/Beale.aspx#2011\u000d\u000a    http:\/\/www.tue.nl\/fileadmin\/content\/universiteit\/Academische_plechtigheden\/Dies_Natalis\/2011\/Frank_Kelly.pdf\u000d\u000a    [11] Evidence of engineering impact: http:\/\/www.nae.edu\/56166.aspx\/\u000d\u000a    [12] Evidence of insight transferring to other large-scale systems:\u000d\u000a    \"A national infrastructure for the 21st century\", Council for Science and\u000d\u000a      Technology, 2009. (Page 34, cascade effects):\u000d\u000a      http:\/\/webarchive.nationalarchives.gov.uk\/+\/http:\/\/www.cst.gov.uk\/reports\/files\/national-infrastructure-report.pdf\u000d\u000a    [13] Statement from a former executive at Lucent (and then\u000d\u000a      Lucent-Alcatel) on the impact of Kelly's research in shaping the thinking\u000d\u000a      of the technical community and in the engineering of the Internet.\u000d\u000a    [14] Statement from senior researcher at Microsoft.\u000d\u000a    ","Title":"\u000d\u000a    Design and optimization of communication networks\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    In the 1980s, Professor Kelly (Professor of Mathematics of Systems at the\u000d\u000a      Department of Pure Mathematics and Mathematical Statistics, University of\u000d\u000a      Cambridge 1990 to present) and his collaborators made important\u000d\u000a      contributions to the design of routing algorithms for circuit-switched\u000d\u000a      networks such as telephony networks. In the early 1990s Professor Kelly\u000d\u000a      was involved with the considerable international research effort between\u000d\u000a      telecommunication companies and academics looking for ways to design what\u000d\u000a      were then called multiservice networks, using developments of the models\u000d\u000a      which had been so successful for telephony. It was widely thought in the\u000d\u000a      telecommunications world that packet-switching (as used in the then\u000d\u000a      miniscule Internet) could not scale to support systems comparable in size\u000d\u000a      to the telephony network. This was a view not shared in the computer\u000d\u000a      communication world, but the two worlds spoke different languages.\u000d\u000a    From 1997-2005 Professor Kelly and his co-authors published a series of\u000d\u000a      papers [1-6], based on their research in that period in the University of\u000d\u000a      Cambridge. These papers provided a mathematical framework showing that the\u000d\u000a      important distinction between circuit &#8212; and packet-switched networks was\u000d\u000a      not whether bits are arranged in streams or files, but between different\u000d\u000a      control architectures. The framework made clear, in a precise language\u000d\u000a      comprehensible to both worlds, that the different control architectures\u000d\u000a      corresponded to either a primal or a dual approach to the solution of a\u000d\u000a      certain optimization problem. Algorithms based on the two approaches were\u000d\u000a      shown to be stable about a system optimum characterized by a proportional\u000d\u000a      fairness criterion. Stability was established by showing that, with an\u000d\u000a      appropriate formulation of the overall optimization problem, the network's\u000d\u000a      implicit objective function provided a Lyapunov function for the dynamical\u000d\u000a      system defined by the control algorithm. Both classes of algorithm were\u000d\u000a      generalized to include routing control, and provided natural\u000d\u000a      implementations of proportionally fair pricing. The framework allowed\u000d\u000a      natural treatments of stochastic effects and of time delays, with simple\u000d\u000a      and scalable conditions for fairness and stability that are especially\u000d\u000a      important for wide-area networks where links vary in capacity by many\u000d\u000a      orders of magnitude, as do the propagation delays that are a consequence\u000d\u000a      of geographical diversity and the finite speed of light. The framework\u000d\u000a      made clear that there was no in principle reason why a (primal)\u000d\u000a      architecture based on end-system control (such as the Internet) could not\u000d\u000a      scale to arbitrary size.\u000d\u000a    Research on communication networks is a subtle dialogue conducted over\u000d\u000a      decades and involving many disciplines. Mathematical models provide the\u000d\u000a      language that allows the disciplines to speak to each other. The key\u000d\u000a      importance of end-system control was apparent to the pioneering engineers\u000d\u000a      of the Internet. Indeed it was Jacobson's reference to earlier work of\u000d\u000a      mathematical scientists (Aldous, Hajek and Kelly) on random access\u000d\u000a      communication in his 1988 invention of TCP's congestion control algorithm\u000d\u000a      that first prompted Professor Kelly to think about the possibility that\u000d\u000a      the developing algorithms of the Internet could be addressed\u000d\u000a      systematically as distributed mechanisms to solve large-scale optimization\u000d\u000a      problems.\u000d\u000a    "},{"CaseStudyId":"17637","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Science and Technology Facilities Council","Particle Physics and Astronomy Research Council","Royal Society"],"ImpactDetails":"\u000a    Since 2008 Hawking has created several books and TV series that bring the\u000a      new developments in physics and cosmology arising from his research since\u000a      2003 on information loss, the no boundary condition, and the inflationary\u000a      universe to a general audience. They provide access to ideas whose\u000a      sophistication would normally preclude coverage in the mainstream media.\u000a      Hawking places research in a wider cultural context. This deepens public\u000a      understanding of research in physics and cosmology, produces widespread\u000a      media interest, and reiterates the worth of these fields to society.\u000a    Books:\u000a    Building on the phenomenal success of A Brief History of Time\u000a      with over 10 million copies in print, in more than 45 languages &#8212; the\u000a      follow-up volume A Briefer History of Time explaining the no\u000a      boundary proposal [3] has sold 580,000 copies since 2008. The book The\u000a        Universe in A Nutshell describing cosmological aspects of the no\u000a        boundary condition and black holes has sold 823,000 copies since\u000a      2008.The Grand Design, co-authored with Leonard Mlodinow, has sold\u000a      1.43 million copies since publication in 2010 (all data supplied by Random\u000a      House [8]). In The Grand Design, the authors discuss\u000a      Hawking's own research since 2003 on cosmology, particularly the\u000a      no-boundary proposal [3] and its implications for inflation and initial\u000a      conditions for the expanding universe [2,3]. A selection of the many\u000a      national and international press reactions can be seen from reports in The\u000a        New York Times (20\/09\/2010) [15] and the BBC News\u000a      (02\/11\/2010) [16].\u000a    Hawking also co-authored two books for young children with his daughter\u000a      Lucy Hawking: George's Cosmic Treasure Hunt (Corgi 2009, sales\u000a      75,692) and George and the Big Bang (Corgi 2011, sales 27,185).\u000a      These titles completed a trilogy of children's books begun in 2007 with George's\u000a        Secret Key to the Universe (sales 176,220). Sales figures provided\u000a      by Transworld Publishers [11]. These books introduce children aged 8\u000a      upwards to physics and cosmology, harnessing their fascination with the\u000a      universe, space travel and alien life, as well as the search for the\u000a      frontiers of knowledge. They introduce science through engaging adventure\u000a      stories and communicate what motivates scientists in their quest to\u000a      understand the universe.\u000a    TV series:\u000a    Hawking also participated in high-profile TV series about his research\u000a      and on cosmology and physics more generally. These programmes had high\u000a      impact, as can be seen from the viewing figures [9] (which will continue\u000a      to grow dramatically when the series are reshown or dubbed into foreign\u000a      languages and released in DVD format). The fact that such programmes are\u000a      commissioned and broadcast also provides evidence of the very considerable\u000a      impact that Hawking's previous work has had upon the world of the media\u000a      and the general public. Most recently, Hawking's major TV impacts have\u000a      occurred through the series Into the Universe with Stephen Hawking\u000a      (2010, Discovery Channel), Genius of Britain (2010,Channel 4 had\u000a      901,400 viewers for the 5th programme in the series, which\u000a      featured Hawking), Brave New World with Stephen Hawking (2011,\u000a      Channel 4, four-programme series had 892,100 viewers), Stephen\u000a        Hawking's Grand Design (2012, Discovery Channel 4, the 3 episodes\u000a      had in total 3.14 million viewers, Into the Universe with Stephen\u000a        Hawking (2010-11, Discovery Channel, the 6 episodes had in total 5.9\u000a      million viewers), and Stephen Hawking: Master of the Universe\u000a      (2008, Channel 4, the 4 programmes had a total of 1,399,900 viewers\u000a      [9]).These last three series encourage the public to explore cosmology and\u000a      physics. Each series draws on Hawking's recent research, including eternal\u000a      inflation (refs [2, 5]) and the no-boundary proposal in the context of the\u000a      string theory landscape (ref [3]).\u000a    A film entitled Hawking, directed by Stephen Finnegan, has been\u000a      made by Vertigo Films in collaboration with PBS\/Channel 4 (duration 89\u000a      mins) and was shown at the Cannes Film Festival, 15th -26th\u000a      May, 2013 and the Edinburgh Film Festival 19-30 June 2013 [13]. Its UK\u000a      Gala Premiere is at the Cambridge Film Festival on 19 September 2013\u000a      (Hawking will also open the Film Festival) [13]. It will be released in\u000a      cinemas across the UK on 20 September 2013 and shown by Channel 4 later in\u000a      2013.\u000a    Paralympic Opening:\u000a    Hawking's impact as one of the best-known scientists in the world was\u000a      notably demonstrated when he was chosen for the role of narrator for the\u000a      2012 Paralympics Opening Ceremony watched by a peak of 11.2 million UK\u000a      viewers (and many more overseas) [14 ]. This was the largest ever Channel\u000a      4 audience. While championing the cause of those living and competing in\u000a      the Games despite disabilities, Hawking urges his listeners to believe\u000a      there are no boundaries to human endeavour, In his narration Hawking\u000a      mentions explicitly his cosmological research on the no boundary condition\u000a      for the universe: \"There ought to be something very special about the\u000a      boundary conditions of the Universe, and what can be more special than\u000a      that there is no boundary?\" [10]. Anthony Faiola for the The Washington\u000a      Post (29 August 2012) reported: `London raises curtain on Paralympic\u000a        Games \"With an ode to science, human perseverance and the disabled\u000a      physicist Stephen Hawking ... Performers with disabilities soared in the\u000a      air on zip lines as Hawking, arguably the globe's most celebrated living\u000a      scientist, used his trademark voice box to deliver a metaphorical message:\u000a      \"Look up at the stars, and not down at your feet.\"[12]' In recognition of\u000a      the enormous impact of his work, in 2009 Stephen Hawking was awarded the\u000a      Presidential Medal of Freedom, America's highest civilian honour, by\u000a      President Obama. The citation states [7]:\u000a    'Persistent in his pursuit of knowledge, Stephen Hawking has unlocked\u000a        new pathways of discovery and inspired people around the world. He has\u000a        dedicated his life to exploring the fundamental laws that govern the\u000a        universe, and he has contributed to some of the greatest scientific\u000a        discoveries of our time. His work has stirred the imagination of experts\u000a        and lay persons alike. Living with a disability and possessing an\u000a        uncommon ease of spirit, Stephen Hawking's attitude and achievements\u000a        inspire hope, intellectual curiosity, and respect for the tremendous\u000a        power of science.'\u000a    ","ImpactSummary":"\u000a    The research underpinning Stephen Hawking's books, TV appearances and\u000a      lectures has shaped public attitudes towards frontier research in\u000a      cosmology. It attracts large audiences to learn about his research, and he\u000a      is the most well-known scientist in the world. Highlights include the\u000a      publication of his 2010 popular-science book The Grand Design, and\u000a      the Discovery Channel series Into the Universe with Stephen Hawking.\u000a      Further evidence of the impact of Hawking's research was the award of the\u000a      2009 Presidential Medal of Freedom (America's highest civilian honour) and\u000a      his role as narrator in the 2012 Paralympic Opening Ceremony watched by\u000a      over 11M UK viewers.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of Cambridge\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] Inflation with Negative Lambda. James B. Hartle. S.W.\u000a      Hawking, Thomas Hertog. Jul 2012. 4 pp. [LANL archive: arXiv:1207.6653]\u000a    \u000a\u000a[2] *Local Observation in Eternal inflation. James Hartle (UC,\u000a      Santa Barbara), S.W. Hawking (Cambridge U., DAMTP), Thomas Hertog (APC,\u000a      Paris &amp; Intl. Solvay Inst., Brussels). Sep 2010. 4 pp. Published in\u000a        Phys. Rev. Lett. 106 (2011) DOI: 10.1103\/PhysRevLett.106.141302\u000a    \u000a\u000a[3] *No-Boundary Measure of the Universe. James B. Hartle (UC,\u000a      Santa Barbara), S.W. Hawking (Cambridge U., DAMTP), Thomas Hertog (APC,\u000a      Paris &amp; Intl. Solvay Inst., Brussels). Nov 2007. 4 pp. Published in\u000a        Phys. Rev. Lett. 100 (2008) DOI:10.1103\/PhysRevLett.100.201301\u000a    \u000a\u000a[4] *Information loss in black holes. S.W. Hawking (Cambridge U.,\u000a      DAMTP). DAMTP-2005-66. Jul 2005. 5 pp. Published in Phys. Rev. D72\u000a        (2005) DOI:10.1103\/PhysRevD.72.084013\u000a    \u000a\u000a[5] Why does inflation start at the top of the hill? S.W Hawking,\u000a      Thomas Hertog (Cambridge U., DAMTP). Apr 2002. 21 pp. Published in\u000a        Phys. Rev. D66 (2002) DOI:10.1103\/PhysRevD.66.123509\u000a    \u000a\u000a[6] Open inflation without false vacua. S.W. Hawking, Neil Turok\u000a      (Cambridge U.). Feb 1998. 10 pp. Published in Phys. Lett. B425 (1998)\u000a        25-32 DOI:10.1016\/S0370-2693(98)00234-2\u000a    \u000aThis work has been supported continuously during this period by STFC\u000a      rolling grants (now consolidated grants), the latest of which is entitled,\u000a      Fundamental Physics and Observational Cosmology, 2011-14\u000a      (ST\/I002006\/1 amount &#163;1,138,424, PI Shellard). Previous STFC\/PPARC grants\u000a      supporting Hawking's work have been ST\/F002998\/1 (01\/04\/2008 - 31\/03\/2011,\u000a      amount &#163;1,227,537, PI Hawking) PP\/C501676\/1 (01\/04\/2005 - 21\/03\/2008,\u000a      amount &#163;634,294, PI Hawking) PPA\/G\/O\/2001\/000476 (01\/10\/2002 - 31\/03\/2005,\u000a      amount &#163;442,363, PI Hawking) PPA\/G\/O\/1999\/00603 (01\/10\/2000 - 30\/09\/2002\u000a      amount &#163;366,791, PI Hawking).\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"2","Level2":"2","Subject":"Atomic, Molecular, Nuclear, Particle and Plasma Physics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000a    [7] Presidential Medal of Freedom citation: http:\/\/www.whitehouse.gov\/the-press-office\/background-medal-freedom-ceremony\"&gt;office\/background-medal-freedom-ceremony\u000a    [8] Book sales data from Bantam Random House (supporting statement\u000a      available)\u000a    [9a] TV viewing figures from Senior Legal and Business Affairs Executive,\u000a      IWC Media (supporting statement available)\u000a    [9b] TV viewing figures from The Discovery Channel (spreadsheet\u000a      available).\u000a    [10] Transcript of Paralympic presentation: https:\/\/twitter.com\/gemgemloulou\/status\/241438450061045760\u000a    [11] Sales figures for children's books from Transworld Publishers\u000a      (statement available)\u000a    [12] The Washington Post (29 August 2012) http:\/\/articles.washingtonpost.com\/2012-08-29\/world\/35492460_1_martine-wright-ludwig-guttmann-paralympic-games\"&gt;29\/world\/35492460_1_martine-wright-ludwig-guttmann-paralympic-games\u000a    [13] `Hawking' film 2013: Edinburgh International Film Festival http:\/\/www.cine-vue.com\/2013\/06\/eiff-2013-hawking-review.html\u000a    [14] Paralympic Opening Ceremony viewing figures and analysis: http:\/\/www.theguardian.com\/media\/2012\/aug\/30\/paralympics-opening-ceremony-8m-viewers\u000a    [15] The New York Times (20\/09\/2010) http:\/\/nyti.ms\/bkLyZS\u000a    [16] BBC News (02\/11\/2010) http:\/\/www.bbc.co.uk\/news\/uk-11161493\u000a    \u000a    ","Title":"\u000a    Stephen Hawking\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Stephen Hawking (Lucasian Professor of Mathematics from 1979 to 2009 and\u000a      Director of Research from 2009 to present, University of Cambridge\u000a      Department of Applied Mathematics and Theoretical Physics [DAMTP] and 2006\u000a      Copley Medallist of the Royal Society) has made historic contributions to\u000a      our understanding of the possible origin and evolution of our Universe and\u000a      the properties of black holes. Hawking is responsible for many important\u000a      and interlinked breakthroughs in cosmology, black holes, and quantum\u000a      gravitation. He developed new mathematical approaches to joining the study\u000a      of quantum fields to gravitating systems governed by general relativity.\u000a      Most dramatically, this allows the quantum theory to be applied to the\u000a      study of entire universes [1-6].\u000a    Hawking's highlighted researches, refs. [1]-[3], between 2008-12 are in\u000a      collaboration with James Hartle, (UC Santa Barbara) and Thomas Hertog, (KU\u000a      Leuven). Hawking has provided new geometrical and topological ideas about\u000a      the problems of initial conditions and inflation which his collaborators\u000a      have then developed and helped prepare for publication. The research has\u000a      focussed on mathematical ways of defining the likelihood (or `measure' )\u000a      for different types of universe (including a simplified version of the one\u000a      that we observe today) to arise from a particular quantum prescription\u000a      near its initial state in the distant past when boundary conditions are\u000a      placed on its quantum wave function. This is difficult because the size of\u000a      infinite collections of possibilities depends upon the way they are\u000a      counted. An important example of a possible quantum initial state is the\u000a      famous Hartle-Hawking `no boundary' condition, introduced first in 1982-3.\u000a      It introduces an imaginary time coordinate so that, in the quantum limit\u000a      of the early universe, time becomes another dimension of space. This was\u000a      described in the best-selling book A Brief History of Time (1988)\u000a      but has remained a focal point of research into quantum cosmology by\u000a      Hawking and others. In refs [1]-[3] and [5]-[6] it was extended by Hawking\u000a      in a new way to include the introduction of a `string landscape' of\u000a      possible cosmological vacuum states into the theory. Hawking has also\u000a      studied whether the most likely quantum initial states subsequently lead\u000a      to inflationary expansion in the early universe [5] and, if so, what the\u000a      resulting form of inflation is likely to be, and what the observational\u000a      signals in the microwave background radiation of this form of inflation\u000a      should look like. This also included the unusual new possibility of\u000a      inflation with a negative cosmological constant [1].\u000a    In ref [2] Hartle and Hawking also introduce a further new consideration\u000a      into quantum cosmology by asking what constraints are introduced on the\u000a      probability distribution of outcomes for the universe by the fact that we\u000a      exist in a low-density environment after the universe has expanded for\u000a      more than 10 billion years [2]. Hawking continues to develop the\u000a      mathematical framework for calculations of the wave function of the\u000a      universe; for example, investigating the relationship between inflation\u000a      and the cosmological constant [1].\u000a    In a much-cited work (ref. [6]) carried out with Neil Turok (Professor of\u000a      Mathematical Physics (1967) from 1996 to 2009, DAMTP), Hawking also showed\u000a      that an `open' (i.e. negatively curved) universe could be created without\u000a      invoking a special type of inflationary expansion. Hawking has also\u000a      studied the longstanding problem of information loss in the process of\u000a      black hole formation and subsequent `Hawking' evaporation into radiation,\u000a      [4], which he first predicted in 1975. This problem has important\u000a      implications for the quantum theory of gravity and the extent to which a\u000a      theory of space and time can be created using the concept of\u000a      `information'. His recent work on this information loss problem, from 2005\u000a      to the present [4], has led to significant high-profile scientific\u000a      exchanges about Hawking's research on quantum information loss [4] with\u000a      Kip Thorpe (Caltech) and Leonard Susskind (Stanford) that were reported in\u000a      Susskind's best-selling popular book, The Black Hole War: My Battle\u000a        with Stephen Hawking to Make the World Safe for Quantum Mechanics\u000a      (2008).\u000a    "},{"CaseStudyId":"17638","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000d\u000a    The effect of the Warner-McIntyre (hereafter WM) gravity wave scheme was\u000d\u000a      carefully considered\u000d\u000a      by the Met Office during the period in which the desirability of using\u000d\u000a      `vertically extended' models,\u000d\u000a      including a detailed representation of the stratosphere, for operational\u000d\u000a      purposes was established\u000d\u000a      and a robust, high-quality version of the extended model developed. Since\u000d\u000a      2006 the UK Met Office\u000d\u000a      has used such `vertically extended' models, including the WM gravity-wave\u000d\u000a      scheme, for operational\u000d\u000a      purposes, for numerical weather prediction (implemented in 2006), for\u000d\u000a      seasonal forecasting\u000d\u000a      (implemented in 2010, Fereday et al 2012) and for climate prediction\u000d\u000a      simulations used for the 5th\u000d\u000a      IPCC Assessment report (implemented in 2011, Hardiman et al 2012).\u000d\u000a    Improvement of Short-Term Weather Prediction and Seasonal Forecasting\u000d\u000a    On the short time scales of a few days accessible to conventional\u000d\u000a      numerical weather prediction\u000d\u000a      extending the model to include the stratosphere, including use of the WM\u000d\u000a      scheme, provides\u000d\u000a      significant advantages for data assimilation in the stratospheric levels\u000d\u000a      of the model. On longer\u000d\u000a      seasonal forecast time scales, e.g. months, the WM scheme gives skill in\u000d\u000a      prediction of the\u000d\u000a      evolution of the equatorial Quasi Biennial Oscillation (QBO) which has\u000d\u000a      significant implications for\u000d\u000a      the extratropical circulation, including the low-level circulation in the\u000d\u000a      North Atlantic\/European\u000d\u000a      region. (Marshall and Scaife 2009).\u000d\u000a    The advantages of using the extended model for seasonal forecasting are\u000d\u000a      discussed in detail by\u000d\u000a      Fereday et al (2012) who show that the severe 2009\/10 winter was better\u000d\u000a      predicted by an\u000d\u000a      extended version of the Met Office Glosea 4 seasonal forecast system which\u000d\u000a      included the WM\u000d\u000a      scheme than by the standard version that was in operational use in that\u000d\u000a      winter. Fereday et al\u000d\u000a      (2012) argue that the better representation of the QBO in the extended\u000d\u000a      model, for which the WM\u000d\u000a      scheme is absolutely essential, together with improved representation of\u000d\u000a      the stratospheric aspects\u000d\u000a      of the communication of a tropical El Nino signal to the extratropics were\u000d\u000a      important in giving the\u000d\u000a      improved prediction. For the 2010\/11 and 2011\/12 winters the Met Office\u000d\u000a      implemented the\u000d\u000a      vertically extended version of the seasonal forecast system and their\u000d\u000a      seasonal predictions for\u000d\u000a      these winters were correct insofar that the correct phase of the North\u000d\u000a      Atlantic Oscillation pattern\u000d\u000a      (which has a major controlling effect over UK and European weather) was\u000d\u000a      predicted.\u000d\u000a    Seasonal forecasts are passed by the Met Office to UK government\u000d\u000a      Departments such as the\u000d\u000a      Cabinet Office, the Department of Energy and Climate Change (DECC),\u000d\u000a      Department for Transport\u000d\u000a      (DfT) and DEFRA where they are used for resilience planning purposes.\u000d\u000a    Impact on Climate Modelling\u000d\u000a    The Met Office Hadley Centre currently uses the HadGEM2 family of models\u000d\u000a      for climate prediction\u000d\u000a      (Met Office news item 2010). The vertically extended version of this model\u000d\u000a      is one of the three\u000d\u000a      models being used by the Hadley Centre in their contribution to the 5th\u000d\u000a      Assessment Report of the\u000d\u000a      Intergovernmental Panel for Climate Change (IPCC). This contribution is\u000d\u000a      through the Coupled\u000d\u000a      Model Intercomparison Project (CMIP5) which is an internationally\u000d\u000a      coordinated activity to perform\u000d\u000a      climate model simulations to a common set of specifications across all the\u000d\u000a      world's major climate\u000d\u000a      modelling centres. (Hardiman et al 2012, Met Office news item November\u000d\u000a      2012).\u000d\u000a    By comparing simulations with standard and extended versions of the\u000d\u000a      climate model Hardiman et\u000d\u000a      al (2012) show that the inclusion of a well-resolved stratosphere does\u000d\u000a      improve the impact of\u000d\u000a      atmospheric teleconnections on surface climate, in particular the response\u000d\u000a      to El Nino-Southern\u000d\u000a      Oscillation, the quasi-biennial oscillation, and midwinter stratospheric\u000d\u000a      sudden warmings (i.e., zonal\u000d\u000a      mean wind reversals in the middle stratosphere). Thus, including a\u000d\u000a      well-represented stratosphere\u000d\u000a      is expected to improve climate simulation on intraseasonal to interannual\u000d\u000a      time scales.\u000d\u000a    Scaife et al (2012) show by considering several models, including the Met\u000d\u000a      Office model, that the\u000d\u000a      inclusion of a well-resolved stratosphere changes predictions for Northern\u000d\u000a      Hemisphere winter\u000d\u000a      regional climate change. In the models with a well-resolved stratosphere\u000d\u000a      there is a weakening and\u000d\u000a      equatorward shift of the stratospheric polar vortex and a corresponding\u000d\u000a      equatorward shift of the\u000d\u000a      typical path of tropospheric weather systems. There are corresponding\u000d\u000a      differences in the predicted\u000d\u000a      changes in low-level circulation, storminess and rainfall, with these\u000d\u000a      differences being particularly\u000d\u000a      large in western Europe. Scaife et al (2012) conclude, for example, that\u000d\u000a      the increase in frequency\u000d\u000a      under CO2 increase of 1 in 50 daily heavy winter rainfall events in\u000d\u000a      western Europe (10 W-20 E\u000d\u000a      and 40-55 N) is predicted, by the extended Met Office model to be twice as\u000d\u000a      great as predicted by\u000d\u000a      the standard model. Detailed representation of the stratosphere and the\u000d\u000a      associated implementation\u000d\u000a      of the WM scheme has, therefore, had a first order impact on European\u000d\u000a      climate projections.\u000d\u000a    Participation in the IPCC as described above is the primary way in which\u000d\u000a      the UK contributes to\u000d\u000a      formulation of international policy on climate change and the WM scheme is\u000d\u000a      an important\u000d\u000a      component of the model used for these projections. The Met Office is also\u000d\u000a      the primary source of\u000d\u000a      climate information to UK government to advise on national policy\u000d\u000a      formulation. For example, Met\u000d\u000a      Office climate predictions are provided to the UK Climate Projections\u000d\u000a      database.\u000d\u000a    The WM scheme is regarded by the Met Office as an essential component of\u000d\u000a      this extended model.\u000d\u000a      The Head of Monthly to Decadal Prediction at the Met Office Hadley Centre\u000d\u000a      states, \"The Met Office\u000d\u000a        regards the Warner-McIntyre gravity-wave parametrization as an essential\u000d\u000a        part of the improved\u000d\u000a        representation of the stratosphere in weather and climate prediction\u000d\u000a        models which has given direct\u000d\u000a        benefits for seasonal forecasting and climate prediction.\"\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The Warner-McIntyre parametrization scheme for non-topographic\u000d\u000a      atmospheric gravity waves,\u000d\u000a      developed at the Department of Applied Mathematics and Theoretical Physics\u000d\u000a      (DAMTP),\u000d\u000a      University of Cambridge, during the period from 1993 to 2004, has since\u000d\u000a      2010 been used by the\u000d\u000a      UK Met Office in their operational models for seasonal forecasting and\u000d\u000a      climate prediction .The\u000d\u000a      parametrization is regarded by the Met Office as a vital part of improved\u000d\u000a      representation of the\u000d\u000a      stratosphere in those models, which in turn has been shown to lead to\u000d\u000a      significant operational\u000d\u000a      benefits.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Cambridge\u000d\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1. *Warner, CD; McIntyre, ME, 1996: On the propagation and dissipation of\u000d\u000a      gravity wave\u000d\u000a      spectra through a realistic middle atmosphere. J. Atmos. Sci., 53,\u000d\u000a      3213-3235. DOI:\u000d\u000a      10.1175\/1520-0469(1996)053&lt;3213:OTPADO&gt;2.0.CO;2\u000d\u000a    \u000a\u000a2. Warner, CD; McIntyre, ME, 1999: Toward an ultra-simple spectral\u000d\u000a      gravity wave\u000d\u000a      parameterization for general circulation models. (Proceeding of\u000d\u000a      International Symposium on\u000d\u000a      Dynamics and Structure of the Mesopause Region, Kyoto Univ., Kyoto, Japan,\u000d\u000a      March 16-21\u000d\u000a      1998. Earth, Planets and Space, 51, 7-8. 475-484.\u000d\u000a    \u000a\u000a3. Scaife, AA, Butchart, N, Warner, CD, Stainforth, D., Norton, WA,\u000d\u000a      Austin, J., 2000: Realistic\u000d\u000a      quasi-biennial oscillations in a simulation of the global climate.\u000d\u000a      Geophys. Res. Lett., 27,\u000d\u000a      3481-3484. DOI: 10.1029\/2000GL011625.\u000d\u000a    \u000a\u000a4. *Warner, CD, McIntyre, ME, 2001: An ultrasimple spectral\u000d\u000a      parameterization for\u000d\u000a      nonorographic gravity waves. J. Atmos. Sci., 58, 1837-1857. DOI:\u000d\u000a      10.1175\/1520-0469(2001)058&lt;1837:AUSPFN&gt;2.0.CO;2\u000d\u000a    \u000a\u000a5. *Scaife AA, Butchart, N, Warner, CD, Swinbank, RC, 2002: Impact of a\u000d\u000a      spectral gravity\u000d\u000a      wave parameterization on the stratosphere in the met office unified model.\u000d\u000a      J. Atmos. Sci.,\u000d\u000a      59, 1473-1489. DOI: 10.1175\/1520-0469(2002)059&lt;1473:IOASGW&gt;2.0.CO;2\u000d\u000a    \u000a\u000a6. Warner, CD, Scaife, AA, Butchart, N, 2005: Filtering of parametrized\u000d\u000a      nonorographic gravity\u000d\u000a      waves in the Met Office Unified Model. J. Atmos. Sci., 62, 1831-1848. doi:\u000d\u000a      10.1175\/JAS3450.1\u000d\u000a    \u000a* References which best reflect the quality of the underpinning research\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"4","Level2":"5","Subject":"Oceanography"},{"Level1":"9","Level2":"11","Subject":"Maritime Engineering"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000d\u000a    Statement from the Head Monthly to Decadal Prediction at the Met Office\u000d\u000a      Hadley Centre\u000d\u000a    Fereday, DR, Maidens, A, Arribas, A, Scaife, AA, Knight, JR, 2012.\u000d\u000a      Seasonal forecasts of northern\u000d\u000a      hemisphere winter 2009\/10. Environmental Research Letters, 7, 034031: DOI:\u000d\u000a      10.1088\/1748-\u000d\u000a      9326\/7\/3\/034031.\u000d\u000a    Hardiman, SC. Butchart, N, Hinton, TJ, Osprey, SM, Gray, LJ. 2012: The\u000d\u000a      effect of a well-resolved\u000d\u000a      stratosphere on surface climate: differences in CMIP5 simulations with\u000d\u000a      high and low top versions of\u000d\u000a      the Met Office climate model. J. Climate, 25, 7083-7099. DOI:\u000d\u000a      10.1175\/JCLI-D-11-00579.1.\u000d\u000a    Scaife, AA, Spangehl, T, Fereday, DR, Cubasch, U, Langematz, U, Akiyoshi,\u000d\u000a      H, Bekki, S,\u000d\u000a      Braesicke, P, Butchart, N, Chipperfield, MP, Gettelman, A, Hardiman, SC,\u000d\u000a      Michou, M, Rozanov, E,\u000d\u000a      Shepherd, TG, 2011: Climate change projections and\u000d\u000a      stratosphere-troposphere interaction.\u000d\u000a      Climate Dynamics, 38, 2089-2097. DOI 10.1007\/s00382-011-1080-7\u000d\u000a      Institute of Physics news item. http:\/\/www.iop.org\/news\/12\/sep\/page_57337.html\u000d\u000a    Marshall, AG, Scaife, AA, 2009: Impact of the QBO on surface winter\u000d\u000a      climate. J. Geophys. Res.,\u000d\u000a      114, D18110, doi:10.1029\/2009JD011737.\u000d\u000a    Met Office news item 2010:\u000d\u000a      http:\/\/www.metoffice.gov.uk\/research\/modelling-systems\/unified-model\/climate-models\/hadgem2\u000d\u000a    Met Office news item 14 September 2012:\u000d\u000a      http:\/\/www.metoffice.gov.uk\/news\/releases\/archive\/2012\/improved-winter-guidance\u000d\u000a    Met Office news item, 29 November 2012: Met Office delivers new climate\u000d\u000a      simulations to\u000d\u000a      international modelling activity. http:\/\/www.metoffice.gov.uk\/research\/news\/cmip5\u000d\u000a    ","Title":"\u000d\u000a    Gravity-wave parametrization in weather forecast and climate models\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Numerical models used for weather forecasting and climate prediction\u000d\u000a      cannot explicitly represent\u000d\u000a      several processes that have small spatial scale but which have systematic\u000d\u000a      effects on the larger\u000d\u000a      scale. Instead these processes must be represented by parametrization,\u000d\u000a      i.e. by extra terms\u000d\u000a      included in the model equations. One such process is momentum transport by\u000d\u000a      gravity waves, in\u000d\u000a      particular by waves that are not associated with topography. (Gravity\u000d\u000a      waves are small-scale waves\u000d\u000a      that result from internal density gradients in the atmosphere. An example\u000d\u000a      is the topographic gravity\u000d\u000a      waves generated by flow over mountains that are manifested in cloud\u000d\u000a      patterns. Non-topographic\u000d\u000a      gravity waves are generated by processes such as thunderstorms.) The\u000d\u000a      potential role of non-topographic\u000d\u000a      gravity waves in determining aspects of the large-scale atmospheric\u000d\u000a      circulation,\u000d\u000a      particularly in the stratosphere and mesosphere, and consequently the need\u000d\u000a      to represent such\u000d\u000a      waves in numerical models was recognised in the 1980s and 1990s. In 1993\u000d\u000a      researchers at the\u000d\u000a      University of Cambridge Department of Applied Mathematics and Theoretical\u000d\u000a      Physics (DAMTP),\u000d\u000a      C.D. Warner (DAMTP Research Associate from 1992, Senior Research Associate\u000d\u000a      from 2003) and\u000d\u000a      M.E. McIntyre (DAMTP Professor throughout the period), began work on the\u000d\u000a      formulation of a\u000d\u000a      parametrization scheme in which the propagation and dissipation of a field\u000d\u000a      of waves made up of a\u000d\u000a      broad spectrum of frequencies was calculated by following individual\u000d\u000a      spectral elements [Ref 1].\u000d\u000a      Compared to other schemes existing at that time, this scheme (the `full'\u000d\u000a      scheme) gave the\u000d\u000a      advantage of physical and mathematical clarity, but was too\u000d\u000a      computationally expensive to be\u000d\u000a      useable in numerical models of the large-scale atmosphere. In following\u000d\u000a      work [Refs 2 and 4] an\u000d\u000a      `ultra-simple' version of the scheme, that required only a small number of\u000d\u000a      variables to characterize\u000d\u000a      the gravity wave field as a function of height and time at each horizontal\u000d\u000a      location, was developed\u000d\u000a      and validated by Warner and McIntyre by testing against their full scheme.\u000d\u000a      Alongside this work\u000d\u000a      developing a robust and properly formulated gravity-wave scheme there was\u000d\u000a      a collaborative\u000d\u000a      programme of work with the UK Met Office to establish, as new versions of\u000d\u000a      the gravity-wave\u000d\u000a      scheme became available, the effect on the circulation as represented by a\u000d\u000a      global-scale model.\u000d\u000a      (The equations describing the parametrization were added to the large\u000d\u000a      computer code defining the\u000d\u000a      model.) The global model was a version, at that time used for research\u000d\u000a      purposes only, of the Met\u000d\u000a      Office Unified Model, `vertically extended' to include a detailed\u000d\u000a      representation of the stratosphere.\u000d\u000a      Results showing the effect of the gravity-wave scheme on the modelled\u000d\u000a      circulation were reported in\u000d\u000a      Refs 3, 5 and 6.\u000d\u000a    "},{"CaseStudyId":"18042","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000a    The new nozzle design is capable of producing much smaller bubbles of a\u000a      more uniform size that (a) increased the effectiveness of the air\u000a      introduced, (b) allowed the air to be introduced at a lower pressure, and\u000a      (c) substantially reduced the breakup of the flocculated particles in the\u000a      water supply. The indirect benefits of this include reductions in the\u000a      energy requirements for the production of dissolved air, a reduction in\u000a      the concentration of chemical additives required to achieve flocculation,\u000a      an improvement in the quality of the treated water, and an increase in the\u000a      throughput of existing dissolved air flotation systems. Additionally, as\u000a      an indirect consequence of the improved water quality produced by the DAF\u000a      system, the costs of subsequent filtration and maintenance\/cleaning\u000a      requirements of downstream systems were reduced.\u000a    Such was the success of this new nozzle design that Yorkshire Water\u000a      immediately instigated plans to roll it out across its dissolved air\u000a      flotation. This roll out has be prioritised on water works that (with\u000a      their original nozzles) would not meet water quality requirements if\u000a      operated at designed throughput. As of December 2012, Yorkshire Water have\u000a      retrofitted the nozzles at six sites, with installation at four further\u000a      sites operated by their sister company, Kelda Water Services. These\u000a      nozzles have also been installed at two new treatment works, the more\u000a      recent commissioned at the end of 2012.\u000a    The initial field trials suggested that 20% more raw water could be\u000a      treated whilst maintaining an improved water quality (Zhang et al.\u000a      2009). The potential saving in capital investment through avoiding the\u000a      need to construct new or additional flotation systems is substantial.\u000a      Earlier work had shown that reducing the pressure at which water was\u000a      saturated with dissolved gases led to fewer, larger bubbles, thus\u000a      requiring very high saturation pressures to be used. The energy cost in\u000a      achieving this is substantial. However, the new nozzle is able to operate\u000a      successfully at significantly lower saturation pressures. Yorkshire Water\u000a      estimates energy savings at pumping sites in the region of 10-15% due to\u000a      nozzle use (more accurate disaggregation of benefit is difficult since\u000a      nozzle installation has been part of a refurbishment programme.)\u000a      (Commercial Optimisation Manager, Yorkshire Water)\u000a    Patent protection in the UK and USA has been gained for these nozzles\u000a      with Dalziel, Leppinen and Zhang listed as inventors and the rights\u000a      assigned to Yorkshire Water Services. Under this Yorkshire Water have\u000a      licensed the use of the new nozzles to three further water companies:\u000a      United Utilities, Anglian Water and Northern Ireland Water.\u000a    ","ImpactSummary":"\u000a    By modelling the formation of micro-bubbles and the flows induced by\u000a      them, researchers at the University of Cambridge Department of Applied\u000a      Mathematics and Theoretical Physics developed a new, low-cost nozzle\u000a      design that could be retrofitted to existing Dissolved Air Flotation (DAF)\u000a      systems. This new design dramatically improved the performance of DAF\u000a      systems, used by the water industry for the production of drinking water.\u000a      Specifically, this research has enabled a substantial increase in\u000a      throughput and effectiveness of the flotation process, whilst\u000a      simultaneously providing a dramatic decrease in the energy requirement.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Cambridge\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000aLeppinen, D.M. &amp; Dalziel, S.B. 2001 A light attenuation technique for\u000a      void fraction measurement of microbubbles; Experiments in Fluids 30,\u000a      214-220, DOI: 10.1007\/s003480000158\u000a    \u000a\u000aLeppinen, D.M., Dalziel, S.B. &amp; Linden, P.F. 2001 Modelling the\u000a      global efficiency of dissolved air flotation. Water Science and Technology\u000a      43 (8), 159-166, URL:\u000a      http:\/\/www.iwaponline.com\/wst\/04308\/wst043080159.htm.\u000a    \u000a\u000aLeppinen, D.M. &amp; Dalziel, S.B. 2004 Bubble size distribution in\u000a      dissolved air flotation tanks. Journal of Water Supply Research and\u000a      Technology - Aqua 53, 531-543, URL:\u000a      http:\/\/www.iwaponline.com\/jws\/053\/jws0530531.htm.\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"9","Level2":"14","Subject":"Resources Engineering and Extractive Metallurgy"},{"Level1":"3","Level2":"6","Subject":"Physical Chemistry (incl. Structural)"},{"Level1":"9","Level2":"4","Subject":"Chemical Engineering"}],"Sources":"\u000a    Yorkshire Water Services Limited (incorporated in the United Kingdom)\u000a      Inventors: Dalziel, Stuart B; Leppinen, David M; Zhang, Yanmin\u000a      Dissolved gas flotation system and nozzle assembly\u000a      UKC Headings: B2F B2H Int Cl B05B\u000a      1\/00(2006.01) B03B 5\/28(2006.01)\u000a      B03D 1\/02(2006.01) B03D\u000a      1\/14(2006.01) B03D 1\/16(2006.01)\u000a    US Patent: US 2008\/0277329 A1\u000a      Jet nozzle arrangements for optimising gas bubble size in flotation\u000a      Inventors: Yanmin Zhang (Leeds, GB), Stuart B. Dalziel (Cambridge, G.B),\u000a      David Leppinen (Staffordshire, G.B.)\u000a    Zhang, Y., Leppinen, D.M. &amp; Dalziel, S.B. 2009 A new nozzle for\u000a      dissolved air flotation. Water Science and Technology: Water Supply 9 (6),\u000a      611-617. doi:10.2166\/ws.2009.229\u000a    Email from R, D &amp; I Project Manager at Yorkshire Water confirming use\u000a      of Dissolved Air Floatation Nozzles\u000a    Email From Commercial Optimisation Manager at Yorkshire Water confirming\u000a      energy savings resulting from use of new nozzles \u000a    ","Title":"\u000a    Dissolved air flotation\u000a    ","UKLocation":[{"GeoNamesId":"2653941","Name":"Cambridge"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The principle behind Dissolved Air Flotation is to generate very small\u000a      bubbles that attach to flocculated particles in the water, making them\u000a      buoyant enough to rise to the surface of the water and form a foam that\u000a      can be removed. Such devices are in widespread use within the water\u000a      industry for the purification of drinking water, and the research\u000a      described here focuses on improving the efficiency of the process. This\u000a      research was predicated by the concern within the water industry of the\u000a      decreasing quality of the water sources (especially from upland areas\u000a      where the decrease in `acid rain' has led to an increase in the `colour'\u000a      of the runoff) at the same time as regulations are driving towards\u000a      increased quality and safety. Additionally, privatisation of the water\u000a      industry has brought into sharp conflict the rapidly increasing overall\u000a      demand for potable water and the capital investment requirement to secure\u000a      new water sources and\/or increase the capacity of treatment works.\u000a      Increasingly, societal concerns over land and the environment introduce\u000a      further factors.\u000a    Research at the University of Cambridge Department of Applied Mathematics\u000a      and Theoretical Physics (DAMTP) into the fluid mechanics of dissolved air\u000a      flotation tanks commenced during the mid-1990s with funding from Yorkshire\u000a      Water plc. Initially, DAMTP staff members Stuart Dalziel (DAMTP researcher\u000a      from 1989, Assistant Director of Research from 1994, Lecturer from 2000,\u000a      Senior Lecturer from 2001 and Reader from 2012 to present) and Paul Linden\u000a      (DAMTP researcher from 1972, Reader from 1991-1998, Professor from\u000a      2010-present), along with postdoctoral researcher David Leppinen (DAMTP\u000a      Research Associate 1997-2004), concentrated on the mean dynamics within\u000a      flotation tanks induced by the combination of a through flow and the\u000a      buoyancy introduced through bubble formation. This work identified the key\u000a      rate-limiting factors governing the flotation process. In a follow-on\u000a      project in 2002, Dalziel and Leppinen developed methods for in-situ field\u000a      measurements of the bubbles and flocs involved in the flotation, and found\u000a      that not only was most of the dissolved air being wasted through the\u000a      generation of relatively large bubbles, but also that the manner in which\u000a      it was being introduced was damaging the flocs and reducing the\u000a      probability of successful floc-bubble interactions. Consequently, they\u000a      turned their attention to the bubble formation process and used a simple\u000a      model for bubble growth to analyse the conflicting requirements for energy\u000a      dissipation and pressure reduction for the introduction of dissolved air\u000a      into the system. Using theoretical and laboratory modelling, they\u000a      developed a `ventilated shrouded' nozzle through which water saturated\u000a      with air at high pressure is injected into the flotation tank. The final\u000a      phase of work was a field study in conjunction with Yanmin Zhang\u000a      (Yorkshire Water). This field study (2004) saw prototype nozzles of the\u000a      new design installed in one of Yorkshire Water's dissolved air flotation\u000a      tanks, thus allowing Dalziel and Leppinen to confirm their modelling with\u000a      in-situ field measurements, and convince Yorkshire Water of the value of\u000a      these new nozzles.\u000a    "},{"CaseStudyId":"18054","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"660013","Name":"Finland"},{"GeoNamesId":"719819","Name":"Hungary"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The Joint Research Centre (JRC) is the scientific arm of the European\u000a      Commission and provides advice to the Commission to inform policy-making.\u000a      In collaboration with the JRC, the research conducted at Queen Mary has\u000a      had impact in a number of ways. Firstly, through collaboration with JRC\u000a      our research has influenced European security policy. Secondly, with JRC\u000a      and the Directorate General Taxation and Customs Union our research has\u000a      informed security performance standards in the container shipping\u000a      industry. Thirdly, our collaboration with JRC and relationships with E.On\u000a      and the National Emergency Supply Agency of Finland (NESA) has informed\u000a      the security of electricity and gas networks. And fourthly, through\u000a      collaboration with HM Treasury our research has informed the delivery of\u000a      the High Speed 2 rail network in the UK to identify opportunities for a\u000a      shared corridor. The following section explains each of these impact\u000a      areas.\u000a    Influencing security policy\u000a    Our research and the associated modelling techniques [1, 2, 3, 4] have\u000a      been adopted by the JRC who, in turn, have impact on European legislation\u000a      for infrastructure projects. A key mission of the JRC is to provide\u000a      research that supports EC policymakers in their efforts to ensure global\u000a      security and the protection of European citizens from accidents,\u000a      deliberate attacks, fraud and other illegal activity. In 1996, Guti&#233;rrez\u000a      joined Arrowsmith's group as a PhD student and together they published\u000a      research on structural engineering. Guti&#233;rrez joined the European\u000a      Commission as a researcher in the JRC where later, in collaboration with\u000a      Arrowsmith, he provided research expertise in the fields of gas and\u000a      electricity supply security, building upon the methods described above in\u000a      Section 2.\u000a    The data-mining exercise mentioned in Section 2 allowed us to identify\u000a      load- and fault-tolerant backbones of the trans-European gas pipeline\u000a      network by combining topological data with information on inter-country\u000a      flows [1]. This has made it possible to estimate the global load of the\u000a      European gas network and its tolerance to failures by applying two\u000a      complementary methods generalised from measures of betweenness centrality\u000a      and the maximum flow. We found that the gas pipeline network has grown to\u000a      satisfy a dual purpose. On one hand, the major pipelines are crossed by a\u000a      large number of direct connections thereby increasing the efficiency of\u000a      the network; on the other hand, a non-operational pipeline causes only a\u000a      minimal impact on network capacity, implying that the network is error\u000a      tolerant. Our findings conclude that the trans-European gas pipeline\u000a      network is robust &#8212; it is tolerant to failures of high load links. The\u000a      data for this activity has been made available by the Head of Unit, Energy\u000a      Security, EC Joint Research Centre at Petten, and Queen Mary conducted the\u000a      mining and analysis exercise.\u000a    The collaboration between JRC and our researchers has contributed to the\u000a      ongoing evaluation of technologies and the improvement of security\u000a      standards which have an impact on EU regulations as confirmed by\u000a      Guti&#233;rrez, now a researcher at the JRC [8]: \"I would like to record\u000a        the significance of the collaboration between the European Laboratory\u000a        for Structural Assessment of the European Commission's Joint Research\u000a        Centre (JRC), Ispra, and the School of Mathematical Sciences of Queen\u000a        Mary, University of London, most notably with Prof. D.K. Arrowsmith.\u000a        This collaboration has enabled the JRC to have an impact on the\u000a        development of a framework for standardisation, regulation and\u000a        legislation with regards to supply chain container management and\u000a        security. The scientific input and the unique expertise of SMS\/QMUL have\u000a        been a key ingredient for its collaboration with the JRC, whose role is\u000a        to provide scientific and technical support to European Commission\u000a        services by delivering guidelines with a view to promote standards and\u000a        legislation\".\u000a    Addressing security performance standards in the container shipping\u000a          industry\u000a    Arrowsmith and Guti&#233;rrez have also collaborated on research into the\u000a      robustness of containerised transport networks and the ability of sensors\u000a      to provide appropriate indicators of secured goods. There were over 400\u000a      million container movements in 2010 and containerised transport is\u000a      particularly at risk of being targeted for the smuggling of people or\u000a      goods, as indicated in the \"EC Technical Report, An overview of\u000a        research programmes and prospective technology in the development of\u000a        more secure supply chains: The Case of Shipping Containers\" [11].\u000a      The scope of this report, a joint activity of our researchers, JRC, and\u000a      The Directorate General Taxation and Customs Union covers \"on the one\u000a        hand the two primary technological considerations (materials technology\u000a        and production, ad-hoc sensor networks deployment) and, on the other,\u000a        the economic and trade statistics backdrop of the container industry. By\u000a        considering these aspects &#8212; in conjunction with the technological\u000a        developments and policies in container security of the EU's major\u000a        trading partners &#8212; it is intended to set the backdrop to the potential\u000a        innovative technologies in the area of tamper-proof intermodal\u000a        containers. The aim is to provide support to European Commission\u000a        services in their policies of motivating the development, capability\u000a        testing and evaluation of technologies that could meet the security\u000a        performance standards in the container shipping industry, and in matters\u000a        relating to meeting the EU's international cooperation agreements on\u000a        supply chain security\" [11]. Arrowsmith and Guti&#233;rrez's\u000a      collaborative research in MANMADE and RAVEN [5,6] led to an invitation to\u000a      address security performance standards in the container shipping industry\u000a      [12]. The project research has been applied in matters relating to meeting\u000a      the EU's international cooperation agreements on supply chain security.\u000a    Ranking and the impact on security of gas networks\u000a    In 2012 Arrowsmith, Carvalho and Guti&#233;rrez founded a research group named\u000a      Real World Networks (RWnets). This group produced an energy network\u000a      analysis that ranked the security of gas networks in Europe. This research\u000a      led to a novel method of ranking countries in terms of the vulnerability\u000a      of their infrastructure to degradation, natural disaster or deliberate\u000a      attack. Furthermore, as a result of the development of expertise in the\u000a      analysis of real networks within MANMADE [6], which was further developed\u000a      by EPSRC funding in RAVEN [5], we now have a memorandum of understanding\u000a      with the EC Joint Research Centre, Petten, which has enabled the release\u000a      of key energy network data to develop novel ideas of fairness of flow\u000a      indicators [2] (January 2012, data via the Head of Unit for Energy\u000a      Security, EC Joint Research Centre, Petten). Much of this research is to\u000a      address issues of the Grand Challenge energy scenarios that Research\u000a      Councils UK is now supporting, such as the supply of energy becoming\u000a      increasingly pan-European with the potential for political involvement in\u000a      the transport of energy across the continent.\u000a    The research group which was set up at Queen Mary as part of the MANMADE\u000a      project [6] collaborated with the energy supplier E.ON in Hungary [9] to\u000a      develop a dynamic power grid model with a view to understanding likely\u000a      cascade scenarios at a national level, and with the National Emergency\u000a      Supply Agency of Finland (NESA) [7] who attended all six management\u000a      meetings over the three years of the project to monitor the outcome of\u000a      MANMADE, for them to feedback on national energy security.\u000a    Evidence of impact on UK infrastructure development: High Speed 2\u000a    Our research resulted in an approach from HM Treasury [10] to offer\u000a      advice and expertise on one of the largest infrastructure projects in\u000a      Europe. Specifically, the Treasury's approach was stimulated by interest\u000a      in our investigation of the European gas network [1], which led to our\u000a      involvement with HS2. Our research outputs have been discussed by the HS2\u000a      team, as described by the Interdependencies Lead within the Enterprise and\u000a      Growth Group at HM Treasury [13]: \"Your report mapping HS2 route\u000a        against electricity and gas network density has been read with interest\u000a        here and I have used the results with DfT and the HS2 delivery team to\u000a        explain how we can go about analysing potential opportunities for shared\u000a        corridor.... In particular, we are working with the HS2 team to make a\u000a        case for a fibre backbone down the HS2 route\". This indicates impact\u000a      in the decision making process on future high-profile infrastructure\u000a      projects.\u000a    ","ImpactSummary":"\u000a    Researchers at Queen Mary have applied mathematical modelling techniques\u000a      to understand how and when problems may arise in complex man-made\u000a      infrastructure networks including electricity, gas, global shipping and\u000a      haulage networks. Many of these networks have points of vulnerability\u000a      where a local issue such as an earthquake, a terrorist attack or even a\u000a      simple engineering problem can bring down widespread areas of the network.\u000a      Our research and the associated modelling techniques have impacted on\u000a      organisations including the UK Treasury Office and the European\u000a      Commission's Joint Research Centres at both Petten and Ispra, where it has\u000a      been used to inform UK and European policy guidelines and legislation for\u000a      infrastructure projects.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Queen Mary University of London (QMUL)\u000a    ","Institutions":[{"AlternativeName":"Queen Mary, University of London","InstitutionName":"Queen Mary, University of London","PeerGroup":"A","Region":"London","UKPRN":10007775}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a1. R. Carvalho, L. Buzna, F. Bono, E. Guti&#233;rrez, W. Just, D.K.\u000a      Arrowsmith, `Robustness of Trans-European Gas Networks', Phys. Rev. E,\u000a      016106 (2009) (doi:10.1103\/PhysRevE.80.016106)\u000a    \u000a\u000a2. R. Carvalho, L. Buzna, W. Just, D. Helbing, and D. K. Arrowsmith,\u000a      `Fair sharing of resources in a supply network with constraints', Phys.\u000a        Rev. E 85, 046101 (2012)\u000a    \u000a\u000a3. M. Woolf, Z. Huang, and R.J. Mondragon, `Building catastrophes:\u000a      networks designed to fail by avalanche-like breakdown', New Journal of\u000a        Physics, 9, 174 (2007) (doi:10.1088\/1367-2630\/9\/6\/174)\u000a    \u000a\u000a4. R.J. Mondragon, `Topological modelling of large networks', Phil.\u000a        Trans. R. Soc. A 366 (2008) (doi:10.1098\/rsta.2008.0008)\u000a    \u000a5. Arrowsmith, Resilience, Adaptability and Volatility of complex\u000a        Energy Networks (RAVEN), EPSRC EP\/H04812X\/1, awarded March 2010,\u000a      grant value &#163;350k\u000a    \u000a6. http:\/\/manmade.maths.qmul.ac.uk\/\u000a      Arrowsmith, Diagnosing vulnerability, emergent phenomena, and\u000a        volatility in manmade networks, FP6-STREP (2007-2009)\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"8","Level2":"6","Subject":"Information Systems"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000a      \u000a      Senior Researcher, National Emergency Supply Agency (NESA). [Impact of\u000a      the research performed in the EU-STREP Manmade for a key aim of NESA, the\u000a      security of the Scandinavian infrastructure network].\u000a    Senior Scientific Officer, EC Joint Research Centre, Ispra, Italy.\u000a      [Impact on the development of a framework for standardisation, regulation\u000a      and legislation with regards to supply chain container management and\u000a      security].\u000a    Commercial Advisor, Infrastructure UK, HM Treasury. [Impact of the\u000a      research on vulnerability of networks done within Manmade for\u000a      infrastructure security (gas) and planned transport network projects\u000a      (HS2)]\u000a    An overview of research programmes and prospective technology in the\u000a      development of more secure supply chains: The Case of Shipping Containers\u000a      Report EUR 25298 (EN 2012) JRC Scientific and Policy Reports, E. Gutierrez\u000a      (DG-JRC), W. van Heeswijk (DG-TAXUD), D.K. Arrowsmith (Queen Mary,\u000a      University of London), EUR 25298 EN, ISBN 978-92-79-24168-0 (pdf),ISBN\u000a      978-92-79-24167-3 (print).\u000a    Policy Officer, Supply Chain Security and Technology expert, European\u000a      Commission. [Impact of the research on European Commission services in\u000a      their policies for the development, capability testing and evaluation of\u000a      technologies to meet the security performance standards in the container\u000a      shipping industry].\u000a    Interdependencies Lead, Enterprise and Growth Group, HM Treasury.\u000a      [Impact of the research on the HS2 project, in particular how to analyse\u000a      potential opportunities for shared corridor].\u000a    \u000a\u0009","Title":"\u000a    Informing policy and mitigating risk - modelling infrastructure networks\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Our research in the study of infrastructure networks has focused on:\u000a    \u000a      Building new datasets, particularly of European electricity and gas\u000a        network data, overlaying these networks to determine important common\u000a        nodes, and offering a range of new insights using mathematical measures\u000a        of robustness that are based on network topology; and\u000a      Applying mathematical analysis to networks using real-world data and\u000a        understanding the problems and opportunities this raises.\u000a    \u000a    Our expertise in this area was demonstrated by the development of an\u000a      EU-wide series of collaborations on the subject of network vulnerability.\u000a      The MANMADE project (2007-09) [6], which was EU sponsored, addressed the\u000a      diagnosis of vulnerability in networks, emergent phenomena and volatility\u000a      in man-made networks. The grant resulted from the collaboration between\u000a      Arrowsmith and the European Commission Joint Research Centre, Ispra (JRC)\u000a      and grew to include the National Emergency Supply Agency (NESA) in Finland\u000a      and academic institutions across Europe, including the prestigious\u000a      Collegium Budapest. This collaboration supported the analysis of real\u000a      networks from both the mathematical and engineering perspectives.\u000a      Industrial, government, European Commission and commercial stakeholders\u000a      have contributed real data from continental, country and city-wide\u000a      networks. An indicator of our success came towards the end of the MANMADE\u000a      project when we were awarded an EPSRC grant on Resilience, Adaptability\u000a      and Volatility of complex Energy Networks (RAVEN) [5]. The thrust and\u000a      novelty of the project is that the investigation of the mathematical\u000a      models and methods are data rather than theory driven [5].\u000a    Work by Carvalho, Arrowsmith and others at Queen Mary [1, 2] has\u000a      uncovered insights into network robustness according to various criteria\u000a      of importance. The extensive data mining that we have undertaken has, for\u000a      example, allowed us to:\u000a    a) consider error-tolerance of gas transmission networks and network\u000a      redundancy in greater detail than ever before at the European scale;\u000a    b) investigate the probability distribution of the impact on society as a\u000a      result of disruptions to electricity and gas supply across Europe after an\u000a      earthquake;\u000a    c) consider the critical infrastructure aspects of road networks in major\u000a      cities and attempt to identify the critical building blocks, the so-called\u000a      motifs, in these networks; and\u000a    d) identify load- and fault-tolerant backbones of the trans-European gas\u000a      pipeline network by combining topological data with information on pipe\u000a      diameters and inter-country flows.\u000a    This research was carried out between 2009 and 2012 and was led by\u000a      Carvalho (postdoctoral research assistant of MANMADE and RAVEN, supported\u000a      partially by ImpactQM) and Arrowsmith (Principal Investigator of MANMADE\u000a      and RAVEN, Head of School, 2003-11).\u000a    The research establishes that overloading transmission lines in transport\u000a      or energy networks can trigger a cascade of failures resulting in a\u000a      critical breakdown. To address this we have developed models that allow us\u000a      to make predictions about robustness that are of significant use to\u000a      network designers. We have developed network algorithms that produce\u000a      graphs which are acutely prone to cascade breakdown [3, 4]. By\u000a      characterising these graphs we can determine the extent to which real\u000a      networks differ from these extreme constructs. This research has resulted\u000a      in models that allow us to investigate the connectivity and dynamics that\u000a      result from these graph-building scenarios, which in turn allows us to\u000a      propose counter-measures which can be employed to prevent cascade failure.\u000a      This research was carried out between 2007 and 2009 and was led by\u000a      Arrowsmith (see above) and Mondragon (co-investigator of MANMADE and\u000a      RAVEN, senior lecturer in electronic engineering).\u000a    "},{"CaseStudyId":"18055","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2921044","Name":"Germany"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    Predicting the distribution of vibrational energy in large coupled\u000d\u000a      systems is an important and\u000d\u000a      challenging task of major interest to industry. For example, cars contain\u000d\u000a      many moving components\u000d\u000a      that may produce resonances resulting in unwanted rattles, vibrations and\u000d\u000a      possibly dangerous\u000d\u000a      structural weaknesses. Avoiding or eliminating these resonances is a major\u000d\u000a      design task and\u000d\u000a      requires a detailed knowledge of the vibrational spectrum of the coachwork\u000d\u000a      and chassis.\u000d\u000a    There are two standard approaches to the problem &#8212; Finite Element Methods\u000d\u000a      (FEM) and Statistical\u000d\u000a      Energy Analysis (SEA) &#8212; however, these approaches are either restricted to\u000d\u000a      the low frequency\u000d\u000a      regime or rely on assumptions that are hard to control. In 2011, Bandtlow\u000d\u000a      and Tanner (University of\u000d\u000a      Nottingham) began a collaboration with inuTech (Innovative Numerical\u000d\u000a      Technologies) to overcome\u000d\u000a      these limitations and contribute to a black-box implementation of this\u000d\u000a      novel approach.\u000d\u000a    While for low frequencies FEM is preferable, and for high frequencies SEA\u000d\u000a      is the method of choice,\u000d\u000a      the novel scheme really demonstrates its value and works best in the\u000d\u000a      mid-frequency area, where it\u000d\u000a      outperforms FEM and SEA. The method uses finite element meshes based on\u000d\u000a      the actual topology\u000d\u000a      of the problem, and does not need any information about eigenmodes.\u000d\u000a    Based in Nuremberg, inuTech [6] is an SME targeting engineering\u000d\u000a      challenges that require\u000d\u000a      application of mathematical solutions. Within inuTech there is a team of\u000d\u000a      innovative and highly\u000d\u000a      qualified employees, possessing many years of research and development\u000d\u000a      experience, as well as\u000d\u000a      profound scientific competence. InuTech has 21 employees with revenue of\u000d\u000a      1.65 Million Euros. As\u000d\u000a      part of their work they develop products aimed at vibration prediction and\u000d\u000a      have used Bandtlow's\u000d\u000a      research and methods [1,2,3,4] to develop these products. In 2011, inuTech\u000d\u000a      engaged with\u000d\u000a      Bandtlow specifically as a result of his research around operator theory,\u000d\u000a      with a view towards the\u000d\u000a      prediction of the distribution of vibrational energy in large coupled\u000d\u000a      systems. The automotive\u000d\u000a      industry is an example beneficiary of inuTech's products &#8212; cars contain\u000d\u000a      many moving components\u000d\u000a      that may produce resonances resulting in unwanted rattles, vibrations and\u000d\u000a      possibly dangerous\u000d\u000a      structural weaknesses. The application of Bandltow's research and novel\u000d\u000a      methods ensure that\u000d\u000a      resonances are minimised and support successful design.\u000d\u000a    Between January to March 2011, Bandtlow was partly based at inuTech in\u000d\u000a      Nuremberg to\u000d\u000a      collaborate with inuTech, contributing to a black-box implementation of\u000d\u000a      their approach. The\u000d\u000a      software has progressed as a result of the application of Bandtlow's\u000d\u000a      research. The algorithms\u000d\u000a      developed are now stable and are able to cope with increasingly complex\u000d\u000a      situations. In March\u000d\u000a      2011, the decision was taken to incorporate the algorithms into the\u000d\u000a      software package. The\u000d\u000a      improvements made to the software considerably speeds up and improves the\u000d\u000a      accuracy of this\u000d\u000a      part of the design process, resulting in cheaper and safer cars. InuTech\u000d\u000a      acts as a consultant and\u000d\u000a      software provider for other companies, such as Airbus, opening up\u000d\u000a      possibilities for further\u000d\u000a      application. In addition to EADS (Airbus), commercially valuable contacts\u000d\u000a      to Range Rover, Hapag-Lloyd,\u000d\u000a      and Bombardier have been established and interest in the software has been\u000d\u000a      expressed by\u000d\u000a      those parties. So far, the software and the consultancy expertise has been\u000d\u000a      used to approach\u000d\u000a      problems posed by Range Rover on shock-absorber towers of cars, by\u000d\u000a      Bombardier for train\u000d\u000a      design, by EADS for the Airbus fuselage, and by the German Lloyd for the\u000d\u000a      design of ship hulls.\u000d\u000a    The founder and CEO [6] of inuTech says: \"The expertise offered by\u000d\u000a        Bandtlow plays an essential\u000d\u000a        role for our project to predict vibrational energy distributions in\u000d\u000a        large built-up structures. The\u000d\u000a        economic success of the implementation of these ideas depends crucially\u000d\u000a        on his input and I am\u000d\u000a        looking forward to continue this fruitful and beneficial collaboration\u000d\u000a        on related projects\". Impact and\u000d\u000a      collaboration have been put on a sustainable basis by regular meetings and\u000d\u000a      visits, the latest in\u000d\u000a      February 2013, which focused on considerable improvements of the software\u000d\u000a      implementation as a\u000d\u000a      result of the use of refined spectral approximations for optimal chassis\u000d\u000a      design.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Designs for complex structures like cars, aeroplanes and modern buildings\u000d\u000a      suffer from\u000d\u000a      unpredictable vibrations that lead to anything from irritating noises to\u000d\u000a      dangerous structural failures.\u000d\u000a      Predicting the distribution of vibrational energy in large coupled systems\u000d\u000a      is an important and\u000d\u000a      challenging task of major interest to industry. Until recently there was\u000d\u000a      no reliable method to predict\u000d\u000a      vibrations at the important mid-to-high frequency ranges.\u000d\u000a    There is a need to gain accurate predictions of vibrations at the design\u000d\u000a      stage. However, previous\u000d\u000a      techniques developed in the context of Quantum Chaos are too cumbersome to\u000d\u000a      be used in a fast-moving\u000d\u000a      commercial design setting. Bandtlow has used his expertise to develop a\u000d\u000a      novel method\u000d\u000a      that computes a very close approximation to these predictions but in a\u000d\u000a      reasonable time.\u000d\u000a      Bandtlow's method of constructing an efficient mathematical model for\u000d\u000a      spectral vibrations has\u000d\u000a      informed inuTech's latest product and led to enhanced performance of\u000d\u000a      automobiles and aircraft.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    Queen Mary University of London (QMUL)\u000d\u000a    ","Institutions":[{"AlternativeName":"Queen Mary, University of London","InstitutionName":"Queen Mary, University of London","PeerGroup":"A","Region":"London","UKPRN":10007775}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2861650","Name":"Nurnberg"},{"GeoNamesId":"2861650","Name":"Nuremberg"}],"References":"\u000d\u000a    \u000a1. OF Bandtlow (2004). `Estimates for norms of resolvents and an\u000d\u000a      application to the perturbation\u000d\u000a      of spectra'. Math. Nachr. 267, 3-11.\u000d\u000a    \u000a\u000a2. OF Bandtlow (2008). `Resolvent estimates for operators belonging to\u000d\u000a      exponential classes'.\u000d\u000a        Integr. Equ. Oper. Theory 61 (2008) 21-43.\u000d\u000a    \u000a\u000a3. OF Bandtlow and Oliver Jenkinson (2008). `On the Ruelle eigenvalue\u000d\u000a      sequence'. Ergod. Th. &amp;\u000d\u000a        Dynam. Sys. 28 1701-1711.\u000d\u000a    \u000a\u000a4. OF Bandtlow and O Jenkinson (2008) `Explicit eigenvalue estimates for\u000d\u000a      transfer operators\u000d\u000a      acting on spaces of holomorphic functions'. Adv. Math. 218\u000d\u000a      902-925.\u000d\u000a    \u000a\u000a5. O. Jenkinson (2002) EPSRC Fast Stream Grant GR\/R64650\/01 \"Approximating\u000d\u000a        Spectral Data\u000d\u000a        of Ruelle Transfer Operators\".\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    6. CEO and founder, inuTech (www.inuTech.de)\u000d\u000a      [impact of the research for the software\u000d\u000a      development (mainly concerning products for structural engineering) at\u000d\u000a      inuTech and the benefit\u000d\u000a      for the company in economic (financial) terms].\u000d\u000a    ","Title":"\u000d\u000a    Spectral theory to improve the accuracy of vibrational energy predictions\u000d\u000a      in\u000d\u000a      complex structures such as cars, aeroplanes and buildings\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Predicting the distribution of vibrational energy in large coupled\u000d\u000a      systems is a challenging task. The\u000d\u000a      two standard approaches to this problem, Finite Element Methods (FEM) and\u000d\u000a      Statistical Energy\u000d\u000a      Analysis (SEA), are either restricted to the low frequency regime or rely\u000d\u000a      on assumptions that are\u000d\u000a      hard to control. One of the mathematical challenges at the heart of Queen\u000d\u000a      Mary's research in this\u000d\u000a      area is to determine high-quality numerical approximations to transfer\u000d\u000a      operators, mathematical\u000d\u000a      objects that play a central role in the modern theory of dynamical\u000d\u000a      systems. This is an area where\u000d\u000a      Bandtlow has extensive research experience.\u000d\u000a    Operator Theory is concerned with linear transformations on\u000d\u000a      infinite-dimensional linear spaces.\u000d\u000a      Our research is concerned with the behaviour of a linear operator on a\u000d\u000a      Banach space (a vector\u000d\u000a      space) under perturbations. For example, given a linear operator, our\u000d\u000a      research is addressing how\u000d\u000a      spectral data of the operator (its spectrum, its isolated eigenvalues and\u000d\u000a      corresponding\u000d\u000a      eigenvectors) can be approximated in a consistent way. In practice,\u000d\u000a      spectral data of the transfer\u000d\u000a      operator is difficult to calculate. Part of Bandtlow's research in this\u000d\u000a      area has been concerned with\u000d\u000a      methods for the computation of high quality approximations to the true\u000d\u000a      spectral data and their\u000d\u000a      application to concrete problems. One such method, which is particularly\u000d\u000a      suitable for piecewise\u000d\u000a      analytic Markov transformations, is the so-called finite section method.\u000d\u000a      The underlying idea of this\u000d\u000a      method [4] is to approximate the transfer operator by a sequence of finite\u000d\u000a      rank operators.\u000d\u000a    The eigendata of the finite-rank approximants is readily computable and\u000d\u000a      is usually taken to provide\u000d\u000a      increasingly accurate approximations to the true eigendata. While a number\u000d\u000a      of authors have\u000d\u000a      recently used this procedure in concrete cases observing a rapid and\u000d\u000a      empirically stable\u000d\u000a      approximation, no proof of the convergence of this method has appeared in\u000d\u000a      the literature so far.\u000d\u000a    Bandtlow's research in this area was established in 2002 while employed\u000d\u000a      as a research assistant\u000d\u000a      in the School of Mathematical Sciences at Queen Mary under the EPSRC Fast\u000d\u000a      Stream Grant [5].\u000d\u000a      Bandtlow and Jenkinson [3,4] proved that under general conditions, the\u000d\u000a      finite section method\u000d\u000a      converges, and that the speed of convergence is exponential (or at least\u000d\u000a      stretched exponential,\u000d\u000a      depending on the dimension of the underlying system); moreover, using\u000d\u000a      abstract perturbation\u000d\u000a      results [1,2], rigorous and explicitly computable error-bounds for the\u000d\u000a      approximation have been\u000d\u000a      derived. These results have been applied to obtain convergence and error\u000d\u000a      estimates for the\u000d\u000a      spectral approximation of transfer operators for the Gauss map, justifying\u000d\u000a      empirical results\u000d\u000a      previously obtained by other authors.\u000d\u000a    "},{"CaseStudyId":"18056","Continent":[],"Country":[],"Funders":["Royal Society"],"ImpactDetails":"\u000d\u000a    Gilmour's research has led to changes and improvements in guidelines on\u000d\u000a      statistical procedures issued by the CEC, and has led to Shell Global\u000d\u000a      Solutions incorporating his methodology in their portfolio of statistical\u000d\u000a      techniques used to estimate fuel economy and other benefits from\u000d\u000a      experimental data. His work has refined and improved the technical quality\u000d\u000a      of their processes, with a concomitant financial benefit although this is\u000d\u000a      difficult to quantify.\u000d\u000a    Test methods for ascertaining the performance of fuels and lubricants\u000d\u000a      need to be precise and reliable. In order to ensure the former, round\u000d\u000a      robin studies are performed, where identical samples of, for example, a\u000d\u000a      fuel are tested repeatedly within the same and across different\u000d\u000a      laboratories.\u000d\u000a    Important numerical outcome measures of this process are the\u000d\u000a      repeatability and reproducibility. These precision measures reflect the\u000d\u000a      consistency of repeat measurements made at the same and at different\u000d\u000a      laboratories respectively. The formulae for calculating these measures\u000d\u000a      involve variance components, which are estimated from the data. Since in\u000d\u000a      cost-constrained experimentation the number of times which measurements\u000d\u000a      within labs are replicated is usually small, the methods in the ISO 5725\u000d\u000a      standard used by industry can lead to negative estimates of the\u000d\u000a      laboratory-to-laboratory variance component. This is unrealistically\u000d\u000a      assumed to be zero in standard computer packages and procedures. This\u000d\u000a      phenomenon has been observed by Shell and at CEC. The approach of Gilmour,\u000d\u000a      which underpins this impact, offers a solution to this problem.\u000d\u000a    The methodology has been used in round robin experiments for two test\u000d\u000a      methods (nozzle fouling and low temperature pumpability) by the Senior\u000d\u000a      Consultant Statistician at Shell, and the Chairman of the CEC Statistical\u000d\u000a      Development Group and Statistical Advisor at Infineum who formulate,\u000d\u000a      manufacture and market petroleum additives. A presentation has been made\u000d\u000a      to other members of the CEC Statistical Development Group on the\u000d\u000a      methodology, including examples of WinBUGS and SAS code, and this is\u000d\u000a      stored online in the private CEC-SDG working area.\u000d\u000a    Impact on European guidelines \u000d\u000a    CEC is the Coordinating European Council, the European Fuels and\u000d\u000a      Lubricants Performance Test Development Organisation. It represents the\u000d\u000a      motor, oil and petroleum additive industries and develops test methods to\u000d\u000a      evaluate the likely performance of different fuels and lubricants in the\u000d\u000a      field. Their test methods are used in member laboratories throughout\u000d\u000a      Europe and their performance is monitored by the regular testing of\u000d\u000a      reference fluids of known performance. Industrial statisticians support\u000d\u000a      each test method to ensure consistency and accuracy of application.\u000d\u000a      Normally laboratories would follow the ISO 5725 Accuracy [trueness and\u000d\u000a      precision] of measurement methods and results standard, as detailed in the\u000d\u000a      CEC manual. However, CEC began to notice that, in some of their round\u000d\u000a      robin testing, there was a problem with negative variance components,\u000d\u000a      particularly in studies constrained by cost where samples are tested just\u000d\u000a      once or twice in a limited number of laboratories. This was leading to\u000d\u000a      unreliable estimates of repeatability and reproducibility. Given the\u000d\u000a      significant impact of unreliable precision data across the industry, CEC\u000d\u000a      sought to revise their approach. As a result of Shell's Senior Consultant\u000d\u000a      Statistician presenting to both Shell and CEC on the Gilmour method, our\u000d\u000a      research [1] now features as an element of the CEC Statistics Manual\u000d\u000a      Procedures [8] as an alternative to ISO 5725. The Chair of the CEC\u000d\u000a      Statistical Development Group [7] confirms that \"...the Gilmour method\u000d\u000a        tends to give precision estimates which look more reasonable since they\u000d\u000a        have a sensible prior which the estimates shrink towards when there is\u000d\u000a        lack of information\". He also indicated that the methodology is\u000d\u000a      likely to be used in further round robin analyses for these two methods\u000d\u000a      and\/or others. It might also prove useful when the number of participating\u000d\u000a      laboratories and hence the degrees of freedom for reproducibility are\u000d\u000a      small.\u000d\u000a    Redefining statistical protocols for industry\u000d\u000a    After attending Gilmour's talk on `Multi-Stratum Response Surface Designs\u000d\u000a      in Industrial Experiments' at the International Symposium on Business and\u000d\u000a      Industrial Statistics, Shell Global Solutions first became interested in\u000d\u000a      his work and its potential application in their fuel technology\u000d\u000a      development programme. Following the subsequent publication of [1], and\u000d\u000a      its conclusion that a Bayesian approach culminates in a more appropriate\u000d\u000a      analysis, members of the Shell Global Solutions statistics team [9, 10]\u000d\u000a      contacted Gilmour for details about the method as they were keen to employ\u000d\u000a      it at their Research and Technology Centre at Thornton in Cheshire. Their\u000d\u000a      Senior Consultant Statistician [9] stated that Gilmour \"seemed to have\u000d\u000a        a solution to a problem we had identified\".\u000d\u000a    The Global Solutions team were involved in the testing of different fuels\u000d\u000a      in a number of cars to measure fuel economy. Because of the limited number\u000d\u000a      of experimental runs possible and logistical constraints, Shell's research\u000d\u000a      group cannot always use conventional, fully randomized, experimental\u000d\u000a      designs. Some variables are harder to change than others and cannot be\u000d\u000a      altered on a trial-to-trial basis. For example, the car age and engine\u000d\u000a      setup is harder to change than the fuel formulation. The real-life driving\u000d\u000a      conditions encountered during testing cannot be controlled at all. As a\u000d\u000a      result, experimental design and analysis becomes extremely challenging. In\u000d\u000a      a `simple' run of experiments, where the variables can be altered from\u000d\u000a      test to test ad lib, all treatment effects can be tested against a single\u000d\u000a      error term in the subsequent analysis of variance. However, in Shell's\u000d\u000a      case, some variables can only be changed from day-to-day rather than from\u000d\u000a      run-to-run for example. Such variables need to be tested against an\u000d\u000a      appropriate combination of the day-to-day and within-day variance\u000d\u000a      components. But as the data available from experiments is often sparse,\u000d\u000a      the estimates of the error terms were unreliable. As a result, the\u000d\u000a      experiments that Shell were running were not as informative as they\u000d\u000a      required. With the support of a Knowledge Transfer Partnership grant,\u000d\u000a      Gilmour placed his PhD student Lutfor Rahman in the Senior Consultant\u000d\u000a      Statistician at Shell's department for a three-month placement, during\u000d\u000a      which time they developed a step-by-step method for the implementation of\u000d\u000a      their new analytical method so that it could be used in Shell's ongoing\u000d\u000a      programme of fuel technology development. The Gilmour method was used to\u000d\u000a      analyse fuel economy data sets which had a day-to-day component of\u000d\u000a      variance which was difficult to estimate as the number of days' testing\u000d\u000a      was generally small. Standard methods were often arriving at negative\u000d\u000a      estimates, which the Shell team recognised as unrealistic. By using the\u000d\u000a      Gilmour method in addition to their own methods during the project, the\u000d\u000a      Shell team were able to use the Gilmour Bayesian analysis to gain\u000d\u000a      confidence in the results and as a consequence not recommend further\u000d\u000a      expensive testing. Shell Global Solutions have now added to their\u000d\u000a      repertoire of techniques this type of multi-stratum analysis as a solution\u000d\u000a      to an identified class of problems.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The petrochemical industry is eager to develop advanced fuels which\u000d\u000a      improve fuel efficiency both for economic and environmental reasons.\u000d\u000a      Statistics plays a crucial role in this costly process. Innovative\u000d\u000a      Bayesian methodology developed by Gilmour was applied at Shell Global\u000d\u000a      Solutions to data from fuel experiments to solve a recurring statistical\u000d\u000a      problem. The usefulness of this approach to the wider petrochemical\u000d\u000a      industry has been recognized by the industry-based Coordinating European\u000d\u000a      Council (CEC) for the Development of Performance Tests for Fuels,\u000d\u000a      Lubricants and other Fluids, who in their statistics manual have included\u000d\u000a      Gilmour's method as an alternative to procedures in the ISO 5725 standard.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    Queen Mary University of London (QMUL)\u000d\u000a    ","Institutions":[{"AlternativeName":"Queen Mary, University of London","InstitutionName":"Queen Mary, University of London","PeerGroup":"A","Region":"London","UKPRN":10007775}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1. S. G. Gilmour and P. Goos (2009) `Analysis of data from nonorthogonal\u000d\u000a      multi-stratum designs in industrial experiments'. Journal of the Royal\u000d\u000a        Statistical Society, Series C, 58, 467-484.\u000d\u000a    \u000a\u000a2. M. Y. Baba and S. G. Gilmour (2007) `Bayesian estimation from\u000d\u000a      saturated factorial designs'. In Bayesian Process Monitoring, Control\u000d\u000a        and Optimization (eds. B. M. Colosimo and E. del Castillo), Chapman\u000d\u000a      &amp; Hall\/CRC, New York, 2007, pp.311-322.\u000d\u000a    \u000a\u000a3. L. A. Trinca and S. G. Gilmour (2001) `Multi-stratum response surface\u000d\u000a      designs'. Technometrics, 43, 25-33.\u000d\u000a    \u000a\u000a4. S. G. Gilmour, J. M. Pardo, L. A. Trinca, K. Niranjan and D. S.\u000d\u000a      Mottram (2000) `A split-unit response surface design for improving aroma\u000d\u000a      retention in freeze dried coffee'. Proceedings of the 6th European\u000d\u000a        Conference on Food-Industry and Statistics, 18.0-18.9.\u000d\u000a    \u000a\u000a5. P. Goos and S. G. Gilmour (2012) `A general strategy for analysing\u000d\u000a      data from split-plot and multistratum experimental designs'. Technometrics,\u000d\u000a      54, 340-354.\u000d\u000a    \u000a\u000a6. S. G. Gilmour and L. A. Trinca (2012) `Optimal design criteria for\u000d\u000a      statistical inference (with discussion)'. Applied Statistics, 61,\u000d\u000a      345-401.\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a    \u000d\u000a      Chairman of the CEC Statistical Development Group [Impact of the\u000d\u000a        research on European guidelines for fuels and lubricants performance\u000d\u000a        testing, in particular, on modifications by CEC to the ISO 5725\u000d\u000a        standard.]\u000d\u000a      \u000awww.cectests.org\/cec-constitution.asp\u000d\u000a        [click on Statistics; Statistics Manual &#8212; Procedure 1]\u000d\u000a      Senior Consultant Statistician, Shell Global Solutions, UK [inclusion\u000d\u000a        of techniques based on QMUL research in Shell's portfolio of analysing\u000d\u000a        precision studies and assessing test methods, with improved technical\u000d\u000a        quality of processes and boosting financial efficiency.\u000d\u000a      Consultant Statistician, Shell Global Solutions, UK [inclusion of\u000d\u000a        techniques based on QMUL research in Shell's portfolio of analysing\u000d\u000a        precision studies and assessing test methods, with improved technical\u000d\u000a        quality of processes and boosting financial efficiency]\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Economical Experiments for the Fuel Efficiency Industry\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The underpinning research and methodology was developed by Gilmour,\u000d\u000a      Reader (then Professor) in Statistics at Queen Mary from 2000 until 2010.\u000d\u000a      During this period he made important contributions to the design and\u000d\u000a      analysis of industrial experiments and developed the advanced Bayesian\u000d\u000a      statistical methods used at Shell and by participating CEC laboratories.\u000d\u000a    Since the mid-1990s there has been an increasing realisation that many\u000d\u000a      industrial factorial experiments involve some factors whose levels are\u000d\u000a      more difficult to reset than others and this leads naturally to the use of\u000d\u000a      multi-stratum designs, such as nonorthogonal split-plot designs. These\u000d\u000a      imply the use of linear mixed models, with random error terms\u000d\u000a      corresponding to each stratum in the design, and the experiments must be\u000d\u000a      designed with such models in mind. The first paper [3] to deal directly\u000d\u000a      with designing such experiments, rather than adapting existing designs to\u000d\u000a      the required structures, was by Trinca and Gilmour.\u000d\u000a    As well as developing the methodology, Gilmour was closely involved in\u000d\u000a      applying it to specific experiments, e.g. a food processing experiment\u000d\u000a      which investigated the effects of five process factors on the drying rate\u000d\u000a      and the retention of several volatile compounds for freeze-dried coffee\u000d\u000a      [4]. One of the factors, the pressure, could only be reset once per day,\u000d\u000a      while the other factors could be reset for five runs each day. While the\u000d\u000a      research on multi-stratum designs provided a useful and informative design\u000d\u000a      for this experiment, the data analysis had one surprising feature. The\u000d\u000a      state-of- the-art method of analysis for data from multi-stratum designs\u000d\u000a      involves estimating the random effects by residual maximum likelihood\u000d\u000a      (REML) and the fixed effects by empirical generalized least squares (GLS).\u000d\u000a      In the freeze dried coffee experiment, the variance component for days was\u000d\u000a      estimated to be zero. One consequence of this is that the standard errors\u000d\u000a      of the effects of pressure are estimated to be the same as they would have\u000d\u000a      been if the pressure had been reset for every run, which might be too\u000d\u000a      optimistic.\u000d\u000a    The estimation of variance components for high strata to be zero was\u000d\u000a      found in several other data sets from multi-stratum designs and in 2006,\u000d\u000a      Goos published results showing that there was a high probability of\u000d\u000a      obtaining zero estimates, even if the true value was substantially greater\u000d\u000a      than zero. In many cases such estimates are unrealistic and arise due to a\u000d\u000a      flat likelihood in designs which are rather uninformative about these\u000d\u000a      variance components. Gilmour and Goos obtained Royal Society international\u000d\u000a      joint project grant 2007\/R2 to study the problem further.\u000d\u000a    The specific research that underpins the impact is published in the paper\u000d\u000a      [1] by Gilmour and Goos, combining ideas from the analysis of saturated\u000d\u000a      designs [2] with the mixed models used for analysing data from\u000d\u000a      multi-stratum designs. Paper [1] details a general method of statistical\u000d\u000a      analysis for multi-stratum designs that incorporates prior information\u000d\u000a      into the experimental analysis when estimating variance components.\u000d\u000a      Gilmour and Goos compared their Bayesian approach to the commonly-used\u000d\u000a      REML-GLS for non-orthogonal split-plot designs. They discovered that REML-\u000d\u000a      GLS estimation in non-orthogonal split-plot designs with few main plots\u000d\u000a      gives misleading conclusions, whereas their Bayesian approach led to a\u000d\u000a      more informative analysis.\u000d\u000a    The proposed method uses Bayesian analysis [1, 2] incorporating a prior\u000d\u000a      distribution for the main plot variance component, which in the round\u000d\u000a      robin experiments of fuels considered in the impact case corresponds to\u000d\u000a      the laboratory-to-laboratory variance component. The method is implemented\u000d\u000a      by using Markov chain Monte Carlo sampling [1] using the WinBUGS software\u000d\u000a      and SAS code. With this Bayesian approach informative priors will outweigh\u000d\u000a      uninformative data, and informative data will outweigh uninformative\u000d\u000a      priors. As a consequence, negative and zero estimates of variance\u000d\u000a      components can be avoided.\u000d\u000a    The design and analysis of multi-stratum experiments has become one of\u000d\u000a      the most vibrant areas of research in the design and analysis of\u000d\u000a      experiments. In addition to introducing the Bayesian analysis, Gilmour and\u000d\u000a      Goos outlined a general strategy for choosing an appropriate model to\u000d\u000a      analyse data from multi-stratum designs in a paper [5] which won the 2012\u000d\u000a      American Statistical Association Award for Statistics in Chemistry.\u000d\u000a      Meanwhile, Gilmour was PI on Engineering and Physical Sciences Research\u000d\u000a      Council research grant EP\/C541715\/1, `Unifying approaches to design of\u000d\u000a      experiments' (&#163;435,000) from 2005-2010, one of the themes of which was on\u000d\u000a      multi- stratum designs. This lead, amongst other publication to an RSS\u000d\u000a      discussion paper [6], which introduced the idea of designing experiments\u000d\u000a      specifically to ensure good and robust estimation of the required variance\u000d\u000a      components.\u000d\u000a    "},{"CaseStudyId":"18325","Continent":[{"GeoNamesId":"6255146","Name":"Africa"},{"GeoNamesId":"6255150","Name":"South America"},{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"286963","Name":"Oman"},{"GeoNamesId":"3144096","Name":"Norway"},{"GeoNamesId":"3469034","Name":"Brazil"},{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"2300660","Name":"Ghana"},{"GeoNamesId":"1269750","Name":"India"}],"Funders":[],"ImpactDetails":"\u000a    The GAMLSS growth curve methodology has improved and extended the\u000a      previous major existing methodology of the LMS method. The researchers\u000a      also provide free appropriate software for fitting the models and most\u000a      importantly checking their adequacy. This allows the user to find the most\u000a      appropriate model for their data.\u000a    The impact of any statistical methodological contribution, such as the\u000a      GAMLSS growth curve methodology, can only be measured by its usefulness in\u000a      practical applications and the impact those applications have to the wider\u000a      community. In what it follows we will argue that this is the case with\u000a      GAMLSS methodology. Firstly, we will consider the impact that the WHO\u000a      Growth Reference Study has and will have upon world health.\u000a    The following quote from their website explains the aims of their study:\u000a      \"The WHO Multicentre Growth Reference Study (MGRS) was undertaken between\u000a      1997 and 2003 to generate new growth curves for assessing the growth and\u000a      development of infants and young children around the world. The MGRS\u000a      collected primary growth data and related information from approximately\u000a      8500 children from widely different ethnic backgrounds and cultural\u000a      settings (Brazil, Ghana, India, Norway, Oman and the USA). The new growth\u000a      curves are expected to provide a single international standard that\u000a      represents the best description of physiological growth for all children\u000a      from birth to five years of age and to establish the breastfed infant as\u000a      the normative model for growth and development.\"\u000a    Upon the completion of the analysis WHO published three books, WHO (2006,\u000a      2007, 2009), which are available from their website http:\/\/www.who.int\/childgrowth\/standards\/en\/.\u000a      The impact of the release of Growth Reference Study curves is explained by\u000a      the article \"Worldwide implementation of the WHO Child Growth Standards\"\u000a      of de Onis et al. (2012) from which the following quote is taken\u000a      \"By April 2011, 125 countries had adopted the WHO standards, another\u000a      twenty-five were considering their adoption and thirty had not adopted\u000a      them. Preference for local references was the main reason for\u000a      non-adoption.\" The article also provided a map to show which countries\u000a      adopted the WHO standards curves.\u000a    The important point to make here is that results, from applying the\u000a      GAMLSS methodology, are used and will be used for the next decades in a\u000a      majority of countries of the world for checking the health and the\u000a      wellbeing of children.\u000a    A further major application of the methodology is its use by the Global\u000a      Lung Function Initiative. The initiative uses GAMLSS methodology to\u000a      provide equations for obtaining the lung function centile 'z- score' given\u000a      values of age, height, ethnic group and gender, Quanjer et al. (2012a).\u000a      The equations have been adopted by a variety of commercial companies, as\u000a      can be seen in the page 'Manufacturers'\u000a      in the www.lungfunction.org\u000a      page.\u000a    ","ImpactSummary":"\u000a    The methodology for growth curve estimation developed by Rigby and\u000a      Stasinopoulos (2004,2006) has been used worldwide. The World Health\u000a      Organisation used the methodology and the related software exclusively for\u000a      developing child growth standards, WHO (2006, 2007, 2009). The Global Lung\u000a      Function Initiative (GLFI), (www.lungfunction.org,\u000a      Stanojevic et al. 2008, Cole et al. 2009, ) use it for\u000a      providing a unified worldwide approach to lung function in growth and\u000a      ageing. The methodology is now the gold standard for developing growth\u000a      curves.\u000a    ","ImpactType":"Health","Institution":"\u000a    ","Institutions":[{"AlternativeName":"London Metropolitan University","InstitutionName":"London Metropolitan University","PeerGroup":"D","Region":"London","UKPRN":10004048}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a2004 Rigby, R. A. &amp; Stasinopoulos, D. M. Smooth\u000a        centile curves for skew and kurtotic data modelled using the Box-Cox\u000a        power exponential distribution. Statistics in Medicine, 23,\u000a      3053-3076\u000a    \u000a\u000a2005 Rigby R. A. and Stasinopoulos M.D. Generalised additive models for\u000a      location scale and shape, (with discussion), Appl. Statist., 54, part 3,\u000a      pp 507-554.\u000a    \u000a\u000a2006 Rigby R. A. and Stasinopoulos M.D. (2006) Using the Box-Cox t\u000a      distribution in GAMLS modelling of skewness and kurtosis. Statistical\u000a      Modelling, 6, pp 209-229.\u000a    \u000a\u000a2006 Borghi E, de Onis M, Garza C, Van den Broeck J, Frongillo EA,\u000a      Grummer-Strawn L, Van Buuren S, Pan H, Molinari L, Martorell R, Onyango\u000a      AW, Martines JC, Construction of the World Health Organization child\u000a      growth standards: selection of methods for attained growth curves Statistics\u000a        in Medicine 2006;25(2):247-65.\u000a    \u000a(the first three papers were submitted in that last RAF)\u000a    ","ResearchSubjectAreas":[{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"8","Level2":"6","Subject":"Information Systems"}],"Sources":"\u000a    The following three volumes provide evidence for the use of the GAMLSS\u000a      methodology and software in the creation of the child growth standards by\u000a      WHO:\u000a    WHO\u000a        Child Growth Standards: Methods and development: Length\/height-for-age,\u000a        weight-for- age, weight-for-length, weight-for-height and body mass\u000a        index-for-age. (2006) Geneva: World Health Organization\u000a    WHO\u000a        Child Growth Standards: Methods and development: Head\u000a        circumference-for-age, arm circumference-for-age, triceps\u000a        skinfold-for-age and subscapular skinfold-for-age. (2007) Geneva:\u000a      World Health Organization,\u000a    WHO\u000a        Child Growth Standards: Methods and development (2009) Growth\u000a      velocity based on weight, length and head circumference. Geneva: World\u000a      Health Organization.\u000a    The following articles support the use of the GAMLSS methodology and\u000a      software by the Global Lung Function Initiative:\u000a    Cole, T. J., Stanojevic, S., Stocks, J., Coats, A. L., Hankinson, J. L.\u000a      and Wade, A. M. (2009) Age-\u000a        and size related reference ranges: A case study of spirometry through\u000a        childhood and adulthood. Statistics in Medicine,28,\u000a      880-898.\u000a    Stanojevic, S., Wade, A.M., Stocks, J., Hankinson, J.L., Allan, L.,\u000a      Coates, A.L., Pan, H., Rosenthal, M., Corey, M., Lebecque, B., and Cole,\u000a      T.J. (2008) Reference\u000a        ranges for spirometry across all ages: a new approach. American\u000a        Journal of Respiratory and Critical Care Medicine, 177,\u000a      253-260.\u000a    Quanjer, P.H., Stanojevic, S., Cole, T.J., Baur, X., Hall, G.L., Culver,\u000a      B.H., Enright, P.L., Hankinson, J.L., Ip, I.M.S.M. Zheng, J., Stocks, J.\u000a      and the ERS Global Lung Function Initiative (2012a Multi-ethnic\u000a        reference values for spirometry for the 3-95-yr age range: the global\u000a        lung function 2012 equations. European Respiratory Journal,\u000a      40, 1324-1343.\u000a    Quanjer, P.H., Stanojevic, S., Stocks, J. and Cole, T.J. (2012b) GAMLSS\u000a      in action. Available from www.lungfunction.org\u000a    \u000a    ","Title":"\u000a      Centile and growth curves estimation (London Metropolitan\u000a          University)\u000a     ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The most popular method for growth curve estimation was the LMS method\u000a      developed by Cole and Green (1992). Rigby and Stasinopoulos (2004, 2005,\u000a      2006) extended the LMS method (which allows for location, scale and\u000a      skewness but not for kurtosis in the data), by introducing the 4-\u000a      parameter Box-Cox power exponential (BCPE) and the Box-Cox t (BCT)\u000a      distributions and called the resulting centile and quantile estimation\u000a      methods LMSP and LMST, respectively. The BCCG (equivalent to the LMS\u000a      method), BCPE and BCT distributions are part of the GAMLSS general\u000a      framework of models. The GAMLSS methodology for growth curves also\u000a      generalised the LMS method by allowing multiple explanatory variables and\u000a      factors in the model for each of the four parameters of the BCPE or BCT\u000a      distributions something which was found useful in the GLFI. The interest\u000a      of the two researchers in growth curve methodology started around 2000. In\u000a      September 2002 Dr Rigby and Prof. Stasinopoulos were contacted by the\u000a      Department of Nutrition for Health and Development of the World Health\u000a      Organisation, expressing an interest in the then little known GAMLSS\u000a      models as a potential method for the construction of growth curves. This\u000a      resulted in a close collaboration, which led to improvements in the GAMLSS\u000a      software with the inclusion of several diagnostic techniques appropriate\u000a      for comparing and evaluating the fitted growth curves. In the summer of\u000a      2003 the decision was taken by WHO that the LMSP method using the BCPE\u000a      distribution was the most appropriate method for the construction of the\u000a      standard growth curves for the merged data collected from six\u000a      representative countries of the world. The arguments for choosing the\u000a      GAMLSS methodology for constructing the growth curves, as opposed to at\u000a      least 30 other competing methodologies, were published by Borghi et\u000a        al. (2006). The WHO subsequently published the actual child growth\u000a      standards curves in three volumes, WHO (2006, 2007, 2009).\u000a    Another example of worldwide application of the GAMLSS methodology is the\u000a      Global Lung Function Initiative (GLFI). The GLFI aim is to set a unified\u000a      worldwide approach to lung function in growth and ageing. The GAMLSS\u000a      methodology was used to provide equations for obtaining the lung function\u000a      centile 'z-score' given values of age, height, ethnic group and gender.\u000a      These are two major applications of the GAMLSS methodology in growth curve\u000a      fitting, although the method is also now used widely for the construction\u000a      of growth curves.\u000a    Both Dr Rigby and Prof. Stasinopoulos have been researching continuously\u000a      at London Metropolitan University (University of North London before 2001)\u000a      for more than twenty years.\u000a    "},{"CaseStudyId":"18326","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2963597","Name":"Ireland"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The work by Rudge and Gilchrist has been used as evidence on the health\u000d\u000a      effects of fuel poverty by consumer and advisory groups, NHS groups, UK\u000d\u000a      central and local government and various overseas organisations. As such,\u000d\u000a      it has influenced the decisions of a wide variety of government and health\u000d\u000a      authorities, and informed the public debate on fuel poverty and poor\u000d\u000a      housing.\u000d\u000a    Thus, in proposing policies to discuss and alleviate fuel poverty, the\u000d\u000a      following organisations, inter alia, have cited the Rudge\/Gilchrist papers\u000d\u000a      listed in section 3:\u000d\u000a    NHS:\u000d\u000a    \u000d\u000a      NHS Rotherham (Keeping warm in later life project),\u000d\u000a      NHS Oldham, (Winter Warmth fact sheet),\u000d\u000a      NHS Medway (Medway Keep Well Keep Warm),\u000d\u000a      NHS Nottingham City (Excess Winter Deaths: A Health Needs Assessment\u000d\u000a        for NHS),\u000d\u000a      NHS Hillingdon (Excess Winter Deaths)Winter Deaths\u000a\u000d\u000a    \u000d\u000a    UK central and local authorities:\u000d\u000a    \u000d\u000a      UK Equality and Human Rights Commission see [1],\u000d\u000a      Kirklees Council [2],\u000d\u000a      Lewisham Council (Fuel poverty and Excess Winter Deaths in Lewisham),\u000d\u000a      Carlisle City Council (Report to Chief Exec of Carlisle Council),\u000d\u000a      Harrow Council (Wider determinants that affect health and well being:\u000d\u000a        Housing and Health),\u000d\u000a      Cumbria County Council (Anti Poverty Strategy),\u000d\u000a      South East Regional Public Health Group,\u000d\u000a      Health Protection Agency (An overview of winter planning &amp;\u000d\u000a        preparedness.), Mayor of London (The London climate change adaptation\u000d\u000a        strategy draft report.)\u000d\u000a    \u000d\u000a    Consumer groups:\u000d\u000a    \u000d\u000a      Consumer Focus [3],\u000d\u000a      Friends of the Earth [4],\u000d\u000a      National Women's Council of Ireland,\u000d\u000a      The British Columbia Public Interest Advocacy Centre [5]\u000d\u000a    \u000d\u000a    Advisory groups:\u000d\u000a    \u000d\u000a      Energy Action Scotland (The Relationship between Fuel Poverty and\u000d\u000a        Health: A Discussion paper),\u000d\u000a      Scottish Fuel Poverty Forum: (Review of the Scottish Government's Fuel\u000d\u000a        Poverty Strategy, Interim Report, May 2012)\u000d\u000a      Integrated Care Network (advises NHS, Local Government and 3rd\u000d\u000a        Sector).\u000d\u000a    \u000d\u000a    Overseas organisations:\u000d\u000a    \u000d\u000a      Organisation for Health Impact Assessment (USA),\u000d\u000a      Australian Building Codes Board [6],\u000d\u000a      Christchurch City Council, NZ [7],\u000d\u000a      Otago Government (NZ),\u000d\u000a      Connecticut Legislative District (USA) [8] .\u000d\u000a    \u000d\u000a    In addition to the papers in section 3, Rudge and Gilchrist made\u000d\u000a      presentations at substantive workshops and conferences, including at the\u000d\u000a      House of Commons. Rudge was invited to contribute to the WHO publication\u000d\u000a      on the burden of disease of inadequate housing on the basis of a Healthy\u000d\u000a      Housing conference, published in 2011 [9].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This report discusses the impact of the work by Rudge and Gilchrist on\u000d\u000a      the relationship between fuel poverty and poor health.This work has been\u000d\u000a      used as evidence on the health effects of fuel poverty by consumer and\u000d\u000a      advisory groups, NHS groups, UK central and local government and various\u000d\u000a      overseas organisations.\u000d\u000a    ","ImpactType":"Political","Institution":"\u000d\u000a    ","Institutions":[{"AlternativeName":"London Metropolitan University","InstitutionName":"London Metropolitan University","PeerGroup":"D","Region":"London","UKPRN":10004048}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2639093","Name":"Rotherham"},{"GeoNamesId":"2641022","Name":"Oldham"},{"GeoNamesId":"4831725","Name":"Connecticut"},{"GeoNamesId":"5909050","Name":"British Columbia"},{"GeoNamesId":"2641170","Name":"Nottingham"},{"GeoNamesId":"2643743","Name":"London"}],"References":"\u000d\u000a    The two principal papers cited by users of this research are:\u000d\u000a    \u000aRudge J. &amp; Gilchrist R. (2005) Excess winter morbidity among older\u000d\u000a      people at risk of cold homes: a population-based study in a London\u000d\u000a      borough. Journal of Public Health, 27,4, 353-358. doi:\u000d\u000a      10.1093\/pubmed\/fdi051\u000d\u000a    \u000a\u000aRudge J. &amp; Gilchrist, R.(2007). Measuring the health impact of\u000d\u000a      temperatures in dwellings: investigating excess winter morbidity and cold\u000d\u000a      homes in the London Borough of Newham. Energy and Buildings, 39,\u000d\u000a      847-858. . Doi: 10.1016\/j.enbuild.2007.02.007\u000d\u000a    \u000aThese papers were entered by Gilchrist as outputs 1 and 3 in the\u000d\u000a      Statistics and Operational Research (UOA22) submission of London\u000d\u000a      Metropolitan University, in the 2008 RAE.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    [1] Equality and Human Rights Commission: Socio-economic inequalities in\u000d\u000a      older people's access to and use of public services\u000d\u000a      http:\/\/justageing.equalityhumanrights.com\/socio-economic-inequalities-in-older-peoples-access-to-and-use-of-public-services\/\u000d\u000a    [2] Liddell C., Morris C., Lagdon S. (2011) Kirklees Warm Zone: The\u000d\u000a      project and its impacts on well-being. University of Ulster,\u000d\u000a      http:\/\/www.kirklees.gov.uk\/community\/environment\/energyconservation\/warmzone\/ulsterreport.pdf\u000d\u000a    [3] Fahmy, E. (2011) The definition and measurement of fuel poverty.\u000d\u000a      Consumer Focus briefing paper. http:\/\/www.consumerfocus.org.uk\/files\/2011\/06\/The-definition-and-measurement-of-fuel-poverty-Dr-Eldin-Fahmy.pdf\u000d\u000a    [4] The Marmot Review Team (2011) The Health Impact of Cold Homes and\u000d\u000a      Fuel Poverty.\u000d\u000a      http:\/\/online.eastherts.gov.uk\/moderngov\/documents\/s8254\/Housing%20and%20Health.pdf\u000d\u000a      Friends of the Earth.\u000d\u000a    [5] Kelly Liz (2007) Affordable Energy. The British Columbia Public\u000d\u000a      Advocacy Centre\u000d\u000a      http:\/\/www.bcuc.com\/Documents\/Proceedings\/2008\/DOC_18821_C10-2-1_Attachment_3_Clean-Copy.pdf\u000d\u000a    [6] Williamson T., Grant E. Hansen A. Pisaniello D. and Andamon M. (2009)\u000d\u000a      An Investigation of Potential Health Benefits from Increasing Energy\u000d\u000a      Efficiency Stringency Requirements Building Code of Australia. The\u000d\u000a      Australian Building Codes Board.\u000d\u000a      http:\/\/www.abcb.gov.au\/~\/media\/Files\/Download%20Documents\/Archived\/Major%20Initiatives\/Energy%20Efficiency\/Residential%20Housing\/31114%20An%20Investigation%20of%20Potential%20Health%20Benefits%20from%20Increasing%20Energy%20Efficiency%20Stringency%20Requirements.pdf\u000d\u000a    [7] Christchurch NZ City Council (2008) Community Wellbeing Research\u000d\u000a      review\u000d\u000a      http:\/\/resources.ccc.govt.nz\/files\/CommunityResearchProject-fullreport-2009-11-23.pdf\u000d\u000a    [8] Colton R. D (2011)F isher, Sheehan &amp; Colton Home Energy\u000d\u000a      Affordability Gap. Connecticut Legislative Districts.\u000d\u000a      http:\/\/www.operationfuel.org\/wp-content\/uploads\/Connecticut-2011-HEAG-Final.pdf\u000d\u000a    [9] Braubach, M., Jacobs, D.E. and Ormandy, D. (2011). Environmental\u000d\u000a      burden of disease associated with inadequate housing. Methods for\u000d\u000a      quantifying health impacts of selected housing risks in the WHO European\u000d\u000a      Region. Summary report. ISBN 978 92 890 0239 4 http:\/\/www.euro.who.int\/en\/what-we-publish\/abstracts\/environmental-burden-of-\u000d\u000a        disease-associated-with-inadequate-housing.-summary-report\u000d\u000a    ","Title":"\u000d\u000a    Fuel poverty modelling (London Metropolitan University)\u000d\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Research on morbidity relating to fuel poverty was carried out by Dr\u000d\u000a      Janet Rudge (Research Fellow) and Dr Robert Gilchrist (Professor) in the\u000d\u000a      period 2003-2008. The relationship between older people's health and fuel\u000d\u000a      poverty in a London Borough was examined using morbidity data at small\u000d\u000a      area level (enumeration districts, EDs). Excess winter morbidity modelling\u000d\u000a      was based on winter and non-winter counts of emergency hospital episodes\u000d\u000a      for respiratory diagnosis. Explanatory variables included energy\u000d\u000a      inefficient housing. A proposed Fuel Poverty Index, based on combined\u000d\u000a      variables, was found to be a predictor of excess winter morbidity.\u000d\u000a    Traditional approaches for analysing count data have used Poisson\u000d\u000a      regression. In this work, the authors modelled morbidity by a wide range\u000d\u000a      of distributions that allow for over-dispersion and they found that the\u000d\u000a      Poisson Inverse Gaussian distribution was superior for analysis to the\u000d\u000a      existing methodology.\u000d\u000a    Fossil fuel energy use in heating and cooling buildings is considered to\u000d\u000a      be a major contributor to observed climate change effects, so there is an\u000d\u000a      environmental imperative to reduce energy use in buildings. In one paper,\u000d\u000a      the authors reviewed epidemiological research to illustrate problems\u000d\u000a      associated with measuring the direct health impact of indoor temperatures,\u000d\u000a      for which evidence remains limited. Conventionally, temperature-related\u000d\u000a      health effects have been discussed in terms of seasonal excess deaths. The\u000d\u000a      authors developed a population-based study in London that considered\u000d\u000a      morbidity rather than mortality. The analysis indicated a link between the\u000d\u000a      risk of cold homes and excess winter hospital episodes, demonstrating its\u000d\u000a      potential for identifying small areas for priority action on improving\u000d\u000a      domestic energy efficiency in terms of health as well as the environment.\u000d\u000a    "},{"CaseStudyId":"19857","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2921044","Name":"Germany"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The Ross procedure, introduced in the 1960s, involves the replacement of\u000d\u000a      a failing aortic valve with the patient's own pulmonary valve. This\u000d\u000a      procedure did not gain popularity until the late 1980s, when the technique\u000d\u000a      for this operation was described in detail. The procedure became\u000d\u000a      increasingly popular during the 1990s, and a registry of patients going\u000d\u000a      under the Ross procedure was initiated. At this point, little was known\u000d\u000a      about the long-term outcome of this procedure. However, in the early\u000d\u000a      2000s, the fallibility of the pulmonary implant started to become apparent\u000d\u000a      [see Section 5, C1]. Dr Robinson's work on analysing data resulting from\u000d\u000a      patients undergoing the Ross procedure dates back to 2000. The initial\u000d\u000a      analysis, however, was based on data from the Cardiac Unit of the\u000d\u000a      University of L&#252;beck alone, and was later extended to larger datasets.\u000d\u000a      Around 2007, a group of physicians and researchers at L&#252;beck (Department\u000d\u000a      of Cardiac Surgery, University of L&#252;beck, Germany) decided to address\u000d\u000a      fears about the long-term success rate of the operation. This was\u000d\u000a      motivated by two factors: (a) the widespread interest in this procedure in\u000d\u000a      the early 1990s and (b) emerging good-quality data that could be used to\u000d\u000a      provide a quantitative analysis of the success rate of this procedure in\u000d\u000a      terms of the life expectancy and quality of life of patients undergoing\u000d\u000a      this procedure.\u000d\u000a    Using data from the large patient population of the Dutch-German Ross\u000d\u000a      Registry, Dr Robinson carried out a rigorous statistical analysis on 1,620\u000d\u000a      Ross-operated patients over a follow-up of 10,747 patient-years [R1, R2].\u000d\u000a      Dr Robinson's statistical analysis, and its interpretation by clinicians,\u000d\u000a      showed that the outcomes using this procedure compared well with the\u000d\u000a      outcomes of other approaches using artificial vales, and that the survival\u000d\u000a      rate of adult patients following the operation was similar to the expected\u000d\u000a      survival rate of the population.\u000d\u000a    In particular, the analysis confirmed that the autograft procedure is a\u000d\u000a      valid option to treat aortic- valve disease in selected patients. Namely,\u000d\u000a      the Ross procedure works well for young patients, and active people (e.g.\u000d\u000a      sportsmen\/women) who want to preserve their life quality by avoiding the\u000d\u000a      daily use of anticoagulants. Moreover, the Ross procedure compares\u000d\u000a      favourably for patients on dialysis [C2.2, C4 and C6], where the lifespan\u000d\u000a      is reduced but there is an increase in life quality due to the avoidance\u000d\u000a      of the regular use of anticoagulants. The result of Dr Robinson and his\u000d\u000a      co-workers have contributed (a) to changes in the current practice\u000d\u000a      guidelines of the European Society of Cardiologists [C2.1, C3 and C4 via\u000d\u000a      C7] and (b) to reversing the negative impact of papers published in the\u000d\u000a      1990s on the Ross procedure. Dr Charitos commented that `I believe our\u000d\u000a      work has lead to a renaissance and reappreciation of the Ross procedure in\u000d\u000a      the European area' [see C5 for relevant statistics].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The statistical analysis of large datasets has contributed to the\u000d\u000a      rehabilitation of the Ross procedure (the replacement of a failing aortic\u000d\u000a      valve with the patient's own pulmonary valve) for specific patient groups,\u000d\u000a      such as those above 50 years old who want to avoid daily anticoagulation\u000d\u000a      treatment, and those with a reduced life span, especially patients on\u000d\u000a      dialysis. The results of the research have (a) contributed to changes in\u000d\u000a      the current practice guidelines of the European Society of Cardiologists\u000d\u000a      and (b) have shown that, in contrast to previous beliefs, the Ross\u000d\u000a      procedure can still be safely performed when the aortic valve\u000d\u000a      malfunctions.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    University of Sussex\u000d\u000a    ","Institutions":[{"AlternativeName":"Sussex (University of)","InstitutionName":"University of Sussex","PeerGroup":"B","Region":"South East","UKPRN":10007806}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2875601","Name":"Lübeck"}],"References":"\u000d\u000a    \u000aR1 Sievers, H.-H., Stierle, U., Charitos, E.I., Hanke, T.,\u000d\u000a      Misfield, M., Matthias Bechtel, J.F., Gorski, A., Franke, U.F.W., Graf,\u000d\u000a      B., Robinson, D.R., Bogers, A.J., Dodge-Khatami, A.,\u000d\u000a      Boehm, J.O., Rein, J.G., Botha, C.A., Lange, R., Hoerer, J., Moritz, A.,\u000d\u000a      Wahlers, T., Breuer, M., Ferrari-Kuehne, K., Hetzer, R., Huebler, M.,\u000d\u000a      Ziemer, G., Takkenberg, J.J.M. and Hemmer, W. (2010) `Major adverse\u000d\u000a      cardiac and cerebrovascular events after the ross procedure: a report from\u000d\u000a      the German-Dutch Ross Registry', Circulation, 122(11 Suppl):\u000d\u000a      S216-223.\u000d\u000a    \u000a\u000aR2 Hanke, T., Charitos, E.I., Stierle, U., Robinson, D.R.,\u000d\u000a      Hemmer, W., Moritz, A., Lange, R. and Sievers, H.H. (2010) `The Ross\u000d\u000a      operation: a feasible and safe option in the setting of a bicuspid aortic\u000d\u000a      valve?', European Journal of Cardio-Thoracic Surgery, 38(3):\u000d\u000a      333-339.\u000d\u000a    \u000a\u000aR3 Hoerer, J., Hanke, T., Stierle, U., Takkenberg, J.J., Bogers,\u000d\u000a      A.J., Hemmer, W., Rein, J.G., Hetzer, R., Huebler, M., Robinson, D.R.,\u000d\u000a      Sievers, H.H. and Lange, R. (2009) `Homograft performance in children\u000d\u000a      after the Ross operation', Annals of Thoracic Surgery, 88(2): 609-\u000d\u000a      615.\u000d\u000a    \u000a\u000aR4 Hoerer, J., Hanke, T., Stierle, U., Takkenberg, J.J., Bogers,\u000d\u000a      A.J., Hemmer, W., Rein, J.G., Hetzer, R., Huebler, M., Robinson, D.R.,\u000d\u000a      Sievers, H.H. and Lange, R. (2009) `Neoaortic root diameters and aortic\u000d\u000a      regurgitation in children after the Ross operation', Annals of\u000d\u000a        Thoracic Surgery, 88(2): 594-600.\u000d\u000a    \u000a\u000aR5 Hanke, T., Charitos, E.I., Stierle, U., Robinson, D.R.,\u000d\u000a      Gorski, A., Sievers, H.H. and Misfeld, M. (2009) `Factors associated with\u000d\u000a      the development of aortic valve regurgitation over time after two\u000d\u000a      different techniques of valve-sparing aortic root surgery', Journal of\u000d\u000a        Thoracic and Cardiovascular Surgery, 137(2): 314-319.\u000d\u000a    \u000a\u000aR6 Hanke, T., Stierle, U., Boehm, J.O., Botha, C.A., Bechtel,\u000d\u000a      M.F., Erasmi, A., Misfeld, M., Hemmer, W., Rein, J.G., Robinson, D.R.,\u000d\u000a      Lange, R., Horer, J., Moritz, A., Ozaslan, F., Wahlers, T., Franke,\u000d\u000a      U.F.W., Hetzer, R., Hubler, M., Ziemer, G., Graf, B., Ross, D.N. and\u000d\u000a      Sievers, H.H. (2007) `Autograft regurgitation and aortic root dimensions\u000d\u000a      after the Ross procedure', Circulation, 116(11 Suppl): 251-258.\u000d\u000a    \u000aOutputs R1, R2, R3 best indicate the quality of the underpinning\u000d\u000a      research. Outputs can be supplied by the University on request.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    Below are testimonies from physicians, with direct reference to the\u000d\u000a      papers which Dr Robinson has co-authored and contributed to.\u000d\u000a    C1 David, T.E. (2009) `Ross procedure at the crossroads', Circulation,\u000d\u000a      119(2): 207-209.\u000d\u000a    C2 Professor Dr J.F. Matthias Bechtel, Heart and Chest Clinic,\u000d\u000a      University of Bochum.\u000d\u000a    One of the physicians involved (Dr J.F. Matthias Bechtel, now at the\u000d\u000a      Heart and Chest Clinic, university of Bochum, Germany) says:\u000d\u000a    C2.1 `I also believe that the many manuscripts on the\u000d\u000a      Ross-operation and the different ways it can be done contributed to the\u000d\u000a      fact that, for example in the current practice guidelines of the European\u000d\u000a      Society of Cardiologists, the Ross-operation is mentioned more detailed\u000d\u000a      and positive than before. I believe the analysis of the Dutch-German\u000d\u000a      registry contributed to this change'.\u000d\u000a    C2.2 `Today biological valves are preferred in dialysis patients;\u000d\u000a      for years, mechanical valves were preferred because it was feared that\u000d\u000a      there is an accelerated degeneration of biological valves in dialysis\u000d\u000a      patients'.\u000d\u000a    C3 Dr Efstratios I. Charitos, Cardiac and Thoracic Vascular\u000d\u000a      Surgery linic, University of L beck.\u000d\u000a    C4 The Joint Task Force on the Management of Valvular Heart\u000d\u000a      Disease of the European Society of Cardiology (ESC) and the European\u000d\u000a      Association for Cardio-Thoracic Surgery (EACTS) (2012) `Guidelines on the\u000d\u000a      management of valvular heart disease', European Heart Journal, 33:\u000d\u000a      2451- 2496.\u000d\u000a    C5 The number of Ross procedures performed during the REF period\u000d\u000a      shows an increase from a low of 87 in 1999 to a high of 144 in 2011 as\u000d\u000a      given by the Ross Registry. See analysis of data in an email from Dr\u000d\u000a      Charitos, dated 23 February 2013.\u000d\u000a    C6 Bechtel, T., Matthias, J.F., Detter, C. and Fischlein, T. et\u000a        al. (2008) `Cardiac surgery in patients on dialysis: decreased\u000d\u000a      30-day mortality, unchanged overall survival', Annals of Thoracic\u000d\u000a        Surgery, 85(1): 147-53. Dr Robinson is acknowledged `for expert\u000d\u000a      statistical analysis and his kind advice when planning the study'.\u000d\u000a    C7 Takkenberg, J.J.M., Klieverik, L.M.A., Schoof, P.H., van\u000d\u000a      Suylen, R.-J., van Herwerden, L.A., Zondervan, P.E., Roos-Hesselink, J.W.,\u000d\u000a      Eijkemans, M.J.C., Yacoub, M.H. and Bogers, A.J.J.C. (2008) `The Ross\u000d\u000a      procedure: a systematic review and meta-analysis', Circulation,\u000d\u000a      119(2): 222-228. \u000d\u000a    ","Title":"\u000d\u000a    The statistical analysis of heart-surgery data influences practice\u000d\u000a      guidelines and choice of procedures\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Dr Robinson's research interests are in the development of statistical\u000d\u000a      methodologies for tackling problems arising in engineering and medicine\u000d\u000a      [see Section 3, R1-R6], with previous work on the optimal control of\u000d\u000a      stochastic processes. He works with doctors in several hospitals in\u000d\u000a      Britain (e.g. the Royal Brompton and Harefield NHS Trust, St Thomas'\u000d\u000a      Hospital London, Aintree University Hospital Merseyside, the Royal Sussex\u000d\u000a      County Hospital, Brighton) and in Germany (e.g. the Department of Cardiac\u000d\u000a      and Thoracic Vascular Surgery at the University of L&#252;beck) on the design\u000d\u000a      and analysis of medical studies.\u000d\u000a    Statistical analysis of data arising from cardiac procedures most often\u000d\u000a      involves the development of survival and\/or longitudinal models of the\u000d\u000a      changing condition of a patient subsequent to a medical procedure being\u000d\u000a      carried out. Survival modelling typically involves the separate modelling\u000d\u000a      of short-term survival (e.g. using logistic regression) and long-term\u000d\u000a      survival (e.g. using Cox regression). The analysis usually aims to\u000d\u000a      determine the factors associated with longer survival, but other\u000d\u000a      considerations may also be important. For example, comparing patients'\u000d\u000a      survival with that of the general population, i.e. showing on a survival\u000d\u000a      graph not only how patients in the study have fared but also showing the\u000d\u000a      expected survival curve based on the national life table, matched by age\u000d\u000a      and gender. Another survival comparison is with published survival curves\u000d\u000a      based on earlier studies; bootstrap methods can be used to see whether\u000d\u000a      these curves differ significantly from the current study.\u000d\u000a    Longitudinal data have usually been modelled using multilevel and mixed\u000d\u000a      models adapted to the response being measured. Polynomial or piecewise\u000d\u000a      linear models have been used for continuous measurements, while ordinal\u000d\u000a      regression or Markov chain models have been used for ordinal measurements.\u000d\u000a    While Dr Robinson's many published research papers involve a large number\u000d\u000a      of co-authors, mainly from the medical profession, he is solely\u000d\u000a      responsible for the data-processing and the statistical analysis. All the\u000d\u000a      research relevant to this case study has been carried out at the\u000d\u000a      University of Sussex, where Dr Robinson has worked since 1989.\u000d\u000a    "},{"CaseStudyId":"19871","Continent":[{"GeoNamesId":"6255146","Name":"Africa"}],"Country":[{"GeoNamesId":"2440476","Name":"Niger"},{"GeoNamesId":"2453866","Name":"Mali"},{"GeoNamesId":"337996","Name":"Ethiopia"},{"GeoNamesId":"2245662","Name":"Senegal"},{"GeoNamesId":"2361809","Name":"Burkina Faso"}],"Funders":["Royal Society"],"ImpactDetails":"\u000d\u000a    To combat the devastating effect of meningococcal meningitis on\u000d\u000a      communities in the African\u000d\u000a      `meningitis belt', the international Meningitis Vaccine Project, funded by\u000d\u000a      the Gates Foundation and\u000d\u000a      the WHO, has been working since 2001 on developing an effective and cheap\u000d\u000a      vaccine to be\u000d\u000a      deployed in affected countries. The resulting MenAfriVac&#8482; vaccine has\u000d\u000a      completed its trials; in\u000d\u000a      2010 it was rolled out in Burkina Faso, Mali and Niger and, in\u000d\u000a      October-December 2012, it was\u000d\u000a      introduced in 7 more countries. The goal is to cover all 26 countries by\u000d\u000a      2016.\u000d\u000a    From a public-health perspective, there are two major issues with the\u000d\u000a      introduction of the\u000d\u000a      MenAfriVac&#8482; vaccine. The first concerns the logistical constraints of\u000d\u000a      optimising a vaccination\u000d\u000a      campaign to target those individuals most at risk of infection, and the\u000d\u000a      second is the need for a\u000d\u000a      robust means of assessment of the population-wide efficiency of the\u000d\u000a      vaccine. The model\u000d\u000a      developed by Blyuss and his colleagues is helping public-health\u000d\u000a      professionals on the ground to\u000d\u000a      address both of these issues. Through close collaboration with the\u000d\u000a      MenAfriCar Consortium, we\u000d\u000a      have ensured that the results of the research do not remain academic but\u000d\u000a      rather are translated into\u000d\u000a      practical recommendations for the design of optimal vaccination strategies\u000d\u000a      in vaccine deployment\u000d\u000a      and for assessment of the efficacy of the new vaccine. Experts from the\u000d\u000a      international MenAfriCar\u000d\u000a      Consortium have used the model and its subsequent developments (an\u000d\u000a      age-structured and meta-population\u000d\u000a      version of the model) to understand the prevalence, incidence and relative\u000d\u000a      impact of\u000d\u000a      different risk factors in the endemic areas. Furthermore, they have used\u000d\u000a      this work to develop\u000d\u000a      targeted, age-structured vaccination strategies [see Section 5, C3].\u000d\u000a    Besides MenAfriVac&#8482; vaccine deployment, the results of the underpinning\u000d\u000a      research have also\u000d\u000a      been taken up by the MERIT (Meningitis Environment Risk Information\u000d\u000a      Technologies) Project\u000d\u000a      coordinated by the World Health Organization for the purposes of disease\u000d\u000a      surveillance [C4], and\u000d\u000a      epidemiologists from the GAVI (Global Alliance for Vaccines and\u000d\u000a      Immunisation) are developing\u000d\u000a      further detailed models for the assessment of effects of vaccine\u000d\u000a      interventions based on the Blyuss\u000d\u000a      model [C7].\u000d\u000a    Since its publication in March 2012, this work has received substantial\u000d\u000a      interest [C5-C8] from\u000d\u000a      epidemiologists and public-health professionals. As, at present, there are\u000d\u000a      several alternative\u000d\u000a      hypotheses for the role of different epidemiological and environmental\u000d\u000a      factors in the dynamics of\u000d\u000a      meningococcal meningitis, the work has provided a new level of\u000d\u000a      understanding of the relative\u000d\u000a      contributions of those factors. This has resulted in a radically improved\u000d\u000a      understanding of the\u000d\u000a      dynamics of meningococcal meningitis by epidemiologists and clinical\u000d\u000a      scientists, thus helping them\u000d\u000a      to design and deliver efficient public-health policies aimed at combating\u000d\u000a      the disease.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Meningococcal meningitis affects up to 100,000 people and causes around\u000d\u000a      10,000 deaths annually\u000d\u000a      in the African `meningitis belt', a region of sub-Saharan Africa\u000d\u000a      stretching from Senegal in the west\u000d\u000a      to Ethiopia in the east. Dr Blyuss has developed a mathematical model that\u000d\u000a      is able to explain the\u000d\u000a      observed patterns of dynamics of this disease in terms of immunity and\u000d\u000a      seasonality. This model is\u000d\u000a      currently used by the Meningitis Vaccine Project to design optimal\u000d\u000a      strategies for the control of\u000d\u000a      meningococcal meningitis in the endemic areas, to inform specific\u000d\u000a      public-health decisions\u000d\u000a      regarding the deployment of the MenAfriVac&#8482; vaccine, and to assess its\u000d\u000a      effectiveness. Other\u000d\u000a      epidemiologists, including those at the World Health Organization (WHO),\u000d\u000a      are also using the model\u000d\u000a      to improve public-health policies aimed at combating meningitis.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    University of Sussex\u000d\u000a    ","Institutions":[{"AlternativeName":"Sussex (University of)","InstitutionName":"University of Sussex","PeerGroup":"B","Region":"South East","UKPRN":10007806}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    The mathematical model described in Section 2, that underpins the impact\u000d\u000a      of this case study, has\u000d\u000a      been published in:\u000d\u000a    \u000aR1 Irving, T.J., Blyuss, K.B., Colijn, C. and Trotter, C.L. (2012)\u000d\u000a      `Modelling meningococcal\u000d\u000a      meningitis in the African meningitis belt', Epidemiology and Infection,\u000d\u000a      140(5): 897-905.\u000d\u000a    \u000aThis paper proposes a model of the dynamics of meningococcal meningitis,\u000d\u000a      which provides a\u000d\u000a      comprehensive explanation of observed patterns of the disease in terms of\u000d\u000a      duration of the\u000d\u000a      immunity period, as well as seasonal variation in the transmissibility of\u000d\u000a      infection or the rate of\u000d\u000a      disease progression. It utilises the dynamical systems methodology as used\u000d\u000a      previously by\u000d\u000a      K.B. Blyuss in the studies of other infectious diseases.\u000d\u000a    Blyuss' previous expertise in this area:\u000d\u000a    \u000aR2 Blyuss, K.B. and Kyrychko, Y.N. (2010) `Stability and\u000d\u000a      bifurcations in an epidemic model with\u000d\u000a      varying immunity period', Bulletin of Mathematical Biology, 72(2):\u000d\u000a      490-505.\u000d\u000a    \u000a\u000aR3 Blyuss, K.B. and Gupta, S. (2009) `Stability and bifurcations\u000d\u000a      in a model of antigenic variation\u000d\u000a      in malaria', Journal of Mathematical Biology, 58(6): 923-937.\u000d\u000a    \u000a\u000aR4 Recker, M., Blyuss, K.B. and Simmons, C.P. et al.\u000d\u000a      (2009) `Immunological serotype\u000d\u000a      interactions and their effect on the epidemiological pattern of dengue', Proceedings\u000d\u000a        of the\u000d\u000a        Royal Society B, 276(1667): 2541-2548.\u000d\u000a    \u000aOutputs R1, R2, R4 best indicate the quality of the underpinning\u000d\u000a      research.\u000d\u000a    Outputs can be supplied by the University on request.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"11","Level2":"8","Subject":"Medical Microbiology"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    C1 Member of the MenAfriCar Consortium, Cambridge University).\u000d\u000a    Can corroborate how the mathematical model we derived and analysed has\u000d\u000a      influenced and\u000d\u000a      has been the basis for development of optimal vaccination strategies aimed\u000d\u000a      at controlling\u000d\u000a      meningococcal meningitis in the African meningitis belt.\u000d\u000a    C2 Programme Manager of the MenAfriCar Consortium, London School\u000d\u000a      of Hygiene and Tropical\u000d\u000a      Medicine).\u000d\u000a    Can confirm that the mathematical model has had a major effect on how\u000d\u000a      epidemiologists on\u000d\u000a      the ground in the African meningitis belt view and interpret the\u000d\u000a      population-level dynamics of\u000d\u000a      meningococcal meningitis, and on the development of optimal vaccination\u000d\u000a      strategies in\u000d\u000a      preparation for the deployment of the new meningococcal vaccine.\u000d\u000a    References shown below indicate some recent publications where the\u000d\u000a      results of the analysis were\u000d\u000a      interpreted in the light of our work highlighting the role of immunity in\u000d\u000a      the dynamics of meningitis.\u000d\u000a    C3 MenAfriCar Newsletter, August 2012.\u000d\u000a      (http:\/\/www.menafricar.org\/sites\/www.menafricar.org\/files\/Newsletter%20final.pdf)\u000d\u000a    C4\u000d\u000a    \u000d\u000a      Thomson, M.C., Firth, E., Jancloes, M., Mihretie, A., Onoda, M.,\u000d\u000a        Nickovic, S., Bertherat,\u000d\u000a        E. and Hugonnet, S. (2011) `Climate and public health &#8212; the MERIT\u000d\u000a        initiative: a climate\u000d\u000a        and health partnership to inform public health decision makers'. World\u000d\u000a        Climate Research\u000d\u000a        Programme.\u000d\u000a      Thomson, M.C., Firth, E., Jancloes, M., Mihretie, A., Onoda, M.,\u000d\u000a        Nickovic, S., Broutin, H.,\u000d\u000a        Sow, S., Perea, W., Bertherat, E. and Hugonnet, S. (2013) `A climate and\u000d\u000a        health\u000d\u000a        partnership to inform the prevention and control of meningococcal\u000d\u000a        meningitis in sub-Saharan\u000d\u000a        Africa: the MERIT initiative', in Asrar, G.R. and Hurrell, J.W. (eds) Climate\u000aScience\u000d\u000a          for Serving Society: Research, Modeling and Prediction Priorities.\u000d\u000a        Springer, 459-484.\u000d\u000a    \u000d\u000a    C5 Paireau, J., Girond, F., Collard, J.-M., Ma&#239;nassara, H.B. and\u000d\u000a      Jusot, J.-F. (2012) `Analysing\u000d\u000a      spatio-temporal clustering of meningococcal meningitis outbreaks in Niger\u000d\u000a      reveals\u000d\u000a      opportunities for improved disease control', PLOS Neglected Tropical\u000d\u000a        Diseases, 6: e1577.\u000d\u000a    `Recently, Irving et al. suggested that population immunity may\u000d\u000a      be a key factor in causing the\u000d\u000a      unusual epidemiology of meningitis in the Belt'.\u000d\u000a    C6 Agier, L., Deroubaix, A., Martiny, N., Yaka, P. Djibo, A. and\u000d\u000a      Broutin, H. (2013) `Seasonality of\u000d\u000a      meningitis in Africa and climate forcing: aerosols stand out', Journal\u000d\u000a        of the Royal Society\u000d\u000a        Interface, 10(79): 1742-5662.\u000d\u000a    `The current epidemiological [Irving et al.]...models for\u000d\u000a      meningitis considered ... seasonality\u000d\u000a      of the meningitis transmission dynamics. We now suggest integrating dust\u000d\u000a      data into these\u000d\u000a      models to make them more realistic and usable in a public health\u000d\u000a      perspective'.\u000d\u000a    C7 Papaevangelou, V. and Spyridis, N. (2012) `MenACWY-TT vaccine\u000d\u000a      for active immunization\u000d\u000a      against invasive meningococcal disease', Expert Review of Vaccines,\u000d\u000a      11(5): 523-537.\u000d\u000a    C8 Trotter, C.L. Yaro, S. and Njanpop-Lafourcade B.M. et al.\u000d\u000a      (2013) `Seroprevalence of\u000d\u000a      bactericidal, specific IgG antibodies and incidence of meningitis due to\u000d\u000a      group A Neisseria\u000d\u000a      meningitides by age in Burkina Faso', PLoS ONE, 8(2): e55486.\u000d\u000a    `Such hypothesis needs to be considered critically, as the regular\u000d\u000a      recurrence of epidemic\u000d\u000a      waves strongly suggest, according to general infectious disease dynamics\u000d\u000a      and recent\u000d\u000a      modelling evaluation [Irving et al.], a major role of acquisition\u000d\u000a      and waning of natural\u000d\u000a      immunity'.\u000d\u000a    ","Title":"\u000d\u000a    The mathematical modelling of meningococcal meningitis and implications\u000d\u000a      for\u000d\u000a      the control of meningitis in sub-Saharan Africa\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    In collaboration with colleagues from the University of Bristol, Blyuss\u000d\u000a      has developed a\u000d\u000a      mathematical model of transmission of meningococcal meningitis, which is\u000d\u000a      able to qualitatively\u000d\u000a      reproduce different types of observed disease patterns.\u000d\u000a    Twenty-six countries in sub-Saharan Africa suffer from a higher incidence\u000d\u000a      of meningococcal\u000d\u000a      meningitis every dry season and experience major outbreaks of the disease\u000d\u000a      every 6-14 years,\u000d\u000a      causing tens of thousands of deaths, with a case-fatality rate of 5-15%.\u000d\u000a      Substantial\u000d\u000a      epidemiological and clinical data on meningococcal meningitis are\u000d\u000a      available from different areas\u000d\u000a      inside the meningitis belt &#8212; primarily from Niger, Mali and Burkina Faso &#8212;\u000d\u000a      and several models have\u000d\u000a      been put forward to explain these data. A number of environmental factors\u000d\u000a      are believed to be\u000d\u000a      important in explaining the observed seasonality in meningitis incidence,\u000d\u000a      and several alternative\u000d\u000a      hypotheses have been proposed to explain how these factors affect disease\u000d\u000a      transmission. Despite\u000d\u000a      some successes with the existing models, the precise causes of observed\u000d\u000a      irregularities in the\u000d\u000a      dynamics of meningococcal meningitis and the relative roles played by\u000d\u000a      different factors have\u000d\u000a      remained poorly understood. One of the biggest challenges for current\u000d\u000a      understanding of the\u000d\u000a      meningococcal meningitis specifically in the `meningitis belt' is that the\u000d\u000a      available data for this region\u000d\u000a      are in contradiction with the classic Goldschneider paradigm that asserts\u000d\u000a      an inverse relationship\u000d\u000a      between the age-specific disease risk and immunity.\u000d\u000a    The model developed by Blyuss and his colleagues explicitly includes\u000d\u000a      temporary immunity, as well\u000d\u000a      as two possible types of seasonality: variation in disease transmission\u000d\u000a      and changes in the rate of\u000d\u000a      progression from carriage to invasive disease [see Section 3, R1]. Having\u000d\u000a      fixed other demographic\u000d\u000a      parameters at certain biologically realistic values, numerical simulations\u000d\u000a      have been performed for\u000d\u000a      various values of the disease transmission rate and the duration of\u000d\u000a      temporary immunity to identify\u000d\u000a      different dynamical regimes, as well as to explore the effects of these\u000d\u000a      parameters on the inter-epidemic\u000d\u000a      period. The main academic significance of this work lies in the\u000d\u000a      highlighting of a\u000d\u000a      fundamental role of temporary immunity and its interactions with\u000d\u000a      seasonality in the dynamics of\u000d\u000a      meningococcal meningitis. It also adds weight to an alternative\u000d\u000a      explanation of why observed data\u000d\u000a      contradict the classical view of the relation between disease risk and\u000d\u000a      immunity.\u000d\u000a    Konstantin Blyuss began his work in this area looking at models of\u000d\u000a      antigenic variation in malaria\u000d\u000a      and the dynamics of dengue fever (R3, R4). On moving to Sussex in 2010 he\u000d\u000a      began the work on\u000d\u000a      meningococcal meningitis, that underpins this case study. Blyuss worked\u000d\u000a      with PhD student Tom\u000d\u000a      Irving (University of Bristol), Caroline Colijn (University of Bristol)\u000d\u000a      and Caroline Trotter (University of\u000d\u000a      Bristol). Blyuss's previous experience in mathematical modelling of\u000d\u000a      infectious diseases [R2-R4]\u000d\u000a      meant that he was responsible for development and analysis of the model\u000d\u000a      for the dynamics of\u000d\u000a      meningococcal meningitis. This included performing analytical calculations\u000d\u000a      and numerical\u000d\u000a      simulations, and interpreting the results.\u000d\u000a    "},{"CaseStudyId":"20176","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Wellcome Trust","Medical Research Council"],"ImpactDetails":"\u000a    There are two main areas where IMPUTE software has made an economic\u000a      impact on companies\u000a      working in the area of genetics and pharmaceuticals:\u000a    \u000a      IMPUTE has had a significant impact on the company Affymetrix. It has\u000a        led to the\u000a        introduction of new products and has significantly changed a design\u000a        process. The company\u000a        has benefited by recently winning a genotyping contract worth ~&#163;25M.\u000a      IMPUTE has led to the improvement of a drug response study carried out\u000a        by Roche. This\u000a        saved the company an estimated ~$1,000,000.\u000a    \u000a    Affymetrix licensed the source code for both IMPUTE v1 (2009) and v2\u000a      (2010) from Oxford\u000a      University for &#163;250,000 [A]. Affymetrix use Impute v2 as a central part of\u000a      the process of designing\u000a      both generic and custom-made SNP chips (a chip is a collection of\u000a      microscopic DNA spots\u000a      attached to a solid surface). In addition, licences for use of the\u000a      software, without the source code,\u000a      worth ~&#163;70,000 in total have been sold to Genentech (2008),\u000a      GlaxoSmithKline (2008),\u000a      Biocomputing Platforms Ltd. (2009) and PGxHealth (2010) [A]. IMPUTE has\u000a      also been used in a\u000a      study of drug response by Roche via a 2011 consultancy agreement with\u000a      Marchini.\u000a    Optimizing product design at Affymetrix using IMPUTE\u000a    Genotype imputation is now a central method in human genetics utilized by\u000a      researchers carrying\u000a      out GWAS. The method is usually applied to data collected from genome-wide\u000a      SNP arrays.\u000a      Affymetrix is a $300M US company that makes such arrays together with the\u000a      equipment and\u000a      reagents to run the experiments. Such equipment is essential in any lab\u000a      carrying out its own\u000a      GWAS.\u000a    The company has used IMPUTE in the design process for a new series of\u000a      population-specific\u000a      arrays called the \"AxiomTM Genome-Wide EUR, EAS, LAT and AFR Arrays\",\u000a      targeted at the\u000a      European, East Asian, Latino and African populations. These arrays are\u000a      sold commercially to\u000a      research groups carrying out GWAS [B,C]. Affymetrix recently won the bids\u000a      to genotype &gt;500,000\u000a      participants for the UKBiLEVE (http:\/\/www.mrc.ac.uk\/Newspublications\/News\/MRC008925)\u000a      and\u000a      UK Biobank (http:\/\/www.ukbiobank.ac.uk\/)\u000a      studies, with a total study cost of ~&#163;25M. The UK\u000a      Biobank project is the largest single genotyping study on record in the\u000a      world as well as the largest\u000a      single project in the Affymetrix Genotypic revenue base [B]. The company\u000a      states that \"a significant\u000a        competitive advantage of the Affymetrix proposal was a custom GWAS grid\u000a        that draws significant\u000a        power from using IMPUTE2 in its design\" [D].\u000a    The Vice President for Informatics at Affymetrix says in [B] \"The\u000a        impute software that we licenced\u000a        from Oxford University has been used extensively at Affymetrix and is an\u000a        essential tool used to\u000a        compute and describe the coverage of our genotypic arrays. [...]\u000a        In particular, the SNPs on these\u000a        arrays were selected in such a way as to maximize imputation coverage.\u000a        This has made a\u000a        significant impact in the way we design arrays and could not have\u000a        happened without using\u000a        IMPUTE2, which has been shown to be the most accurate method\u000a        of imputation in the literature.\u000a      [...] Affymetrix had total revenues of about $300 million in 2012 and\u000a        is using IMPUTE2 in the\u000a        design and dissemination of all its genotyping products Affymetrix has a\u000a        significant and rapidly\u000a        growing share of the worldwide market for genotyping arrays, the size of\u000a        which is on the order of\u000a        $600m million annually.\"\u000a    Roche saved ~$1,000,000 by using IMPUTE in a study of drug response\u000a    Pharmacogenetics is a particular type of GWAS applied to subjects that do\u000a      or do not have an\u000a      adverse reaction to a particular medication. Many medications exhibit a\u000a      variable response rate that\u000a      is thought to be partly genetic. Therefore, there is a great interest in\u000a      discovering biomarkers that\u000a      aid physician decision making, through the identification of patients who\u000a      will or will not respond,\u000a      and therefore derive greater benefit from a particular therapy.\u000a    The pharmaceutical company Roche has investigated the genetics of\u000a      response to the drug\u000a      tocilizumab for the treatment of rheumatoid arthritis (RA). Tocilizumab is\u000a      prescribed to RA patients\u000a      who had inadequate response to disease modifying anti-rheumatic drugs.\u000a      Genotype imputation\u000a      using IMPUTE v2 was used in this study to combine studies together for\u000a      greater power. Since the\u000a      subjects in these studies had a variety of different ancestries the use of\u000a      IMPUTE v2 together with\u000a      the HapMap3 reference panel provided an ideal and practical solution to\u000a      the prediction of the\u000a      unobserved genotypes in each study. The study was able to implicate the\u000a      involvement of 8 loci in\u000a      the patient response to tocilizumab treatment. Patients carrying the\u000a      specific genetic markers had a\u000a      higher remission compared to those who did not [F].\u000a    The Roche study used three different Illumina genotyping chips (550K,\u000a      Human1M-Duo and\u000a      HumanOmni1-Quad) on different sets of individuals. A Senior Statistical\u000a      Scientist at Roche [E]\u000a      states \"The IMPUTE program was used and generated high quality data for\u000a        the union set of SNPs\u000a        on the three chips. This allowed us to analyse the data from all 1600\u000a        patients together.....Without\u000a        the genetic data imputation carried out with IMPUTE, the best way to\u000a        reproduce the study would be\u000a        to genotype all study samples using an IlluminaOmni1-Quad chip. This\u000a        would have involved re-\u000a        genotyping 1,157 samples at a cost of $750 each plus an additional\u000a        operational cost of 20%.\u000a      Therefore the total cost saving is ~$1,000,000. In addition to the cost\u000a        saving, the imputation work\u000a        also allowed us to save time and complete the analysis in time to meet\u000a        decision timelines set by\u000a        the development program\".\u000a    ","ImpactSummary":"\u000a    In genetic studies of human disease it is now routine for studies to\u000a      collect genetic data on\u000a      thousands of individuals with and without a particular disease. However,\u000a      the genetic data collected\u000a      is incomplete, with many millions of sites of the genome unmeasured. The\u000a      novel methods and\u000a      software (IMPUTE) developed by researchers at the University of Oxford\u000a      predict unobserved\u000a      genetic data using reference datasets.\u000a    IMPUTE has been adopted by the company Affymetrix in the design of custom\u000a      genotyping chips.\u000a      Affymetrix recently won the tenders by the UK Biobank and UKBiLEVE studies\u000a      to genotype\u000a      &gt;500,000 participants, with a total study cost of ~&#163;25M. The company\u000a      states that IMPUTE gave\u000a      their project bid a significant competitive advantage. Affymetrix also\u000a      purchased the IMPUTE source\u000a      code for &#163;250,000. In addition, Roche Pharmaceuticals have used the\u000a      software in their research on\u000a      the genetic basis of drug response. The use of imputation has saved Roche\u000a      ~$1,000,000.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Oxford\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a*[1] The Wellcome Trust Case Control Consortium (2007) Genomewide\u000a      association study of\u000a      14,000 cases of seven common diseases and 3,000 shared controls. Nature\u000a      447 661-78.\u000a      doi:10.1038\/nature05911.\u000a    \u000a\u000a*[2] J. Marchini, B. Howie, S. Myers, G. McVean and P. Donnelly (2007) A\u000a      new multipoint method\u000a      for genome-wide association studies via imputation of genotypes. Nature\u000a        Genetics 39 906-\u000a      913. doi:10.1038\/ng2088.\u000a    \u000a\u000a[3] E. Zeggini, L. Scott, R. Saxena, B. Voight, J. Marchini et al. (2008)\u000a      Meta-analysis of genome-\u000a      wide association data and large-scale replication identifies additional\u000a      susceptibility loci for\u000a      type 2 diabetes. Nature genetics 2008;40;5;638-45.\u000a      doi:10.1038\/ng.120.\u000a    \u000a\u000a*[4] B. Howie, P. Donnelly, J. Marchini (2009) A Flexible and Accurate\u000a      Genotype Imputation\u000a      Method for the Next Generation of Genome-Wide Association Studies. PLoS\u000a        Genetics 5(6):\u000a      e1000529. doi:10.1371\/journal.pgen.1000529.\u000a    \u000a\u000a[5] B. Howie, J. Marchini, M. Stephens (2011) Genotype Imputation with\u000a      Thousands of\u000a      Genomes. G3 : Genes, Genomes, Genetics. doi: 10.1534\/g3.111.001198.\u000a    \u000a\u000a[6] B. Howie, C. Fuchsberger, M. Stephens, J. Marchini, and G. R.\u000a      Abecasis (2012) Fast and\u000a      accurate genotype imputation in genome-wide association studies through\u000a      pre-phasing.\u000a      Nature Genetics 44, 955-959. doi: 10.1038\/ng.2354.\u000a    \u000aThe three asterisked outputs best indicate the quality of the\u000a      underpinning research. All six papers\u000a      are in high quality internationally refereed journals.\u000a    ","ResearchSubjectAreas":[{"Level1":"6","Level2":"4","Subject":"Genetics"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    [A] Letter from Technology Transfer Team Leader, ISIS Innovation, Oxford,\u000a      held by the\u000a      University of Oxford, which corroborates licensing deals and software\u000a      sales for IMPUTE.\u000a    [B] Letter from Vice President Informatics, Affymetrix, held by the\u000a      University of Oxford, which\u000a      corroborates how Affymetrix have made use of IMPUTE.\u000a    [C] Affymetrix press release giving details of their Axiom arrays and how\u000a      IMPUTE was used to\u000a      design the arrays, copy held by the University of Oxford\u000a    [D] Affymetrix press release giving details of contract with UK Biobank,\u000a      copy held by the\u000a      University of Oxford\u000a    [E] Letter from Senior Statistical Scientist, Roche, held by the\u000a      University of Oxford, describing\u000a      the use of IMPUTE in their pharmacogenetic study of Tocilizumab for the\u000a      treatment of\u000a      rheumatoid arthritis.\u000a    [F] Paper describing Roche's pharmacogenetic study of Tocilizumab,\u000a      confirming the use of\u000a      IMPUTE in their study.\u000a    Wang J et al. (2011) Genome-wide association analysis implicates the\u000a      involvement of 8 loci\u000a      with response to tocilizumab for the treatment of rheumatoid arthritis The\u000aPharmacogenomics\u000a        Journal 44, 955-960, doi: 10.1038\/tpj.2012.8\u000a    \u000a    ","Title":"\u000a    Pharmaceutical and biotechnology companies gain economic benefits from\u000a      novel statistical methods for imputing genotypes\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Genome-wide association studies (GWAS) aim to identify genes that\u000a      increase risk of developing a\u000a      disease under study. A typical study will measure up to a million variable\u000a      positions across the\u000a      genome, called single nucleotide polymorphisms (SNPs), in thousands of\u000a      subjects, and look for\u000a      significant differences between individuals with and without the disease.\u000a      The identification of these\u000a      disease genes can help understanding of the disease mechanisms. Since only\u000a      a fraction of sites\u000a      that are known to vary between humans are measured, there is a substantial\u000a      amount of genetic\u000a      data that is unobserved. However, reference databases such as the 1000\u000a      Genomes Project (TGP)\u000a      contain many more SNPs. The July 2012 TGP release contains 38 million\u000a      SNPs. The methodology\u000a      developed at the University of Oxford combines the data from a GWAS with\u000a      the TGP database and\u000a      predicts the unobserved genotypes.\u000a    The first approach at predicting, or imputing, unobserved genotypes was\u000a      developed by Dr Marchini\u000a      and Professor Donnelly, both faculty members at the University of Oxford,\u000a      as part of their\u000a      involvement in the Wellcome Trust Case Control Consortium (WTCCC) [1],\u000a      during the period of\u000a      2006-2007. They realized that genetic studies of human disease could be\u000a      substantially improved if\u000a      unobserved genotypes could be predicted using the existing reference\u000a      databases, and that\u000a      recently developed Hidden Markov models developed in the area of\u000a      population genetics could be\u000a      adapted to carry out this task. Their approach, IMPUTE v1 [2], was\u000a      developed by Marchini and\u000a      applied successfully to all 7 disease studies carried out by the WTCCC.\u000a      This paper has over 1,000\u000a      citations since 2007. The figure below illustrates the typical imputation\u000a      scenario, where a reference\u000a      panel of haplotypes is combined with a GWAS. The figure highlights that a\u000a      large fraction of\u000a      genotypes are unobserved (indicated by question marks). IMPUTE can predict\u000a      this missing data\u000a      using shared patterns of haplotypes between the two datasets. For common\u000a      genetic variants of\u000a      interest, the accuracy of imputation is over 95%.\u000a    There have been over 1,350 published GWAS since 2005 (www.genome.gov\/gwastudies).\u000aImputation\u000a      has been used in the vast majority of these, evidenced by the large number\u000a      of citations\u000a      of our papers on imputation. One key benefit of the method is that once\u000a      unobserved genotypes\u000a      have been predicted in several different studies, they can then be\u000a      combined, via meta-analysis, to\u000a      produce much more powerful studies. This approach has changed the field of\u000a      human genetics and\u000a      groups now routinely share data via this approach. One of the earliest\u000a      examples of this was in the\u000a      study of Type 2 Diabetes and lead to the discovery of 6 new disease genes\u000a      [3].\u000a    Subsequently, Marchini and Donnelly realized that as reference panels\u000a      increase in size, through\u000a      ongoing projects such as the TGP, the method IMPUTE v1 would not scale\u000a      well. Marchini led the\u000a      development of IMPUTE v2 which extends the approach by adaptively\u000a      selecting a subset of the\u000a      reference database to use for predicting each individual. Another insight\u000a      was that this approach\u000a      naturally allows the use of reference panels from multiple populations.\u000a      For example, when\u000a      predicting genotypes in an individual with European ancestry the method\u000a      would select the subset\u000a      of the reference database that matches the individual's ancestry [4,5].\u000a\u000a\u000a\u000a    A further paper published in Nature Genetics [6] develops a new two-step\u000a      imputation process, first\u000a      by estimating haplotypes in the GWAS sample, then using haploid\u000a      imputation. The second step is\u000a      very fast and reduces the computational cost needed by a factor of at\u000a      least 20.\u000a    From 2002-2005, Marchini held a Wellcome Trust Postdoctoral Fellowship at\u000a      the University of\u000a      Oxford and since 2005 has been a University Lecturer in Statistical\u000a      Genomics. Donnelly has been\u000a      a Professor of Statistical Science since 1996. From 2007 he has also been\u000a      Director of the\u000a      Wellcome Trust Centre for Human Genetics, University of Oxford.\u000a    "},{"CaseStudyId":"20178","Continent":[{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"1269750","Name":"India"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The impact of the research falls into the category of better informed\u000d\u000a      public policy-making and the beneficiaries are the Indian Government and\u000d\u000a      people living with HIV\/AIDS in India.\u000d\u000a    The HIV epidemic has been evolving in India since the first detected case\u000d\u000a      in 1986 and India is among the top three countries in the world in terms\u000d\u000a      of HIV burden. As a result, there has been sustained activity from the\u000d\u000a      Indian Government to implement effective control and prevention\u000d\u000a      strategies. From 1998 to 2005 the number of people living with HIV\/AIDS\u000d\u000a      rose from an estimated 3.5 million to 5.206 million with significant\u000d\u000a      variations from state to state. After two initial stages of control\u000d\u000a      activities, NACP III was launched in 2006 for a 5 year period with the\u000d\u000a      goal of reducing new infections by 60-80 percent in different regions in\u000d\u000a      India.\u000d\u000a    Pathway from Research to Impact:\u000d\u000a      The research carried out by Maini and his colleagues had a direct impact\u000d\u000a      on the Indian Government strategy for NAPC III. One of the novel features\u000d\u000a      of the modelling approach was to incorporate the effects of preventive\u000d\u000a      measures on people already infected with HIV. For example, the modelling\u000d\u000a      showed that treatment with anti-retroviral therapy could actually increase\u000d\u000a      HIV prevalence because it would prolong the life of those infected with\u000d\u000a      HIV, potentially leading to them passing on the disease. The research\u000d\u000a      considered a number of levels of future responses which were being\u000d\u000a      proposed by the Indian Government, and made predictions about the number\u000d\u000a      of people living with HIV\/AIDS in each case.\u000d\u000a    Maini helped to formulate the model and to lay down the plan of model\u000d\u000a      validation and interrogation, and then assisted in the interpretation of\u000d\u000a      results. In particular two figures (3 and 6) and tabular data in [1] were\u000d\u000a      used for the Strategy and Implementation Plan [A] (and appear explicitly\u000d\u000a      as figures 2.4 and 2.7 in that document) as part of the NACP III for the\u000d\u000a      period 2007-12. Figure 2.7 in [A] is shown above as Figure 1. Through\u000d\u000a      collaborations during the period 2005-2012 Maini's role was pivotal in\u000d\u000a      extending the model building to enable further strategies to be considered\u000d\u000a      which accounted for variations in behaviour on a district level [B], and\u000d\u000a      the effects of anti-retroviral therapy [A], in addition to the published\u000d\u000a      work [1]. A member of the NACP III and IV planning teams (and co-author of\u000d\u000a      [1]) reports [C] that \"The output of these models were used extensively\u000d\u000a        in finalizing the implementation plans for the country. [...] We\u000d\u000a        are grateful to Professor Maini and Dr Rao for their continued\u000d\u000a        assistance in guiding these policy decisions and their contribution to\u000d\u000a        enabling scenarios to be assessed and results to be evaluated and help\u000d\u000a        those working in these difficult areas\".\u000d\u000a    In addition to presentation directly into the NACP the research work was\u000d\u000a      circulated more widely into Indian AIDS prevention activities [eg D,E]. It\u000d\u000a      was presented at a collaborative meeting between the Indian Clinical\u000d\u000a      Epidemiology Network, a network of academic health care researchers across\u000d\u000a      135 Medical colleges\/Institutions in India, and the Indian Statistical\u000d\u000a      Institute, Kolkata, which was providing technical assistance to NACP. It\u000d\u000a      also formed part of a Capacity Building Workshop on Operations Research in\u000d\u000a      HIV\/AIDS for the Northeast States during September 2010. The research was\u000d\u000a      further disseminated through activities of the Postgraduate Institute of\u000d\u000a      Medical Education and Research School of Public Health, first at a\u000d\u000a      Technical meeting in June\/July 2009, and then in their School of Public\u000d\u000a      Health Impact Study in June 2010, and also at the Indian Council for\u000d\u000a      Medical Research Institute in Kolkata in December 2009.\u000d\u000a    The direct impact of the research was facilitated by two of the\u000d\u000a      co-authors of [1] who were members of the NACP III planning team. The work\u000d\u000a      formed the cornerstone of documents and presentations produced by this\u000d\u000a      team, including predictions directly from the paper [1], forming a central\u000d\u000a      element of the Strategy and Implementation Plan [A].\u000d\u000a    Nature and Extent of the Impact\u000d\u000a      One of the key outcomes from the research was to influence the\u000d\u000a      recommendation in the Strategy and Implementation Plan [A] based around\u000d\u000a      the groups modelled as high risk, specifically commercial sex workers,\u000d\u000a      injecting drug users and men having sex with men. In 2007, the model\u000d\u000a      projections indicated that, should the interventions of NACP II be\u000d\u000a      continued, there would be 2.08 million people living with HIV\/AIDS by the\u000d\u000a      end of 2011. This value is very close to the data for 2011 released by the\u000d\u000a      Indian Ministry of Health in 2012 showing the number of people living with\u000d\u000a      HIV\/AIDS was 2,088,642. Subsequently the model predictions helped inform\u000d\u000a      the mid-term review of the NACP III plan initiated by the National AIDS\u000d\u000a      Control Organisation (NACO) of the Ministry of Health in 2009. The\u000d\u000a      mathematical model predictions, including using the model to predict AIDS\u000d\u000a      levels at a district level [B], exploited data collected by the NACO from\u000d\u000a      May 2009 until the beginning of 2011. These predictions were presented in\u000d\u000a      2011 [F] to the Director General of NACO and the Heads of Department and\u000d\u000a      then to the NACO All Stake Holder meeting, and served as a source of\u000d\u000a      reference in developing the follow-on programme NACP IV (2012-17). In\u000d\u000a      particular, a member of the NACP planning team at the time (and co-author\u000d\u000a      of [1]) reports [C] that \"This model was used in quantifying the\u000d\u000a        benefit and impact of different intervention of NACP lll in 2009 and\u000d\u000a        prioritizing the programs for NACP lV during 2012. Different scenarios\u000d\u000a        were considered and presented by the team at meetings of relevant lndian\u000d\u000a        Government Health Department ministers with great effect. As a\u000d\u000a        consequence of this input government decided to extend full support for\u000d\u000a        the program during the next 5 years in spite of pressure from many\u000d\u000a        quarters to scale down the interventions due to competing priorities.\u000d\u000a        Resulting policy proved that the planned interventions to be [sic] very\u000d\u000a        effective with AIDS prevalence in India reducing from 23.95 lakh\u000d\u000a      [hundred thousand] in 2009 to current level of 20.89 lakh.\"\u000d\u000a    The success of this modelling study has resulted in Dr Rao being invited\u000d\u000a      to serve on the planning team for AIDS policy for the fourth phase\u000d\u000a      (2012-2017). Maini, who serves the planning team as a consultant,\u000d\u000a      continues to provide crucial input in relation to model formulation and\u000d\u000a      papers presented to government.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research at Oxford University directly influenced Indian Government\u000d\u000a      policy through its projections of the population of HIV\/AIDS sufferers in\u000d\u000a      India under several possible strategic interventions. It formed a central\u000d\u000a      part of the policy documents and presentations of the Indian National AIDS\u000d\u000a      Control Programme (NACP) Planning Team, and was presented to Indian\u000d\u000a      Government officials in 2006 in order to plan the third phase of the\u000d\u000a      control programme, NACP III (2006-2011). The projected HIV\/AIDS\u000d\u000a      populations then served as a reference for mid-term evaluations of NACP\u000d\u000a      III in 2009. The study was used to measure the impact of the intervention\u000d\u000a      programs, and acted as a source of reference during the planning of the\u000d\u000a      next phase, NACP IV (2012-17).\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    University of Oxford\u000d\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"1275004","Name":"Kolkata"}],"References":"\u000d\u000a    \u000a*[1] Rao, A.S.R.S., Thomas, K., Sudhakar, K. and Maini, P.K. (2009),\u000d\u000a      HIV\/Aids epidemic in India and predicting the impact of the national\u000d\u000a      response: mathematical modeling and analysis. Mathematical Biosciences\u000d\u000a        &amp; Engineering 6, 4, pp. 779-813. doi:\u000d\u000a      10.3934\/mbe.2009.6.779\u000d\u000a    \u000aThe asterisked output best indicates the quality of the underpinning\u000d\u000a      research. Mathematical Biosciences &amp; Engineering is an international\u000d\u000a      refereed journal.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"}],"Sources":"\u000d\u000a    [A] National AIDS Control Programme, Phase III 2006 - 2011: Strategy and\u000d\u000a      Implementation Plan (2006), which can be found at:\u000d\u000a      http:\/\/aidsdatahub.org\/dmdocuments\/India_Strategy_and_Implementation_Plan_NACO_Programme_PhaseIII_2006_2011.pdf.pdf\u000d\u000a      Copy held by the University of Oxford.\u000d\u000a    [B] District Level Mathematical Modeling of HIV\/AIDS in Tamil Nadu,\u000d\u000a      September 2010, submitted to Project Director, AIDS Prevention and Control\u000d\u000a      Project, The Voluntary Health Services, Taramani, Chennai 600113, INDIA.\u000d\u000a      Copy held by the University of Oxford.\u000d\u000a    [C] Letter from a Member of the NACP III and IV Planning Teams (and\u000d\u000a      co-author of paper [1], current Director of Research at the Oman Medical\u000d\u000a      Speciality Board) confirming the pathway to impact and the significance.\u000d\u000a      Copy held by the University of Oxford .\u000d\u000a    [D] Copy of email from the National AIDS Control Organisation to Dr Rao\u000d\u000a      requesting he presents the model and its results to the \"Dissemination on\u000d\u000a      HIV\/AIDS Research 19-21 Jan 2011\", held by the University of Oxford.\u000d\u000a    [E] Copy of email inviting Dr Rao to attend the Advisory Committee\u000d\u000a      Meeting of India Impact Study at NACO on 1 Oct 09, held by the University\u000d\u000a      of Oxford.\u000d\u000a    [F] Powerpoint presentation by one of the authors of [1] to the Director\u000d\u000a      General of the National AIDS Control Organisation and the Heads of\u000d\u000a      Department, and to the National AIDS Control Organisation All Stake Holder\u000d\u000a      meeting while preparing for NACP IV. Copy held by the University of\u000d\u000a      Oxford.\u000d\u000a    ","Title":"\u000d\u000a    Influencing Indian Government policy through mathematical modelling of\u000d\u000a      the HIV\/AIDS epidemic in India\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    After two phases of AIDS control activities in India, the third phase,\u000d\u000a      NACP III, was launched in July 2007. Professor Philip Maini, permanent\u000d\u000a      faculty member of the Mathematical Institute, University of Oxford since\u000d\u000a      1990, was a key member of an international multidisciplinary research team\u000d\u000a      who developed a mathematical model [1] with the purpose of predicting the\u000d\u000a      number of people living with HIV\/AIDS in India to assist the NACP III\u000d\u000a      planning team in determining appropriate strategies for targeting groups\u000d\u000a      during the project period.\u000d\u000a    As part of the planning of NACP III, Dr Arni Rao (Indian Statistical\u000d\u000a      Institute, Kolkata) was commissioned by members of the NACP Planning Team\u000d\u000a      to provide predictions of the effectiveness of various HIV\/AIDS\u000d\u000a      strategies. To complement existing statistical approaches, Dr Rao visited\u000d\u000a      Prof Maini at Oxford University's Centre for Mathematical Biology who\u000d\u000a      provided the necessary mathematical modelling capability for the programme\u000d\u000a      [1]. An example of the results from this modelling is given in Figure 1.\u000d\u000a    The research involved constructing dynamical models which capture the\u000d\u000a      mixing patterns between susceptibles and infectives in both low-risk and\u000d\u000a      high-risk groups in the population. The aim was to project forward the HIV\u000d\u000a      estimates by taking into account general interventions for susceptibles,\u000d\u000a      targeted interventions among high risk groups, the provision of\u000d\u000a      anti-retroviral therapy, and behaviour change among HIV-positive\u000d\u000a      individuals. The standard compartmental model framework was extended and\u000d\u000a      adapted to account for possible interactions between different categories\u000d\u000a      of risk behaviour amongst male and females, sex workers, intravenous drug\u000d\u000a      users, and blood transfusion recipients. The resulting system of coupled\u000d\u000a      nonlinear ordinary differential equations was extensively analysed by\u000d\u000a      numerical simulation and validated against existing data for disease\u000d\u000a      spread in various states in India. The model parameters were determined by\u000d\u000a      various means including literature searches, fitting of submodels to\u000d\u000a      specific data, and de-convolution methods. A sensitivity analysis of the\u000d\u000a      parameters was also carried out. The model was then interrogated to\u000d\u000a      compare the effects of different control and intervention strategies. The\u000d\u000a      model projections based on the NACP II and III scenarios indicated that\u000d\u000a      prevention programmes which were directed towards the general and\u000d\u000a      high-risk populations, as well as HIV-positive individuals, would be key\u000d\u000a      in determining the decline or stabilization of the epidemic.\u000d\u000a    \u000d\u000a   Figure 1. Projections based on the mathematical model for the number of\u000d\u000a      people living with HIV\/AIDS. Five sets of projections for HIV estimates,\u000d\u000a      using different strategies, are shown based on revised estimates by NACO\u000d\u000a      released in 2007. Reproduced from Fig 6 [1] and used as Fig 2.7 [2].\u000d\u000a   \u000d\u000a    "},{"CaseStudyId":"20200","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000a    The University of Oxford's research on extensional thin layer flows has\u000a      resulted in significant\u000a      economic impact since 2008. The beneficiaries are the glass manufacturers\u000a      [text removed for publication], Schott AG and Pilkington.\u000a    Pathways to impact:\u000a      [text removed for publication]\u000a    Between 2002 and 2006, the University of Oxford team (led by Prof. John\u000a      Ockendon FRS) was a\u000a      node in the &#8364;1.4m EU Research Training Network Math ematics for the Glass\u000a      Industry:\u000a      Computing and Analysis (known as MAGICAL) which aimed to promote\u000a      collaborations between\u000a      Universities and glass companies across the EU, including Schott AG.\u000a      Schott AG posed specific\u000a      research questions which were tackled by the MAGICAL team at the\u000a      University of Oxford, and\u000a      gained access to the University of Oxford's pre-existing research base\u000a      relevant to glass flows.\u000a    Finally, Pilkington (now NSG group) has had a long standing\u000a      relationship with the University of\u000a      Oxford's Mathematical Insitute through Industrial Workshops organised by\u000a      the Oxford Centre for\u000a      Industrial and Applied Mathematics (OCIAM) and was also involved in\u000a      MAGICAL. The current\u000a      Head of the Float and Rolled Glass Technology Group says [B] \"Pilkington\u000a        Group research &#8212; in\u000a        recent years the NSG European Technology Centre, have for many years\u000a        found the OCIAM\u000a        group to be much the most valuable point of contact for problems raising\u000a        complex mathematical\u000a        issues in glassmaking and glass products.\"\u000a    Nature and extent of the impact:\u000a    The glass industry constantly seeks improved models for production and\u000a      processing\u000a      technologies, so that it can reliably design and build improved,\u000a      fault-free products while cutting\u000a      development costs and innovating new advanced materials. It is\u000a      understandable that major\u000a      industrial glass producers, such as [text removed for publication]\u000a      and Schott, were interested\u000a      in the University of Oxford's research into modelling thin viscous sheets.\u000a    [text removed for publication]\u000a    At Schott AG, another world-leading glass and materials company\u000a      with global sales of &#8364;2 billion\u000a      (in 2011\/12), research from the University of Oxford has been used even\u000a      more widely. Like [text\u000a        removed for publication], Schott has developed software, based on\u000a      Howell's methodology for\u000a      modelling thin viscous sheets and fibres, which is key in the improvement\u000a      and development of\u000a      glass forming processes. A senior scientist in the Mathematical Simulation\u000a      and Optimization\u000a      group states [D] \"At Schott, we now have models based on these methods\u000a        for all our drawing\u000a        processes. A typical application is the prediction of suitable process\u000a        conditions (heater power\u000a        distribution, top roller speeds etc) for a desired glass sheet thickness\u000a        and net width in the float\u000a        process.\" Schott use the models to save significant sums in\u000a      development costs. The senior\u000a      scientist states [D] \"In many cases, our only option in the development\u000a        process are experiments\u000a        in the actual production plant. A day in a typical production plant\u000a        costs about 50,000 Euro. I am\u000a        quite certain that the models saved us years of such experiments at the\u000a        plants.\" In a subsequent\u000a      email he confirms that these years of effort were post-2008.\u000a    One of Schott's major production processes, tube drawing, has also been\u000a      heavily influenced by\u000a      the University of Oxford's research into pressure-driven flows and\u000a      hollow-fibre production. In\u000a      particular, the asymptotic solutions developed at the University of Oxford\u000a      have been used to\u000a      create models which Schott claims offer improved accuracy over competing\u000a      models. The senior\u000a      scientist in the Mathematical Simulation and Optimization group states [D]\u000a      \"The results in [5,\u000a      published in 2008, and other papers] are of special significance for\u000a        the tube drawing process, one\u000a        of Schott's major production processes. These asymptotic solutions for\u000a        non-circular tube\u000a        geometries are in my opinion superior to \"brute force\" numerical\u000a        approaches which suffer from\u000a        inaccuracies arising from the high glass viscosities downstream.\"\u000a      This allows manufacturing\u000a      processes for proposed new products to be reliably tested and optimised\u000a      computationally, leading\u000a      to significant savings in wastage and money.\u000a    Finally, work from the University of Oxford has allowed Schott to produce\u000a      glass with fewer faults.\u000a      Research into levitating thin sheets of glass on air cushions has allowed\u000a      Schott to reduce\u000a      dramatically the occurrence of instabilities in numerous moulding\u000a      processes, and understanding\u000a      of the presence of defects such as bubbles has helped them reduce the\u000a      incidence of faults in\u000a      many different glass-forming processes.\u000a    Pilkington is one of the leading glass suppliers in the UK and is\u000a      now part of the Japan-based\u000a      NSG Group which has manufacturing operations in 29 countries and global\u000a      sales of over &#163;4\u000a      billion. The company uses research carried out at the University of Oxford\u000a      in several areas\u000a      concerned with risk management, which is an essential part of production\u000a      processes. The\u000a      support the University of Oxford has provided with solving the problem of\u000a      bubbles bursting on the\u000a      surface of glass has given Pilkington invaluable understanding of the\u000a      underlying physical\u000a      processes (subsequently verified experimentally) as well as significant\u000a      financial savings by\u000a      avoiding lost production costs. The Head of the Float and Rolled Glass\u000a      Technology Group at\u000a      NSG states [B] \"The idea that distinctions which could be drawn between\u000a        various types of bubble\u000a        develops a way of analysing melting problems which results in a better\u000a        chance of taking the most\u000a        appropriate action\".\u000a    A common procedure in the manufacture of windscreens involves heating a\u000a      sheet of glass so that\u000a      it sags under gravity into the desired shape. The inverse problem of\u000a      determining the heating\u000a      profile required to produce a particular shape after sagging is tackled in\u000a      the industry using\u000a      computational modelling, which is found to be extremely delicate. The\u000a      University of Oxford's\u000a      mathematical research on this problem has been very useful for Pilkington,\u000a      who found that\u000a      \"guidance as to how much can safely be left to automated algorithms and\u000a        what requires active\u000a        intervention by the user is hugely valuable in reducing product\u000a        development time\" [B].\u000a    ","ImpactSummary":"\u000a    The glass industry uses theoretical modelling to control, improve, and\u000a      reduce the cost of\u000a      designing and manufacturing novel glass products. Market-leaders [text removed for\u000a\u0009  publication], Schott AG and Pilkington have developed modelling\u000a      software which is underpinned\u000a      by equations stemming from research at the University of Oxford.\u000a    [text removed for publication]. The same modelling approach is\u000a      used in software developed by\u000a      Schott which is now used in all of its modelling of drawing processes to\u000a      reduce both development\u000a      costs and the incidence of faults. Pilkington have implemented research\u000a      performed at the\u000a      University of Oxford to decrease the risk associated with manufacturing\u000a      processes.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Oxford\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a*[1] P.D. Howell. Models for thin viscous sheets, Euro. J. Appl.\u000a        Math. (1996), 7:321-343. DOI:\u000a      10.1017\/S0956792500002400.\u000a    \u000a\u000a[2] P.D. Howell. The draining of a two-dimensional bubble. J. Engrg.\u000a        Math. (1999), 35:251-272.\u000a      DOI: 10.1023\/A:1004399105606.\u000a    \u000a\u000a*[3] C.J.W. Breward and P.D. Howell. The drainage of a foam lamella, J.\u000a        Fluid Mech. (2002),\u000a      458:379-406. DOI: 10.1017\/S0022112002007930.\u000a    \u000a\u000a[4] P.D. Howell and M. Siegel. The evolution of a slender\u000a      non-axisymmetric drop in an\u000a      extensional flow. J. Fluid Mech. (2004), 521:155-180.\u000a      DOI:10.1017\/S002211200400148X.\u000a    \u000a\u000a*[5] I.M. Griffiths and P.D. Howell. Mathematical modelling of\u000a      non-axisymmetric capillary tube\u000a      drawing, J. Fluid Mech. (2008), 605:181-206. DOI:\u000a      10.1017\/S002211200800147X.\u000a    \u000a\u000a[6] D. Salazar and R. Westbrook. Inverse problems of mixed type in linear\u000a      plate theory, Euro.\u000a        J. Appl. Math. (2004), 15:129-146. DOI: 10.1017\/S0956792503005345.\u000a    \u000aThe three asterisked outputs best indicate the quality of the\u000a      underpinning research. All six papers\u000a      are in high quality internationally refereed journals. This is not an\u000a      exhaustive list: results from\u000a      many other papers have also led to impact on glass manufacturers.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000a    [A] [text removed for publication]\u000a    [B] Letter from Group Head, Float and Rolled Glass Technology Group, NSG,\u000a      describing the\u000a      influence of Oxford's research on their product development. Copy held by\u000a      the University of\u000a      Oxford.\u000a    [C] [text removed for publication]\u000a    [D] Letter and emails from a Senior Scientist in the Mathematical\u000a      Simulation and Optimisation\u000a      Group, Schott AG, describing the impact of 10 relevant Oxford Mathematics\u000a      publications on\u000a      Schott's activities. Copies held by the University of Oxford.\u000a    ","Title":"\u000a    Mathematics in the design and manufacture of novel glass products\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Peter Howell has been working on extensional thin layer flows with\u000a      application to the glass\u000a      industry for the last 20 years. He first studied the evolution of thin\u000a      sheets and jets of viscous fluid\u000a      by taking a mathematical limit where the aspect ratio (the ratio of the\u000a      thickness to the length or\u000a      width) is small, precisely the situation in the manufacture of glass\u000a      windows, tubing and optical\u000a      fibres, for example. The result of the research [1] is a systematic\u000a      framework for reducing the full\u000a      Navier-Stokes equations to a simplified lower-dimensional system, which\u000a      gives greater insight\u000a      into possible instabilities and allows for much more efficient\u000a      computation. These simplified\u000a      models allow glass processes to be more effectively controlled to produce\u000a      flawless products with\u000a      optimised properties and to avoid catastrophic process failures.\u000a    In a glass furnace, many tiny gas bubbles are produced as the raw\u000a      material melts and reacts. It is\u000a      essential that all of these bubbles are eliminated before the glass leaves\u000a      the furnace for further\u000a      processing, to avoid defects in the finished product. To this end, the\u000a      drainage of a bubble at the\u000a      surface of a viscous fluid was modelled mathematically by Howell [2];\u000a      explicit formulae were\u000a      found for the suction of fluid out of the thin film between the bubble and\u000a      the atmosphere and for\u000a      the expected timescale for a bubble to burst. Marangoni effects were\u000a      incorporated in [3]; this\u000a      allowed the influence of impurities on bubble bursting to be quantified.\u000a      These analyses allow the\u000a      required furnace residence time required to remove all bubbles, to be\u000a      determined. Further\u000a      research [4] allowed the deformation of any bubbles that do make it into\u000a      the processing stage to\u000a      be quantified.\u000a    Another important facet of the University of\u000a      Oxford's glass modelling concerns the\u000a      drawing of non-axisymmetric glass tubing. A\u000a      key question is: what die shape is needed to\u000a      make tubes of a given cross-sectional\u000a      shape? Using ideas from perturbation theory\u000a      and partial differential equations, researchers\u000a      at the University of Oxford were able to solve\u000a      this inverse problem explicitly [5, and other\u000a      papers], as shown in the figure, which shows\u000a      the die shape required to draw glass tubing\u000a      with a square cross-section.\u000a    \u000a    \u000a    \u000a    The gravitational sagging of heated glass\u000a      sheets to form windscreens also provides an inverse problem, namely, to\u000a      predict the temperature\u000a      profile required to produce a final desired windscreen shape. In research\u000a      carried out at the\u000a      University of Oxford [6], this problem was analysed mathematically and\u000a      found to reduce to an ill-posed\u000a      partial differential equation which inevitably changes type from elliptic\u000a      to hyperbolic across\u000a      some interior line in the glass sheet.\u000a    The key researchers: Peter Howell (postdoc 1994-95, research fellow\u000a      1996-99 faculty 2001 to\u000a      date); Chris Breward (postdoc 2001-03, research fellow 2003-08, faculty\u000a      2008 to date); Ian\u000a      Griffiths (postdoc 2008-2010, Research fellow 2010 to date); Domingo\u000a      Salazar (postdoc 1998-2002)\u000a      were all at the University of Oxford when the research was carried out. At\u000a      least four other\u000a      researchers, including John Ockendon, participated in the University of\u000a      Oxford's glass research\u000a      programme.\u000a    "},{"CaseStudyId":"20248","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Wellcome Trust","Royal Society"],"ImpactDetails":"\u000d\u000a    The HapMap project has led to three areas of impact. Firstly, it has made\u000d\u000a      an economic impact on biotech and diagnostic companies by facilitating the\u000d\u000a      introduction of new products and services, which have led to substantial\u000d\u000a      wealth creation for these companies since 2008 (the official release of\u000d\u000a      HapMap was 20 December 2007, see hapmap.ncbi.nlm.nih.gov). Secondly, the\u000d\u000a      project has led to a new diagnostic test that has been widely adopted that\u000d\u000a      can guide doctors and patients when considering drug treatment for\u000d\u000a      hepatitis C virus. Finally, the project has had an impact on society by\u000d\u000a      increasing the public interest in, and engagement with, science. Specific\u000d\u000a      details of these impact areas are as follows.\u000d\u000a    Economic impact: facilitating biotech companies to develop new\u000d\u000a        products\u000d\u000a    Prior to the HapMap Project, genetic variants at known locations in the\u000d\u000a      genome were typically analysed, in both research and clinical labs,\u000d\u000a      through small-scale experiments. Such technologies did not scale (in terms\u000d\u000a      of cost or throughput) to genome-wide analysis. The HapMap Project\u000d\u000a      demonstrated that most of the common genetic variation in the genome could\u000d\u000a      be `captured' through selected use of a few hundred thousand SNPs.\u000d\u000a      Realising the major potential for such genome-wide products, a number of\u000d\u000a      companies developed technologies for enabling massively parallel\u000d\u000a      genotyping. These included Perlegen Sciences (which was awarded the\u000d\u000a      contract to genotype the Phase 2 SNPs for the project) and Illumina Inc.\u000d\u000a      (currently a NASDAQ listed company). Products from Illumina Inc. were\u000d\u000a      specifically designed from the HapMap project (for example, the\u000d\u000a      HumanOmniExpress chip [A], [B]) so as to maximise the power of studies\u000d\u000a      using these `SNP chips' (a chip is a collection of microscopic DNA spots\u000d\u000a      attached to a solid surface). Their customers include genomic research\u000d\u000a      centers, pharmaceutical companies, academic institutions, clinical\u000d\u000a      research organizations and biotechnology companies. The Associate Director\u000d\u000a      for Scientific Research at Illumina states [B] \"This letter is to\u000d\u000a        outline the utility of the HapMap project as a vital resource for\u000d\u000a        developing the products used for genome wide association studies (GWAS).\u000d\u000a        The array portion of the GWAS market is estimated around $1.25 billion\u000d\u000a        USD over the past 5 years. The data generated by the HapMap project was\u000d\u000a        the primary resource used to develop these arrays. [...]\u000d\u000a        Illumina's current line of GWAS array products includes the OmniExpress\u000d\u000a        which consists of over 700k of SNP content derived solely from the\u000d\u000a        HapMap. [...] Without the data available from the HapMap project\u000d\u000a        these arrays would be significantly less powerful for detecting regions\u000d\u000a        of the genome association\". The number of individuals genotyped\u000d\u000a      using chips that trace back to the HapMap project is well over 1 million\u000d\u000a      [C].\u000d\u000a    Health impact: Providing a framework for discoveries of genetic risk\u000d\u000a        factors that have made clinical impacts\u000d\u000a    The HapMap Project did not directly analyse the genetic contribution to\u000d\u000a      human disease. However, it provided a framework for the wealth of\u000d\u000a      discoveries about the genetic contribution to common complex disease and\u000d\u000a      pharmacological risk via the GWAS approach. Evidence of the success and\u000d\u000a      scope of GWAS is documented at the National Human Genome Research\u000d\u000a      Institute GWAS Catalog [C] which lists 1,449 published GWA in 237\u000d\u000a      different diseases and traits. For many diseases GWAS has led to the\u000d\u000a      discovery of multiple disease genes. These discoveries have led to a\u000d\u000a      greater understanding of the disease etiology, and functional work that\u000d\u000a      might lead to a clinical impact is still ongoing. As one example, the\u000d\u000a      details of a GWAS discovery that has led to a substantial clinical impact\u000d\u000a      are given below.\u000d\u000a    A certain polymorphism of the IL28B gene was found in individuals\u000d\u000a      infected with the most common type of hepatitis C virus (HCV), HCV\u000d\u000a      genotype 1, to aid in identifying those patients who are twice as likely\u000d\u000a      to eliminate the HCV virus on a sustained basis when treated with\u000d\u000a      pegylated interferon-ribavirin combination therapies [D]. This study used\u000d\u000a      the Illumina Human610-quad BeadChip that was designed using the HapMap\u000d\u000a      resource.\u000d\u000a    A test for these genetic variants is sold by LabCorp [E, F] and used by\u000d\u000a      clinicians when treating HCV and in clinical trials. HCV is the most\u000d\u000a      common chronic blood-borne infection in the US. The Senior Vice President\u000d\u000a      for Science and Technology at LabCorp confirms [E] \"I understand that\u000d\u000a        the Oxford team were key players in the HapMap project, which underpins\u000d\u000a        downstream genome-wide association studies, and that you are interested\u000d\u000a        in a practical example of where association studies have led to clinical\u000d\u000a        impact. [...] The IL-28B test is a significant contributor to\u000d\u000a        the testing volume we do in the area of pharmacogenetics and has been\u000d\u000a        since its launch in 2010, continuing to today. Recent estimates indicate\u000d\u000a        that an estimated 2.7 to 3.9 million people are affected by HCV annually.\"\u000d\u000a    Impact on society: widening the public understanding of human genetic\u000d\u000a        variation\u000d\u000a    The last ten years has seen a major growth in the public interest and\u000d\u000a      understanding of genetic variation, both in relation to disease, but also\u000d\u000a      more generally in relation to ancestry and origins. The publication of the\u000d\u000a      first phase of the HapMap Project was widely reported in international\u000d\u000a      non-specialist media including, in the UK, interviews with Donnelly on\u000d\u000a      Newsnight and Radio 4's Today programme [G]. The contribution of the\u000d\u000a      HapMap Project was acknowledged in the House of Lord's report into Genomic\u000d\u000a      Medicine [H] (published in 2009; Donnelly was interviewed as part of the\u000d\u000a      committee enquiries), which in turn has resulted in the founding of the\u000d\u000a      Human Genomics Strategy Group [I], which advises the government and NHS on\u000d\u000a      how genomics can be integrated into a national healthcare programme.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The International HapMap project was a major international research\u000d\u000a      collaboration to map the structure of common human genetic variation\u000d\u000a      across populations from Europe, Asia and Africa. Mathematical Scientists\u000d\u000a      from the University of Oxford played key roles in the development of\u000d\u000a      statistical methods for the project, along with its overall design and\u000d\u000a      management of the International HapMap Project.\u000d\u000a    Companies have used HapMap as the primary resource to design genome-wide\u000d\u000a      microarrays to make novel discoveries in, for example, pharmacogenetic\u000d\u000a      studies. The size of this market is estimated at $1.25 billion.\u000d\u000a    One novel discovery has led to a genetic test that is predictive of\u000d\u000a      sustained viral suppression in patients treated for chronic hepatitis C.\u000d\u000a      An estimated 2.7 to 3.9 million people are affected by HCV infection. This\u000d\u000a      test is sold commercially by the company LabCorp and is a significant\u000d\u000a      contributor to the company's testing volume. Finally, the project has been\u000d\u000a      important in widening the public understanding of genetic variation.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Oxford\u000d\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a*[1] M Stephens, N Smith and P Donnelly (2001) A New Statistical Method\u000d\u000a      for Haplotype Reconstruction from Population Data. American Journal of\u000d\u000a        Human Genetics 68:978-989. DOI: 10.1086\/319501\u000d\u000a    \u000a\u000a*[2] J Marchini, D Cutler, N Patterson, M Stephens, E Eskin, E Halperin,\u000d\u000a      S Lin, Z Qin, H Munro,\u000d\u000a      G Abecasis, P Donnelly, and International HapMap Consortium (2006) A\u000d\u000a      Comparison of\u000d\u000a      Phasing Algorithms for Trios and Unrelated Individuals. Amercan\u000d\u000a        Journal of Human\u000d\u000a        Genetics, 78 437-450. DOI:10.1086\/500808\u000d\u000a    \u000a\u000a*[3] The International HapMap Consortium. (2005) A haplotype map of the\u000d\u000a      human genome. Nature 437, 1299-1320 DOI:10.1038\/nature04226\u000d\u000a    \u000a\u000a[4] The International HapMap Consortium (2007) A second generation human\u000d\u000a      haplotype map of over 3.1 million SNPs. Nature 449, (7164):851-61.\u000d\u000a      DOI:10.1038\/nature06258\u000d\u000a    \u000a\u000a[5] GA McVean, SR Myers, S Hunt, P Deloukas, DR Bentley, and P Donnelly.\u000d\u000a      (2004) The fine-scale structure of recombination rate variation in the\u000d\u000a      human genome. Science. Apr 23; 304 (5670): 581-4.\u000d\u000a      DOI:10.1126\/science.1092500\u000d\u000a    \u000a\u000a[6] SR Myers, C Freeman, A Auton, P Donnelly, G McVean. (2008) A common\u000d\u000a      sequence motif associated with recombination hot spots and genome\u000d\u000a      instability in humans. Nat Genet. 40, 1124-1129.\u000d\u000a      doi:10.1038\/ng.213\u000d\u000a    \u000aThe three asterisked outputs best indicate the quality of the\u000d\u000a      underpinning research. All six papers are in high quality internationally\u000d\u000a      refereed journals.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"6","Level2":"4","Subject":"Genetics"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    [A] Product description for Illumina HumanOmniExpress BeadChip that\u000d\u000a      describes the use of the HapMap project data in product design. Copy held\u000d\u000a      by University of Oxford. www.illumina.com\/documents\/products\/datasheets\/datasheet_human_omni_express.pdf\u000d\u000a    [B] Letter from the Associate Director for Scientific Research at\u000d\u000a      Illumina that confirms how the HapMap Project data was used in the design\u000d\u000a      of Illumina's SNP-chips. Copy held by University of Oxford.\u000d\u000a    [C] National Human Genome Research Institute (NHGRI) Genome-Wide\u000d\u000a      Association (GWA) Catalog, which provides evidence of the success and\u000d\u000a      scope of GWAS. The catalog lists 1,449 published GWAS in 237 different\u000d\u000a      diseases and traits. www.genome.gov\/26525384\u000d\u000a    [D] Main paper on the discovery of the IL28B variants. This study used\u000d\u000a      the Illumina Human610-quad BeadChip that was designed using the HapMap\u000d\u000a      resource: Ge, D., Fellay, J., Thompson, A. J., Simon, J. S., Shianna, K.\u000d\u000a      V., Urban, T. J., et al. (2009). Genetic variation in IL28B predicts\u000d\u000a      hepatitis C treatment-induced viral clearance. Nature, 461(7262),\u000a      399-401. doi:10.1038\/nature08309\u000d\u000a    [E] Letter from the Senior Vice President for Science &amp; Technology at\u000d\u000a      LabCorp providing details of the IL-28B test that they sell. Copy held by\u000d\u000a      University of Oxford.\u000d\u000a    [F] Press release on LabCorp's IL28B test. Copy held by University of\u000d\u000a      Oxford.\u000d\u000a    [G] Non-academic media coverage of the publication of the first phase of\u000d\u000a      the project. See, for example, news.bbc.co.uk\/1\/hi\/health\/4378624.stm.\u000d\u000a      Copy held by University of Oxford.\u000d\u000a    [H] Discussion of the role of the HapMap Project in medical genetics.\u000d\u000a      House of Lords report on Genomic Medicine (2008-2009). Copy held by\u000d\u000a      University of Oxford.\u000d\u000a    [I] Human Genomics Strategy Group Report. Copy held by University of\u000d\u000a      Oxford. \u000d\u000a    ","Title":"\u000d\u000a    Driving clinical genetic testing and biotechnology development based on\u000d\u000a      the International HapMap Project\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The International HapMap Project was an international collaboration of\u000d\u000a      research institutions from the UK, US, Canada, Japan, China and Nigeria\u000d\u000a      (for details see www.hapmap.org). The\u000d\u000a      project assayed genetic data on a sample of individuals from around the\u000d\u000a      world in order to map the patterns of common genetic variation. The HapMap\u000d\u000a      project had two main strands: data production and data analysis. The data\u000d\u000a      analysis group was co-chaired at different times by Professor Peter\u000d\u000a      Donnelly and Professor Gil McVean and they and other statisticians from\u000d\u000a      the University of Oxford (Dr Jonathan Marchini, Dr Simon Myers) played key\u000d\u000a      roles in developing statistical methods that were used in the project to\u000d\u000a      generate and analyse the data; they made a very substantial contribution\u000d\u000a      to the analysis work. Furthermore, Professor Donnelly sat on the main\u000d\u000a      HapMap committee and played a pivotal role in the feedback from analysis\u000d\u000a      to experimental design.\u000d\u000a    The human genome consists of a sequence of 3 billion base pairs, but only\u000d\u000a      a small fraction of these positions vary between individuals. The\u000d\u000a      experiments carried out by the HapMap project discovered many millions of\u000d\u000a      these variable positions, known as Single Nucleotide Polymorphisms (SNPs).\u000d\u000a      The analysis of the data provides an understanding of the structure of\u000d\u000a      common human genetic variation, in particular, how SNPs at nearby\u000d\u000a      locations in the genome are arranged into common combinations or\u000d\u000a      haplotypes. The project showed that well-powered genome-wide analyses of\u000d\u000a      SNPs can be carried out by assaying only a fraction of all SNPs in the\u000d\u000a      genome, through exploiting the correlation structure of nearby SNPs.\u000d\u000a    The main product of the HapMap project was the set of haplotypes on the\u000d\u000a      270 individuals. Each individual's genome consists of two copies of each\u000d\u000a      chromosome, one from each of their parents. These copies are known as\u000d\u000a      haplotypes. Haplotypes are not directly observed by the genotyping\u000d\u000a      technologies that were used by the project, so statistical methods were\u000d\u000a      needed to infer the haplotypes from the genotypes. In a precursor paper\u000d\u000a      [1], Donnelly and Dr Matthew Stephens (a PDRA) developed a method to\u000d\u000a      achieve this. As the HapMap project progressed, it became necessary to\u000d\u000a      select from a number of competing methods to perform this inference.\u000d\u000a      Marchini and Donnelly led the effort within the project to compare\u000d\u000a      different methods and develop new methods for haplotype estimation. This\u000d\u000a      work was published in the American Journal of Human Genetics in 2006 [2]\u000d\u000a      and the methods of Donnelly and Stephens, as further developed by Marchini\u000d\u000a      and Donnelly, were used to estimate haplotypes in the project's two main\u000d\u000a      papers in Nature [3,4].\u000d\u000a    In parallel, Myers, McVean and Donnelly developed a method for estimating\u000d\u000a      fine-scale recombination rates from the project data. The recombination\u000d\u000a      maps provided an insight into the underlying evolutionary forces that\u000d\u000a      shape the patterns of genetic variation found in the project data [5].\u000d\u000a      This work has further led directly to the identification of the first\u000d\u000a      sequence motifs that are associated with hotspot activity in humans and\u000d\u000a      evidence that these same motifs mark sites of recurrent disease-causing\u000d\u000a      genomic rearrangements in humans [6]. The fine-scale genetic maps produced\u000d\u000a      by Myers, McVean and Donnelly and their research groups have been used in\u000d\u000a      effectively all the subsequent genome-wide association studies (GWAS) to\u000d\u000a      delimit regions of associations, and to pinpoint the natural candidate\u000d\u000a      genes underpinning the association findings.\u000d\u000a    The first phase of the project was completed and published, with very\u000d\u000a      substantial public interest, in 2005 [3]. Donnelly was co-chair of the\u000d\u000a      Analysis Group during the first phase, co-wrote [3] and is joint\u000d\u000a      corresponding author. A second phase of the project, published in 2007\u000d\u000a      [4], produced genotypes at 3.1 million SNPs from 270 people of European,\u000d\u000a      African, Japanese and Chinese ancestry. McVean was co-chair of the\u000d\u000a      Analysis Group in Phase 2, wrote [4] and is joint corresponding author.\u000d\u000a    Work was carried out during the period 1999-2008. All the key researchers\u000d\u000a      were at the University of Oxford when the research was carried out.\u000d\u000a      Donnelly has been Professor of Statistical Science since 1996. From 2007\u000d\u000a      he has been Director, Wellcome Trust Centre for Human Genetics. McVean was\u000d\u000a      a Royal Society University Research Fellow from 2000-2004. Since then he\u000d\u000a      has been a University Lecturer in Mathematical Genetics and has been on\u000d\u000a      secondment as Head of Bioinformatics and Statistical Genetics at the\u000d\u000a      Wellcome Trust Centre for Human Genetics, since 2010. Marchini held a\u000d\u000a      Wellcome Trust Postdoctoral Fellowship from 2002-2005. In 2005 he became a\u000d\u000a      University Lecturer in Statistical Genetics. Myers was a Nuffield Trust\u000d\u000a      Fellow in Medical Mathematics from 2002-2005. He was then a Broad Fellow\u000d\u000a      at the Broad Institute of MIT and Harvard, USA from 2005-2007. Since 2007\u000d\u000a      he has been a University Lecturer in Bioinformatics.\u000d\u000a    Stephens was a postdoctoral researcher at the University of Oxford from\u000d\u000a      1997-2000.\u000d\u000a    "},{"CaseStudyId":"20286","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Since 2008, the University of Oxford's work on GPUs has had both economic\u000d\u000a      impact and impact on practitioners and professional services, via\u000d\u000a      improvement of existing software and the provision of consulting services.\u000d\u000a      The beneficiaries are NAG, NVIDIA, the Apache Foundation, and the large\u000d\u000a      number of people who use the improved software produced by these\u000d\u000a      companies.\u000d\u000a    The impact has been achieved through the transfer of software developed\u000d\u000a      by Giles as part of his research. This has gone into libraries developed\u000d\u000a      and maintained by NAG, NVIDIA, and Apache. With NAG and NVIDIA, this came\u000d\u000a      about through long-standing research collaborations and personal contacts.\u000d\u000a      In the case of the Apache Foundation (which develops open-source\u000d\u000a      software),the organisation asked for Giles' software as a result of\u000d\u000a      reading his papers.\u000d\u000a    The first impact of this research was on the Numerical Algorithms Group\u000d\u000a      (NAG) and through their subsequent dissemination of the software to the\u000d\u000a      financial industry. NAG is an Oxford-based world leader in the development\u000d\u000a      of mathematical computer software libraries, whose products are used\u000d\u000a      worldwide in both academia and industry. NAG and Giles have long-standing\u000d\u000a      connections, including the recent development of a wholly new GPU-based\u000d\u000a      library [B] targeted at the needs of the finance industry. The Vice\u000d\u000a      President of Sales at NAG states [A] \"Thanks to your contributions, we\u000d\u000a        developed the GPU random number generation library quite quickly; I\u000d\u000a        think you were responsible for over half of the original code before it\u000d\u000a        went into our quality assurance process\". He further states that \"It\u000d\u000a      [the NAG GPU library] is being used by two major banks and two others\u000d\u000a        have purchased related consultancy services and training....We have also\u000d\u000a        benefited indirectly from this project, for example one major Tier 1\u000d\u000a        bank made a significant licence upgrade of ~&#163;100,000 and this upgrade\u000d\u000a        only became possible by NAG establishing new contacts within the bank\u000d\u000a        through our GPU work. It is important that we are seen by our customers\u000d\u000a        as being at the cutting edge of scientific computing research, and our\u000d\u000a        work in areas like the GPU library is key to that and does help us to\u000d\u000a        keep existing customers and bring in new ones; over the past 3 years the\u000d\u000a        percentage of top banks who use our software has increased to 60%.\"\u000d\u000a      NAG report [C] that the French bank BNP Paribas reported excellent speed\u000d\u000a      up results (between 150x and 240x relative to a single-threaded CPU\u000d\u000a      simulation, depending on the number of simulations).\u000d\u000a    Giles has contributed fundamental software components to two of the\u000d\u000a      NVIDIA's mathematical libraries for sparse linear algebra (CUSPARSE) and\u000d\u000a      random number generation (CURAND) [D]. Both are integral to many\u000d\u000a      scientific applications using NVIDIA's GPUs, without which the scientific\u000d\u000a      applications would be unable to take advantage of the acceleration\u000d\u000a      hardware in the leading supercomputers. The inverse error function routine\u000d\u000a      (erfinv) is now part of their standard mathematics library, while the\u000d\u000a      Sobol quasi-random generator is part of the CURAND random number\u000d\u000a      generation library. The implementation of the mrg32k3a pseudo-random\u000d\u000a      generator is based on [2] referenced above, and the sparse matrix-vector\u000d\u000a      product (spMV) routine was put into the CUSPARSE library to replace the\u000d\u000a      previous version developed internally by NVIDIA. Both of these algorithm\u000d\u000a      libraries were developed and analysed at Oxford. The spMV routine is also\u000d\u000a      a foundation for NVIDIA's new NVAMG solver, an algebraic multigrid solver\u000d\u000a      which is the basis for a new GPU version of Ansys' Fluent computational\u000d\u000a      fluid dynamics (CFD) software, in turn probably the leading commercial CFD\u000d\u000a      solver worldwide. This illustrates the layered nature of software\u000d\u000a      development, with high-level packages addressing specific applications\u000d\u000a      layered on top of lower- level, more generic, more fundamental software.\u000d\u000a    The Senior Manager for CUDA Libraries and Algorithms at NVIDIA states in\u000d\u000a      his support letter [D] \"It is clear that high quality efficient\u000d\u000a        software libraries are important to users implementing their algorithms\u000d\u000a        on our hardware, and if we did not have our CUDA libraries then NVIDIA\u000d\u000a        would not hold the powerful position within HPC which it does. This is\u000d\u000a        well illustrated by the fact that the Titan system at Oak Ridge National\u000d\u000a        Laboratory, the top supercomputer in the world according to the Top500\u000d\u000a        list, is based on our new Kepler GPUs. Other indications, quoting from\u000d\u000a        our CEO's keynote presentation in the 2013 GTC conference [E],\u000d\u000a        are that in 2012 we sold 100M CUDA-capable GPUs, and the CUDA\u000d\u000a        development kit, including all of the libraries, was downloaded 1.6M\u000d\u000a        times.....Although it is hard to quantify the impact, indications of our\u000d\u000a        appreciation of the impact of your work are that we made you one of our\u000d\u000a        inaugural CUDA Fellows in 2008 (there are still only 11 worldwide and\u000d\u000a        you are the only one in the UK), and we made Oxford University a CUDA\u000d\u000a        Centre of Excellence (CCoE) in 2012\". This award included a donation\u000d\u000a      of $100k to support undergraduate research internships, and hardware\u000d\u000a      donations with a value of approximately another $100k, with further\u000d\u000a      donations likely in future years.\u000d\u000a    Finally, the inverse error function approximation software has also been\u000d\u000a      adopted by the Apache Software Foundation for its Java-based Apache\u000d\u000a      Commons Math library [F]. This has widespread use across numerous sectors,\u000d\u000a      where it is used to convert uniformly-distributed random numbers into\u000d\u000a      normally-distributed random numbers for stochastic simulations written in\u000d\u000a      Java, in application areas as diverse as financial option pricing,\u000d\u000a      biochemical reaction modelling, engineering uncertainty quantification,\u000d\u000a      and the simulation of groundwater flow in nuclear waste repositories.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Many of the top supercomputers use Graphical Processing Units (GPUs) to\u000d\u000a      accelerate scientific computing applications with less energy consumption\u000d\u000a      and lower overall cost. GPUs achieve this by having comparatively large\u000d\u000a      numbers of simple processing elements when compared against CPUs, which\u000d\u000a      have fewer, more sophisticated, elements. However, to take full advantage\u000d\u000a      of GPUs requires quite different algorithms and implementation techniques\u000d\u000a      for mathematical software libraries. Researchers at the University of\u000d\u000a      Oxford have developed a number of such algorithms and implementation\u000d\u000a      techniques over the period 2008-2013, which have been incorporated into\u000d\u000a      software libraries distributed by NAG, NVIDIA and the Apache Foundation\u000d\u000a      and have enhanced the performance up to 150x compared with single thread\u000d\u000a      CPU calculations and 20x relative to multithreaded CPU calculations. These\u000d\u000a      libraries are used by large numbers of application developers worldwide.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    University of Oxford\u000d\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a* [1] A. Lee, C. Yau, M.B. Giles, A. Doucet, C.C. Holmes. 'On the utility\u000d\u000a      of graphics cards to perform massively parallel simulation of advanced\u000d\u000a      Monte Carlo methods'. Journal of Computational and Graphical\u000d\u000a        Statistics, 19(4): 769-789, 2010.\u000d\u000a      DOI: 10.1198\/jcgs.2010.10039 (Google Scholar: 94 citations, Web of\u000d\u000a      Knowledge: 17 citations)\u000d\u000a    \u000a\u000a* [2] T. Bradley, J. du Toit, M.B. Giles, R. Tong, P. Woodhams.\u000d\u000a      'Parallelisation techniques for random number generators'. pp.231-246 in\u000d\u000a      GPU Computing Gems, Emerald Edition, Morgan Kaufmann, 2011. ISBN:\u000d\u000a      0123849888\u000d\u000a    \u000a\u000a* [3] M.B. Giles. 'Approximating the erfinv function'. pp.109-116 in GPU\u000d\u000a      Computing Gems, Jade Edition, Morgan Kaufmann, 2011. ISBN:\u000d\u000a      978-0-12-385963-1\u000d\u000a    \u000a\u000a[4] I. Reguly, M.B. Giles. 'Efficient sparse matrix-vector multiplication\u000d\u000a      on cache-based GPUs' IEEE Refereed Proceedings of Innovative Parallel\u000d\u000a      Computing Conference, 2012. DOI: 10.1109\/InPar.2012.6339602\u000d\u000a    \u000aThe three asterisked outputs best indicate the quality of the\u000d\u000a      underpinning research. [1] is in a major international journal, while [2]\u000d\u000a      and [3] are chapters in research monographs instigated by NVIDIA.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"8","Level2":"6","Subject":"Information Systems"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"}],"Sources":"\u000d\u000a    [A] Letter from Vice President of Sales, Numerical Algorithms Group\u000d\u000a      (NAG), dated 12 June 2013, detailing the significance of GPU computing and\u000d\u000a      Giles' influence on GPU computing at NAG. Copy held by the University of\u000d\u000a      Oxford.\u000d\u000a    [B] Information on GPU computing on the NAG website which mentions Mike\u000d\u000a      Giles by name:\u000d\u000a      http:\/\/www.nag.co.uk\/numeric\/GPUs\/index.asp\u000d\u000a    [C] NAG presentation detailing performance results generated by BNP\u000d\u000a      Paribas\u000d\u000a      http:\/\/www.nag.co.uk\/numeric\/gpus\/FinanceNVIDIA.pdf\u000d\u000a    [D] Letter from the Senior Manager, CUDA Libraries and Algorithms,\u000d\u000a      NVIDIA, dated 27 March 2013, detailing Giles' contribution to CUDA\u000d\u000a      libraries and their impact on NVIDIA. Copy held by the University of\u000d\u000a      Oxford\u000d\u000a    [E] 2013 presentation by NVIDIA CEO Jen-Hsun Huang.\u000d\u000a      http:\/\/www.ustream.tv\/recorded\/30095793\u000a        confirms the number of downloads of the library\u000d\u000a    [F] Email from Independent Contributor to Apache Commons Math library,\u000d\u000a      confirming their use of Giles' implementation of erfinv. Copy held by\u000d\u000a      University of Oxford \u000d\u000a    ","Title":"\u000d\u000a    Development and implementation of mathematical algorithms enhance\u000d\u000a      performance of software libraries on GPUs \u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    In 2007, Professor Mike Giles began research on the use of GPUs for Monte\u000d\u000a      Carlo simulations. This exploited the new capability to use NVIDIA\u000d\u000a      graphics cards for high performance computing (HPC) applications through\u000d\u000a      writing programs using CUDA, NVIDIA's proprietary extension to the\u000d\u000a      computer language C. The performance benefits proved to be substantial,\u000d\u000a      with many of the newly developed algorithms exhibiting a speed-up of over\u000d\u000a      150x compared with single thread CPU execution [2].\u000d\u000a\u000d\u000aFigure 1: Graph showing the number of theoretical Floating-Point Operations per Second for CPUs\u000d\u000aand GPUs, clearly showing the enhanced computing power achievable using GPUs (figure taken\u000d\u000afrom NVIDIA&#8217;s CUDA website).\u000d\u000a\u000d\u000a    This led to a collaboration with Professor Chris Holmes at the University\u000d\u000a      of Oxford's Statistics Department and Professor Arnaud Doucet (who was\u000d\u000a      then at University of British Columbia but moved to the University of\u000d\u000a      Oxford in 2012), in which they demonstrated the performance that could be\u000d\u000a      achieved for more challenging statistical applications such as particle\u000d\u000a      filters; the primary challenge is the re-weighting of the particles which\u000d\u000a      is not easily parallelised [1].\u000d\u000a    Part of Giles' research programme involved the massively parallel\u000d\u000a      implementation of random number generators, including both L'Ecuyer's\u000d\u000a      mrg32k3a pseudo-random generator, and Sobol's quasi-random generator. This\u000d\u000a      work is documented in [2] which has co-authors from both NAG and NVIDIA;\u000d\u000a      both companies have adopted these generators in their respective random\u000d\u000a      number libraries (see below).\u000d\u000a    One standard method of converting uniform random numbers into Normal\u000d\u000a      random numbers is through inverting the Normal cumulative distribution\u000d\u000a      function. This is a simple affine transformation of the inverse error\u000d\u000a      function which is a standard function of many mathematical libraries, but\u000d\u000a      the standard way in which it is approximated performs very poorly on GPUs\u000d\u000a      because of their vector computing nature. This led to Giles developing a\u000d\u000a      new approximation which is detailed in [3].\u000d\u000a    Another key component in many engineering and scientific applications is\u000d\u000a      sparse matrix-vector multiplication. It is easy to implement this\u000d\u000a      efficiently on CPUs, but much harder on GPUs due to the very limited\u000d\u000a      amount of level 1 cache (memory) available to the large number of compute\u000d\u000a      threads. Giles addressed this in a novel way by using multiple compute\u000d\u000a      threads to cooperate to compute each one of the output elements, resulting\u000d\u000a      in a factor 2x speedup compared with NVIDIA's existing CUSPARSE\u000d\u000a      implementation [4].\u000d\u000a    "},{"CaseStudyId":"20291","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000d\u000a    The primary impact of the research described in Section 2 is economic,\u000d\u000a      and the beneficiaries are HSBC and other financial institutions. There is\u000d\u000a      secondary societal impact in the adoption of RORO as a standard term in\u000d\u000a      media coverage of financial markets. All impacts have occurred since 2008.\u000d\u000a    From research to impact\u000d\u000a    The HSBC Foreign Exchange Group were key partners in the development of\u000d\u000a      the underpinning research, three HSBC employees were coauthors, and HSBC\u000d\u000a      coined the phrase RORO. The underpinning research was taken up by the\u000d\u000a      Foreign Exchange Research Group at HSBC, the world's third largest bank by\u000d\u000a      market capitalisation. The bank has a major presence in global markets\u000d\u000a      with large trading operations in all significant asset classes. Trading is\u000d\u000a      undertaken on behalf of clients, including institutional and sovereign\u000d\u000a      fund managers, central banks, charities, and supra-national organisations.\u000d\u000a      HSBC Research actively engages with its client base, both enhancing the\u000d\u000a      clients' background understanding of markets and providing advice on\u000d\u000a      specific investment and trading strategies.\u000d\u000a    The Head of FX Quantitative Strategy at HSBC states [A] \"The RORO\u000d\u000a        research was translated from the original PRE paper into two major HSBC\u000d\u000a        Global Research publications [Risk On-Risk Off: a paradigm is born\u000d\u000a      (2010) and Risk On-Risk Off: Fixing a broken investment process (2012)],\u000d\u000a        aimed at Market Practitioners\". These publications explore the\u000d\u000a      far-reaching consequences of RORO for market participants, and describe\u000d\u000a      techniques explicitly devised to combat the phenomenon, with strategies to\u000d\u000a      aid market participants in incorporating this new view of the market into\u000d\u000a      their planning. HSBC also devised the HSBC RORO Index (illustrated below)\u000d\u000a      to quantify the RORO effect.\u000d\u000a    Nature and extent of the impact\u000d\u000a    Investors have traditionally relied on certain guiding principles, some\u000d\u000a      of which are consistent with standard finance theory such as the Capital\u000d\u000a      Asset Pricing Model (assets respond to their own economic fundamentals as\u000d\u000a      well as overall market conditions, risk reduction through diversification\u000d\u000a      is achievable across asset classes) and others which may not be (enhanced\u000d\u000a      returns can be generated though \"active\" strategies such as relative value\u000d\u000a      trades and stock picking). The RORO phenomenon means these principles are\u000d\u000a      much less useful than they once were. Furthermore, and crucially for\u000d\u000a      investors, it provides a replacement framework within which they can\u000d\u000a      construct new and effective asset allocation strategies.\u000d\u000a\u000d\u000a\u000d\u000a\u000d\u000a    Immediate impact for HSBC can be measured in terms of client take-up.\u000d\u000a      Research is distributed via a web-site, and by an emailed web-link,\u000d\u000a      enabling active downloads to be tracked. The Head of FX Quantitative\u000d\u000a      Strategy at HSBC writes [A]: \"In 2012, HSBC published over [text\u000d\u000a        removed for publication] research reports. Scored by distinct\u000d\u000a        hits, the report \"RORO: Fixing a Broken Investment Process\" was ranked 3rd\u000d\u000a        globally. [...] It is a testament to the importance of RORO to\u000d\u000a        traditional practitioners as well as `quants' and points to RORO\u000d\u000a        becoming `mainstream and widely relevant. [...] HSBC also\u000d\u000a        distributes research in online video format and two videos were produced\u000d\u000a        ... Of over [text removed for publication] video releases in 2012, these\u000d\u000a        two videos were ranked 2nd and 3rd most watched.\"\u000d\u000a    As a consequence of client demand, the PCA-based graphical and\u000d\u000a      quantitative tools, as developed by the University of Oxford and HSBC, are\u000d\u000a      now updated weekly by HSBC and provided to clients on a subscription\u000d\u000a      basis. The immediate commercial impact for HSBC is exemplified in the\u000d\u000a      letter from the Head of FX Quantitative Strategy at HSBC [A], which states\u000d\u000a      [text removed for publication]. Trading businesses are high volume,\u000d\u000a      low margin operations. A higher ongoing volume of client business is thus\u000d\u000a      extremely valuable. Numbers are too commercially sensitive to state, but\u000d\u000a      are significant enough that HSBC has now launched a dedicated Emerging\u000d\u000a      Market version of the PCA-based tools to further its Asian, Latin American\u000d\u000a      and Middle Eastern franchises.\u000d\u000a    Deeper impact is seen in requests for in-depth project work by HSBC from\u000d\u000a      top-tier clients. \"We also conducted bespoke research for individual\u000d\u000a        clients, including central banks, corporates and investment managers\u000d\u000a        looking to adapt their businesses to the RORO phenomenon. The combined\u000d\u000a        AUM [assets under management] of these clients totalled hundreds\u000d\u000a        of billions of dollars\" states the Head of FX Quantitative Strategy\u000d\u000a      at HSBC [A].\u000d\u000a    The research has directly benefited the wider investment management\u000d\u000a      community, with many substantial investors using the HSBC RORO index as a\u000d\u000a      key quantitative tool for making investment decisions. As an example of\u000d\u000a      typical usage, the website of Institutional Asset Manager, reporting a\u000d\u000a      presentation [B] by Peter Rigg, Global Head of HSBC Alternative Investment\u000d\u000a      Group and an early adopter of the use of RORO, states: \"Quite simply,\u000d\u000a        until the RORO Index shows signs of falling, Rigg does not envisage a\u000d\u000a        transition from Scenario 1 [relatively pessimistic] to Scenario\u000d\u000a        2 [cautiously optimistic]\"; it goes on to explain that his\u000d\u000a      investment strategy will be determined by this signal: \"`We're\u000d\u000a        currently positioned for Scenario 1 but we can move quickly into\u000d\u000a        Scenario 2 when required', said Rigg\". HSBC AIM is the largest\u000d\u000a      Alternative manager in the UK with $30 Bn AUM.\u000d\u000a    The Global Chief Investment Officer of [text removed for publication]\u000d\u000a      writes (to the University of Oxford) [C]: \"Your published research in\u000d\u000a        this area has been instrumental for practitioners endeavouring to\u000d\u000a        measure and adapt to this once-in-a-generation shift in market\u000d\u000a        structure. I consider your work to be groundbreaking and it has had\u000d\u000a        significant impact within the asset management community and beyond\".\u000d\u000a      A Director at [text removed for publication] writes [D]: \"[RORO] helped\u000d\u000a        my team better understand our positioning during a difficult time in the\u000d\u000a        markets\", listing areas where [text removed for publication] uses\u000d\u000a      it, including stress testing positioning (analysis of portfolio behaviour\u000d\u000a      under extreme scenarios) and assisting clients in total portfolio\u000d\u000a      construction. A Senior Portfolio Manager at [text removed for\u000d\u000a        publication], states [E]: \"The hallmark of the RORO phenomenon\u000d\u000a        was a dramatic increase in cross-asset correlations: this had profound\u000d\u000a        consequences for the asset management community. [...] the\u000d\u000a        insight we derived from your research has had a direct impact on the\u000d\u000a        construction methodologies we adopted for our funds.\"\u000d\u000a    Use of the RORO framework has now extended well beyond HSBC and other\u000d\u000a      professional investment managers. After featuring in specialist\u000d\u000a      publications such as Risk Magazine (\"Everyone is perplexed by these\u000d\u000a        risk-on or risk-off days, where it feels like you can actually see the\u000d\u000a        correlation increase\" [F], para 2) and FX Week (headline: Risk-on,\u000d\u000a      risk-off markets boost demand for active currency management [G], RORO\u000d\u000a      began to feature regularly in the generalist financial press, often with\u000d\u000a      explicit reference to the HSBC research team. There have been many\u000d\u000a      appearances in the Financial Times and Wall Street Journal, for example \"In\u000d\u000a        the scale of Risk On\/Risk Off trading days, this looked usual\" [H]\u000d\u000a      from the FT and the WSJ headline \"Bernanke's 'Risk-On, Risk-Off' Monetary\u000d\u000a      Policy\" [I]; they include columns, blogs, feature articles and inclusion\u000d\u000a      in a Private Wealth Management supplement. The Financial Times has\u000d\u000a      recently added RORO to the official list of tags it uses to index its\u000d\u000a      website FT.com, where a search for RORO gives more than 50 results [J].\u000d\u000a    The RORO paradigm has now moved on to become a staple of mainstream\u000d\u000a      journalism. For example, articles in the Times (headline: \"Risk on, risk\u000d\u000a      off as stock market's RoRo goes into sharp reverse\" [K]) and the New York\u000d\u000a      Times Business Day `Your Money' section (\"Why are markets so highly\u000d\u000a      correlated? The answer may be found in \"risk on, risk off,\" a bit of\u000d\u000a      jargon favored by financial traders and strategists.\" [L]) are aimed at\u000d\u000a      mainstream retail audiences, while [M] looks at the repercussions of RORO\u000d\u000a      for retirement planning and saving for children's education. Likewise,\u000d\u000a      mainstream retail investor websites use the terminology routinely; for\u000d\u000a      example, in an article entitle Rising star fund managers,\u000d\u000a      Investor's Chronicle simply quoted Jason Hollands from Bestinvest:. \"A\u000d\u000a        false call on the risk on\/risk off trade could turn a manager from hero\u000d\u000a        to zero overnight\" [N].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This case study charts the influence of the Risk On \/ Risk Off (RORO)\u000d\u000a      paradigm, developed in research at the University of Oxford in\u000d\u000a      collaboration with investment bank HSBC. Since 2008, RORO has had a\u000d\u000a      significant economic impact on HSBC as well as wider impact on the\u000d\u000a      thinking and actions of investors and other global market participants.\u000d\u000a      Having begun as a specialised research tool within HSBC's foreign exchange\u000d\u000a      team, the RORO methodology was publicised in the advice that HSBC supply\u000d\u000a      to a wide range of major fund managers, corporate institutions and central\u000d\u000a      banks. The research has led directly to a change in the way that asset\u000d\u000a      managers think about investment decisions, with consequent impact on the\u000d\u000a      investment and risk management strategies they undertake. RORO is\u000d\u000a      regularly featured in the financial press and is becoming increasingly\u000d\u000a      mainstream, with coverage in national and international media aimed at\u000d\u000a      retail investors.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    University of Oxford\u000d\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a* [1] Fenn DJ, Porter MA, Williams S, McDonald M, Johnson NF &amp; Jones\u000d\u000a      NS. Temporal evolution of financial-market correlations. Phys.\u000d\u000a      Rev. E 84, 026109, 2011. DOI 10.1103\/PhysRevE.84.026109.\u000d\u000a    \u000a\u000a* [2] Fenn DJ, Porter MA, McDonald M, Williams S, Johnson NF, &amp; Jones\u000d\u000a      NS. Dynamic Communities in Multichannel Data: An Application to the\u000d\u000a        Foreign Exchange Market During the 2007-2008 Credit Crisis, Chaos, 19,\u000d\u000a      033119, 2009. DOI 10.1063\/1.3184538.\u000d\u000a    \u000a\u000a* [3] Fenn DJ, Porter MA, Mucha PJ, McDonald M, Williams S, Johnson, NF,\u000d\u000a      &amp; Jones NS, Dynamical Clustering of Exchange Rates,\u000d\u000a      Quantitative Finance, 12, 1493-1520. 2012. DOI\u000d\u000a      10.1080\/14697688.2012.668288.\u000d\u000a    \u000aThe three asterisked outputs best indicate the quality of the\u000d\u000a      underpinning research. All these papers are in high quality\u000d\u000a      internationally refereed journals.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"15","Level2":"2","Subject":"Banking, Finance and Investment"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a    [A] Letter from Head of FX Quantitative Strategy at HSBC, describing the\u000d\u000a      development, use and impact of RORO in HSBC. Copy held by University of\u000d\u000a      Oxford.\u000d\u000a    [B] http:\/\/www.institutionalassetmanager.co.uk\/2012\/02\/07\/161783\/hsbc-alternative-\u000d\u000ainvestments-limited-hail-remains-defensive-2012-while-risk-%E2%80%93-risk,\u000d\u000a      2012.\u000d\u000a    [C] Letter from CIO, [text removed for publication]. Copy held by\u000d\u000a      University of Oxford.\u000d\u000a    [D] Letter from Director at [text removed for publication] Copy\u000d\u000a      held by University of Oxford.\u000d\u000a    [E] Letter from Senior Portfolio Manager, [text removed for\u000d\u000a        publication]. Copy held by University of Oxford.\u000d\u000a    [F] http:\/\/search.proquest.com\/docview\/753944406\/13D07A448C83210A5E5\/6?accountid=130\u000d\u000a      42\u000d\u000a    [G] http:\/\/www.fxweek.com\/fx-week\/news\/2188455\/amundi-signs-mim-currency-account\u000d\u000a    [H] James Mackintosh, 'The Short View', The Financial Times, 5\u000d\u000a      January 2012,\u000d\u000a      http:\/\/www.ft.com\/cms\/s\/0\/40c5f0ea-37bd-11e1-a5e0-00144feabdc0.html#axzz2OIbGgj00\u000d\u000a    [I] http:\/\/online.wsj.com\/article\/SB10000872396390443524904577649793013124710.html?KEYWORDS=%22risk+on%22+%22risk+off%22 &#8212; note the attached tags. 2012.\u000d\u000a    [J] http:\/\/search.ft.com\/search?ftsearchType=type_news&amp;queryText=Roro\u000d\u000a    [K] http:\/\/www.thetimes.co.uk\/tto\/business\/markets\/article3424248.ece,\u000d\u000a      2013.\u000d\u000a    [L] http:\/\/www.nytimes.com\/2012\/01\/29\/your-money\/how-long-can-the-stock-market-forget-about-the-pain.html?, 2012.\u000d\u000a    [M] http:\/\/www.nytimes.com\/2011\/04\/03\/your-money\/03stra.html.,\u000d\u000a      2011.\u000d\u000a    [N] \u000d\u000ahttp:\/\/www.investorschronicle.co.uk\/2013\/01\/14\/funds-and-etfs\/the-big-theme\/rising-star-fund-managers-262S1OKsmlBjoCcTo1CVcJ\/article.html, 2013.\u000d\u000a    [C]-[E] confirm the reach and significance of the economic impact of RORO\u000d\u000a      among major fund managers. [F]-[N] corroborate the reach and significance\u000d\u000a      of the societal impact of RORO. The University of Oxford holds copies of\u000d\u000a      all webpages.\u000d\u000a    ","Title":"\u000d\u000a    Risk On \/ Risk Off: from academic research to financial market staple\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The collapse of Lehman Brothers in 2008 precipitated a dramatic and\u000d\u000a      enduring change in the correlation structure of global financial markets.\u000d\u000a      This has become widely known by investment professionals as the Risk\u000d\u000a      On\/Risk Off (RORO) phenomenon &#8212; a term coined by the authors of the\u000d\u000a      research considered below, and now common market parlance.\u000d\u000a    Researchers in the University of Oxford started thinking about the\u000d\u000a      temporal evolution of multi-asset correlations and their relationship to\u000d\u000a      macro-economic and geo-political events in 2008. In [1] these were\u000d\u000a      characterised and measured using evolving correlation matrices, which were\u000d\u000a      constructed from a large dataset covering 98 major asset prices over 12\u000d\u000a      years from January 1999. The research compares the realised correlations\u000d\u000a      with those that would result from a random-matrix equivalent, revealing\u000d\u000a      that there is structure in the data that is not present in the Gaussian\u000d\u000a      model used in standard financial theory. This structure is investigated in\u000d\u000a      more detail via a Principal Component Analysis. A key result is that\u000d\u000a      before the collapse of Lehman Brothers in September 2008, the first\u000d\u000a      component is generally unremarkable and many assets are close to\u000d\u000a      uncorrelated (see the left-hand panel of the figure below, in which the\u000d\u000a      colour indicates the strength of correlation). Post-Lehman, a dominant\u000d\u000a      component emerges, the strength of which is tightly coupled to market\u000d\u000a      events, as illustrated in the right-hand panel of the figure below, in\u000d\u000a      which all bonds are closely correlated, as are all equities.\u000d\u000a\u000d\u000a\u000d\u000a\u000d\u000a    The research [1] provided a view of the markets which came to be known as\u000d\u000a      RORO. This is the direct manifestation of the Principal Component Analysis\u000d\u000a      in [1], and is backed up by the parallel studies [2,3] which use\u000d\u000a      techniques from network analysis to develop algorithms to detect large\u000d\u000a      clusters which may indicate the presence of changes of risk states. RORO\u000d\u000a      uses two states to characterise market conditions, and individual assets\u000d\u000a      are characterised as being either risky assets, or safe-havens. \"Risk On\"\u000d\u000a      periods see investors buy risky assets and employ traditional\u000d\u000a      correlation-based strategies, although even in these periods, correlation\u000d\u000a      levels are much higher than was typical before the financial crisis. Thus,\u000d\u000a      risky assets rise in value and safe-haven instruments fall. In \"Risk Off\"\u000d\u000a      periods these moves swing into reverse as investors all move into safe\u000d\u000a      havens, which all become highly correlated. RORO has profound consequences\u000d\u000a      for asset managers as well as for other market participants such as hedge\u000d\u000a      funds, corporate institutions and central banks.\u000d\u000a    A second significant outcome of the research, also described in [1], was\u000d\u000a      the development of PCA- based graphical and quantitative methods to\u000d\u000a      analyse the evolution of correlation structure, track the RORO phenomenon\u000d\u000a      and monitor its influence on particular assets.\u000d\u000a    The underpinning research was carried out at the University of Oxford\u000d\u000a      between 2008 and 2011 together with colleagues from HSBC. The key\u000d\u000a      researchers were Mason Porter (University Lecturer, 2007 to date) and Dr\u000d\u000a      Nick Jones (Research Fellow, 2008-2012).\u000d\u000a    "},{"CaseStudyId":"20293","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000a    Not only has Billmonitor had a substantial economic impact, it\u000a      has also affected public policy and had a significant impact on society.\u000a      The main beneficiaries are the parent company Optimor and the\u000a      general public.\u000a    Billmonitor is the sole product of the parent company Optimor,\u000a      which was established specifically in order to bring Billmonitor\u000a      to market. The resulting price comparison website, launched in 2009, was\u000a      the first of its kind to be recognised by the independent regulator,\u000a      Ofcom. The Mobile Phone Guide [A], published by Ofcom's Consumer\u000a      Focus and Communications Consumer Panel, explains how Billmonitor\u000a      can help consumers track down the best deal. Since the launch of Billmonitor,\u000a      over 110,000 users have provided details of their mobile phone usage\u000a      through the site. The savings available over the lifetime of a contract\u000a      (typically 12-24 months) identified for those users total over &#163;35 million\u000a      [B]. Reports such as http:\/\/www.billmonitor.com\/billmonitor-national-mobile-report-2012\u000a      by the Billmonitor team, which estimate a total overspend on UK\u000a      mobile phone contracts totalling literally billions of pounds per annum,\u000a      have received extensive press coverage.\u000a    How research underpins impact\u000a      The founder of Optimor discussed the idea of the company with\u000a      Holmes and Meinshausen, who realised that their previous research provided\u000a      just the insights required for building an accurate algorithm for\u000a      predicting an individual's future mobile phone usage. He states [B] \"The\u000a        research done by Professor Chris Holmes and Professor Nicolai\u000a        Meinshausen of the University of Oxford was an essential component which\u000a        has enabled us to build an accurate algorithm for predicting an\u000a        individual's future mobile phone usage and we have used this to develop\u000a        Billmonitor.\"\u000a    Users provide access to their mobile phone call plan, call histories and\u000a      billing and Billmonitor finds a telephone contract optimised to\u000a      the individual's pattern of use (see figure). The user's call history is\u000a      treated as a time series, and makes predictive inference for future calls.\u000a      For each potential contract, the algorithm uses robust bootstrap methods\u000a      for adaptive prediction to construct a distribution over future\u000a      pseudo-bills. Two particular problems present themselves when trying to\u000a      use this to predict an individual's future mobile phone usage. First, user\u000a      behaviour evolves with time, resulting in regime shifting and consequently\u000a      a bias-variance trade-off associated with window scale selection. The\u000a      insights provided by Holmes's previous work were key to the development of\u000a      the windowing approach taken in Billmonitor. The second problem is\u000a      that users show occasional large deviations from their `usual' behaviour,\u000a      exposing them to potentially heavy losses. The importance of the\u000a      distribution of loss in the context of regime switching, which was so\u000a      important in Holmes's work on segmental classification of genetic\u000a      sequences [2], becomes the accurate estimation of the distribution of the\u000a      upper tail of the `pseudo-bills' for each contract. The choice of window\u000a      size in Billmonitor is optimised via a bootstrap study of the\u000a      population exploiting a blocked bootstrap method. Prototyping was\u000a      completed using R.\u000a    \u000a    \u000a    \u000a    Once prototyping of the statistical tool was complete, the front-end user\u000a      interface of the programme was designed by engineers at Optimor,\u000a      with Holmes and Meinshausen advising on the graphical displays of\u000a      information.\u000a    Nature and extent of impact\u000a      The parent company, Optimor, employs a managing director and four\u000a      developers. Holmes and Meinshausen were directors of Optimor\u000a      (Holmes from April 2008 until January 2012 and Meinshausen from April 2008\u000a      until September 2011) and continue to act as scientific advisers, and\u000a      there are three further commercial advisers. The company's financial model\u000a      allows it to provide a completely unbiased price comparison tool: Billmonitor\u000a      receives a small affiliate commission if users purchase via the links on\u000a      the site. However, they will pay no more than if they had bought direct.\u000a      The links are non-biased and advice is given exclusively on the basis of\u000a      which plan provides the best deal for the user's projected usage.\u000a    In 2009, Billmonitor was the first price comparison site to be\u000a      awarded the price accreditation scheme logo from Ofcom, having met the\u000a      terms of a rigorous independent audit, which tests whether information\u000a      given to consumers is accessible, accurate, transparent, comprehensive and\u000a      up to date. The head of company performance at Consumer Focus, the\u000a      statutory organisation campaigning for consumer rights, explained at the\u000a      time why this was such an important step: \"Accreditation for comparison\u000a        sites is vital. With tens of thousands of mobile phone tariffs on the\u000a        market consumers need peace of mind that before they take up a new deal\u000a        they [can] get comprehensive and unbiased advice\" [C]. In\u000a      2010 Billmonitor featured in Ofcom's guide to obtaining the best\u000a      mobile phone deal [A].\u000a    Since the launch of Billmonitor, a number of rival comparison\u000a      sites have been launched. However, as the online technology magazine CNET\u000a      puts it \"There are enough mobile phone price comparison sites out\u000a        there that someone should launch a price comparison comparison site.\u000a        Except that they don't need to, because Bill Monitor is simply the best\"\u000a      [D]. TheBillmonitor site is also ranked top for accuracy by the\u000a      online consumer advice service MoneySavingsExpert [E] who describe it as\u000a      the \"Best for any decent handset\".\u000a    In 2011, the Billmonitor team published an analysis, based on\u000a      28,500 UK mobile phone bills, which revealed that three out of four users\u000a      were paying an average of &#163;195 too much for their mobile phone contracts,\u000a      equating to almost half their annual spend. This report received\u000a      widespread media coverage, with articles in The Times, The Guardian, The\u000a      Telegraph, The Daily Mail, The Financial Times and The Wall Street\u000a      Journal; and features on Sky News and Channel 5's Gadget show [F]. In a\u000a      feature on BBC1's The One Show, Billmonitor was used to help the\u000a      people of Bristol save &#163;1,000 in one hour [G].\u000a    The 2012 Billmonitor National Mobile Report painted a similar\u000a      picture, and recommended a \"tariff diet\" for Britons to help put an end to\u000a      an estimated &#163;6 billion pounds annually wasted on mobile phone bills. The\u000a      team also published a separate smartphone data report, featured, for\u000a      example, on the BBC news website [H].\u000a    The Founder and Managing Director of Optimor states [B]: \"Since its\u000a        inception, Billmonitor has analysed over 1.5 million phone bills, an\u000a        average of 14 months worth for each of a total of over 110,000 users,\u000a        and identified &#163;35 million of savings. We estimate that the overspend on\u000a        UK mobile phone contracts totals literally billions of pounds per annum.\"\u000a      According to URLmetrics [I], the Billmonitor site receives an average of\u000a      725 visits per day. As one might expect for an internet service, it\u000a      receives a huge amount of coverage on the internet. An internet search on\u000a      `Billmonitor' reveals literally hundreds of blogs and websites\u000a      recommending the service. Typical, is a piece on the personal finance\u000a      website Money to the Masses, \"Billmonitor I salute you as I will save\u000a        over &#163;360 a year\" [J].\u000a    ","ImpactSummary":"\u000a    Since its launch in 2009, the mobile phone package price comparison tool\u000a      Billmonitor has identified &#163;35 million worth of savings available\u000a      to the 110,000 users whose bills have been analysed. It was the first\u000a      price comparison tool to be accredited by Ofcom and it has been widely\u000a      praised in the media. Exploiting techniques that they had developed for\u000a      applications in finance and genetics, University of Oxford researchers\u000a      Chris Holmes and Nicolai Meinshausen developed the statistical algorithms\u000a      underpinning the package, which uses simulation-based inference and\u000a      careful statistical modelling to analyse users' phone bill data. It\u000a      searches over 2.4 million available packages to identify the best mobile\u000a      phone deal for each user's particular pattern of usage. Widely quoted in\u000a      the press, reports in 2011 and 2012 from the Billmonitor team\u000a      estimated that approximately three quarters of mobile phone customers are\u000a      on the wrong tariff, with an overspend of around 40%.\u000a    ","ImpactType":"Economic","Institution":"\u000a    University of Oxford\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a*[1] Dellaportas P, Denison D and Holmes C (2007) \"Flexible threshold\u000a      models for modelling interest rate volatility\". Econometric Reviews.\u000a      Special Issue on Bayesian Dynamic Econometrics. 26(2). 419-437 DOI:\u000a      10.1080\/07474930701220600\u000a    \u000a\u000a*[2] Yau, C., Holmes, C. (2013), \"A decision theoretic approach for\u000a      segmental classification using Hidden Markov models\", Annals of\u000a        Applied Statistics, 7(3), 1814-1835. DOI: 10.1214\/13-AOAS657\u000a    \u000aThe two asterisked outputs best indicate the quality of the underpinning\u000a      research. Both papers are in high quality internationally refereed\u000a      journals.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000a    [A] Ofcom's \"The Mobile Phone Guide\", specifically recommends Billmonitor\u000a      and demonstrates the reach and significance of the impact:\u000a      consumers.ofcom.org.uk\/files\/2010\/03\/mobiledeal_v2b.pdf\u000a    [B] Letter from Founder and Managing Director of Optimor, highlighting\u000a      the pathway from research to impact and the significance of Billmonitor.\u000a      Copy held by University of Oxford.\u000a    [C] Quote from Head of Company Performance at Consumer Focus, describing\u000a      the importance of accreditation: http:\/\/www.guardian.co.uk\/money\/2009\/may\/21\/mobile-phones-billmonitor\u000a    [D] CNET Magazine snippet comparing mobile phone comparison sites:\u000a      http:\/\/reviews.cnet.co.uk\/gadgets\/best-money-saving-tech-50004215\/\u000a    [E] MoneySavingExpert website, demonstrating significance of Billmonitor:\u000a      http:\/\/www.moneysavingexpert.com\/phones\/mobile-phone-cost-cutting.\u000a    [F] Billmonitor in the press: http:\/\/www.billmonitor.com\/what-others-are-saying.html?alt=false\u000a    [G] Billmonitor on One Show, BBC, demonstrating significance\u000a      www.youtube.com\/watch?v=079u38-zCTA\u000a    [H] BBC News report on Billmonitor's smartphone data report www.bbc.co.uk\/news\/technology-21959032\u000a    [I] URL metrics analysis of the Billmonitor website http:\/\/urlm.co.uk\/www.billmonitor.com#web\u000a    [J] Article on Money to the Masses website, indicating the significance\u000a      of Billmonitor\u000a      http:\/\/moneytothemasses.com\/how-i-cut-my-phone-bill-by-60-in-2-minutes\u000a    ","Title":"\u000a    Billmonitor: predicting the best mobile phone contract for users\u000a    ","UKLocation":[{"GeoNamesId":"2654675","Name":"Bristol"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The identification of an effective statistical method for Billmonitor\u000a      was itself a research problem in applied statistics, whose resolution\u000a      rested on the insights gained from research in other, at first sight\u000a      unconnected, application areas. Chris Holmes, Professor of Statistics at\u000a      the University of Oxford since 2004, and Nicolai Meinshausen (University\u000a      Lecturer at the University of Oxford, 2007-2013) were able to combine the\u000a      insights that they had gained in overcoming parallel difficulties in their\u000a      research in finance and genetics to build an effective forecasting tool\u000a      for an individual's mobile phone usage.\u000a    Greatest costs are incurred when mobile phone users stray outside their\u000a      monthly allowances and so, in order to reliably forecast the expected cost\u000a      of a tariff, the model used must accurately approximate the tails of the\u000a      user behaviour (the unusual months). Billmonitor rests on a bespoke\u000a      bootstrap algorithm, developed by Holmes and Meinshausen, to make the\u000a      predictions.\u000a    In any time series in which there is regime shifting, more recent\u000a      observations are more reliable in predicting the future evolution than\u000a      older ones. Bootstrap samples are therefore restricted to a time window.\u000a      However, if this window is too short, sample size is reduced and\u000a      prediction variance is inflated. If the window is too long, the predictive\u000a      distribution is not adapted to regime-switching and bias is inflated.\u000a      Holmes's work on Bayesian inference for regime switching in threshold\u000a      models for time-series variability [1] underpinned his analysis of\u000a      adaptation to user regime-switching in phone bills. Holmes and Meinshausen\u000a      discussed notions of statistical uncertainty and translated this into a\u000a      bootstrap setting for the application. The switch to a windowing approach\u000a      was inspired by Holmes's experience working with data partitioning\u000a      methods. A blocked bootstrap method is used to optimise the algorithm.\u000a    Another strand of Holmes's research at the time was developing\u000a      loss-functions for recovering regime switching (\"segmental\u000a      classification\") along genetic sequences with time-series-like structure\u000a      [2]. Traditional approaches, such as reporting the most probable state\u000a      sequence or the most probable set of marginal predictions, correspond to\u000a      particular choices of loss function that may be inappropriate for\u000a      segmental analysis of sequence data. The new class of Markov loss\u000a      functions proposed in [2] penalises misclassification of both state\u000a      occupancy and transitions. The sequence of minimum expected loss is then\u000a      enumerated using dynamic programming methods.\u000a    Meinshausen completed the prototyping for Billmonitor using R.\u000a    "},{"CaseStudyId":"20302","Continent":[{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"1269750","Name":"India"}],"Funders":[],"ImpactDetails":"\u000a    The impact is on society through public interest and engagement with\u000a      science and the stimulation of public discourse. The questions of the\u000a      `origin of the universe', `what happened before the big bang?', and `how\u000a      space-time might emerge from a more fundamental theory' are some of the\u000a      most frequently addressed issues in popular science and stimulate wide\u000a      interest (and controversy). Roger Penrose's writings have opened up new\u000a      avenues in this debate that have led to much interest outside academia.\u000a      The beneficiaries since 2008 have been the general public who gain an\u000a      understanding of current models for the evolution of the universe.\u000a    The research of Paul Tod demonstrated rigorously that the Weyl curvature\u000a      hypothesis gave mathematical sense to the question of whether the\u000a      conformal geometry whose existence is demonstrated mathematically has any\u000a      physical meaning. This underpinned Penrose's Conformally Cyclic Cosmology\u000a      proposal. His recent broad audience book Cycles of Time: An\u000a        Extraordinary New View of the Universe (published in 2010) explains\u000a      the conformal cyclic cosmology which is underpinned by the research\u000a      described in Section 2.\u000a    Cycles of Time has been hugely successful. More than 95,000 copies\u000a      have been sold worldwide since 2010 [A], it has been translated into\u000a      German, Polish, Italian and Russian, and released as an audio book. The\u000a        Nature of Space and Time has sold 10,502 copies in the period [B]\u000a      and the Road to Reality, another pre-cursor building on the themes\u000a      in the Nature of Space and Time, has sold nearly 120,000 copies since 2004\u000a      including 45,400 in the REF period [A]. The books are also promoted with\u000a      accompanying public presentations at large book fairs (e.g., 25\/11\/10 at\u000a      Toppings, The Paragon, Bath, Somerset or 26\/3\/12 Sunday Times Literary\u000a      Festival, Oxford). A new book by Penrose is an automatic choice for review\u000a      in quality media of all kinds.\u000a    Penrose's books are highly unusual among popular science books in\u000a      celebrating rather than obscuring the mathematical equations and\u000a      geometrical pictures that underlie the physical ideas. They take the\u000a      audience seriously as intellectuals and do not oversimplify. Readers\u000a      engage directly with Penrose's writing, as evidenced by a 2008 reader\u000a      review [C] of The Road to Reality on Amazon: \"If you\u000a        assiduously go through every sentence until you understand its meaning,\u000a        if you consult outside references as necessary, if you really absorb\u000a        this material- it is phenomenal. Penrose will equip you with a visual\u000a        and intuitive comprehension of the advanced math necessary to really\u000a        understand the big theories of physics. [...] In terms of the\u000a        breadth and scope, I could compare this to the Feynman lectures.\u000a      [...] This is a book to live in for a long time. There is nothing else\u000a        quite like it. The rewards justify the large amount of reader time and\u000a        effort that will need to be committed. Five stars, absolutely\".\u000a      Professional and reader reviews of Cycles of Time also attest to\u000a      Penrose's success in engaging with the wider public:\u000a    \"The most influential person to develop the general theory of\u000a        relativity since Einstein\" [D]\u000a    \"A gifted popularizer of science [...] Roger Penrose has\u000a        attracted a popular audience with thought- provoking books on physics,\u000a        consciousness and the theory of computation\" [E]\u000a    \"Doing what most find impossible has long been Penrose's stock in\u000a        trade in mathematics and physics, even when it comes to publishing\"\u000a      [F]\u000a    \"One of the world's best science writers\" [G]\u000a    \"As usual, one gets a clear, simply, but new view of physics from\u000a        Penrose. The depth is much greater than many popular science books, but\u000a        I find I can grasp the ideas. I have wondered for a long time how to get\u000a        an understanding of metrics which seem all important in modern physics.\u000a        I looked at a collection of math books which would provide the knowledge\u000a        but felt that it would take the rest of my life to get there. Penrose\u000a        gave me as much knowledge of the ideas and usage of the methods as I\u000a        need in one page\" [H]\u000a    This approach makes his contributions to the public engagement with\u000a      science particularly important as it allows the public to see scientific\u000a      debate as a rigorous evaluation of mathematical argument and empirical\u000a      data rather than as `anything-goes' speculation with one person's theory\u000a      being on a democratic footing with anyone else's.\u000a    Another unusual feature of Penrose's public engagement is that he is\u000a      rarely reviews other people's ideas except to counterpoint them against\u000a      his own. He engages people directly with his own research and that of his\u000a      close associates following the programmes that he has initiated. The\u000a      public engagement described in this study arises directly from the\u000a      research described here and not from that of others or older ideas. This\u000a      point is reinforced by his use of hand-drawn diagrams which have attracted\u000a      much favourable comment from readers of his books and online viewers of\u000a      his lectures, for example \"He's probably the only world famous speaker\u000a        that draws his own slides with color marker pens! Not just a brilliant\u000a        mind but also a talented illustrator!\" comments one YouTube watcher\u000a      in 2011 [I].\u000a\u000a\u000a\u000a    The wide impact of these ideas and Penrose's presentational style is\u000a      evidenced by regular invitations to give distinguished public lectures and\u000a      to appear on the media the world over. Penrose featured prominently in a\u000a      2010 BBC Horizon programme which attracted more than 1.8 million\u000a      viewers [J]. His public lectures typically pack out the largest available\u000a      auditoria, sometimes with overflow rooms equipped with video-links. For\u000a      example, a capacity audience of 477 saw him at the Royal Institution in\u000a      2010 [K], while an audience of more than 700 saw him lecture after\u000a      receiving the 2011 Fonseca prize. Of those who filled in a feedback\u000a      questionnaire following the Royal Institution lecture, 94% had come `to\u000a      find out more' and 82% wanted `to find out more following the event' [K].\u000a      He gave the Tagore, Bose and Chandrasekhar memorial lectures in Kolkata,\u000a      the Chandrasekhar lecture Delhi, the Neils Bohr lecture in Copenhagen, the\u000a      Trotter Public lecture after receipt of the Trotter prize at Texas\u000a      A&amp;M, and the Copernicus Institute Lecture in Warsaw among many others\u000a      around the world.\u000a    The impact of Penrose's public lectures is not limited to the few hundred\u000a      people that attend each of them, as they are often filmed and posted\u000a      online. For example, YouTube lists 344 videos in response to a search for\u000a      `Roger Penrose'; not all of these feature him directly, but total views of\u000a      those that do, and were posted since 2008, number well over 500,000 [L]. A\u000a      good example is his 2009 Christmas Lecture Aeons before the Big Bang\u000a      with more than 52,000 views [M]; another is his 2010 TEDx lecture aimed at\u000a      encouraging young people to engage with STEM subjects, with more than\u000a      32,000 views [N]; and a third is a public lecture on Twistors and\u000a        Quantum non-locality, broadcast on TV in Ontario in 2011, with more\u000a      than 40,000 views [O]. Many of the posted videos generate comments\u000a      numbering in the hundreds and often stimulate online dialogues about the\u000a      content of the lectures.\u000a    These are outstanding impacts on a world stage. As can be seen, Penrose\u000a      is in great demand and has a worldwide following.\u000a    ","ImpactSummary":"\u000a    This case study describes public engagement with the University of\u000a      Oxford's research in Mathematical Physics via the popularization of\u000a      science through the writings, public lectures and media appearances of Sir\u000a      Roger Penrose. Published in 2010, Penrose's book Cycles of Time\u000a      deals directly with the research contributions and has reached broad\u000a      audiences via books, public lectures, TV appearances, and YouTube\u000a      postings. The impact has been to engage large numbers of the public with\u000a      modern theories of the origin of the universe in a mathematically\u000a      non-trival way.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of Oxford\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"1275004","Name":"Kolkata"},{"GeoNamesId":"1273294","Name":"Delhi"},{"GeoNamesId":"6093943","Name":"Ontario"}],"References":"\u000a    \u000a*[1] K. Anguige and K.P. Tod, \"Isotropic cosmological singularities. 1.\u000a      Polytropic perfect fluid space-times,'' Annals Phys., 276 (1999)\u000a      257 [gr-qc\/9903008], 29 citations. DOI: 10.1006\/aphy.1999.5946\u000a    \u000a\u000a*[2] K. Anguige and K.P. Tod,\"Isotropic cosmological singularities. 2.\u000a      The Einstein-Vlasov system,'' Annals Phys., 276, (1999) 294\u000a      [gr-qc\/9903009], 16 citations. DOI: 10.1006\/aphy.1999.5947\u000a    \u000a\u000a[3] S Hawking and R Penrose, the Nature of Space and Time, New Edition,\u000a      Princeton University Press (April 01, 2010) ISBN: 9780691145709.\u000a      Originally published in 1997.\u000a    \u000a\u000a*[4] K.P. Tod,\"Isotropic cosmological singularities in\u000a      spatially-homogeneous models with a cosmological constant,\" Class.\u000a        Quant. Grav., 24, (2007) 2415 [arXiv:0704.2506 [gr-qc]].\u000a      DOI:10.1088\/0264-9381\/24\/9\/017\u000a    \u000a\u000a[5] R.Penrose, \"Before the big bang: An outrageous new perspective and\u000a      its implications for particle physics,'' Conf. Proc. C 060626,\u000a      (2006) 2759. Journal article available at\u000a      http:\/\/accelconf.web.cern.ch\/AccelConf\/e06\/PAPERS\/THESPA01.PDF.\u000a    \u000aThe three asterisked outputs best indicate the quality of the\u000a      underpinning research and are contained in high quality internationally\u000a      refereed journals.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000a    [A] Email from Literary Consultant, The Zeno Agency, confirming sales\u000a      numbers for the books of Roger Penrose, copy held by the University of\u000a      Oxford.\u000a    [B] Email from Executive Editor, Princeton University Press, confirming\u000a      sales numbers for The Nature of Space and Time, copy held by the\u000a      University of Oxford.\u000a    [C] Reader review at http:\/\/www.amazon.co.uk\/review\/R2SS0HKLRGGNT9\/ref=cm_cr_pr_perm?ie=UTF8&amp;ASIN=0099440687&amp;linkCode=&amp;nodeID=&amp;tag=\u000a    [D] Review in Nature of Cycles of Time\u000a      http:\/\/www.nature.com\/nature\/journal\/v467\/n7319\/full\/4671034a.html.\u000a    [E] Review in The Wall Street Journal of Cycles of Time\u000a      http:\/\/online.wsj.com\/article\/SB10001424052748703730804576317072124312488.html\u000a    [F] Review in The Guardian of Cycles of Time\u000a      http:\/\/www.guardian.co.uk\/books\/2010\/oct\/16\/cycles-time-roger-penrose-review.\u000a    [G] Review in The New York Journal of Books of Cycles of Time\u000a      http:\/\/www.nyjournalofbooks.com\/review\/cycles-time-extraordinary-new-view-universe\u000a    [H] Reader review at http:\/\/www.amazon.co.uk\/review\/R1PWAJGLN1V1U6\/ref=cm_cr_pr_perm?ie=UTF8&amp;ASIN=0224080369&amp;linkCode=&amp;nodeID=&amp;tag=\u000a    [I] Viewer review at http:\/\/www.youtube.com\/watch?v=oBkOYQ02chs:\u000a    [J] Horizon, Before the Big Bang, 11 October 2010, BBC2, over 1.82\u000a      million viewers,\u000a      http:\/\/www.bbc.co.uk\/programmes\/b00vdkmj,\u000a        number of viewers found on www.barb.co.uk\u000a    [K] Email from Public Programme Manager at the Royal Institution. Contact\u000a      details held by the University of Oxford.\u000a    [L] Viewing figures for Penrose videos on youtube:\u000a      www.youtube.com\/results?search_sort=video_view_count&amp;search_query=roger+penrose\u000a    [M] Aeons Before the Big-Bang, Sir Roger Penrose Christmas Lecture,\u000a      December 2009,\u000a      http:\/\/www.youtube.com\/watch?v=OutKE3tyG94\u000a    [N] Space-Time Geometry and a New Cosmology, 6 March 2010 TEDx, Warwick\u000a      University,\u000a      http:\/\/www.youtube.com\/watch?v=oBkOYQ02chs.\u000a      TEDX organisers. Contact details held by the University of Oxford.\u000a    [O] Twistors and Quantum Non-Locality, Public Lecture and broadcast on TV\u000a      Ontario, 6 April 2011, Ontario Canada, at http:\/\/www.youtube.com\/watch?v=hAWyex1GKRU\u000a    [C]-[I] exemplify the significance of the public engagement activities;\u000a      [J]-[O] exemplify the reach.\u000a    ","Title":"\u000a    Cycles of Time; Public engagement with conformal infinity\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    In 1999, Professor Paul Tod, faculty member at Oxford University,\u000a      published [1,2] a mathematical study of Penrose's Weyl curvature\u000a      hypothesis as described in [3], giving a rigorous analytic treatment of\u000a      solutions to the partial differential equations at an initial singularity\u000a      and an optimal geometric framework for their study. Tod gave a definitive\u000a      statement of the Weyl curvature hypothesis and showed that it could be\u000a      re-framed as the condition that the conformal structure of space-time can\u000a      be smoothly continued through an initial singularity. This gives a clear\u000a      mathematical meaning to `the universe before the big bang' and underpins\u000a      Penrose's subsequent proposals for the nature of physics before the big\u000a      bang.\u000a    The singularity theorems imply that there must have been an initial\u000a      singularity, the big bang, approximately 14 billion years ago, in which\u000a      the universe that we see started essentially from a point. This is now an\u000a      integral part of the standard modern view of our universe. The existence\u000a      of the big bang invites questions such as: What controlled the big bang?\u000a      Did anything precede it? If so, what? A theme emphasized by Penrose is\u000a      that the second law of thermodynamics provides overwhelming evidence for\u000a      special initial conditions for the evolution of the universe; he proposed\u000a      the Weyl curvature hypothesis, namely that the `Weil curvature' should\u000a      vanish as the initial singularity is approached. With this assumption,\u000a      Tod's work demonstrated that it is meaningful to continue the conformal\u000a      geometry of the universe back in time to before the big bang [4].\u000a    Penrose's concept of conformal infinity gives a geometric realization to\u000a      those events that occur at infinitely late times, exploiting an analogy\u000a      with the perspective ideas of projective geometry as a surface at infinity\u000a      whose conformal structure again has a smooth continuation through\u000a      infinity. More recently, Nobel prize winning studies of observational data\u000a      showed that the universe is controlled by a positive cosmological constant\u000a      (also known as accelerated expansion or dark energy). Penrose observed\u000a      that with such a positive cosmological constant, conformal infinity has\u000a      the same conformal geometry as the big bang in Tod's work. This led him to\u000a      propose that the big bang is conformally glued to the future infinity of a\u000a      previous aeon or epoch of the universe with positive cosmological constant\u000a      [3], answering the question as to what precedes the big bang. Two\u000a      consecutive aeons are separated by infinite time and massive particles\u000a      cannot communicate from one aeon to the next. However, massless particles\u000a      such as photons or gravitational waves continue through from one aeon to\u000a      the next leading to observable consequences. The paper [5] was the first\u000a      announcement of the new conformally cyclic cosmology proposals.\u000a    Key researchers from the University of Oxford\u000a    K.P. Tod: University Lecturer at the Mathematical Institute, 1985-date.\u000a      R. Penrose: Rouse-Ball Professor at the Mathematical Institute 1973-1998,\u000a      subsequently retired but working in the Mathematical Institute as an\u000a      emeritus Professor.\u000a    "},{"CaseStudyId":"20329","Continent":[{"GeoNamesId":"6255147","Name":"Asia"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"294640","Name":"Israel"}],"Funders":[],"ImpactDetails":"\u000d\u000a    R is ubiquitous. It is found in any arena in which people analyse,\u000d\u000a      visualise or manipulate data. It\u000d\u000a      has huge economic impact, benefitting millions of users from every sector\u000d\u000a      of industry, from\u000d\u000a      pharmaceutical to finance. It has resulted in improvements in performance\u000d\u000a      in existing companies,\u000d\u000a      by providing a platform which combines state of the art data analysis\u000d\u000a      tools with high quality data\u000d\u000a      visualisation, and it has spawned new companies, most notably Revolution\u000d\u000a      Analytics, whose\u000d\u000a      purpose since 2009 has been to make impact through their REvolution\u000d\u000a      software, designed\u000d\u000a      specifically for commercial users. R is used by Government agencies,\u000d\u000a      not-for-profit organisations,\u000d\u000a      search giants like Google and even dating agencies [A].\u000d\u000a    How research underpins impact\u000d\u000a    With MASS included, the first non-beta version of R - 1.0.0 &#8212; was\u000d\u000a      released on 29th February 2000.\u000d\u000a      When Ripley joined the R project in January 1999 the code repository was\u000d\u000a      1.5Mb in size. By\u000d\u000a      January 2008, it had grown to 15Mb and in May 2013 it stood at 25Mb.\u000d\u000a      Analysis of SubVersion\u000d\u000a      commits &#8212; the measurement used to assess which users are updating\u000d\u000a      collaboratively developed\u000d\u000a      software &#8212; (http:\/\/www.ohloh.net\/p\/rproject\/commits\/summary)\u000d\u000a      shows that, from 1999 to 2013, just\u000d\u000a      over 50% per cent were by Ripley and that this rate of contribution has\u000d\u000a      been sustained over the\u000d\u000a      entire period. This phenomenal output constitutes literally millions of\u000d\u000a      lines of code.\u000d\u000a      R is a research output which is placed directly in the hands of the user.\u000d\u000a      Some of the R core team,\u000d\u000a      including Brian Ripley, run a repository (cran.r-project.org) of publicly\u000d\u000a      available extension packages\u000d\u000a      which currently contains over 4700 packages. The active community of users\u000d\u000a      and developers\u000d\u000a      enables R to constantly adapt to incorporate the very latest research in\u000d\u000a      data analysis and meet the\u000d\u000a      rapidly shifting needs of R-users.\u000d\u000a    The software, along with many original insights on its use in applied\u000d\u000a      statistics, are laid out in\u000d\u000a      Modern applied statistics with S-plus (R is an implementation of\u000d\u000a      the S programming language\u000d\u000a      combined with lexical scoping semantics). This classic book, written by\u000d\u000a      Ripley and co-author\u000d\u000a      Venables, was published by Springer in 1994 and now in its 4th\u000d\u000a      edition. It has sold over 90,000\u000d\u000a      copies. It is complemented by more recent specialised texts, often aimed\u000d\u000a      at practitioners, such as\u000d\u000a      those in the Springer series `UseR!'.\u000d\u000a    Nature and extent of impact\u000d\u000a    Since it is a free, open-source project, the full impact of R is\u000d\u000a      impossible to quantify. Nonetheless,\u000d\u000a      there is compelling evidence of extremely widespread use. Perhaps most\u000d\u000a      obviously, in 2009 it\u000d\u000a      spawned its own, refereed, online journal, called R Journal (http:\/\/journal.r-project.org\/),\u000d\u000a      which\u000d\u000a      contains both articles about the development of R itself and about its\u000d\u000a      applications. Consulting\u000d\u000a      company Revolution Analytics (formerly known Revolution Computing until\u000d\u000a      2009 when a $9m\u000d\u000a      injection of capital by Intel and North Bridge Venture Partners was\u000d\u000a      accompanied by a new strategy\u000d\u000a      aimed at R end users) undertook a survey in August 2011 which reported\u000d\u000a      over 2 million active\u000d\u000a      users of R, of whom 1.2 million were based in industry [B]. Their website\u000d\u000a      provides links to 55 Local\u000d\u000a      R User Groups, whose members meet up to brainstorm, network and listen to\u000d\u000a      guest speakers.\u000d\u000a      Groups span Australia, Asia, Europe, Israel and North and South America\u000d\u000a      [C]. Some, such as the\u000d\u000a      Chinese Financial R users group, focus on specific areas of application.\u000d\u000a    The Revolution Analytics website also provides a glimpse of the vast\u000d\u000a      range of organisations using\u000d\u000a      R and how the continually evolving R library enables them to stay at the\u000d\u000a      cutting edge of data\u000d\u000a      analysis [A]. It also links to powerful testimonies, such as that of\u000d\u000a      Antonio Possolo of the US\u000d\u000a      National Institute of Science and Technology, who was charged with making\u000d\u000a      sense of the\u000d\u000a      conflicting estimates on the rate of oil flowing from the BP Oil spill at\u000d\u000a      Deepwater Horizon; \"The\u000d\u000a        quality that you have built into R, through public open examination, is\u000d\u000a        the greatest strength and\u000d\u000a        source of confidence that I could have asked for\" [C]. Possolo's use\u000d\u000a      of R to run uncertainty\u000d\u000a      analysis and harmonize estimates was crucial to decision makers in\u000d\u000a      coordinating the scale and\u000d\u000a      scope of the response to the emergency. Revolution Analytics' more than 60\u000d\u000a      non-academic\u000d\u000a      customers include Merck, who use their software to collect and analyse\u000d\u000a      massive data sets in\u000d\u000a      clinical drug trials and Pfizer, who use it to analyze genetic data,\u000d\u000a      perform predictive modelling, and\u000d\u000a      carry out exploratory data analysis [D].\u000d\u000a    R is used in every sector of industry. As examples, the Bank of America\u000d\u000a      uses R for capital\u000d\u000a      adequacy modelling, decision systems design and predictive analytics [E];\u000d\u000a      and Google use R to\u000d\u000a      help analyse its marketing data [E]. Use of R is so widespread at Google\u000d\u000a      that it has its own R style\u000d\u000a      guide, while there is an annual meeting R\/Finance, devoted entirely to\u000d\u000a      applied finance using R [F],\u000d\u000a      and the first `R in insurance' event took place in 2013 [G].\u000d\u000a    Personal testaments received by the University of Oxford in 2013 also\u000d\u000a      confirm the ubiquity of R\u000d\u000a      (we include the maximum of 5 allowed):\u000d\u000a    \u000d\u000a      The Head of Discovery Informatics at e-Therapeutics Plc [H] states \"The\u000d\u000a          R environment is now\u000d\u000a          the de-facto standard for bioinformatics data analysis and forms a\u000d\u000a          critical part of our\u000d\u000a          computational toolbox at e-Therapeutics. The R environment allows us\u000d\u000a          to `stand on the\u000d\u000a          shoulders of giants', utilise the vast statistical and bioinformatics\u000d\u000a          knowledge embedded within\u000d\u000a          the software and concentrate on solving our specific problems rather\u000d\u000a          than reinventing the wheel\u000d\u000a          by rewriting standard data analysis algorithms. I'd estimate that this\u000d\u000a          alone saves us months of\u000d\u000a          work on any project utilising such data. [...] R is a critical\u000d\u000a          part of the computational resources\u000d\u000a          used by e-Therapeutics and personally I find it hard to believe that\u000d\u000a          the impact of R needs to be\u000d\u000a          justified.\"\u000d\u000a      The Head of Data Science R&amp;D at dunnhumby says [I] \"One tool we\u000d\u000a          are finding increasingly\u000d\u000a          useful for exploring advanced techniques, investigating new algorithms\u000d\u000a          and rapidly prototyping\u000d\u000a          new concepts, is the open source statistical language R. We have a\u000d\u000a          core research and\u000d\u000a          development team who are regular users of R\"\u000d\u000a      The HR Manager from Tessella, a consulting firm, states [J] \"In\u000d\u000a          life sciences, R is a popular tool\u000d\u000a          \/ environment with many of our clients who value and use R over and\u000d\u000a          above the traditional use\u000d\u000a          of the SAS product that has previously been the mainstay of data\u000d\u000a          analysis in clinical\u000d\u000a          development. We commonly use R on projects to support\u000d\u000a        analysis\/consultancy done for\u000d\u000a        pharmaceutical clients. Examples include analysing simulated data,\u000d\u000a          modelling of clinical trials\u000d\u000a          data and the development of causal reasoning algorithms. In these\u000d\u000a          projects, R is typically used\u000d\u000a          in the investigative development stages and sometimes pre development\u000d\u000a          of faster codes for use\u000d\u000a          in a more commercial environment. Another example would be\u000d\u000a          work we have undertaken with\u000d\u000a          a large agro chemical business on a project relating to understanding\u000d\u000a          the lifecycle of pesticides\u000d\u000a          within the ground. Tessella have developed a simple tool which\u000d\u000a          implements the FOCUS\u000d\u000a          Kinetics guidance to generate degradation kinetics. This was developed\u000d\u000a          in C# with an R back-end.\u000d\u000a          Further work in this sector involved developing a method to predict\u000d\u000a          and understand fish\u000d\u000a          survival &#8212; the development tool was R.\"\u000d\u000a      The Global Head of Risk and Performance at Investec Asset Management\u000d\u000a        writes [K] \"A number\u000d\u000a          of us at Investec Asset Management have been regular users of R for a\u000d\u000a          number of years. R was\u000d\u000a          perhaps first used seriously late 2006 when I joined the firm. Since\u000d\u000a          that time, R has been\u000d\u000a          integral to trading-model development in an initiative that I\u000d\u000a          launched. More recently, my\u000d\u000a          colleagues have used R as a building block within an equity investment\u000d\u000a          process. [...] Over the\u000d\u000a          years, R has become our standard environment for\u000d\u000a          statistical\/mathematical problem solving.\u000d\u000a        [...] for rolling out pre-built solutions for non-mathematicians the\u000d\u000a          ability to prototype solutions in\u000d\u000a          R prior to programming in a compiled language has been invaluable.\u000d\u000a        [...] representatives from\u000d\u000a          Investec Asset Management have attended R\/Rmetrics, R\/Finance\u000d\u000a          conferences and regular\u000d\u000a          local meetings such as LondonR.\"\u000a\u000d\u000a      The Chief Analyst at the Implementation Unit at the Cabinet Office\u000d\u000a        says [L] \"the following\u000d\u000a          Government Departments and Agencies have told me that they use R:\u000d\u000a          Department of Business,\u000d\u000a          Innovation and Skills; Department for Communities and Local\u000d\u000a          Government; Department for\u000d\u000a          Environment, Food and Rural Affairs; Animal Health and Veterinary\u000d\u000a          Laboratories Agency;\u000d\u000a          Environment Agency; Food and Environment Research Agency; Food\u000d\u000a          Standards Agency;\u000d\u000a          Forestry Commission; Ministry of Justice; Marine Management\u000d\u000a          Organisation; Natural England;\u000d\u000a          Office for National Statistics. I can confirm, as TfL's former\u000d\u000a          Director of Policy Analysis, that\u000d\u000a          Transport for London have also used R.\"\u000d\u000a    \u000d\u000a    ","ImpactSummary":"\u000d\u000a    R is a free and open-source software programming language and software\u000d\u000a      environment for\u000d\u000a      expressing and implementing statistical algorithms and graphics. It has\u000d\u000a      become the lingua franca\u000d\u000a      for developing and implementing new statistical methodologies &#8212; not just\u000d\u000a      in statistics, but in\u000d\u000a      applications across the whole spectrum of industry, from marketing and\u000d\u000a      pharmaceuticals to\u000d\u000a      finance. It is used by companies for research, analysis and production.\u000d\u000a      Its power in analysing and\u000d\u000a      visualising data helps organisations from charities to government. About\u000d\u000a      one half of the core\u000d\u000a      statistical modelling and graphics engine included in R builds on research\u000d\u000a      carried out in Oxford.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Oxford\u000d\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a* [1] Ripley, B. D. (1994) Neural networks and related methods for\u000d\u000a      classification (with\u000d\u000a      discussion). J. Roy. Statist. Soc. B 56, 409-456\u000d\u000a    \u000a\u000a* [2] Ripley, Brian D. (1996) Pattern Recognition and Neural Networks,\u000d\u000a      Cambridge University\u000d\u000a      Press\u000d\u000a    \u000a[3] MASS http:\/\/cran.r-project.org\/web\/packages\/MASS\/MASS.pdf\u000d\u000a    \u000a[4] Brian D. Ripley. Time series in R 1.5.0. R News 2(2):2-7 June\u000d\u000a      2002.\u000d\u000a    \u000a* [5] The R Project for Statistical Computing http:\/\/www.r-project.org\/\u000d\u000a    The three asterisked outputs best indicate the quality of the\u000d\u000a      underpinning research. [1] is in a high\u000d\u000a      quality internationally refereed journal, [2] is a key book and [5] is R\u000d\u000a      itself.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"},{"Level1":"8","Level2":"3","Subject":"Computer Software"}],"Sources":"\u000d\u000a    [A] Revolution Analytics' list of Companies Using R, demonstrating reach\u000d\u000a      of the impact\u000d\u000a      http:\/\/www.revolutionanalytics.com\/what-is-open-source-r\/companies-using-r.php\u000d\u000a    [B] Revolution Analytics user statistics, demonstrating reach of the\u000d\u000a      impact\u000d\u000a      http:\/\/prezi.com\/s1qrgfm9ko4i\/the-r-ecosystem\u000d\u000a    [C] Revolution Analytics Blog, demonstrating the reach of the impact:\u000d\u000a      user groups: http:\/\/blog.revolutionanalytics.com\/local-r-groups.html\u000d\u000a      Deepwater: http:\/\/blog.revolutionanalytics.com\/2010\/08\/rs-role-in-the-national-response-to-\u000d\u000a        the-bp-oil-spill.html\u000d\u000a    [D] Use of R at Merck and Pfizer: http:\/\/www.revolutionanalytics.com\/aboutus\/our-customers.php\u000d\u000a    [E] R is Hot: Part 1 http:\/\/www.r-bloggers.com\/r-is-hot-part-1\/\u000d\u000a    [F] R\/Finance http:\/\/www.rinfinance.com\/\u000d\u000a    [G] \"There is an R in Lloyds\", Head of Exposure Management and\u000d\u000a      Reinsurance, Lloyd's, R in\u000d\u000a      Insurance Conference, London, 15 July 2013 http:\/\/lamages.blogspot.co.uk\/p\/the-first-\u000d\u000a        conference-on-r-in-insurance.html?goback=.gde_4180165_member_197992140#!\u000d\u000a    [H] Letter from Head of Discovery Informatics at e-Therapeutics Plc. Copy\u000d\u000a      held by University of\u000d\u000a      Oxford\u000d\u000a    [I] Letter from Head of Data Science R&amp;D at dunnhumby. Copy held by\u000d\u000a      University of Oxford\u000d\u000a    [J] Letter from the HR Manager from Tessella. Copy held by University of\u000d\u000a      Oxford\u000d\u000a    [K] Letter from the Global Head of Risk and Performance at Investec Asset\u000d\u000a      Management. Copy\u000d\u000a      held by University of Oxford\u000d\u000a    [L] Letter from the Chief Analyst at the Implementation Unit at the\u000d\u000a      Cabinet Office. Copy held by\u000d\u000a      University of Oxford\u000d\u000a    [D]- [L] exemplify the reach and demonstrate the significance and\u000d\u000a      ubiquity of R.\u000d\u000a    ","Title":"\u000d\u000a    R, a free software environment for statistical computing and graphics\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    R is a collaborative research project, run by the R Development Core Team\u000d\u000a      (20 people in 10\u000d\u000a      countries), which has been running in its current form since 1997. The\u000d\u000a      output is a freely available\u000d\u000a      software programming language and environment for statistical computing\u000d\u000a      and graphics, which\u000d\u000a      runs on all major computer platforms, including UNIX, Windows and Mac OS.\u000d\u000a      It has a core set of\u000d\u000a      libraries to which users can add packages. The central purpose of R is to\u000d\u000a      build on mathematical\u000d\u000a      and statistical research and to provide a toolbox of software that others\u000d\u000a      can use.\u000d\u000a    In 1994, Brian Ripley, Professor of Applied Statistics at the University\u000d\u000a      of Oxford (1990-present),\u000d\u000a      published a seminal paper, [1], which sets up a general statistical\u000d\u000a      framework for classification.\u000d\u000a      Within this framework, data analysis methods from the Statistics, Pattern\u000d\u000a      recognition and Machine\u000d\u000a      Learning literatures can be compared. The paper showed how developments in\u000d\u000a      Pattern\u000d\u000a      Recognition could be identified with statistical ideas, to the benefit of\u000d\u000a      both fields. For example, it\u000d\u000a      introduced neural networks to the field of statistics as a classification\u000d\u000a      tool, provided a means of\u000d\u000a      fitting neural networks to statistical problems, and identified neural\u000d\u000a      networks as a form of robust\u000d\u000a      regression analysis. Ripley joined the R project in January 1999, and he\u000d\u000a      implemented these newly\u000d\u000a      introduced classification techniques, and others developed in his book\u000d\u000a      [2], in R as a package called\u000d\u000a      MASS [3]. A major research output in its own right, MASS was first\u000d\u000a      released as part of R in 1999,\u000d\u000a      and now forms an important part of the core statistical modelling and\u000d\u000a      graphics engine, delivering\u000d\u000a      many of the analysis and modelling tools used on a day-to-day basis by\u000d\u000a      R-users.\u000d\u000a    Other substantial elements of the R-core package also result from\u000d\u000a      Ripley's research. For example,\u000d\u000a      the new methods in computational statistics which underlie the core time\u000d\u000a      series package, in\u000d\u000a      particular the development of methods for handling missing data and the\u000d\u000a      identification of\u000d\u000a      appropriate optimization methods, were first described in [4] and\u000d\u000a      implemented by Ripley in a 2002\u000d\u000a      R-release.\u000d\u000a    At its core, R is itself a very substantial research output, [5],\u000d\u000a      underpinned by fundamental statistical\u000d\u000a      research such as [1], [2].\u000d\u000a    "},{"CaseStudyId":"20416","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000d\u000a    The impact of the research falls into the category of economic benefit\u000d\u000a      which we illustrate through the benefits realised by UCB Pharma. Other\u000d\u000a      pharmaceutical companies such as Crysalin Ltd and InhibOx have also\u000d\u000a      benefited. The research also has downstream impact on patient health.\u000d\u000a    Pathway to impact\u000d\u000a      pyFREAD has been made accessible through three different routes:\u000d\u000a    \u000d\u000a      the direct implementation in 2010 of the software by UCB Pharma, who\u000d\u000a        were industrial partners in the research project. The Director of\u000d\u000a        Computational Structural Biology at UCB Pharma states [A] \"... Professor\u000a          Deane kindly provided to us at no charge the software pyFREAD, which\u000d\u000a          was developed in her laboratory\"\u000d\u000a      a web-based computational tool (http:\/\/opig.stats.ox.ac.uk\/sites\/fread\/).\u000a        The Head of Computational Chemistry, Crystalin Ltd, states in a letter\u000d\u000a        in October 2013 [E] \"I downloaded it from your website ... last\u000d\u000a          August\".\u000d\u000a      a freely downloadable version of the FREAD software, published in\u000d\u000a        2010,\u000d\u000a        (http:\/\/opig.stats.ox.ac.uk\/webapps\/fread\/php\/)\u000d\u000a    \u000d\u000a    Nature and extend of the impact\u000d\u000a      The impact of pyFREAD is most readily measured through its use by UCB\u000d\u000a      Pharma. They, as well as other major pharmaceutical companies, use X-ray\u000d\u000a      crystallography and molecular dynamics simulations to guide `lead\u000d\u000a      optimization' in an iterative fashion. Atomic interactions between each\u000d\u000a      compound and the target protein are analyzed and chemical modifications to\u000d\u000a      the lead compound are designed accordingly to improve potency and\u000d\u000a      selectivity. However, X-ray structures of the compound-protein complex\u000d\u000a      often have undefined residues due to experimental limitations; such\u000d\u000a      residues must be modelled before dynamic simulations can be carried out.\u000d\u000a    The Director of Computational Structural Biology at UCB Pharma states [A]\u000d\u000a      \"We used to rely on the software Prime from Schrodinger Inc to model\u000d\u000a        the undefined residues in X-ray structures. [...] This software\u000d\u000a      [pyFREAD] was at least 1000 times faster than Prime and also produced\u000d\u000a        more accurate results! For example, to model a stretch of 14 undefined\u000d\u000a        residues, it took 50 CPU hours with Prime but less than a minute with\u000d\u000a        pyFREAD. In another occasion, we tried to reconstruct 7 stretches of\u000d\u000a        undefined residues within the same X-ray structure; Prime crashed after\u000d\u000a        running for 3 days without producing any useable results, while pyFREAD\u000d\u000a        managed to generate accurate models in merely 3 minutes. Pleasantly\u000d\u000a        surprised by the lightning speed and accuracy of this software, we\u000d\u000a        immediately switched to using pyFREAD for such tasks and have not been\u000d\u000a        disappointed.\"\u000d\u000a    He further summarises the immediate financial benefits [A] \"It is\u000d\u000a        immediately clear that pyFREAD saves us not only &#163;45,000 in annual\u000d\u000a        license fee for Prime, but also thousands of CPU hours for each lead\u000d\u000a        optimization campaign.\"\u000d\u000a    UCB Pharma operates in 40 countries worldwide and had a global revenue of\u000d\u000a      &#8364;3.4 billion in 2012. They identify (in [A]) that lead optimization is one\u000d\u000a      of the most costly steps in drug discovery and development, requiring on\u000d\u000a      average &#163;6 million per campaign (just one stage of the drug discovery\u000d\u000a      process). In order to bring an approved drug onto the market, typically 15\u000d\u000a      lead optimization campaigns have to be carried out, costing a total of &#163;90\u000d\u000a      million. The Director of Computational Structural Biology at UCB writes in\u000d\u000a      [A] \"... that switching to pyFREAD shortened each lead optimization\u000d\u000a        cycle by an average of 3 days. A typical lead optimization campaign\u000d\u000a        lasts 2 years, the cost is &#163;3 million per year, or &#163;58,000 per week.\u000d\u000a        Shortening each optimization cycle by 3 days translates to &#163;35,000 in\u000d\u000a        direct savings per iteration, or &#163;350,000 for a 10-iteration campaign.\u000d\u000a        As 15 campaigns are needed to put one drug onto the market [6], the\u000d\u000a        total savings per drug approval achieved by using pyFREAD is expected to\u000d\u000a        be over &#163;5 million.\" This corresponds to saving UCB at least 5% for\u000d\u000a      each programme in which it has been used.\u000d\u000a    The impact for UCB of this research goes beyond the financial savings.\u000d\u000a      UCB state that [A] \"The research work at Professor Deane's laboratory\u000d\u000a        has generated significant economic value for UCB Pharma through the\u000d\u000a        acceleration of the drug discovery process. More importantly, faster\u000d\u000a        drug discovery means that patients receive better treatment sooner.\u000d\u000a        While the impact on patients' quality of life is hard to quantify, it is\u000d\u000a        what matters most\".\u000d\u000a    Evidence of wider, less quantifiable, impact of the FREAD methodology is\u000d\u000a      seen from its web-based computational version. The data available for 2013\u000d\u000a      show this has performed on average over 60 predictions per month and was\u000d\u000a      visited by over 200 unique users per month from throughout the world [B].\u000d\u000a      As exemplars of the reach of FREAD, the Chief Executive Officer of\u000d\u000a      InhibOx, who have been using the software since 2011, states [C] \"I am\u000d\u000a        writing to confirm that InhibOx has used the program FREAD to support\u000d\u000a        our drug design work. The program was used primarily by one of our staff\u000d\u000a        to assist in a project to build homology models of a malaria parasite\u000d\u000a        serine protease, which we have been working on as a novel anti-malarial\u000d\u000a        drug target\", while the Head of Computational Chemistry at Crysalin\u000d\u000a      Ltd says [D] \"Thank you for permitting me to use FREAD at Crysalin....\u000d\u000a      [it] is providing to be an invaluable addition to our existing tools.\"\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Novel rapid methods for predicting protein structure, particularly\u000d\u000a      functional loop structures, have been developed by researchers at the\u000d\u000a      University of Oxford. These have been made accessible to a large audience\u000d\u000a      through a suite of computational tools. The methods have had general\u000d\u000a      impact through download and online access and specific impact through\u000d\u000a      extensive use within UCB Pharma. The tools are much faster than other\u000d\u000a      methods, creating equal or better predictions in approximately a\u000d\u000a      thousandth of the time. Commonly exploited by UCB Pharma in their drug\u000d\u000a      discovery pipeline, they have cut computational cost, but, more\u000d\u000a      importantly, they have greatly reduced the time for process improvements.\u000d\u000a      UCB Pharma estimate that the tool pyFREAD alone saves over &#163;5 million in\u000d\u000a      the discovery costs for a single drug molecule. FREAD (a version of\u000d\u000a      pyFREAD coded in C) is also being used more widely, for example by\u000d\u000a      Crysalin Ltd and InhibOx.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Oxford\u000d\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a* [1] Choi Y, Deane CM, FREAD revisited: Accurate loop structure\u000d\u000a      prediction using a database search algorithm, Proteins, 2010,\u000d\u000a      78(6), 1431-40. DOI: 10.1002\/prot.22658.\u000d\u000a    \u000a\u000a* [2] Sebastian Kelm, Anna Vangone, Yoonjoo Choi, Jean-Paul Ebejer, Jiye\u000d\u000a      Shi, Charlotte M. Deane, Fragment-based modelling of membrane protein\u000d\u000a      loops &#8212; successes, failures and prospects for the future, Proteins,\u000d\u000a      2013. DOI: 10.1002\/prot.24299.\u000d\u000a    \u000aThe two asterisked outputs best indicate the quality of the underpinning\u000d\u000a      research. Proteins is a international refereed journal.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"3","Level2":"7","Subject":"Theoretical and Computational Chemistry"},{"Level1":"11","Level2":"9","Subject":"Neurosciences"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    [A] Letter from the Director of Computational Structural Biology UCB\u000d\u000a      Pharma, describing their use of pyFREAD and the significance of the\u000d\u000a      impact. Copy held by the University of Oxford.\u000d\u000a    [B] Data from webserver and software pages for pyFREAD:\u000d\u000a      http:\/\/opig.stats.ox.ac.uk\/webapps\/fread\/php\/\u000d\u000a      http:\/\/opig.stats.ox.ac.uk\/sites\/fread\/\u000d\u000a      Copy held by University of Oxford\u000d\u000a    [C] Letter from the Chief Executive Officer, InhibOx, describing their\u000d\u000a      use of FREAD. Copy held by University of Oxford.\u000d\u000a    [D] Letter from the Head of Computational Chemistry, Crysalin, describing\u000d\u000a      their use of FREAD. Copy held by University of Oxford.\u000d\u000a    [B], [C] and [D] provide examples of the reach of the impact of FREAD and\u000d\u000a      pyFREAD. \u000d\u000a    ","Title":"\u000d\u000a    Exploitation of rapid protein structure prediction tools\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2640729","Name":"Oxford"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Proteins perform crucial functions in all biological processes and in\u000d\u000a      general their function is specified by their three-dimensional structure.\u000d\u000a      Understanding a protein's structure is an essential step in the drug\u000d\u000a      discovery process and most pharmaceutical companies make extensive use of\u000d\u000a      structural information. Experimental methods can provide some information\u000d\u000a      but are not always applicable or able to give complete structural\u000d\u000a      information. One of the areas where experimental methods most commonly\u000d\u000a      fail is in the loop-structures of proteins. It is these loops which tend\u000d\u000a      to determine the function of proteins. Thus computational methods for loop\u000d\u000a      prediction offer a powerful addition to the experimentally available data.\u000d\u000a    Loop prediction has been approached in two ways, ab initio and database\u000d\u000a      search. In recent years it had been thought that ab initio methods were\u000d\u000a      more powerful. Researchers at the University of Oxford, led by Professor\u000d\u000a      Charlotte Deane, identified that database methods had been underestimated\u000d\u000a      and found that sequence similarity, as quantified by environment-specific\u000d\u000a      substitution scores, can be used to significantly improve prediction [1].\u000d\u000a      The research gives a method for predicting loop-structures and for\u000d\u000a      calculating any missing structural data.\u000d\u000a    In 2010, Deane and her team at the University of Oxford rewrote the\u000d\u000a      computer program FREAD (and wrote pyFREAD) to incorporate a completely new\u000d\u000a      scoring system which, combined with bigger databases of protein structures\u000d\u000a      and faster computers, resulted in a significant improvement in the ability\u000d\u000a      to predict the protein structure. These improvements in prediction and\u000d\u000a      speed are reported in [1], see for example Figure 1.\u000d\u000a    pyFREAD has been demonstrated to show higher accuracy than comparable\u000d\u000a      tools such as the loop refinement modules in the commercially available\u000d\u000a      Prime or MODELLER packages and to be significantly faster than its\u000d\u000a      competitors. Professor Deane and collaborators at the company UCB Pharma\u000d\u000a      realised that the pyFREAD methodology was also applicable to the much more\u000d\u000a      general problem of model completion and, given its speed and accuracy, it\u000d\u000a      could also be used when multiple segments of data were missing. A specific\u000d\u000a      version of the method was then developed within pyFREAD in order to\u000d\u000a      consider this more general problem and to address specific issues of\u000d\u000a      interest to UCB Pharma. This second phase was completed in September 2012\u000d\u000a      and written up in [2], which includes predictions applied to membrane\u000d\u000a      proteins. The significant speeding up of the algorithms has been shown, as\u000d\u000a      well as the generalisation of the method to allow any fragment of the\u000d\u000a      protein, not just loop structures, to be considered. Paper [2] also\u000d\u000a      demonstrates that results of an even higher quality are obtained by using\u000d\u000a      just the databases most appropriate to a given protein.\u000d\u000a    The key researcher, Prof Charlotte Deane has been a University Lecturer\u000d\u000a      since joining Oxford in 2002.\u000d\u000a    \u000d\u000a    Figure 1. The predictive power of FREAD &#8212; An example prediction. The\u000d\u000a      black structure is the correct answer structure. The grey loop is the top\u000d\u000a      prediction without use of the environment specific substitution score. The\u000d\u000a      white loop is the top prediction by FREAD using the environment specific\u000d\u000a      substitution score. Reproduced from reference [1].\u000d\u000a    \u000d\u000a    "},{"CaseStudyId":"20446","Continent":[{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"3595528","Name":"Guatemala"},{"GeoNamesId":"3595528","Name":"Guatemala"}],"Funders":["Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000a    Published in 2008, Finding Moonshine created a channel via which\u000a      du Sautoy's research has had huge impact on society, stimulating interest\u000a      in, and engagement with, mathematics. The ultimate beneficiaries have been\u000a      millions of members of the general public whose mathematical curiosity has\u000a      been awoken.\u000a    Finding Moonshine provides the general public with a unique\u000a      insight into mathematical research. As the story of the classification of\u000a      finite simple groups unfolds, du Sautoy charts the progress of his own\u000a      research, month by month, during the year following his 40th birthday. Its\u000a      publication cemented Marcus du Sautoy's position as an outstanding\u000a      expositor of mathematics and provided a platform from which he expanded\u000a      his public engagement activities. Millions of people have subsequently\u000a      watched his TV shows, listened to his radio programmes, played his Flash\u000a      games and interacted through Facebook. He has been able to use his\u000a      position as a leading mathematician and public figure to engage with\u000a      Government. Throughout he has maintained close contact with his audiences\u000a      and developed new ideas through more intimate activities such as `Maths in\u000a      the City' and `Pi Day'.\u000a    How research underpins impact\u000a      Marcus du Sautoy's second popular book, Finding Moonshine: A\u000a        Mathematician's Journey into Symmetry was published in 2008.\u000a      Although primarily concerned with the story of the classification of the\u000a      finite simple groups, du Sautoy's own research is central to the\u000a      narrative. The story begins in 2005, on his 40th birthday, and uses the\u000a      literary device of being divided into twelve chapters, one for each month\u000a      of the year. Amid many diversions into the (often colourful) lives of\u000a      famous mathematicians and stories of major mathematical discoveries, it\u000a      has two narrative strands. The first is the story of the classification of\u000a      the finite simple groups. The second is the progress, month by month, of\u000a      du Sautoy's own research. As part of this second strand, he describes the\u000a      problems that he is working on and something of the process of solving,\u000a      or, in the case of the problem that he first describes, not solving, them.\u000a      He describes Higman's PORC conjecture and his growing scepticism about its\u000a      validity. We also see his programme develop: he and a research student\u000a      demolish one of his conjectures by exhibiting a group for which the\u000a      corresponding zeta function does not satisfy a functional equation &#8212; a\u000a      concept elegantly explained in terms of palindromic symmetry &#8212; as the\u000a      conjecture predicts, and so the question is refined. His research, and the\u000a      process of research, are thus brought directly to the reader. Crucially,\u000a      the book demonstrated that, rather than cherry-picking pieces of\u000a      mathematics that are easy to explain, du Sautoy is able to explain the\u000a      mathematical concepts that lie at the heart of his research in a way that\u000a      is accessible and engaging. The next step was to provide his audiences\u000a      with the opportunity to go beyond the passive process of reading and to\u000a      actually participate in the mathematics. This required multiple media, but\u000a      combining his abilities as a broadcaster with his deep knowledge and\u000a      boundless curiosity as a researcher he developed ideas based on these same\u000a      mathematical concepts into TV shows with accompanying web-based Flash\u000a      games.\u000a    Nature and extent of impact.\u000a      The impact of Finding Moonshine has been both direct and indirect.\u000a      It has been translated into ten languages and has sold over 62,311 copies\u000a      worldwide [A]. The Fields medallist Professor Sir Timothy Gowers wrote in\u000a      his review for the Times Higher Educational supplement [B]: \"If\u000a        you are genuinely curious to know what it is like to be a mathematician,\u000a        there is now a better way of satisfying your curiosity: read Finding\u000a        Moonshine.\" The publication of the book gave du Sautoy the platform\u000a      to promote and explain the importance of group theory to a wide audience,\u000a      with diverse mathematical backgrounds, as evidenced by many of the\u000a      comments from readers left on du Sautoy's blog [C]:\u000a    \u000a      \u000a\"I'm a former student of maths. I abandoned my studies 20 years\u000a          ago. Recently, my wife gave me your book about symmetry as a present.\u000a          Now I am enjoying the reading greatly. You awakened in me a kind of\u000a          longing for maths. A math worm that was sleeping for years. [...]\u000a          I just wanted to thank you for making Mathematics so appealing.\"\u000a      \u000aJust finished this book. It really is a very good read even for\u000a          those who have a little understanding of maths. Considering I only\u000a          have a GCSE I didn't find it too bewildering.\"\u000a      \u000a\"I just finished reading Finding Moonshine in the dead of night\u000a          last night :) I so enjoyed it [...] Now I see symmetry\u000a          everywhere &#8212; what a joy!\"\u000a\u000a    \u000a    TV and Radio. With the success of Finding Moonshine, du\u000a      Sautoy found himself in increasing demand as a broadcaster. A string of\u000a      appearances on TV and radio, in which he explained the importance, and\u000a      excitement, of mathematics followed, including:\u000a    \u000a      The Story of Maths (4 part series on BBC4, Autumn 2008): Episode 1,\u000a        0.58 million viewers;\u000a      Horizon: How Long is a Piece of String &#8212; Presented by comedian Alan\u000a        Davies and featuring Marcus du Sautoy on BBC2 on 17th November 2009:\u000a        2.11 million viewers;\u000a      Horizon: What Makes a Genius &#8212; on BBC2 on 17th February 2010: 1.76\u000a        million viewers;\u000a      A Brief History of Mathematics &#8212; a ten part series profiling famous\u000a        mathematicians on BBC Radio 4 in June 2010.\u000a    \u000a    (TV viewing figures here and below are taken from www.barb.co.uk\/viewing\/weekly-top-30.)\u000a    One of Horizon's viewers commented on du Sautoy's blog [C]: \"Your\u000a        enthusiasm is infectious. I repeated your explanation of Euclid's Prime\u000a        Number Proof to my 9 and 12 year olds and they were gripped. I didn't\u000a        understand about mathematical proofs until after I left school! Thanks\u000a        for a great piece of work.\"\u000a    With such clear evidence of his appeal as a broadcaster and of his\u000a      ability to present complex mathematical concepts, right up to the cutting\u000a      edge of research, in a way that was accessible to audiences from very\u000a      mixed mathematical backgrounds, he next went beyond the traditional format\u000a      of these programmes to develop new shows, supported by materials that\u000a      exploit a range of electronic media, which appeal to audiences with a much\u000a      wider demographic profile. Importantly, the public actively participate in\u000a      the mathematics. Building on the success of Finding Moonshine, the\u000a      material of these shows is directly based on mathematical concepts,\u000a      especially the prime numbers and symmetry that lie at the heart of du\u000a      Sautoy's research.\u000a    The first such show was The Code, a three part documentary series\u000a      on BBC2 which started on 27 July 2011. Viewing figures for the three\u000a      episodes were 1.75 million, 1.53 million and 1.33 million. Viewers of The\u000a        Code were invited to participate in an interactive mathematical\u000a      treasure hunt based on the academic ideas presented in the programmes. The\u000a      treasure hunt involved four web-based Flash games, produced by the company\u000a      Six to Start, who state [D] \"all of the games are tied directly\u000a        into concepts shown in the show\", focusing on mathematical concepts\u000a      such as symmetry and prime numbers, hunting for clues in the programmes,\u000a      solving riddles, finding examples of prime numbers in everyday life and\u000a      completing a complex mathematical quiz book. They add that over 1 million\u000a      people played the Flash games, playing for an average of 24 minutes, and\u000a      more than 100,000 took part in the treasure hunt challenge. The level of\u000a      engagement by viewers was staggering, as all of the elements of the\u000a      treasure hunt involved dedication and complex mathematical concepts.\u000a      According to the Six to Start Executive, reported in an external\u000a      blog [E], there are \"300,000+ interactions on the Facebook fan page,\u000a        1000+ photos, videos, 3D models, and a wiki with 100,000 views and 2000+\u000a        edits &#8212; all created by users\". An appraisal of the treasure hunt on\u000a      the same blog emphasises the significance of the engagement: \"The whole\u000a        thing was really involved and quite complex, not to mention on a\u000a        traditionally unpopular subject, maths, so over 100k players for the\u000a        meta-puzzle is impressive [...] it does seem like quite an\u000a        achievement.\"\u000a    The opportunity to engage a quite different kind of audience arrived when\u000a      du Sautoy was chosen as co-presenter of the comedy-maths show, Dara &#211;\u000a        Briain's School of Hard Sums. First aired on Dave on 16th\u000a      April 2012, it attracted 1.7 million viewers. According to the Executive\u000a      Producer of the show [F] \"[it is] the most watched completely new\u000a        title in the history of the channel\" with \"more than double the\u000a        normal ratings in its slot with 555,000 viewers for episode one\".\u000a      She adds that proof that viewers have engaged with the maths that du\u000a      Sautoy introduces on the show is that \"the homework questions were\u000a        viewed a staggering 442,124 times! The title accounted for almost a\u000a        quarter of all page views when it was on air (2,030,796 total page\u000a        views) and has recently been re-commissioned by Dave\".\u000a    Recognition of the huge educational potential that du Sautoy had unlocked\u000a      came in a speech [G] of Michael Gove, Secretary of State for Education: \"Computer\u000a        games developed by Marcus Du Sautoy are enabling children to engage with\u000a        complex mathematical problems that would hitherto have been thought too\u000a        advanced. I am sure that this field of educational games has huge\u000a        potential for maths and science teaching\".\u000a    Maths in the City and Pi day. Public engagement activities must be\u000a      continually refreshed if they are to continue to have impact. Recognising\u000a      the importance of a two-way dialogue, and also the value of smaller scale,\u000a      more intimate, projects than his TV shows, in 2010 du Sautoy began working\u000a      with a team from the University of Oxford's Department of Continuing\u000a      Education to create a way to reach out to the public with maths, ensure\u000a      engagement with the subject and open a two-way dialogue with participants.\u000a      The result was the Maths in the City project: a website with a user forum\u000a      heavily involving social media, and walking tours around Oxford and London\u000a      pointing out examples of maths in everyday life. Just as when he describes\u000a      outings with his son in Finding Moonshine, symmetry, inevitably,\u000a      plays a central role. Du Sautoy helped to design the free tours and\u000a      materials and led some of them before handing over to students from the\u000a      Oxford Mathematical Institute, whom he had trained. Funded by the EPSRC,\u000a      the tours, which were attended by adults and school groups from as young\u000a      as Key Stage 3, were a huge success with over 2500 people engaging via\u000a      social media, over 460 people attending the tours and over 130 examples of\u000a      `maths in the city' from around the world posted on the website. The Maths\u000a      in the City project has been so successful, that the same format is being\u000a      used in the training of mathematics teachers in China.\u000a    Du Sautoy has continued to work with the University of Oxford's\u000a      Department of Continuing Education to create events that not only broaden\u000a      the knowledge of the participants, but also their approach to maths.\u000a      Through the use of online communication, these events allow participants\u000a      to engage both during and after the live event. The first event, Pi day,\u000a      which took place in March 2013, made innovative use of social media in\u000a      arranging for participants to `rediscover Pi' through experiments.\u000a      According to http:\/\/oxfordconnect.conted.ox.ac.uk,\u000a      around 2000 participants from 17 different countries took part.\u000a    Sale of Groups and Common Hope. In a completely different\u000a      direction, in October 2012, du Sautoy offered opportunities to name\u000a      nilpotent groups appearing in his research in recognition of contributions\u000a      to charity. Through this direct application of his research, du Sautoy\u000a      sponsors the Common Hope Charity that supports orphanages in Guatemala.\u000a      Amid widespread, and very positive, publicity for mathematics, such as the\u000a      article [H] in The Times, he also offered one of the groups to\u000a      Queen Elizabeth II in honour of her Diamond Jubilee.\u000a    ","ImpactSummary":"\u000a    By using the progress of his own research over the course of a year as a\u000a      major narrative theme, in Finding Moonshine Marcus du Sautoy\u000a      provides the public with unique insight into the content and nature of his\u000a      mathematical research programme. The success of the book, published in\u000a      2008, in conveying the essence of cutting edge research, in elementary\u000a      terms, attracted the attention of broadcasters and policymakers and\u000a      provided a platform from which du Sautoy has been able to expand his\u000a      public engagement activities to reach millions of people through TV,\u000a      radio, public lectures, social media and interactive projects. His three\u000a      part documentary The Code stimulated over a million viewers to\u000a      play Flash games based directly on mathematical concepts. The phenomenal\u000a      success of his unique brand of engagement in awakening an interest in\u000a      mathematics, in both young and old, has had a great impact on society.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of Oxford\u000a    ","Institutions":[{"AlternativeName":"Oxford (University of)","InstitutionName":"University of Oxford","PeerGroup":"A","Region":"South East","UKPRN":10007774}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a*[1] M du Sautoy and F Grunewald, Zeta functions of groups and rings. International\u000a        Congress of Mathematicians Vol II, 131-149. Eur. Math. Soc., Z&#252;rich,\u000a      2006\u000a      http:\/\/www.icm2006.org\/proceedings\/Vol_II\/contents\/ICM_Vol_2_07.pdf\u000a    \u000a\u000a*[2] M du Sautoy and L Woodward, Zeta functions of groups and rings.\u000a      Lecture Notes in Mathematics Vol 1925, 2008. ISBN: 978-3-540-74701-7\u000a    \u000aThe asterisked outputs best indicate the quality of the underpinning\u000a      research. [1] resulted from du Sautoy's invited lecture at the ICM in 2006\u000a      and [2] is a research monograph.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000a    [A] Email from executive from Greene &amp; Heaton, confirming sales\u000a      details for Finding Moonshine. Copy held by the University of\u000a      Oxford.\u000a    [B] Review by Timothy Gowers of Finding Moonshine for the Times\u000a      Higher Education supplement: \u000ahttp:\/\/www.timeshighereducation.co.uk\/story.asp?storycode=400699.\u000a    [C] Comments from the Finding Moonshine blog, indicating the reach of the\u000a      impact of du Sautoy's work: http:\/\/findingmoonshine.blogspot.co.uk\/.\u000a    [D] Email from Six to Start Executive (the company that produced the\u000a      Flash games and treasure hunt), describing audience engagement. Copy held\u000a      by University of Oxford.\u000a    [E] View of The Code treasure hunt: http:\/\/marthasadie.wordpress.com\/2012\/03\/21\/notes-from-sxswi-adrian-hon-creating-the-code-a-bbc-transmedia-documentary-thecode\/.\u000a    [F] Email from Executive Producer, The School of Hard Sums. Copy held by\u000a      University of Oxford.\u000a    [G] Speech by Michael Gove, 29 June 2011 at the Royal Society,\u000a      demonstrating the impact on school children: http:\/\/www.education.gov.uk\/inthenews\/speeches\/a00191729\/michael-gove-speaks-to-the-royal-society-on-maths-and-science.\u000a    [H] Times article on the gift of a group to Her Majesty the Queen,\u000a      http:\/\/www.thetimes.co.uk\/tto\/science\/article3566284.ece.\u000a    \u000a    ","Title":"\u000a    Finding Moonshine: engaging the public through mathematical research\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Marcus du Sautoy has been at the University of Oxford since 2001, first\u000a      as a Royal Society University Research Fellow (until 2005), then as\u000a      Professor of Mathematics (2005-8) and, since 2008, as the Charles Simonyi\u000a      Chair for the Public Understanding of Science. In the period 2005-6, du\u000a      Sautoy's research programme, centred on zeta functions and Higman's PORC\u000a      conjecture.\u000a    The zeta function of a group G is defined as the sum of the\u000a    Dirichlet series whose nth term is ann-s\u000a    where an is the number of subgroups of index n\u000a    in G. Such zeta functions have an Euler product decomposition in\u000a    which the p-local factor is the zeta function enumerating subgroups\u000a    of p-power index in G. Nilpotent groups are natural objects\u000a    of study in this context because of their polynomial subgroup growth. Du\u000a    Sautoy's work with Grunewald (Heinrich-Heine-Universit&#228;t D&#252;sseldorf) shifted\u000a    the paradigm of comparison from analytic number theory to algebraic\u000a    geometry, specifically zeta functions associated to algebraic varieties. The\u000a    mathematical significance of du Sautoy's programme of work with Grunewald\u000a    was recognized by his invitation to speak at the International Congress of\u000a    Mathematicians in Z&#252;rich in 2006. The corresponding paper, [1], reports on\u000a    \"progress and problems concerning the analytical behaviour of the zeta\u000a    functions of groups and rings\" and provides a technical mathematical\u000a    presentation of this strand of the research described in Finding\u000a      Moonshine.\u000a    Another strand of du Sautoy's research programme during 2005-6 concerned\u000a      his conjecture that all zeta functions of groups satisfy functional\u000a      equations. However, during the period, explicit calculations uncovered a\u000a      counterexample. The mathematical details are published in [2].\u000a    "},{"CaseStudyId":"20702","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"3996063","Name":"Mexico"},{"GeoNamesId":"2661886","Name":"Sweden"},{"GeoNamesId":"3144096","Name":"Norway"},{"GeoNamesId":"1562822","Name":"Vietnam"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"690791","Name":"Ukraine"},{"GeoNamesId":"3017382","Name":"France"}],"Funders":["Royal Society"],"ImpactDetails":"\u000d\u000a    Kenna and Berche's research on critical mass has been reported\u000d\u000a      many times in print and online\u000d\u000a      media and has had impact on research policy all over the world, leading\u000d\u000a      directly to policy changes.\u000d\u000a    Impacts on public policy and services\u000d\u000a      There is documented evidence of Kenna's research having an impact\u000d\u000a      on policy debate in\u000d\u000a      Parliament, EU Directorate and policy-orientated think tanks. There is\u000d\u000a      also evidence that the\u000d\u000a      research has been used by lobbying organisations, professional\u000d\u000a      associations and the media to\u000d\u000a      inform or change policy on research quality.\u000d\u000a    Informing French research policy change: The Conseil National des\u000d\u000a      Universit&#233;s (CNU) is\u000d\u000a      France's national body whose role includes the assessment of academics and\u000d\u000a      evaluation of\u000d\u000a      research groups (similar to the UK's REF but with a broader remit).\u000d\u000a      Professor Georges Landa,\u000d\u000a      President of CNU's section 28 (Physics), states that, prior to engaging\u000d\u000a      with Kenna's research, the\u000d\u000a      CNU assessed academics solely using metrics such as the numbers and\u000d\u000a      quality of publications.\u000d\u000a      However, now it takes the size of the research group into account. Landa\u000d\u000a      believes that, as a result\u000d\u000a      of Kenna's research, the CNU's assessment of individual\u000d\u000a      researchers is now more accurate and\u000d\u000a      fair [a]. He has also stated \"I am totally convinced of the necessity to\u000d\u000a      take into account the size of\u000d\u000a      groups or labs\" [a]. Through its impact on the CNU, Kenna's work\u000d\u000a      has impacted on every\u000d\u000a      university in France and, by extension, to every academic employed in\u000d\u000a      French universities.\u000d\u000a    Influencing EU policy: Dr James Gavigan was Head of the EU's Unit\u000d\u000a      for European Research\u000d\u000a      Area policy in DG Research, from 2006 to 2012. His job was to develop\u000d\u000a      initiatives to increase the\u000d\u000a      amount, impact and overall efficiency of research carried out in the EU.\u000d\u000a      He stated to Kenna \"Your\u000d\u000a      work on critical mass is certainly relevant and interesting for the\u000d\u000a      research policy debate at EU level,\u000d\u000a      as led by the European Commission in Brussels. In that regard, in my role\u000d\u000a      as Head of the\u000d\u000a      European Research Area Policy Unit which I occupied up to the end of\u000d\u000a      August 2012, I read with\u000d\u000a      interest your work and circulated it within the Research and Innovation\u000d\u000a      Directorate General\" [b].\u000d\u000a    Informing UK parliamentary debate: In 2010, Dr Jonathan Adams,\u000d\u000a      Director of Research\u000d\u000a      Evaluation at Evidence, a Thomson Reuters business (now called Research\u000d\u000a      Analytics), wrote: \"We\u000d\u000a      were interested to see your article reported in the Times Higher\u000d\u000a      Education... This is very helpful\u000d\u000a      and we would very much like to get advice on when the papers go into\u000d\u000a      publication... I am passing\u000d\u000a      on your information to colleagues at BIS, and hope that is OK with you.\"\u000d\u000a      As a result Kenna (with\u000d\u000a      Berche, co-author) presented the research evidence to the House of Commons\u000d\u000a      Science &amp;\u000d\u000a      Technology Committee on Peer Review. This is published in the Eighth\u000d\u000a      Report of Session 2010-12\u000d\u000a      Volume 2 at http:\/\/tinyurl.com\/kgxsubr.\u000d\u000a    Research used as the basis for critical reviews of policy in the\u000d\u000a        media: Journalist Paul Jump is\u000d\u000a      the senior science and research reporter at Times Higher Education\u000d\u000a      (THE). One of his three\u000d\u000a      articles on the implications of Kenna's work has been viewed by\u000d\u000a      over 2,000 people online in\u000d\u000a      addition to the 60,000 per week hard-copy readers. Jump believes Kenna's\u000d\u000a      work is relevant to\u000d\u000a      policy-making on \"whether research funding should be concentrated on large\u000d\u000a      research intensive\u000d\u000a      universities, as the Russell Group argue\", or whether size should be taken\u000d\u000a      into account more\u000d\u000a      explicitly in the allocation of research funding [c]. Jump's articles show\u000d\u000a      there has been considered,\u000d\u000a      wider, public engagement with the research, and a policy issue has been\u000d\u000a      raised for research\u000d\u000a      funding in the UK.\u000d\u000a    Stimulation of policy debate: The Foundation for Science and\u000d\u000a      Technology provides a `neutral\u000d\u000a      platform for debate of policy issues that have a science, engineering or\u000d\u000a      technology element'. It is\u000d\u000a      directed by a Council including heads of the Royal Society, British\u000d\u000a      Academy, Research Councils\u000d\u000a      and others. The Foundation provides support to around 140 learned &amp;\u000d\u000a      professional societies.\u000d\u000a      Paper [1] influenced an Editorial by Sir John Enderby on \"the dilemma of\u000d\u000a      science and research\u000d\u000a      funding\" in the Journal of the Foundation for Science and Technology\u000d\u000a      [Vol 20, No. 6 (2011) page\u000d\u000a      4]. The research also influenced the Russell Group policy report Jewels\u000d\u000a        in the Crown: the\u000d\u000a        Importance and Characteristics of the UK's World-Class Universities,\u000d\u000a      Russell Group Papers, Issue\u000d\u000a      4, 2012, which stated `A study using research assessment exercises in the\u000d\u000a      UK and France to look\u000d\u000a      at the relationship between quality and number of researchers submitted,\u000d\u000a      found evidence of\u000d\u000a      maximum and minimum thresholds for group size in a range of subjects' [d].\u000d\u000a    Engagement with practitioners and professional services: Kenna\u000d\u000a      and Berche have been\u000d\u000a      commissioned to write articles for a number professional journals and\u000d\u000a      newsletters. These included\u000d\u000a      Research Intelligence, the membership publication of the British\u000d\u000a      Educational Research\u000d\u000a      Association; Reflets de la Physique, the magazine of the French\u000d\u000a      Physics Society, of which 4,000\u000d\u000a      copies per issue are distributed; Mathematics Today, the\u000d\u000a      membership publication of the Institute of\u000d\u000a        Mathematics and its Applications which has a bimonthly readership of\u000d\u000a      4,500 professional\u000d\u000a      mathematicians; Significance, which is the membership magazine for\u000d\u000a      the Royal Statistical Society,\u000d\u000a      the American Statistical Association and Reports of the National Academy\u000d\u000a      of Sciences of Ukraine\u000d\u000a      in 2013. In addition, Europhysics News published an article based\u000d\u000a      on Kenna's research (The\u000d\u000a        relationship between quality and quantity in research, Vol.41, No.5,\u000d\u000a      2010, page 15). This bimonthly\u000d\u000a      publication is the \"voice of the European Physical Society\" which\u000d\u000a      has 41 national Member\u000d\u000a      Societies, representing over 120,000 members. Europhysics News has\u000d\u000a      25,000 copies per issue\u000d\u000a      which go to relevant university departments and key players the world\u000d\u000a      over. These and other\u000d\u000a      articles were translated into many languages including Greek, Russian,\u000d\u000a      Ukrainian and Vietnamese\u000d\u000a      and reverberated around the world through websites, blogs, online media\u000d\u000a      and discussion forums.\u000d\u000a    Influence on campaigns and debate by international lobbying groups and\u000d\u000a        NGOs: The\u000d\u000a      following give examples of use of the research to support campaigns to\u000d\u000a      protect research funding:\u000d\u000a    \u000d\u000a      Sauvons l'Universite (formerly Sauvons la Recherche) is a French\u000d\u000a        association working to\u000d\u000a        defend values and research funding in the French university system, to\u000d\u000a        promote collegial work\u000d\u000a        and to campaign for reforms in the HE sector. In 2011 they ran a\u000d\u000a        detailed report on papers [1-\u000d\u000a        3]. See http:\/\/www.sauvonsluniversite.com\/spip.php?article4808\u000d\u000a        (in French).\u000d\u000a      The Ukrainian Science Association is \"organized to promote education\u000d\u000a        and science reform in\u000d\u000a        Ukraine\", which \"conducts independent analytical research and monitoring\u000d\u000a        of education,\u000d\u000a        science and technology developments in Ukraine and worldwide.\" In 2010,\u000d\u000a        they ran a report on\u000d\u000a        their website, inspired by Kenna's interview in Times Higher\u000d\u000a          Education. See\u000d\u000a        http:\/\/nauka.in.ua\/news\/archive\/article_detail\/5809\u000d\u000a        (in Ukrainian).\u000d\u000a      The Association of Swedish Higher Education (SUHF in English) is \"an\u000d\u000a        organisation for\u000d\u000a        institutional co-operation\" comprising 41 universities in Sweden. The\u000d\u000a        Association \"aims at\u000d\u000a        safeguarding the external interests of the institutions and at\u000d\u000a        strengthening their internal co-\u000d\u000a        operation.\" In November 2011, an SUHF Experts Committee on Quality\u000d\u000a        issued a report on\u000d\u000a        \"Research Quality and the Role of University Leadership\". Their\u000d\u000a        recommendations regarding\u000d\u000a        group size were influence by [2]. See http:\/\/tinyurl.com\/mfywoy5.\u000d\u000a      RAND (Research and Development) Corporation is \"a non-profit\u000d\u000a        institution that helps improve\u000d\u000a        policy and decision making through research and analysis\". In a 2011\u000d\u000a        review of the research\u000d\u000a        and development system, prepared for the Greek Ministry of Education,\u000d\u000a        Lifelong Learning and\u000d\u000a        Religious Affairs, RAND drew on reference [1] and on Kenna's\u000d\u000a        submission to Parliament to\u000d\u000a        draw conclusions on the influence of research group size. See http:\/\/tinyurl.com\/meg6mrp.\u000d\u000a      The Norwegian Network for Private Higher Education Institutions\u000d\u000a        advocates on issues\u000d\u000a        concerning HE and research policy to the Norwegian parliament. Following\u000d\u000a        publication of\u000d\u000a        Kenna's OECD Higher Education Management and Policy paper\u000d\u000a        [5], in March 2012 the\u000d\u000a        Network placed a review on its website of the research and its\u000d\u000a        implications. See\u000d\u000a        http:\/\/www.nph.no\/?idnr=167\u000d\u000a        (in Norwegian). Dr Arne J Eriksen, Secretary General of the\u000d\u000a        Norwegian Network for Private Higher Education Institutes, stated that Kenna's\u000d\u000a        research \"is\u000d\u000a        important for the development of national and institutional\u000d\u000a        strategies\/policies and priorities\".\u000d\u000a      \u000aKenna's research was cited in a Policy Discussion on the use of\u000d\u000a        metrics in evaluating\u000d\u000a        research, documented in the Proceedings of the International Congress of\u000d\u000a        Mathematicians,\u000d\u000a        Hyderabad 2010, Volume 1 Plenary Lectures and Ceremonies, Ed. Rajendra\u000d\u000a        Bhatia. See\u000d\u000a        section called Round Table: The Use of Metrics in Evaluating Research -\u000d\u000a        page 742 (by J.M.\u000d\u000a        Ball, Oxford). Panel included Doug Arnold (President of SIAM), Malcolm\u000d\u000a        MacCallum (Director\u000d\u000a        of the Heilbronn Institute and Member, Research Policy Committee, London\u000d\u000a        Mathematical\u000d\u000a        Society), Jos&#233; Antonio de Pena, (Director of the Mathematical Institute\u000d\u000a        at the National\u000d\u000a        University of Mexico and former President of UMALCA (Mathematical Union\u000d\u000a        of Latin America\u000d\u000a        and the Caribbean)) and Frank Pacard (Scientific Advisor of Mathematics\u000d\u000a        in the French\u000d\u000a        Ministry of Higher Education and Research).\u000d\u000a    \u000d\u000a    Articles and interviews in media: Articles in Times Higher\u000d\u000a        Education (THE) [e], University\u000d\u000a        Business [f] , Research Professional [g] and the New\u000d\u000a        Zealand Herald appeared in 2010 following\u000d\u000a      interviews with Kenna. THE publishes 28,000 copies per\u000d\u000a      issue, has 60,000 readers per week and\u000d\u000a      THE online registers over 650,000 users globally each month. University\u000d\u000a        Business is read by\u000d\u000a      senior management across 165 Universities and over 350 HE and FE colleges\u000d\u000a      throughout the UK.\u000d\u000a      It has a circulation of 6,000 and a readership of 8,500. In 2011, Research\u000d\u000a        Trends reported on [1-4]\u000d\u000a      [h]. In 2012 another interview with Kenna prompted further\u000d\u000a      articles in THE [e] and University\u000d\u000a        Business [f] following publication of [5]. In 2011 Kenna was\u000d\u000a      also interviewed for Physics World [i].\u000d\u000a      Physics World has a circulation of 35,000 and a readership of\u000d\u000a      110,000. These articles and\u000d\u000a      interviews were translated and reproduced many times worldwide. In 2013,\u000d\u000a      following publication of\u000d\u000a      [6] Kenna was again interviewed by Times Higher Education\u000d\u000a      [e].\u000d\u000a    Influencing a commercial organisation's evaluation of excellence:\u000d\u000a      Verisk Analytics is \"a\u000d\u000a      leading source of information about risk\", offering \"risk-assessment\u000d\u000a      services and decision analytics\u000d\u000a      to professionals in many fields\" (6,000+ employees including 500 with\u000d\u000a      advanced degrees, 200+\u000d\u000a      actuaries with $1.5billion in revenue in 2012). At their Investor Day in\u000d\u000a      2013, Scott Stephenson,\u000d\u000a      president and Chief Executive Officer of Verisk Analytics, used Kenna's\u000d\u000a      work in a presentation on\u000d\u000a      \"Operating Strategy and Innovation\" showing that excellence is achievable\u000d\u000a      at modest scale [j].\u000d\u000a    Conclusion\u000d\u000a      Kenna and co-workers' research has provided much-needed rigour to\u000d\u000a      the concept of `critical mass'\u000d\u000a      in research groups. Their work has directly led to a change in French\u000d\u000a      policy on the assessment of\u000d\u000a      its academics. Furthermore, it has stimulated significant discussion by\u000d\u000a      policy makers and think\u000d\u000a      tanks working in the field, in the UK, EU and beyond.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This case study describes the international impact of research undertaken\u000d\u000a      by Professor Kenna\u000d\u000a      and co-workers into the concept of critical mass in research groups. The\u000d\u000a        main impact arising from\u000d\u000a      the research is upon public policy and services. The research has\u000d\u000a      influenced policy debate in\u000d\u000a      the UK Parliament, in France and more generally. Beneficiaries of\u000d\u000a      the research include policy\u000d\u000a      makers in higher education, governments, think tanks, and public sector\u000d\u000a      organisations and\u000d\u000a      societies.\u000d\u000a    ","ImpactType":"Societal","Institution":"\u000d\u000a    Coventry University\u000d\u000a    ","Institutions":[{"AlternativeName":"Coventry University","InstitutionName":"Coventry University","PeerGroup":"E","Region":"West Midlands","UKPRN":10001726}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2800867","Name":"Bruxelles-Capitale"},{"GeoNamesId":"2907669","Name":"Heilbronn"}],"References":"\u000d\u000a    \u000a1. Kenna, R., &amp; Berche, B. (2010). The extensive nature of\u000d\u000a      group quality. Europhysics Letters, 90\u000d\u000a        (5), 58002. (Impact Factor 2.26)\u000d\u000a    \u000a\u000a2. Kenna, R., &amp; Berche, B. (2011). Critical mass and the\u000d\u000a      dependency of research quality on\u000d\u000a      group size, Scientometrics, 86 (2) 527 - 540. (Impact Factor\u000d\u000a      2.133)\u000d\u000a    \u000a\u000a3. Kenna, R., &amp; Berche, B. (2011). Normalization of\u000d\u000a      peer-evaluation measures of group research\u000d\u000a      quality across academic disciplines. Research Evaluation, 20 (2)\u000d\u000a      107-116. (Impact Factor\u000d\u000a      1.074)\u000d\u000a    \u000a\u000a4. Kenna, R., &amp; Berche, B. (2012). Managing research quality:\u000d\u000a      critical mass and optimal\u000d\u000a      academic research group size. IMA Journal of Management Mathematics,\u000d\u000a        23 (2): 195-207.\u000d\u000a      (Impact Factor 0.59, Association of Business Schools' ranking 2*)\u000d\u000a    \u000a\u000a5. Kenna, R., &amp; Berche, B. (2011). Critical masses for\u000d\u000a      academic research groups and\u000d\u000a      consequences for higher education research policy and management. Higher\u000d\u000a        Education\u000d\u000a        Management and Policy, 23 (3): 9-29\u000d\u000a    \u000a\u000a6. Mryglod, O., Kenna, R., Holovatch, Y., &amp; Berche, B.\u000d\u000a      (2013). Absolute and specific measures of\u000d\u000a      research group excellence. Scientometrics, 95 (1) 115-127. (Impact\u000d\u000a      Factor 2.133)\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"16","Level2":"8","Subject":"Sociology"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000d\u000a    a. Georges Landa, Pr&#233;sident de la section CNU 28 (Information collected\u000d\u000a      by RAND Europe in\u000d\u000a      two interviews, see report PR-514-CU and PR-843-CU)\u000d\u000a    b. Head of the EU's Unit for European Research Area policy in DG\u000d\u000a      Research, from 2006 to 2012\u000d\u000a    c. Paul Jump, Senior Science and Research Reporter for Times Higher\u000d\u000a      Education (Information\u000d\u000a      collected by RAND Europe in an interview, see report PR-514-CU)\u000d\u000a    d. http:\/\/russellgroup.org\/JewelsInTheCrown.pdf\u000d\u000a    e. Truth in numbers: study pinpoints 'critical mass' for research\u000d\u000a        success, by Paul Jump, Times\u000d\u000a      Higher Education, No. 1955, 8-14 July, 2010\u000d\u000a    Do too many researchers spoil the outcome? by Paul Jump, Times\u000d\u000a      Higher Education No.\u000d\u000a      2036, 9-15 February, 2012\u000d\u000a    Skip the REF and count the notes?, by Paul Jump Times Higher\u000d\u000a      Education No.2110, 18-24\u000d\u000a      July 2013\u000d\u000a    f. Research funding most effective when targeted at medium-sized\u000d\u000a        research groups by Lucy\u000d\u000a      Porter, University Business, 9 July 2010\u000d\u000a    Smaller universities often produce better research by Carley Drew,\u000d\u000a      University Business, 13\u000d\u000a      February 2012\u000d\u000a    g. Concentrating research to get quality is a myth, says study by\u000d\u000a      Elizabeth Gibney, Research\u000d\u000a      Professional 8 and 14 July 2010\u000d\u000a    h. Two's company: how scale affects research groups, by Matthew\u000d\u000a      Richardson, Research\u000d\u000a      Trends (issue 25, November 2011)\u000d\u000a    i. Why 13 and 25 are magic numbers for physicists, by Michael\u000d\u000a      Banks, Physics World, Volume\u000d\u000a      24, No. 6 (June 2011) page 7 and Volume 24, No. 12 (December 2011) page 52\u000d\u000a    j. See p106 of document available at http:\/\/tinyurl.com\/odct5ls\u000d\u000a      or from Coventry University\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Changing research policy: the critical mass of research groups\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The concept of critical mass in research had been around for a\u000d\u000a      long time without clear definition.\u000d\u000a      It has been considered, discussed and debated by research managers and\u000d\u000a      policy makers in\u000d\u000a      academia and governments, especially in relation to which research areas\u000d\u000a      to promote.\u000d\u000a    Kenna has been researching statistical physics at Coventry\u000d\u000a      University since his appointment in\u000d\u000a      2002. Drawing on mean-field theory, he developed the first agent-based\u000d\u000a      model, which takes\u000d\u000a      account of interactions between individuals, to explain the relationship\u000d\u000a      between research quality as\u000d\u000a      produced by a group and the number of its members. The model manifests a\u000d\u000a      linear relationship\u000d\u000a      between research quality and group\/departmental size, up to a\u000d\u000a      discipline-dependent \"upper critical\u000d\u000a      mass\" (similar to the Dunbar number in anthropology). Above this\u000d\u000a      size, difficulties in\u000d\u000a      communication akin to the Ringelmann effect in sociology result in\u000d\u000a      a much reduced correlation\u000d\u000a      between research quality and group size. Empirical support for the theory\u000d\u000a      comes from rigorous\u000d\u000a      statistical analysis of the RAE 2008 and its French counterpart. Kenna's\u000d\u000a      research delivers the first\u000d\u000a      and only quantitative definition of critical mass in the context of\u000d\u000a      managing research groups.\u000d\u000a    His initial paper [1] was followed by a second [2] in which critical\u000d\u000a      masses for various academic\u000d\u000a      disciplines were determined. In an interview with Times Higher\u000d\u000a        Education (THE) in 2009, Professor\u000d\u000a      Dame Julia Higgins, Chair of the RAE2008 panel for Physical Sciences\u000d\u000a      lamented the absence of\u000d\u000a      an \"intellectual basis\" to compare RAE results across disciplines and\u000d\u000a      called for \"serious thinking\"\u000d\u000a      on the matter (http:\/\/tinyurl.com\/msluy3r).\u000d\u000a      In response to this, Kenna developed a method to\u000d\u000a      overcome the problem of comparing peer-evaluation between disciplines [3].\u000d\u000a      Kenna's research\u000d\u000a      also focussed on the optimum group\/departmental size [4] and explained, in\u000d\u000a      terms of critical mass,\u000d\u000a      why some smaller, research-focused departments' quality is on a par to\u000d\u000a      that of larger institutions\u000d\u000a      [5]. He also explained why citation-based indicators are poorly correlated\u000d\u000a      with group quality,\u000d\u000a      although correlate well with peer-evaluated measures of group strength\u000d\u000a      [6]. The initial paper [1]\u000d\u000a      was ranked \"Best of 2010\" in European Physics Letters (EPL),\u000d\u000a      placing it in the top 5% of the circa\u000d\u000a      800 papers published in EPL that year. It also featured in EPL's\u000d\u000a      October 2010 \"Highlights from\u000d\u000a      Previous Volumes\". Paper [3] was the 4th, 4th and 45th\u000d\u000a      most-read article in Research Evaluation in\u000d\u000a      Jan, Feb &amp; Mar 2012. Paper [4] was respectively the 13th\u000d\u000a      and 4th and 18th most-read article in the\u000d\u000a      IMA Journal of Management Mathematics in Jan, Feb &amp; Mar 2012.\u000d\u000a    "},{"CaseStudyId":"20910","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"6251999","Name":"Canada"},{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    The southern-route `out of Africa' theory of human dispersal developed by\u000d\u000a      University of Glasgow research has sparked huge ongoing public interest\u000d\u000a      since its publication in 2005. The idea has been discussed in the TV\u000d\u000a      series and popular science book The Incredible Human Journey by\u000d\u000a      Alice Roberts in 2009\/10. University of Glasgow research has also led to\u000d\u000a      the creation of a revised timeline (based on a time-dependent molecular\u000d\u000a      clock) for linking DNA changes with locations and times, which has been\u000d\u000a      adopted by companies such as BritainsDNA,\u000d\u000a      which offer DNA tests for people seeking information about their recent\u000d\u000a      and deep ancestry. The 2013 TV programme Meet the Izzards is\u000d\u000a      another high-profile show which used the Glasgow research in part to trace\u000d\u000a      Eddie Izzard's ancestry back to the evolution of modern humans in east\u000d\u000a      Africa.\u000d\u000a    The Incredible Human Journey was published by Bloomsbury in 2010.\u000d\u000a    Roberts describes the `out of Africa' theory in an accessible manner for a\u000d\u000a    lay audience, and quotes the research papers by Macaulay et al. for those\u000d\u000a    looking for more details. One reviewer on Amazon says, `I am gripped by the\u000d\u000a    central idea that only about 200 families originally emerged out of Africa\u000d\u000a    and between them populated the whole world', one of the statistical\u000d\u000a    inferences made by Macaulay.\u000d\u000a    The TV series was broadcast on BBC 2 from May-June 2009, as a 5-part\u000d\u000a      series following the routes of the families who populated the world. The\u000d\u000a      first part is titled `Out of Africa' and gives a full picture of the\u000d\u000a      earlier theories prior to the Macaulay et al paper in 2005, but each of\u000d\u000a      the episodes recounts the general theory for viewers who had not seen the\u000d\u000a      first one. The later episodes follow Macaulay et al.'s theory of the\u000d\u000a      dispersal of humans across the planet. Over 10.2 million viewers watched\u000d\u000a      the series when it was aired on the BBC, with Part 1, `Out of Africa',\u000d\u000a      being shown on 10 May and achieving an audience of 2.22m. Part 2, `Asia',\u000d\u000a      reached 2.34m; Part 3, `Europe', was watched by 1.66m; Part 4, `Australia'\u000d\u000a      attracted 2.11m viewers; and Part 5, `The Americas', reached an audience\u000d\u000a      of 1.86m. The series is currently available to view on YouTube and the\u000d\u000a      first episode has been viewed 101,562 times since it was added to the site\u000d\u000a      in August 2011. It has also been widely reviewed as a series and book,\u000d\u000a      with The\u000a          Observer noting that it provides `an easily digestible\u000d\u000a      introduction to a complex but fascinating story.'\u000d\u000a    Meet the Izzards is a 2-part documentary, which aired on BBC 1 at\u000d\u000a      a prime-time slot of 9pm on 20 and 21 February 2013, and was available on\u000d\u000a      iPlayer. It was broadcast to an audience of 2.62\u000a        million (figures for Part 1 only). In the documentary, Izzard\u000d\u000a      explains the `out of Africa' concept to the audience: `we all come out of\u000d\u000a      Africa, and we come from the same people. So we were a small group of\u000d\u000a      10,000 people and then we've turned into seven billion people on the\u000d\u000a      planet.' The programme received widespread press coverage. The consultant\u000d\u000a      scientist acknowledges the programme's debt to Macaulay's work, stating\u000d\u000a      that his mtDNA genealogy discoveries; `...define a scientific\u000d\u000a        framework for a number of documentaries, including Meet the Izzards but\u000d\u000a        also other series which seek to educate the public and disseminate human\u000d\u000a        population genetics to the lay public'.\u000d\u000a    The consultant to the BBC programme is also Chief Scientific Officer\u000d\u000a      (CSO) of BritainsDNA, one of a number of companies which have sprung up\u000d\u000a      around the combination of DNA analysis and the public appetite for\u000d\u000a      genealogy. BritainsDNA, established 2011 as ScotlandsDNA, draws upon the\u000d\u000a      `out of Africa' theory to analyse DNA samples from the public. This has\u000d\u000a      tapped into the huge public interest in tracing their ancestors, allowing\u000d\u000a      members of the public to trace their genetic lineage through DNA testing.\u000d\u000a      BritainsDNA draws upon Macaulay et al.'s work to allocate dates and\u000d\u000a      locations in history to the mtDNA samples taken from the public. The CSO\u000d\u000a      for the company states:\u000d\u000a    Dr Macaulay's work underpins a significant part of the genetic\u000d\u000a        ancestry testing business. Specifically his contributions to the\u000d\u000a        understanding of the mtDNA genealogy &#8212; the topology or shape of the\u000d\u000a        tree, the timing of many of the splits in the tree and the distribution\u000d\u000a        of the groups across geography are the bread and butter of the\u000d\u000a        interpretation of mtDNA lineages. This impact is felt not only at\u000d\u000a        BritainsDNA but across all companies offering genetic ancestry testing\u000d\u000a        involving mtDNA &#8212; a multi-million dollar global industry.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research in the School of Mathematics &amp; Statistics in the University\u000d\u000a      of Glasgow has been influential in answering a long-standing question:\u000d\u000a      where do we come from? The fleshing-out of the 'out of Africa' theory has\u000d\u000a      been the focus of two documentary series, The Incredible Human Journey\u000d\u000a      and Meet the Izzards, and has generated income for DNA testing\u000d\u000a      companies in the UK and US by enabling them to offer `deep DNA' tests\u000d\u000a      revealing one's roots from far back in history. The Incredible Human\u000d\u000a        Journey aired on BBC 2 in 2009, reaching 10.2 million viewers\u000d\u000a      altogether, has been watched 100,000 times on YouTube and was broadcast in\u000d\u000a      shorter format in Australia and Canada before being released as a DVD. Meet\u000d\u000a        the Izzards was broadcast on BBC 1 in 2013 to over 3 million people.\u000d\u000a    ","ImpactType":"Cultural","Institution":"\u000d\u000a    University of Glasgow\u000d\u000a    ","Institutions":[{"AlternativeName":"Glasgow (University of)","InstitutionName":"University of Glasgow","PeerGroup":"A","Region":"Scotland","UKPRN":10007794}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1. Torroni, A., Achilli, A., Macaulay, V., Richards, M. and Bandelt,\u000d\u000a      H.-J. (2006). Harvesting the fruit of the human mtDNA tree. Trends in\u000d\u000a        Genetics, 22, 339-345. (doi:10.1016\/j.tig.2006.04.001)\u000d\u000a      *\u000d\u000a    \u000a\u000a2. Macaulay, V., Hill, C., Achilli, A., Rengo, C., Clarke, D., Meehan,\u000d\u000a      W., Blackburn, J., Semino, O., Scozzari, R., Cruciani, F., Taha, A.,\u000d\u000a      Shaari, N. K., Raja, J. M., Ismail, P., Zainuddin, Z., Goodwin, W.,\u000d\u000a      Bulbeck, D., Bandelt, H.-J., Oppenheimer, S., Torroni, A. and Richards, M.\u000d\u000a      (2005). Single, rapid coastal settlement of Asia revealed by analysis of\u000d\u000a      complete mitochondrial genomes. Science, 308, 1034-1036 and 309,\u000d\u000a      1995-1996. (doi:10.1126\/science.1109792)\u000d\u000a      *\u000d\u000a    \u000a\u000a3. Soares, P., Ermini, L., Thomson, N., Mormina, M., Rito, T., R&#246;hl, A.,\u000d\u000a      Salas, A., Oppenheimer, S., Macaulay, V. and Richards, M. B. (2009).\u000d\u000a      Correcting for purifying selection: an improved human mitochondrial\u000d\u000a      molecular clock. American Journal of Human Genetics, 84, 740- 759.\u000d\u000a      (doi:10.1016\/j.ajhg.2009.05.001)\u000d\u000a      *\u000d\u000a    \u000a\u000a4. Thomson, Noel (2010) Bayesian mixture modelling of migration by\u000d\u000a      founder analysis. PhD thesis, University of Glasgow. (http:\/\/theses.gla.ac.uk\/1468\/)\u000d\u000a    \u000a\u000a5. Salas, A., Carracedo, &#193;., Richards, M., and Macaulay, V. (2005).\u000d\u000a      Charting the ancestry of African-Americans. American Journal of Human\u000d\u000a        Genetics, 77, 676-680. http:\/\/dx.doi.org\/10.1086\/491675\u000d\u000a    * best indicators of research quality\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"6","Level2":"4","Subject":"Genetics"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    \u000d\u000a       Testimonial from Consultant Scientist on `Meet the Izzards' and Chief\u000d\u000a        Scientific Officer for BritainsDNA (available from HEI) (confirming\u000d\u000a        importance of research to programme and to genetic ancestry testing\u000d\u000a        industry, and confirming viewing figures for Meet the Izzards)\u000d\u000a       The Guardian, 21 February 2013 (link)\u000d\u000a        (re Meet the Izzards viewing figures)\u000d\u000a       BBC 2 Programmes, The Incredible Human Journey, Out of Africa (link)\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Out of Africa\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2648579","Name":"Glasgow"}],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The research provided the strongest genetic evidence to date of the route\u000d\u000a      by which modern humans migrated out of east Africa some 60,000-80,000\u000d\u000a      years ago. This landmark study carried out by Dr Vincent Macaulay (Reader\u000d\u000a      in Statistics, University of Glasgow, 2003-present) and an international\u000d\u000a      team from 2003-2005 brought into radically sharper focus the `out of\u000d\u000a      Africa' theory of human dispersal. Prior to this it was generally believed\u000d\u000a      that there was a series of dispersals from different areas of Africa,\u000d\u000a      including a northerly wave which was ultimately to populate Europe.\u000d\u000a      Macaulay et al.'s findings demonstrated that only one relatively small\u000d\u000a      group of human ancestors travelled from Africa eventually to spread over\u000d\u000a      the rest of the world; that this group travelled faster and further than\u000d\u000a      previously believed; and that all humans outside Africa are ultimately\u000d\u000a      descended from the same group of travellers.\u000d\u000a    Mitochondrial DNA (mtDNA) is passed down the maternal line, and is\u000d\u000a      important to genetic archaeology because it is inherited intact (in\u000d\u000a      contrast to most of our DNA) and has plenty of variation, generated when\u000d\u000a      mutations occur between mother and child, which makes it possible to\u000d\u000a      reconstruct the ancestry of the sequences in exquisite detail. If the\u000d\u000a      mutation rate of mtDNA can be established, researchers can exploit that\u000d\u000a      rate to convert genetic variation into time estimates to assign mutations\u000d\u000a      to particular eras and places. Macaulay's work focussed on developing\u000d\u000a      statistical models to accomplish these two tasks from the complete\u000d\u000a      mitochondrial DNA sequence data gathered from diverse living humans, in\u000d\u000a      the context of international teams of human geneticists. He used\u000d\u000a      techniques from graph theory (median networks) and likelihood approaches\u000d\u000a      to help to reconstruct the `family tree' of mtDNA [1]. His research then\u000d\u000a      addressed spatial questions &#8212; what physical locations are associated with\u000d\u000a      particular ancestors in this tree [2], before he turned to the temporal\u000d\u000a      aspects, where it was vital better to model the clock which describes how\u000d\u000a      rapidly particular genetic material changes by mutation. A technique for\u000d\u000a      doing this was proposed [3] for the case where DNA is experiencing natural\u000d\u000a      selection, so that in effect the clock appears to slow down as one moves\u000d\u000a      into the past. This allowed a tool [3] to be developed to extract more\u000d\u000a      temporal information from mtDNA sequences than was hitherto possible, so\u000d\u000a      that the variation in mtDNA is much more securely anchored in time, a\u000d\u000a      vital pre-requisite for robust genealogical interpretation at deep time\u000d\u000a      depths.\u000d\u000a    Finally, and most challengingly, between 2003 and 2010 Macaulay modelled\u000d\u000a      the processes that generated the inferred spatio-temporal signal\u000d\u000a      in the tree. For example, these might involve migration, as in the\u000d\u000a      high-profile study [2] which provides the strongest genetic evidence to\u000d\u000a      date of the route by which modern humans migrated out of east Africa\u000d\u000a      60,000-80,000 years ago; by detecting temporal patterning in the\u000d\u000a      distribution of inferred movements of ancestors in the tree (`founder\u000d\u000a      analysis', the statistical properties of which have been explored [4] in\u000d\u000a      the context of the structure coalescent process); or admixture, by\u000d\u000a      dissecting the contribution of different regions of Africa to the\u000d\u000a      Americas, as a result of the slave trade, by Bayesian modelling [5].\u000d\u000a    Macaulay has developed these approaches with three local research\u000d\u000a      students in Statistics, Dr Noel Thomson (supported by a scholarship from\u000d\u000a      the Carnegie Trust), Dr Maarya Sharif (supported by a studentship from\u000d\u000a      EPSRC) and Dr Colette Mair (supported by a University of Glasgow\u000d\u000a      scholarship). The University of Glasgow researchers have been solely\u000d\u000a      responsible for developing the new statistical techniques and have shared\u000d\u000a      the job of applying them, in various collaborative teams with wet-lab\u000d\u000a      colleagues in other institutions in the UK and the rest of Europe (who\u000d\u000a      focus on the sample collection, the DNA sequencing, and bioinformatics\u000d\u000a      issues). The techniques have been most comprehensively applied to\u000d\u000a      understanding genetic variation in human mitochondrial DNA, one of the\u000d\u000a      main genetic loci that have been marketed by the genetic genealogical\u000d\u000a      industry in the last 10 years. The specific University of Glasgow\u000d\u000a      contribution to the impact was the statistical modelling that allowed\u000d\u000a      inferences about time, place and process to be made from contemporary DNA\u000d\u000a      sequences, inferences that have generated the impact in the genetic\u000d\u000a      genealogy industry.\u000d\u000a    "},{"CaseStudyId":"20954","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2264397","Name":"Portugal"},{"GeoNamesId":"2510769","Name":"Spain"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a\u0009  \u000a\u0009  \u000a\u0009  \u000a    Packaging structures based on thermoplastic bio-materials are being\u000a      increasingly used by food\u000a      retailers with the result that, as they are bio-degradable and\/or\u000a      compostable, the amount of waste\u000a      which is being produced is being significantly reduced. This research has\u000a      contributed to the\u000a      steady increase worldwide in the use of biodegradable plant based\u000a      materials in food packaging\u000a      structures. This uptake is, in turn, generating a demand for further\u000a      knowledge of the material\u000a      properties and behaviour of thermoplastic bio-materials.\u000a    The thesis [6] and all the computational results on the forming processes\u000a      for thermoplastic food\u000a      packaging containers were delivered to the company Plantic plc, (http:\/\/www.plantic.com.au), the\u000a      world's largest manufacturer of starch based materials for packaging, who\u000a      sourced the materials.\u000a      In response, Plantic stated that `the company is pleased to receive the\u000a      outcomes of the project\u000a      which will be included on our website and which will provide technical\u000a      information for our clients,\u000a      many of which are multinational firms.' Furthermore, Pactiv plc (http:\/\/www.pactiv.com), one of the\u000a      largest suppliers in the world of food containers for packaging (e.g. to\u000a      Marks &amp; Spencer plc),\u000a      similarly confirmed that `the outcomes from the project will help us to\u000a      modify our designs and\u000a      processing facilities for packages produced from new eco-materials.'\u000a    There is strong motivation from governments worldwide for the adoption of\u000a      biodegradable\u000a      materials in food packaging, both to reduce the amount of non-degradable\u000a      waste that litters the\u000a      world and also to avoid the use of oil-based products in the packaging\u000a      arena.\u000a    The research described above has been disseminated through presentations\u000a      made at a number of\u000a      major international conferences including:\u000a    i) NUMIFORM 2007, 5-day international conference on Materials Processing\u000a      and Design:\u000a      Modeling, Simulation and Applications, University of Porto, Portugal (see\u000a      [5]).\u000a    ii) CMMSE 2010, 4-day international conference on Computational and\u000a      Mathematical Methods in\u000a      Science and Engineering with approximately 130 presentations, Almeria,\u000a      Spain. (Proceedings,\u000a      Volume 3, p 849-852, ISBN13: 978-84-613-5510-5).\u000a      http:\/\/gsii.usal.es\/~CMMSE\/images\/stories\/congreso\/volumen1_10.pdf\u000a    In addition, the work is cited by engineers\/mathematicians world-wide,\u000a      for example:\u000a    i) Thongwichean, T., Pahlakormkule, C., Chaikittiratana, A. Finite\u000a      element analysis for\u000a      thermoforming process of starch\/biodegradable polyester blend. AIJSTPME\u000a      5(2), 33-37, (2012).\u000a    ii) Saedpanah, F. A posteriori error analysis for a continuous space-time\u000a      finite element method for\u000a      a hyperbolic integro-differential equation. BIT Numer Math (2013)\u000a      53:689-716 (2013).\u000a    ","ImpactSummary":"\u000a    The production of plastic (polymer) waste and the difficulties associated\u000a      with its disposal is a major\u000a      environmental challenge. Many polymer food packaging structures are made\u000a      using thermoforming\u000a      processes in which hot thin oil-based polymer sheets are forced under\u000a      pressure into moulds and\u000a      then cooled to become thin-walled packaging structures. These structures\u000a      are not eco-friendly and\u000a      do not degrade after use. Thus unless they are recycled, which is a\u000a      complicated process and\u000a      mostly does not happen, these structures cause major environmental\u000a      problems worldwide.\u000a    Researchers in Brunel Institute of Computational Mathematics (BICOM) have\u000a      undertaken\u000a      extensive computational modelling of the thermoforming of packaging\u000a      structures made from bio-materials\u000a      (thermoplastics). This computational work, together with the necessary\u000a      laboratory\u000a      experiments which were executed by Brunel engineers, has contributed to a\u000a      far better\u000a      understanding of the behaviour of starch-based biodegradable food\u000a      packaging. In turn, the\u000a      availability of such knowledge has contributed to the steady move by food\u000a      packagers and food\u000a      retailers towards the adoption of such packaging which is helping to\u000a      reduce the amount of long\u000a      term non-biodegradable waste produced.\u000a    ","ImpactType":"Technological","Institution":"\u000a    BRUNEL UNIVERSITY (H0113)\u000a    ","Institutions":[{"AlternativeName":"Brunel University","InstitutionName":"Brunel University","PeerGroup":"C","Region":"London","UKPRN":10000961}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Papers in International Journals:\u000a    \u000a1. I. Babuska, J.R. Whiteman, T.Strouboulis. Finite Elements; An\u000a      Introduction to the Method\u000a      and Error Estimation. Oxford University Press, (2011).\u000a    \u000a\u000a2. M.K. Warby, J.R. Whiteman, W-G Jiang, P. Warwick, T. Wright. Finite\u000a      element simulation of\u000a      thermoforming process for polymer sheets. Maths and Computers in\u000a      Simulation, 61, 209-218,\u000a      (2003).\u000a    \u000a\u000a3. W-G Jiang, M.K. Warby, J.R. Whiteman, S. Abbott, W. Shorter, P.\u000a      Warwick ,T. Wright, A.\u000a      Munro, B. Munro. Finite element modelling of high air pressure forming\u000a      processes for\u000a      polymer sheets. Computational Mechanics 31, 163-172, (2003).\u000a    \u000a\u000a4. M. Karamanou, M.K. Warby, J.R. Whiteman. Computational modelling of\u000a      thermoforming\u000a      processes in the case of finite viscoelastic materials. Computer Methods\u000a      in Applied\u000a      Mechanics and Engineering 195, 5220-5238, (2006).\u000a    \u000a\u000a5. D. Szegda, J. Song, M.K. Warby, J.R. Whiteman. Computational Modelling\u000a      of a\u000a      Thermoforming Process for Thermoplastic Starch. American Institute of\u000a      Physics,\u000a      Proceedings 908, 35 - 47, (2007).\u000a    \u000a\u000a6. D Szegda, Experimental Investigation and Computational modelling of\u000a      the Thermoforming\u000a      Process of Thermoplastic Starch. PhD Thesis, School of Engineering, Brunel\u000a      University,\u000a      (2009). http:\/\/bura.brunel.ac.uk\/handle\/2438\/3445\u000a    \u000a\u000a7. S. Shaw, M.K. Warby, J.R. Whiteman. Discretization and modelling error\u000a      in the context of\u000a      the rapid inflation of hyperelastic membranes. IMA Journal of Numerical\u000a      Analysis, 30 302-333, (2010).\u000a    \u000a\u000a8. S. Shaw, M.K. Warby, J.R. Whiteman. Computational Modelling of Some\u000a      Problems of\u000a      Elasticity and Viscoelasticity with Applications to thermoforming\u000a      Projects. IJNAM 3, 320-329,\u000a      (2012). http:\/\/bura.brunel.ac.uk\/handle\/2438\/6699\u000a    \u000aBackground Research Grants and Contracts, with Outcomes\u000a    &#8226; From European Community, 750,000 EU, Jan 1994 &#8212; Dec 1996, Numerical and\u000a      Physical\u000a      Study of Material Forming Processes (Collaboration with Univs of Paris,\u000a      Stuttgart, Aachen,\u000a      Twente, Eindhoven, Palermo, Swansea) &amp; companies from the European\u000a      polymer industry,\u000a      particularly Elf-Atochem AG and Centre de Mise en Forme des Mat&#233;riaux,\u000a      Sophia Anlipolis,\u000a      France. Outcome: Reports on thermo- forming processes produced for the\u000a      network and\u000a      associated companies.\u000a    &#8226; From EPSRC, &#163;176,000, Sept 1999-Aug 2003, Computational Modelling of\u000a      Thermoforming\u000a      and In-Mould-Decoration Processes. Collaboration with Autotype\u000a      International Ltd and John\u000a      McGavigan plc, who contributed an additional &#163;90,000. Grant under the\u000a      EPSRC Material\u000a      Processing for Eng Applics programme. Outcome: Design tools produced based\u000a      on finite\u000a      element models of forming process for transforming thin polymer sheets\u000a      (primarily\u000a      BAYFOL&#174;) into thin-walled structures. Results and software for associated\u000a      In-Mould-Decoration\u000a      processes delivered to companies. PI Whiteman.\u000a    &#8226; From United States Army Research Office, Durham, NC, $223,433, Aug\u000a      2000-Nov 2003,\u000a      Adaptive Space-Time Finite Element Methods for Dynamic Viscoelastic\u000a      Problems.\u000a      Outcome: Numerical schemes produced and tested in collaboration with US\u000a      Army\u000a      Research Laboratory, Langley, Virginia. PI Whiteman.\u000a    &#8226; From EPSRC, &#163;39,341, Jan 2001-July 2004, Nonlinear Modelling and\u000a      Computational\u000a      Simulation in Applied Polymer Viscoelasticity. Grant under the Engineering\u000a      International\u000a      Collaboration Programme. Outcome: Numerical schemes produced in\u000a      collaboration with\u000a      colleagues from the US Army and ICES, University of Texas at Austin. PI\u000a      Whiteman.\u000a    &#8226; From United States Army Research Office, Durham, North Carolina,\u000a      $249,287, Aug 2004 &#8212; July 2007,\u000a      Development of Multi-adaptive Simulation Technologies for Nonlinear Solid\u000a      Polymer Viscoelasticity. Outcome: Numerical schemes produced in\u000a      collaboration with US\u000a      Army Research Laboratory, NASA Langley, Virginia, and reports and code\u000a      delivered. PI\u000a      Whiteman.\u000a    &#8226; From DEFRA Food LINK Programme, 2004 - 2008, Thermoforming of\u000a      Biodegradable\u000a      Starch-Based Materials for Food Packaging. Biodegradable Starch\u000a      Mano-Composites for\u000a      Thermoformable Film Packaging for Food Products (AFM 200) - FT1505. PI\u000a      Song.\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"12","Subject":"Materials Engineering"},{"Level1":"3","Level2":"3","Subject":"Macromolecular and Materials Chemistry"}],"Sources":"\u000a    Pactiv plc was closely involved in the DEFRA funded project. Since the\u000a      end of the funding period\u000a      Pactiv plc has been taken over by the Pregis Corporation. The contact is\u000a      now with Pregis.\u000a    Contactable:\u000a    D and D Manager, Pregis (formerly Pactiv).\u000a    General Manager &#8212; Technology, Plantic Technologies Ltd.\u000a    \u000a    ","Title":"\u000a    Reduction of non-degradable waste from used plastic food packaging\u000a        materials\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Since 1993 Professor J R Whiteman, Drs S Shaw and M K Warby, and research\u000a      students in\u000a      BICOM have undertaken research into the computational modelling of\u000a      thermoforming processes\u000a      for thin polymeric sheets. Initially for oil-based materials, the research\u000a      covered the large\u000a      deformation of hot polymeric sheets into moulds in order to form\u000a      thin-walled packaging structures.\u000a      New applications were treated and numerical techniques based on the\u000a      theoretical work [1] were\u000a      proposed. These computational models required knowledge both of the\u000a      viscoelastic\/elasto-plastic\u000a      behaviour and of the material properties of the oil-based polymeric\u000a      materials. Design tools, based\u000a      on finite element models of forming processes, were delivered to the\u000a      British companies: CMB\u000a      Technology (CarnaudMetalBox plc); Autotype International Ltd; John\u000a      McGavigan Ltd [2]-[4]; and to\u000a      the US Army Research Laboratory, Langley. The state was reached whereby,\u000a      for many relevant\u000a      packaging shapes, the wall thicknesses of the structures could be\u000a      predicted for a range of\u000a      polymeric materials such as Bayfol&#174; and Polypropolene.\u000a    Over the past two decades it has become increasingly clear that oil-based\u000a      polymer packaging is\u000a      creating much non-biodegradable waste, which typically ends up at landfill\u000a      sites. With this in mind,\u000a      Whiteman and Warby collaborated with Professor J Song (Department of\u000a      Mechanical Engineering,\u000a      Brunel) on a project applying computational modelling and experimental\u000a      analysis to thermoforming\u000a      processes for bio-degradable thermoplastic starch sheets. This work,\u000a      funded by the Department\u000a      for Environment, Food and Rural Affairs from 2004 to 2008, brought\u000a      together the industrial\u000a      partners: Dassett Process Engineering Ltd; Marks and Spencer Plc; Heygates\u000a      Ltd; Leistritz\u000a      Extruders; Institute of Food Research; Pactiv Ltd and Northern Foods Plc.\u000a    At the time thermoplastic bio-materials were relatively new and, unlike\u000a      oil-based materials, their\u000a      material properties were not well understood. In particular, the behaviour\u000a      of starch-based bio-materials\u000a      during thermoforming is affected by their moisture content. The\u000a      researchers at BICOM\u000a      developed new computational models for the deformation of a bio-plastic\u000a      membrane during\u000a      thermoforming, under temperature and moisture content variation. The\u000a      parameters required for\u000a      these models were determined experimentally by Song and co-workers using a\u000a      thermoplastic\u000a      material sourced by Plantic plc. This research [5]-[8] provides an insight\u000a      into how different\u000a      parameters (including temperature and moisture content) affect the wall\u000a      thickness and other\u000a      properties (such as strength and stiffness) of a structure during\u000a      thermoforming. Such information is\u000a      essential for the design of moulds and processing of raw material while\u000a      manufacturing starch-based\u000a      bio-degradable food packaging.\u000a    "},{"CaseStudyId":"21197","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6251999","Name":"Canada"},{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Following the trial, the Children's Kidney Unit at the Royal Victoria\u000d\u000a        Hospital (RVI), Newcastle changed its policy, so that alteplase was\u000d\u000a      used in place of heparin as the routine `lock' (i.e. preventative dose of\u000d\u000a      anticoagulant) for venous central lines. In the intervening period, most\u000d\u000a      of the paediatric kidney units in the UK and Ireland have followed the\u000d\u000a      practice at Newcastle and switched to using alteplase. A blocked central\u000d\u000a      line is traumatic, causes delays in treatment and incurs costs for the NHS\u000d\u000a      through taking up the time of medical and nursing staff. The line is used\u000d\u000a      repeatedly and, without anticoagulation, the lumen would become occluded\u000d\u000a      by blood clots between treatments. Repeated blockage of central lines\u000d\u000a      requires re-siting, a traumatic procedure that cannot be repeated\u000d\u000a      indefinitely. If haemodialysis (HD) becomes impossible, the consequences\u000d\u000a      for the child are dire.\u000d\u000a    Impact in significant reduction of re-siting\u000d\u000a    Prior to the Newcastle trial, any child with a central line in for a year\u000d\u000a      would need to have it re-sited for thrombosis with probability 0.7. This\u000d\u000a      failure rate is consistent with rates found elsewhere [E1]. After our\u000d\u000a      research, from 2008 to 2012, similar numbers of children were treated at\u000d\u000a      Newcastle, but none of the lines has been replaced for thrombosis.\u000d\u000a      There are about 10 HD patients at any one time at the unit and each\u000d\u000a      patient needs about 50-150 doses of alteplase per annum, usually until a\u000d\u000a      renal transplant becomes available. Some indication of the financial cost\u000d\u000a      is given by Canadian figures, where one dose costs around $50 whilst the\u000d\u000a      cost of one catheter replacement is estimated to be around $1200, but may\u000d\u000a      incur a further $6000 for hospital treatment costs if there are\u000d\u000a      complications (caused by bacterial infections in a few percent of clotting\u000d\u000a      events) [E1]. The high cost of the drug is therefore partially offset by\u000d\u000a      savings in treatment regimen.\u000d\u000a    Impact on practice in other units\u000d\u000a    Following the research, information was obtained from all 14 paediatric\u000d\u000a      kidney units in the UK and Ireland [E2]. This revealed that 12 have\u000d\u000a      changed to using alteplase. Five centres (Newcastle, Cardiff, Guy's,\u000d\u000a      Glasgow, Manchester) use it as their routine treatment, three times\u000d\u000a      weekly: a further five centres (Nottingham, Belfast, Great Ormond Street,\u000d\u000a      Birmingham, Liverpool) use it three times weekly in some patients and in\u000d\u000a      others use it once per week with heparin on the other two occasions to\u000d\u000a      minimise the expense. The two centres, Dublin and Bristol, use heparin\u000d\u000a      initially, but move to using alteplase three times weekly if they\u000d\u000a      encounter any problems, which is quite common (for example, alteplase is\u000d\u000a      used for eight of the ten children currently treated in Dublin (June,\u000d\u000a      2012)). Typically, at any given time, the total number of children being\u000d\u000a      haemodialysed in these units is around 100.\u000d\u000a    Supply of alteplase\u000d\u000a    The letter [E2] provides evidence, in the form of an e-mail survey, of\u000d\u000a      the effect of the research on this usage of alteplase across the UK and\u000d\u000a      Ireland.\u000d\u000a    Confirmation that the preventative use of alteplase spread from the RVI\u000d\u000a      is provided by the distribution of alteplase from the RVI to other\u000d\u000a      hospitals. At the time of the research, alteplase was marketed as a\u000d\u000a      treatment for pulmonary emboli in adults and was supplied in much larger\u000d\u000a      quantities than required for paediatric HD. Since 2009, the RVI pharmacy\u000d\u000a      has been breaking 300mg ampoules down into 2mg doses suitable for locking\u000d\u000a      dialysis lines and these have been supplied to other NHS trusts. The table\u000d\u000a      below shows the number of alteplase units repackaged and issued by the RVI\u000d\u000a      to other NHS trusts [E3]. The numbers have increased substantially in\u000d\u000a      Newcastle for various reasons including some longer term HD patients. In\u000d\u000a      the other trusts, the numbers are reducing because small doses are now\u000d\u000a      available in a commercial product. This approach means that the drug costs\u000d\u000a      in Newcastle are substantially lower than the Canadian figure quoted\u000d\u000a      above.\u000d\u000a    \u000d\u000a      \u000d\u000a        \u000d\u000a          \u000d\u000a          Newcastle Hospitals\u000d\u000a          Other Trusts\u000d\u000a        \u000d\u000a        \u000d\u000a          Alteplase issued by the RVI 04\/2010-03\/2011\u000d\u000a          1692 UNITS\u000d\u000a          5350 UNITS\u000d\u000a        \u000d\u000a        \u000d\u000a          Alteplase issued by the RVI 04\/2011-03\/2012\u000d\u000a          2470 UNITS\u000d\u000a          3491 UNITS\u000d\u000a        \u000d\u000a      \u000d\u000a    \u000d\u000a    Prevention of harm\u000d\u000a    Further reinforcement of the importance of changing to alteplase is given\u000d\u000a      in [E4] which notes that \"Clots form easily in relatively large central\u000d\u000a        lines, such as those used for haemodialysis, presumably because blood\u000d\u000a        enters the distal lumen\", and that \"Owing to the risk of\u000d\u000a        pulmonary embolus, it is routine practice for children's dialysis lines\u000d\u000a        to be aspirated before every use &#8212; it is apparent that small subclinical\u000d\u000a        clots must be occurring, as evidenced by cardiac and post-mortem studies\u000d\u000a        in children with lines for at least 3 months\".\u000d\u000a    The significance of the problem is also indicated in the article:\u000d\u000a        \"Cumulatively, these small clots may cause morbidity. A child having a\u000d\u000a        clot weighing about 14mg dislodged on 60% of days would have 3g of\u000d\u000a        thrombus showered into their lungs annually.......If cumulative small\u000d\u000a        thrombi did cause the gradual development of pulmonary damage, this\u000d\u000a        might be difficult to detect, or even suspect clinically, especially in\u000d\u000a        children whose other serious disorders had necessitated the use of a\u000d\u000a        long-term central line\".\u000d\u000a    Use of alteplase for locking dialysis lines is beginning to spread to\u000d\u000a      other countries. For example, a US review article from 2012 on tunnelled\u000d\u000a      catheters (TC) [E5], cites Newcastle research in concluding that \"Since\u000d\u000a        the introduction of TCs in the late 1980s, heparin catheter lock has\u000d\u000a        been the standard prophylactic regimen for the prevention of TC\u000d\u000a        dysfunction. More recently, alternative catheter locking agents have\u000d\u000a        emerged, and in some cases have shown to be superior to heparin lock\u000d\u000a        with respect to improving TC patency and reducing TC-associated\u000d\u000a        infections.\"\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Following research carried out at Newcastle, a new anticoagulant is used\u000d\u000a      in 12 of the 14 paediatric kidney units in the UK and Ireland. Substantial\u000d\u000a      distress and delay to therapy can be caused to children undergoing\u000d\u000a      haemodialysis when the central venous lines (CVLs), by which their\u000d\u000a      treatment is delivered, are blocked by blood clots. Our research has shown\u000d\u000a      that preventative use of a new anti-coagulant, alteplase (also known as\u000d\u000a      Rt-PA), is much more successful than the traditional agent, heparin, in\u000d\u000a      preventing blockages. The clinical trial which established the superiority\u000d\u000a      of alteplase required a novel form of optimal crossover design. In one\u000d\u000a      hospital, the annual probability of CVL replacement due to thrombosis was\u000d\u000a      0.7 prior to our work. During the reporting period, no lines have had to\u000d\u000a      be replaced because of thrombosis. This represents a remarkable reduction\u000d\u000a      in the levels of distress to children and allows haemodialysis.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    Newcastle University\u000d\u000a    ","Institutions":[{"AlternativeName":"Newcastle upon Tyne (University of)","InstitutionName":"Newcastle University","PeerGroup":"A","Region":"North East","UKPRN":10007799}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2964574","Name":"Dublin City"},{"GeoNamesId":"2964574","Name":"Dublin"}],"References":"\u000d\u000a    \u000a[P1] Matthews, J. N. S. (1994). Modelling and optimality in the design of\u000d\u000a      crossover studies for medical applications. Journal of Statistical\u000d\u000a      Planning and Inference, 42(1), 89-108. (Google scholar: 29 citations)\u000d\u000a      (Impact Factor: 0.713) [*Key reference]\u000d\u000a    \u000a\u000a[P2] Matthews, J. N. S. (1994). Multi-period crossover trials.\u000d\u000a      Statistical Methods in Medical Research, 3(4), 383-405. (Google scholar:\u000d\u000a      18 citations) (Impact Factor:2.364 ) [*Key reference]\u000d\u000a    \u000a\u000a[P3] Matthews, J. N. S. (2013) An optimal multi-period crossover design\u000d\u000a      for an application in paediatric nephrology. Statistics in Medicine,\u000d\u000a      doi:10.1002\/sim.5981. (Impact Factor: 2.044) [*Key reference]\u000d\u000a    \u000a\u000a[P4] Gittins, N. S., Hunter-Blair, Y. L., Matthews, J. N., &amp;\u000d\u000a      Coulthard, M. G. (2007). Comparison of alteplase and heparin in\u000d\u000a      maintaining the patency of paediatric central venous haemodialysis lines:\u000d\u000a      a randomised controlled trial. Archives of Disease in Childhood, 92(6),\u000d\u000a      499-501. doi: 10.1136\/adc.2006.100065 (Google scholar: 19 citations)\u000d\u000a      (Impact Factor: 3.051)\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"11","Level2":"3","Subject":"Clinical Sciences"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"}],"Sources":"\u000d\u000a    [E1] Brenda R. Hemmelgarn, M.D., Ph.D., Louise M. Moist, M.D., Charmaine\u000d\u000a      E. Lok, M.D., Marcello Tonelli, M.D., S.M., Braden J. Manns, M.D., Rachel\u000d\u000a      M. Holden, M.D., Martine LeBlanc, M.D., Peter Faris, Ph.D., Paul Barre,\u000d\u000a      M.D., Jianguo Zhang, M.Sc., and Nairne Scott-Douglas, M.D., Ph.D., (2011),\u000d\u000a      \"Prevention of Dialysis Catheter Malfunction with Recombinant Tissue\u000d\u000a        Plasminogen Activator\", New England Journal of Medicine. 364,\u000d\u000a      303-312.\u000d\u000a    [E2] Corroboration from Honorary consultant paediatric nephrologist, NHS.\u000d\u000a    [E3] Corroboration from Pharmacist, Newcastle Hospitals NHS Trust.\u000d\u000a    [E4] Short Report. Malcolm G Coulthard and Roderick Skinner, (2007) \"Should\u000a        paediatric central lines be aspirated before use?\", Archives of\u000d\u000a      Disease in Childhood, 92(6): 517-518. doi: 10.1136\/adc.2006.100073\u000d\u000a    [E5] Review Article. Timmy Lee, Charmaine Lok, Miguel Vazquez, Louise\u000d\u000a      Moist, Ivan Maya, and Michele Mokrzycki, (2012), \"Minimizing\u000d\u000a        Hemodialysis Catheter Dysfunction: An Ounce of Prevention\".\u000d\u000a      International Journal of Nephrology. Article ID 170857,\u000d\u000a      doi:10.1155\/2012\/170857\u000d\u000a    ","Title":"\u000d\u000a    Preventing Blood Clots in Children undergoing Kidney Dialysis\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2641170","Name":"Nottingham"},{"GeoNamesId":"2655603","Name":"Birmingham"},{"GeoNamesId":"2653822","Name":"Cardiff"},{"GeoNamesId":"2643123","Name":"Manchester"},{"GeoNamesId":"2648579","Name":"Glasgow"},{"GeoNamesId":"2655984","Name":"Belfast"},{"GeoNamesId":"2644210","Name":"Liverpool"},{"GeoNamesId":"2654675","Name":"Bristol"},{"GeoNamesId":"2641673","Name":"Newcastle-upon-Tyne"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"},{"GeoNamesId":"2641364","Name":"Northern Ireland"},{"GeoNamesId":"2634895","Name":"Wales"},{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Matthews, a member of staff at Newcastle University (from 1987 to the\u000d\u000a      present), devised and applied the statistical methodology for a study to\u000d\u000a      help improve a treatment for children with renal failure by comparing\u000d\u000a      different anticoagulants used to maintain patency in central venous lines\u000d\u000a      used for haemodialysis.\u000d\u000a    Peritoneal dialysis is the preferred treatment for children with renal\u000d\u000a      failure but when this is not suitable, or has failed, haemodialysis is the\u000d\u000a      only other option. Haemodialysis requires attendance at the dialysis unit\u000d\u000a      two or three times weekly, where the patient's blood is dialysed\u000d\u000a      externally. Access to the circulation is usually through an in-dwelling\u000d\u000a      venous central line and it is paramount that this line is kept free from\u000d\u000a      clots between treatments.\u000d\u000a    The assessment of any aspect of paediatric haemodialysis is likely to be\u000d\u000a      constrained by the limited number of patients available for study.\u000d\u000a      However, as they are obliged to attend often, the shortage of patients is\u000d\u000a      partly compensated by observing each patient many times. As such the\u000d\u000a      application is ideal for a crossover design [P1]. However, fewer than 10\u000d\u000a      patients, but who can be studied on tens of occasions, is an extreme case\u000d\u000a      even for a crossover trial. Designs using more than six periods are\u000d\u000a      unusual, and there are very few in the literature for more than 12 periods\u000d\u000a      [P2]. Consequently this study required a specially tailored 30-period\u000d\u000a      design to be derived using optimal design theory [P3].\u000d\u000a    The bespoke crossover clinical trial compared the standard anticoagulant\u000d\u000a      (heparin) with alteplase in haemodialysis patients at the Children's\u000d\u000a      Kidney Unit in the Royal Victoria Infirmary (RVI) in Newcastle in\u000d\u000a      2005 [P4]. The aim was to reduce the weight of clot removed from the line\u000d\u000a      prior to the next treatment. The model for the data assumed that the mean\u000d\u000a      response allowed for the possibility of (i) systematic differences between\u000d\u000a      patients and (ii) the treatment allocated. The modeling of the period\u000d\u000a      effect, a topic discussed in references [P1] &amp; [P2] below, required a\u000d\u000a      special formulation which took account of the point in the dialysis cycle\u000d\u000a      when the observation was made. The information matrix for the treatment\u000d\u000a      effect depends on the design matrices for treatment, dialysis cycle and\u000d\u000a      patient, respectively. The information in the reduced model, where patient\u000d\u000a      terms are omitted, exceeds that in the full model, unless a form of\u000d\u000a      orthogonality applies to the components of the design matrix. The research\u000d\u000a      thereby identified designs which maximised the information from the full\u000d\u000a      model. These designs required equal replication of the treatments on each\u000d\u000a      patient and equal replication on each day of the dialysis cycle. Further\u000d\u000a      details can be found in [P3].\u000d\u000a    The trial found [P4] that a clot was 2.4 times more likely to form when\u000d\u000a      the line was treated with heparin than alteplase and, if a clot did form,\u000d\u000a      it was 1.9 times heavier when heparin had been used.\u000d\u000a    The trial was conducted by Dr M.G.Coulthard (Consultant Paediatric\u000d\u000a      Nephrologist), Dr N.S.Gittins (Research Specialist Registrar) and Mr\u000d\u000a      Y.L.Hunter-Blair (Senior Pharmacist) all of Newcastle Hospitals NHS Trust\u000d\u000a      and Professor J.N.S.Matthews (Professor of Medical Statistics, School of\u000d\u000a      Mathematics and Statistics, Newcastle University).\u000d\u000a    "},{"CaseStudyId":"21204","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Royal Society"],"ImpactDetails":"\u000d\u000a    National Grid (NG) provides the gas transmission infrastructure\u000d\u000a      for the entire country whilst gas\u000d\u000a      distribution networks link this infrastructure to users. Within this\u000d\u000a      transmission system, a sustained\u000d\u000a      level of discrepancy exists in the accounting of gas, leading to Unaccounted\u000d\u000a        for Gas (UAG).\u000d\u000a      Identifying and reducing UAG is of major importance to NG and its\u000d\u000a      regulator, the Office of Gas and\u000d\u000a        Electricity Markets (OFGEM) [E3], as it is a sign of problems such\u000d\u000a      as meter error and results in\u000d\u000a      accrued costs and potentially unfair billing. In 2009\/10, the volume of\u000d\u000a      UAG was estimated at\u000d\u000a      7720GWh (total cost of &#163;100million [E1. Pg1]). Tracking down the source of\u000d\u000a      UAG can be difficult\u000d\u000a      as the causes may include shrinkage, weaknesses in measurement or\u000d\u000a      accounting processes.\u000d\u000a    The research conducted by the Industrial Statistics Research Unit\u000d\u000a      (ISRU) at Newcastle University\u000d\u000a      has produced effective data mining methods for revealing hidden patterns\u000d\u000a      and detecting their\u000d\u000a      causes. The resulting tailored analysis and data mining software has\u000d\u000a      changed the way\u000d\u000a      professionals utilise the data [E4] making it feasible to extract valuable\u000d\u000a      information from operational\u000d\u000a      gas flow data and help National Grid (NG) deal with the issue of Unaccounted\u000d\u000a        for Gas. Although\u000d\u000a      current methods employed by their staff were effective, NG felt that they\u000d\u000a      could be improved and\u000d\u000a      invited Coleman (from 2008 onwards) to explore the application of\u000d\u000a      statistics and data mining on\u000d\u000a      energy balance data to gain further advantage.\u000d\u000a    Newcastle University has helped NG significantly reduce the volume of UAG\u000d\u000a    with benefits that\u000d\u000a    reach to all providers and users of gas throughout the UK. The research has\u000d\u000a    led to a step change\u000d\u000a    in the processes that NG use [E2, E4] to utilise their data and the related\u000d\u000a    working practices. The\u000d\u000a    new decision tree analysis methods helped identify a source of UAG in 2010,\u000d\u000a    resulting in &#163;14\u000d\u000a    million being returned to the community in a single year [E1, pg. 4]. As a\u000d\u000a    result of the benefits\u000d\u000a    achieved by initial changes, NG have adopted these procedures and\u000d\u000a    staff-training in the statistical\u000d\u000a    and data mining methods developed at Newcastle University have been\u000d\u000a    introduced ensuring our\u000d\u000a    work continues to have a lasting effect. The 2013 UAG report shows a further\u000d\u000a    reduction of over 20\u000d\u000a    million kWh of UAG since 2010 [E2, pg. 10] and notes the discussions with\u000d\u000a    ISRU and the potential\u000d\u000a    for further investigation to identify UAG causes.\u000d\u000a    In addition to the benefit of better accounting, some further impacts are\u000d\u000a      presented below:\u000d\u000a    Improving accuracy of leakage estimates\u000d\u000a    Our work validating the collection of information to determine accurately\u000d\u000a      the volume of gas lost\u000d\u000a      from the system due to leakage from assets and the statistical findings\u000d\u000a      provided a sound basis for\u000d\u000a      investment in leakage strategies. The leakage rates were used to develop a\u000d\u000a      shrinkage model\u000d\u000a      (National Leakage Reduction Monitoring Model) which is currently used by\u000d\u000a      all of the UK Gas\u000d\u000a      Distribution Networks, to estimate the impact of mains replacement on\u000d\u000a      leakage, as corroborated in\u000d\u000a      [E5].\u000d\u000a    In [E6, pg. 35, 2011], it is stated: \"Given the rigorous assessment of\u000d\u000a        the sampling, collection and\u000d\u000a        analysis of results carried out in the National Leakage Survey 2002\/3,\u000d\u000a        and the assessment by the\u000d\u000a        ISRU, the AUGE believes the leakage rates used by the GTs for the\u000d\u000a        calculation of shrinkage are\u000d\u000a        reliable and unbiased\" and \"The leakage rates have a 90%\u000d\u000a        confidence interval of &#177;19.4%\". AUGE\u000d\u000a      is the Allocation of Unidentified Gas Expert and GT refers to Gas\u000d\u000a      Transporters (including Gas\u000d\u000a      Distribution Networks whose assets link NG's transmission pipelines to end\u000d\u000a      users). This work, in\u000d\u000a      2010 has been highlighted as an exemplar of research with direct economic\u000d\u000a      impact by the Russell\u000d\u000a      Group [E7, pg. 15].\u000d\u000a    Improving the safe management of the system\u000d\u000a    Our work has been instrumental in achieving a transformation in the use\u000d\u000a      of statistics to support\u000d\u000a      operational and management decision-making [E2, pg. 23]. Newcastle\u000d\u000a      University's Statistical\u000d\u000a        Process Control (SPC) charts provide immediate insight of safety\u000d\u000a      performance by charting\u000d\u000a      measurements with control limits. Dashboard presentation continues to be\u000d\u000a      valued as corroborated\u000d\u000a      in [E4] which states that SPC charts are \"a powerful management\u000d\u000a        decision making tool\" and that\u000d\u000a      the methods \"continue to have a positive impact on the safe and\u000d\u000a        efficient management of the UK\u000d\u000a        gas transmission system\".\u000d\u000a    Improved monitoring of UAG levels &amp; better stakeholder relations\u000d\u000a    We created a process flow map for UAG used by NG [E3] which includes all\u000d\u000a      steps in the gas\u000d\u000a      transmission process; each step has been examined in terms of stakeholders\u000d\u000a      and influences. Our\u000d\u000a      work is essential because it helps distinguish periods when UAG is stable\u000d\u000a      and periods where there\u000d\u000a      are statistically significant changes. Therefore when a significant change\u000d\u000a      arises, staff can identify\u000d\u000a      the likely causes and prioritise which meters to investigate; they have\u000d\u000a      evidence to empower them,\u000d\u000a      which has led to a better dialogue with meter owners and more effective\u000d\u000a      problem solving.\u000d\u000a    NG professionals have used our research findings to be confident in their\u000d\u000a      estimates of the\u000d\u000a      contribution of UAG from the transmission system and to develop effective\u000d\u000a      communication\u000d\u000a      between the different stakeholders. The software and statistical advice\u000d\u000a      continue to have a positive\u000d\u000a      impact on the assessment and exploration of UAG as corroborated in [E3].\u000d\u000a    Newcastle University research has contributed to NG's good relations with\u000d\u000a      OFGEM and the Health\u000d\u000a      and Safety Executive. Our work has led to a high standard of data\u000d\u000a      investigation, and to NG being\u000d\u000a      credited for their sound scientific approach. Our reports have been used\u000d\u000a      to show how serious NG\u000d\u000a      are about continuously improving their processes, particularly during\u000d\u000a      negotiations on pricing and\u000d\u000a      other control policies, for example [E6, see pg. 35 &amp; 36].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Statistical research undertaken by the Industrial Statistics Research\u000d\u000a        Unit (ISRU) at Newcastle\u000d\u000a      University has led to improved accounting of gas in the national\u000d\u000a      transmission system provided by\u000d\u000a      National Grid. A discrepancy, known as unaccounted for gas\u000d\u000a        (UAG), results in accrued costs and\u000d\u000a      potentially unfair billing. In 2009\/10, UAG is estimated to have cost &#163;100\u000d\u000a      million. National Grid has\u000d\u000a      adopted our research results by making fundamental changes in their data\u000d\u000a      utilisation with benefits\u000d\u000a      that reach all the distributers and users of gas throughout the UK. In\u000d\u000a      2010 our methods helped\u000d\u000a      identify a source of UAG, resulting in &#163;14 million being returned\u000d\u000a      to the community. Our reports\u000d\u000a      inform decisions made by regulators and provide data-based evidence to\u000d\u000a      support negotiations\u000d\u000a      between national transmission, local distributors and users.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    Newcastle University\u000d\u000a    ","Institutions":[{"AlternativeName":"Newcastle upon Tyne (University of)","InstitutionName":"Newcastle University","PeerGroup":"A","Region":"North East","UKPRN":10007799}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a[P1] Gardner, J.A., S.Y. Coleman and S.G. Farrow (1994), Start with\u000d\u000a        SPC and Save, Save, Save,\u000d\u000a      Analytical Proceedings of the Royal Society of Chemistry, 31, 71-74.\u000d\u000a    \u000a\u000a[P2] Malone R., Stewardson D., Nelson T. &amp; Bonta J. (1999), Calibration\u000d\u000a        &amp; Quality Control of the\u000d\u000a        Coshocton Weighing Lysimeters, Transactions of the American Society\u000d\u000a      of Agricultural Engineers\u000d\u000a      (Trans ASAE), 42(3) pp701-712, ISSN 0001-2351.\u000d\u000a    \u000a\u000a[P3] Stewardson, D.J. and S.Y.Coleman (2001) Using the summed rank\u000d\u000a        cusum for monitoring\u000d\u000a        environmental data from industrial processes, J. Applied Statistics,\u000d\u000a      28, 469-484.\u000d\u000a    \u000a\u000a[P4] Coleman, S. Y., Gordon, A., &amp; Chambers, P. R. (2001). SPC\u000d\u000a        making it work for the gas\u000d\u000a        transportation industry. J. Applied Statistics, 343-351. [*Key\u000d\u000a      reference]\u000d\u000a    \u000a\u000a[P5] Chambers, P. R., Piggott, J., &amp; Coleman, S. Y. (2001). SPC &#8212;\u000d\u000a        a team effort for process\u000d\u000a        improvement across four Area Control Centres. J. Applied Statistics,\u000d\u000a      307-324. [*Key reference]\u000d\u000a    \u000a\u000a[P6] Coleman, S. Y., Arunakumar, G., Foldvary, F., &amp; Feltham, R.\u000d\u000a      (2001). SPC as a tool for\u000d\u000a        creating a successful business measurement framework. J. Applied\u000d\u000a      Statistics, 325-334. [*Key\u000d\u000a      reference]\u000d\u000a    \u000aAs a result of successful collaboration, Newcastle University were\u000d\u000a      awarded successive contracts in\u000d\u000a      excess of &#163;150,000 for research into the causes of variation in UAG and\u000d\u000a      the application of data\u000d\u000a      mining methods.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    [E1] Open letter from National Grid to Shippers, Suppliers, Customers,\u000d\u000a      Transmission and\u000d\u000a      Transportation System Owners and other Interested Parties (2011)\u000d\u000a      http:\/\/www.nationalgrid.com\/NR\/rdonlyres\/07E7A1E2-7982-48FE-9A5D-F6ACB634F49D\/47329\/UAGIndustryUpdateJune2011.pdf.\u000d\u000a      [accessed 13\/02\/13]\u000d\u000a    [E2] Unaccounted for gas report, National Grid (2013).\u000d\u000a      http:\/\/www.nationalgrid.com\/NR\/rdonlyres\/6C31A7B2-6F15-4665-ACCD-BAA61BB98563\/58740\/UAGReportFebruary2014.pdf.\u000d\u000a      [accessed 22\/04\/13] This document is\u000d\u000a        published to meet special condition C29: Requirement to undertake\u000d\u000a        projects to investigate the\u000d\u000a        causes of Unaccounted for Gas (UAG).\u000d\u000a    [E3] Letter of support from UAG Project Manager, National Grid (2011).\u000d\u000a    [E4] Letter of support and comments regarding on-going use of SPC in\u000d\u000a      system control from Rune\u000d\u000a        Associates (2011).\u000d\u000a    [E5] Corroboration from Senior consultant, Utilities Practice, GL Noble\u000d\u000a      Denton (2013).\u000d\u000a    [E6] Report: Allocation of Unidentified Gas Statement, GL Noble\u000d\u000a      Denton (2011),\u000d\u000a      http:\/\/www.gasgovernance.co.uk\/sites\/default\/files\/2nd%20Draft%20AUGS%202011%202.0.pdf.\u000d\u000a      [accessed 13\/02\/13].\u000d\u000a    [E7] Case study 9: A statistical methodology for asset surveys. In The\u000d\u000a        economic impact of\u000d\u000a        research conducted in Russell Group universities. Russell Group\u000d\u000a      Papers &#8212; Issue 1, Page 15.\u000d\u000a    Russell Pioneering Research Group, (2010).\u000d\u000a      www.russellgroup.ac.uk\/uploads\/RG_ImpactOfResearch2.pdf.\u000d\u000a      [accessed 17\/07\/13]\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Faster Fault Tracking for National Grid Gas\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2641673","Name":"Newcastle-upon-Tyne"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The Statistical Process Control (SPC) and data mining techniques\u000d\u000a      underpinning the fault-tracking\u000d\u000a      procedures now employed by National Grid for their gas transmission system\u000d\u000a      were developed by\u000d\u000a      Coleman and Stewardson, members of the Industrial Statistics Research\u000d\u000a        Unit (ISRU) at Newcastle\u000d\u000a      University. In the period 1993-2001, new SPC methods were\u000d\u000a      developed via research with unstable\u000d\u000a      chemical processes [P1], agricultural equipment (with collaborators Malone\u000d\u000a      R., Nelson T. &amp; Bonta\u000d\u000a      J., from USA) [P2], environmental data [P3] and business processes. One of\u000d\u000a      the special features of\u000d\u000a      this type of SPC was that it involved modelling individual measurements\u000d\u000a      rather than the so-called\u000d\u000a      rational sub-groups of measurements used in manufacturing. Coleman further\u000d\u000a      developed this type\u000d\u000a      of SPC for National Grid (NG) System Control, giving special consideration\u000d\u000a      to issues such as\u000d\u000a      prioritising error detection over the cost of responding to action signals\u000d\u000a      [P4]. Our SPC methodology\u000d\u000a      was successfully adopted by the four UK area gas control centres and was\u000d\u000a      later jointly owned by\u000d\u000a      Gas National Control Centre and Distribution Networks Control Centre to\u000d\u000a      provide statistical\u000d\u000a      monitoring of the movement of gas to detect whether processes are running\u000d\u000a      as efficiently as\u000d\u000a      possible [P5].\u000d\u000a    Our SPC for a safety measurement framework, developed in collaboration\u000d\u000a      with NG [P6], required\u000d\u000a      special consideration of short start-up times for annually collected data\u000d\u000a      and low event rate for\u000d\u000a      incident statistics, varying reporting periods, differences in data\u000d\u000a      availability and quality, and issues\u000d\u000a      of operational ownership. Our SPC work gave us considerable depth of\u000d\u000a      knowledge of the gas\u000d\u000a      transmission system. Further applied statistics projects included\u000d\u000a      statistical analysis of extreme load\u000d\u000a      values in 2004 with Transco (forerunner of NG), and Monte Carlo testing\u000d\u000a      and surveying of assets\u000d\u000a      with Advantica in 2003 and 2008. (Advantica was formed from BG Technology\u000d\u000a      which employed the\u000d\u000a      technical staff of Transco). These projects led to our successful on-going\u000d\u000a      research with NG (from\u000d\u000a      2005) and the impacts from 2008.\u000d\u000a    Vast quantities of gas flow data, temperature, pressure and chemical\u000d\u000a      measurements are collected\u000d\u000a      every second by NG and used to control the system within strict\u000d\u000a      guidelines. Gas flows from north\u000d\u000a      to south and from east to west and can be stored offshore, in containers\u000d\u000a      or by increasing pressure\u000d\u000a      in pipelines. Gas flow data are seasonal and diurnal; the various meter\u000d\u000a      readings give rise to\u000d\u000a      multivariate, time dependent data. Unaccounted-for-gas (UAG) is the\u000d\u000a      difference between meter\u000d\u000a      readings from multiple inputs (approx. 30) and multiple outputs (approx.\u000d\u000a      200). It is of paramount\u000d\u000a      importance to NG to control and reduce UAG.\u000d\u000a    NG needs to identify meter errors causing changes in UAG and to detect\u000d\u000a      patterns in UAG data\u000d\u000a      both to improve their service provision and to meet regulatory\u000d\u000a      obligations. Our time series analysis\u000d\u000a      identified seasonal and temporal effects. Our application of techniques\u000d\u000a      including principal\u000d\u000a      components, regression, decision tree and cluster analysis is new for this\u000d\u000a      type of auto-correlated,\u000d\u000a      multivariate data and required special adaptation in terms of stopping\u000d\u000a      rules and sub-division of\u000d\u000a      clusters. We were able to highlight patterns in the data and provide\u000d\u000a      evidence to support the\u000d\u000a      decision to prioritise and carry out expensive investigations.\u000d\u000a    "},{"CaseStudyId":"21207","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000d\u000a    The amount of nuclear material transferred into and out of the THORP\u000d\u000a      nuclear reprocessing plant in Cumbria is determined through the vessel\u000d\u000a      calibration methodology developed in Newcastle and adopted by BNFL. THORP\u000d\u000a      has the capacity to reprocess 1200 tonnes of nuclear fuel per year, which\u000d\u000a      is 20% of the world's total current annual reprocessing capacity [E1].\u000d\u000a      Since 2008 the amount of nuclear material input into the plant has been\u000d\u000a      determined exclusively through the level\/volume methodology developed in\u000d\u000a      Newcastle. Our methods have ensured that the plant can comply with\u000d\u000a      regulator's requirements and has been able to operate throughout the REF\u000d\u000a      period.\u000d\u000a    Originally a dual system (weight or level\/volume) provided built-in\u000d\u000a      redundancy for nuclear materials accountancy. In 2005 a pipe from a\u000d\u000a      receipt vessel fractured in the containment area. Operations were\u000d\u000a      suspended until 2008 and since restart only the level\/volume systems have\u000d\u000a      been available for determination of nuclear receipts. These rely entirely\u000d\u000a      on the calibration methodology developed in Newcastle. The level\/volume\u000d\u000a      receipt systems were revalidated by Henderson and industry colleagues in\u000d\u000a      2009\/10. Confidence in the systems was confirmed [E2] and operations at\u000d\u000a      THORP have been allowed to continue.\u000d\u000a    1. Safety and environmental impacts\u000d\u000a    Close control of nuclear material is important to prevent the adverse\u000d\u000a      safety and environmental impacts that would follow accidental release of\u000d\u000a      radioactive material.\u000d\u000a    The 2005 leak was detected through nuclear material accountancy based on\u000d\u000a      the vessel calibrations carried out using the Newcastle methodology. In\u000d\u000a      [E3], Section 33, the Health and Safety Executive stated that \"The HSE\u000d\u000a        investigation found that it was not the installed leak detection systems\u000d\u000a        that led to the discovery of the leak. It was the analysis of nuclear\u000d\u000a        materials accountancy (NMA) discrepancies at the end of several fuel\u000d\u000a        shearing campaigns that led to the detailed investigations.... and\u000d\u000a        subsequently to the discovery of the leak.\"\u000d\u000a    2. Security impacts\u000d\u000a    Over 6000 tonnes of spent nuclear fuel from nine countries have passed\u000d\u000a      through THORP calibrated vessels since commissioning. Close control of\u000d\u000a      this fuel is extremely important for quickly identifying any security\u000d\u000a      breaches or misuse of nuclear materials. The International Atomic Energy\u000d\u000a      Agency (IAEA) is charged with monitoring civil programmes in order to ``establish\u000a        and administer safeguards designed to ensure that special fissionable\u000d\u000a        and other materials ... are not used in such a way as to further any\u000d\u000a        military purpose\" [E4].\u000d\u000a    One aspect of this safeguards work is material accountancy, by which\u000d\u000a      records of movements of material are kept and regular inventories taken.\u000d\u000a      The main receipt vessels at THORP are the Head End Accountancy Tanks\u000d\u000a      (HEATs), which are used for input determination. The main issue vessels\u000d\u000a      for plutonium product (HARPs) are used for output determination. Given\u000d\u000a      their importance, the HEAT and HARP vessels were designed to have dual\u000d\u000a      accountancy systems: a primary system based on weight and a secondary\u000d\u000a      system based on volume\/level. Highly accurate weighing systems were\u000d\u000a      installed and calibrated using fairly straightforward models developed in\u000d\u000a      Newcastle during 1993-1994. The more problematic and statistically\u000d\u000a      challenging volumetric systems were installed, calibrated and recalibrated\u000d\u000a      in 2009\/10 using models described in the underpinning research above.\u000d\u000a      Throughout, the European Atomic Energy Community (Euratom), which acts for\u000d\u000a      IAEA in Europe, approved the methods and results [E5].\u000d\u000a    In [E6], the THORP materials custodian says \"The volumetric\u000d\u000a        calibrations of the 12 main process vessels, which are also key\u000d\u000a        measurement points for accountancy purposes, continue to be used on a\u000d\u000a        daily basis and to pass the daily revalidation exercises\". The\u000d\u000a      Newcastle work is key to this.\u000d\u000a    3. Commercial costing impacts\u000d\u000a    THORP is one of only three commercial light water reactor fuel\u000d\u000a      reprocessing plants in the world [E7]. Customers are charged according to\u000d\u000a      the quantities of material received, as measured in the receipt vessels,\u000d\u000a      and THORP charges purchasing customers according to the amount of product,\u000d\u000a      as measured at issue. The material of main interest is plutonium, which is\u000d\u000a      generated during reprocessing of spent nuclear fuel. Extremely accurate\u000d\u000a      measurement is necessary, with relative standard deviations of the order\u000d\u000a      of 0.1% for plutonium being required. The Newcastle calibrations delivered\u000d\u000a      this accuracy. Bias in the measurement system can lead to commercial\u000d\u000a      charges measured in millions of pounds. Exact charges and costs are\u000d\u000a      commercially confidential.\u000d\u000a    In summary, our volumetric system continues to be the sole means of\u000d\u000a      determining transfers of nuclear materials into or out of THORP.\u000d\u000a      Operations could have not have continued after 2008 without regulatory\u000d\u000a      approval of these accurate and reliable accountancy systems.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The Thermal Oxide Reprocessing Plant (THORP) at Sellafield in Cumbria has\u000d\u000a      20% of the world's current annual nuclear reprocessing capacity.\u000d\u000a      Statistical methods developed in Newcastle during commissioning of THORP\u000d\u000a      are integral to the nuclear material accountancy systems that are used in\u000d\u000a      all stages of reprocessing. Since 2008 a Newcastle volumetric calibration\u000d\u000a      system has been the only means of determining input into the plant.\u000d\u000a      Regulatory approval of the system has ensured that THORP has been able to\u000d\u000a      operate throughout the REF period, that customer costing has been\u000d\u000a      accurate, and that the plant has complied with international standards for\u000d\u000a      the close control of nuclear materials.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    Newcastle University\u000d\u000a    ","Institutions":[{"AlternativeName":"Newcastle upon Tyne (University of)","InstitutionName":"Newcastle University","PeerGroup":"A","Region":"North East","UKPRN":10007799}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    (ESARDA is the refereed proceedings of the biennial European\u000d\u000a        Safeguards conference, the primary communication method for scientific\u000d\u000a        developments in the area).\u000d\u000a    \u000a[P1] Henderson, R., McKnespiey, P.N. and Temple, A. (1995). The\u000d\u000a      volumetric calibration of tanks: design of trials. ESARDA, 17, 365-369.\u000d\u000a    \u000a\u000a[P2] McKnespiey, P.N. Henderson, R. and Temple, A. (1995). Volumetric\u000d\u000a      calibration: use of in-tank density determination. ESARDA 17, 371-375.\u000d\u000a    \u000a\u000a[P3] Henderson, R., Temple, A. and McKnespiey, P. (1997). Computer\u000d\u000a      intensive inference for calibration curves: experience at BNF THORP.\u000d\u000a      ESARDA 19, 753-758.\u000d\u000a    \u000a\u000a[P4] Morton-Jones, A., Henderson, R., Hunt, B. and Binks, K. (1999).\u000d\u000a      Optimal control of pressure measurements during volumetric calibration.\u000d\u000a      ESARDA, 21, 285-289.\u000d\u000a    \u000a\u000a[P5] Henderson, R., Morton-Jones, A. and McKnespiey, P. (2000).\u000d\u000a      Reversible jump MCMC for volumetric calibration. Journal of the Royal\u000d\u000a      Statistical Society, Series C (Applied Statistics), 49 (4), pp. 563-576.\u000d\u000a      ISSN 1467-9876 [* Key reference].\u000d\u000a    \u000a\u000a[P6] Morton-Jones, A. and Henderson, R. (2000). Generalized least squares\u000d\u000a      with ignored errors in variables. Technometrics, 42(4), pp.366-375. DOI:\u000d\u000a      10.1080\/00401706.2000.10485709. (American Society for Quality and the\u000d\u000a        American Statistical Association)\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    [E1] \"Reprocessing Plants, Worldwide\", European Nuclear Society\u000d\u000a      Encyclopaedia (2012).\u000d\u000a      http:\/\/www.euronuclear.org\/info\/encyclopedia\/r\/reprocessing-plants-ww.htm,\u000d\u000a      [accessed 04\/09\/2013]\u000d\u000a    [E2] Clarke, C.G, Chater, S.P. and Henderson, R. (2010). \"Review of\u000d\u000a        HEAT A liquor measurement performance using volume density system\".\u000d\u000a      Sellafield Ltd Technical Report TTC\/10\/707 (N).\u000d\u000a    [E3] \"Report of the investigation into the leak of dissolver product\u000d\u000a        liquor at the Thermal Oxide Reprocessing Plant (THORP), Sellafield,\u000d\u000a        notified to HSE on 20 April 2005\", Health and Safety Executive\u000d\u000a      (2007), available at: http:\/\/www.hse.gov.uk\/nuclear\/periodic-safety-review\/thorp.htm.\u000d\u000a      [accessed 04\/09\/2013]\u000d\u000a    [E4] \"IAEA Statutes Article III.A.5\", International Atomic Energy\u000d\u000a      Agency (2013) http:\/\/www.iaea.org\/About\/statute.html,\u000a        [accessed 04\/09\/2013]\u000d\u000a    [E5] Corroboration from the Head of Nuclear Fuel Cycle Analysis Section,\u000d\u000a      Division of Information Management, Department of Safeguards,\u000d\u000a      International Atomic Energy Agency &amp; former Euratom inspector.\u000d\u000a    [E6] Corroboration from the Material Custodian, Thorp Chemical Plants,\u000d\u000a      Sellafield Ltd, March 2011.\u000d\u000a    [E7] \"Processing of Used Nuclear Fuel\" (2012) World Nuclear\u000d\u000a      Association, http:\/\/www.world-\u000a        nuclear.org\/info\/inf69.html, [accessed 04\/09\/2013]\u000d\u000a    ","Title":"\u000d\u000a    Keeping Track of Nuclear Fuel in Reprocessing\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2641673","Name":"Newcastle-upon-Tyne"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The underpinning methodology for a volumetric system was developed by\u000d\u000a      Henderson at Newcastle in 1993-1995, generalised in 1995-2000 in\u000d\u000a      co-operation with McKnespiey (Newcastle) and Morton-Jones (Lancaster) and\u000d\u000a      developed further with THORP operators British Nuclear Fuels Ltd (BNFL)\u000d\u000a      from 2005. Recalibration was carried out in 2009\/10 by Henderson in\u000d\u000a      collaboration with Chater (BNFL, now Sellafield Sites) and Clarke\u000d\u000a      (National Nuclear Laboratory). The methods have been on-going throughout\u000d\u000a      the 2008-2013 period.\u000d\u000a    Operators and regulators use material accountancy methods to help\u000d\u000a      safeguard against accidental or illicit diversion of nuclear materials.\u000d\u000a      This requires highly accurate monitoring of all movements of nuclear\u000d\u000a      material and regular inventories of storage. When, as at THORP, the\u000d\u000a      materials are held and transferred in dissolved form, amounts of material\u000d\u000a      can be estimated through a combination of sampling, chemical analysis, and\u000d\u000a      either weight or volume determination, all of which are subject to error.\u000d\u000a      A major statistical challenge during initial calibration is to determine\u000d\u000a      the relationship between level and volume of liquor in a closed vessel\u000d\u000a      containing significant interior pipework. The relationship is used to\u000d\u000a      convert level measurements during operations to volume and in turn mass\u000d\u000a      estimates of nuclear materials.\u000d\u000a    In generic statistical terms, volumetric calibration forms an inverse\u000d\u000a      problem involving longitudinal data analysis with errors in variables. An\u000d\u000a      unknown relationship v=g(l) exists between volume and level in a vessel.\u000d\u000a      The function g(.) is continuous but at a finite but unknown number of\u000d\u000a      points it is not differentiable. Smooth segments between discontinuities\u000d\u000a      in first derivative may be linear or non-linear. During commissioning, p\u000d\u000a      calibration runs are performed, in which a vessel is filled with measured\u000d\u000a      aliquots of water from low to high volume, with ni aliquots in\u000d\u000a      run i. Data (Vij, Lij) are available (i=1,...,p,\u000d\u000a      j=1,...,ni) subject to correlated error in Vij and\u000d\u000a      heteroscedastic error in Lij. The purpose is first to estimate\u000d\u000a      g without bias for all l, and second to describe the associated error\u000d\u000a      structure. Given a future measurement L0, also subject to\u000d\u000a      error, a prediction V(L0) is required together with a\u000d\u000a      prediction interval.\u000d\u000a    The first challenge was design of calibration runs. Both volume and level\u000d\u000a      are subject to measurement error. In reference [P1], theoretical\u000d\u000a      expressions for mean square error were derived for competing methods. A\u000d\u000a      technique based on in-tank density estimation was recommended on the\u000d\u000a      grounds of robustness. This technique has been taken up by BNFL and\u000d\u000a      continues to be used. In reference [P4] the stability of differential\u000d\u000a      pressure measurements was studied, with a dynamic modelling approach\u000d\u000a      recommended. A generic methodology paper [P6] shows that generalised least\u000d\u000a      squares can be much less robust than ordinary least squares in the\u000d\u000a      presence of measurement error, using calibration as an illustration.\u000d\u000a    The next decision was on aliquot sizes and number of runs. The first run\u000d\u000a      begins with an empty vessel. Vessels cannot later be completely emptied:\u000d\u000a      an unknown heel remains. Further, once active liquors are added to the\u000d\u000a      vessel then access becomes impossible. Each calibration run is expensive\u000d\u000a      (vessel capacity is up to 35 tonnes and a single run can take several\u000d\u000a      days) meaning the number of runs needs to be limited. An expression for\u000d\u000a      mean square error for the appropriate class of problem was derived in\u000d\u000a      [P2].\u000d\u000a    The main methodology developed for fitting volume\/level relationships was\u000d\u000a      an early use of reversible jump Markov chain Monte Carlo [P3, P5]. A\u000d\u000a      Bayesian approach was appropriate because prior (but unreliable)\u000d\u000a      information from vessel design drawings is available. The underlying model\u000d\u000a      for cross-sectional area is piecewise constant, leading to a piecewise\u000d\u000a      linear relationship between level and volume, with prior information for\u000d\u000a      slope changes. The Newcastle calibration methods were used during\u000d\u000a      commissioning to calibrate over 100 process vessels within the THORP\u000d\u000a      plant.\u000d\u000a    The calibration methods used bespoke Splus software written by Henderson\u000d\u000a      and later developed into a commercial package based on the reversible jump\u000d\u000a      methodology of Paul McKnespiey [P5], whose Newcastle PhD was partially\u000d\u000a      funded by BNFL. The package was bought by Westlakes Research Institute for\u000d\u000a      &#163;45,000.\u000d\u000a    Recalibration of weighing systems is carried out periodically at THORP,\u000d\u000a      using certified weights sequentially added to load systems. Recalibration\u000d\u000a      of volume systems is more problematic; however, following necessary\u000d\u000a      decommissioning of the weighing systems a bespoke validation analysis was\u000d\u000a      undertaken in 2009\/10 [E2, Section 5 below].\u000d\u000a    "},{"CaseStudyId":"22417","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"2658434","Name":"Switzerland"},{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"3017382","Name":"France"},{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"2510769","Name":"Spain"},{"GeoNamesId":"6251999","Name":"Canada"}],"Funders":["Royal Society"],"ImpactDetails":"\u000a    This research has been the basis of calculations made to assess the\u000a      viability of CCS as a technology to ameliorate the effects of carbon\u000a      emissions and their impacts on climate change. The mathematical models and\u000a      their successful comparisons with field data have provided policy makers\u000a      with the information needed to estimate the quantities of CO2 that can be\u000a      stored and to evaluate the risks associated with leakages from underground\u000a      reservoirs.\u000a    This research has impacted European policy makers through the European\u000a      Academies Science Advisory Council (EASAC). As a result of his research,\u000a      Huppert was invited in 2011 by the president of EASAC to be the Chair of\u000a      the Working Group on Carbon Capture and Storage (CCS). The President of\u000a      EASAC [11] writes \"Professor Huppert was nominated by the Royal Society to\u000a      chair the working group that carried out the study on the basis of his\u000a      research on the fluid dynamics of carbon dioxide stored in geological\u000a      formations, and his extensive efforts to lecture to a wide range of\u000a      audiences on this important and topical issue\". The CCS Working Group has\u000a      published the report, Carbon storage and capture in Europe, (2013,\u000a        pp95) [8] for the European Parliament. The report, which makes\u000a      explicit reference to the research outlined in section 3, was distributed\u000a      to all politicians, scientists and policy makers on energy in Europe in\u000a      May 2013 and will inform political debate and international strategies on\u000a      climate change. The report was released at a press conference on 21 May\u000a      2013 and at the Royal Society in London on 12 June 2013 [13]. This report\u000a      is highlighted on its website by the CCS Association which has industrial\u000a      members representing a wide sector of UK industry [10].\u000a    Huppert's research has also had a significant impact on the policy of the\u000a      Australian Government on CCS. According to the Chief Executive of CO2CRC\u000a      [9],\u000a      \"This work in turn provides confidence to Government and the community at\u000a      large that underground storage of CO2 is understood, that it can be\u000a      monitored and that it works. The impact of this on public policy in\u000a      Australia is that CCS has become recognised as an important mitigation\u000a      option for Australia and the Government has provided significant funding\u000a      (in excess of $1 billion) to support CCS. Obviously this has been the\u000a      consequence of the work of many people in CO2cRC and other organisations,\u000a      but the work by Herbert and his collaborators has certainly contributed.\u000a      It is also appropriate to mention the various public presentations that\u000a      Herbert has given in Australia which have been picked up by the media and\u000a      which have provided a factual and positive account of what the relevance\u000a      of CCS is to the whole issue of climate change and mitigation\".\u000a    On the basis of the research outlined above, Huppert was asked to present\u000a      this work to the All- Party Parliamentary Group for Earth and\u000a      Environmental Sciences, to an audience of around 100 people, including\u000a      both MPs and Peers, on 16 October 2012.\u000a    Huppert was awarded the Bakerian Lectureship of the Royal Society for\u000a      2011 [7]. This is the major lecture in the physical sciences delivered\u000a      each year at the Royal Society. The title was `Carbon storage: caught\u000a      between a rock and climate change' and was based on the whole gamut of\u000a      Huppert's research, explaining the concepts of carbon dioxide\u000a      sequestration and the possible consequences to an audience of over 250.\u000a      The Royal Society [12] states that \"The total viewing figure for \"Carbon\u000a      storage: caught between a rock and climate change\" from it being given on\u000a      24 March 2011 to today (1 May 2013), is 1618, according to our Google\u000a      Analytics record. The webcast has been accessed from 59 countries, with\u000a      the ten most popular being the United Kingdom, United States, Australia,\u000a      Canada, Germany, France, Switzerland, Netherlands, Spain and China\". This\u000a      accessible presentation of this research encouraged the audience to engage\u000a      with current scientific and political debates on solution to climate\u000a      change.\u000a    ","ImpactSummary":"\u000a    Carbon dioxide sequestration is the process by which pressured CO2\u000a      is injected into a storage space within the Earth rather than released\u000a      into the atmosphere. It is one of the major ways that carbon dioxide\u000a      emissions can be controlled.\u000a    Research since 2004 by applied mathematicians at the University of\u000a      Cambridge into the many different effects that might be encountered during\u000a      this process has had considerable impact on government and industry groups\u000a      in determining how the field is viewed and how it should and might be\u000a      industrially developed. The work played a major role in the CO2CRC\u000a      conferences and was subsequently reported to the Australian Government by\u000a      the CO2CRC chair and organisers.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Cambridge\u000a    ","Institutions":[{"AlternativeName":"Cambridge (University of)","InstitutionName":"University of Cambridge","PeerGroup":"A","Region":"East","UKPRN":10007788}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a1. *Lyle, S., Huppert, H.E., Hallworth, M.A., Bickle, M. and Chadwick, A.\u000a      (2005) \"Axysymmetric gravity currents in a porous medium\", J. Fluid Mech.\u000a      543, 293-302. DOI: 10.1017\/S0022112005006713.\u000a    \u000a\u000a2. *Vella, D. and Huppert, H.E. (2006) \"Gravity currents in a porous\u000a      medium at an inclined plane\", J. Fluid Mech. 555, 353-362. DOI: 10.1017\/S0022112006009578\u000a    \u000a\u000a3. Neufeld, J.A., Vella, D. and Huppert, H.E. (2009) \"The effect of a\u000a      fissure on storage in a porous medium\", J. Fluid Mech. 639,\u000a      239-259. DOI: 10.1017\/S0022112009991030.\u000a    \u000a\u000a4. Golding, M.J. and Huppert, H.E. (2010) \"The effect of confining\u000a      impermeable boundaries on gravity currents in a porous medium\", J. Fluid\u000a      Mech. 649, 1-17. DOI: 10.1017\/S0022112009993223.\u000a    \u000a\u000a5. Vella, D., Neufeld, J.A., Huppert, H.E. and Lister, J.R. (2011)\u000a      \"Leakage from gravity currents in a porous medium. Part II. A line sink\",\u000a      J. Fluid Mech. 666, 414-427. DOI: 10.1017\/S002211201000491X\u000a    \u000a\u000a6. *Boait, F.C., White, N.J., Bickle, M.J., Chadwick, R.A., Neufeld, J.A.\u000a      and Huppert, H.E. (2012) \"Spatial and temporal evolution of injected CO2\u000a      at the Sleipner Field, North Sea\", J. Geophys. Res. 117, B03309.\u000a      DOI: 10.1029\/2011JB008603.\u000a    \u000a*References which best represent the 2*+ quality of the underpinning\u000a      research\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"14","Subject":"Resources Engineering and Extractive Metallurgy"},{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"6","Level2":"2","Subject":"Ecology"}],"Sources":"\u000a      \u000a     Royal Society, 2011. 2011 Bakerian Lecture: Professor Herbert\u000a        Huppert FRS [online] Available at http:\/\/royalsociety.org\/events\/2011\/carbon-storage\/\u000a      [Accessed 17 May 2013].\u000a     EASAC CCS report, 23 May 2013 Carbon capture and storage in Europe\u000a      [online] Available at http:\/\/www.easac.eu\/home\/reports-and-statements\/detail-view\/article\/easac-report.html\u000a      [Accessed 14 June 2013].\u000a     Statement from Chief Executive of CO2CRC, CBE, CEO, CO2CRC, Canberra,\u000a      Australia, corroborating of the impact of the group's work in CO2CRC\u000a      conferences\u000a     Scientific Adviser to the European Parliament who attended the launch\u000a      of the EASAC CCS report and is carrying the matter forward to the European\u000a      President and MEPs.\u000a     Statement from the President of EASAC corroborating Huppert's\u000a      invitation and contribution to EASAC\u000a     Statement from Manager, Science Communication, Royal Society\u000a      corroborating viewing figures for \"Carbon storage: caught between a rock\u000a      and climate change\"\u000a     Royal Society press release on Carbon storage and capture in Europe\u000a      report:\u000a      http:\/\/blogs.royalsociety.org\/in-verba\/2013\/06\/24\/capturing-an-opportunity-or-storing-up-trouble-ccs-in-the-uk-and-europe\/\u000a\u000a\u0009  \u000a    ","Title":"\u000a    Carbon dioxide sequestration\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    This research addressed the process and the consequences of carbon\u000a      dioxide sequestration in porous rock and also possible leakage from the\u000a      storage space. The research was carried out by members of the Department\u000a      of Applied Mathematics and Theoretical Physics (DAMTP): Professor Huppert\u000a      (Professor from 1989-2011), Professor John Lister (Royal Society URF from\u000a      1992, University Lecturer from 1997, Reader from 2001, Professor from 2006\u000a      to present), Dr Jerome Neufeld (Research Associate from 2007-2009, now a\u000a      Lecturer at the Department of Earth Sciences). There were collaborations\u000a      with Professor Michael Bickle of the Department of Earth Sciences and Dr\u000a      Andrew Chadwick, member of the British Geological Survey.\u000a    The research carried out in DAMTP began in 2004 and consisted of the\u000a      development of mathematical models for the spread and leakage of carbon\u000a      dioxide injected into a porous medium. Informed by novel laboratory\u000a      experiments in the G.K. Batchelor Laboratory in DAMTP models were\u000a      developed for rate of spread as a gravity current of a carbon dioxide\u000a      plume directed towards a horizontal cap rock. It was found that, treated\u000a      as a one-phase fluid, the axisymmetric current, fed at a constant flux,\u000a      increases its area at a rate directly proportional to time This same\u000a      result was found, somewhat surprisingly, if the intruding carbon dioxide\u000a      is considered as a two-phase fluid that incorporates effects due to\u000a      surface tension.\u000a    This increase in area with time was clearly seen in the seismic data\u000a      obtained in 1999, 2001, 2002, 2004, 2006 and 2008 from the longest-living,\u000a      large-scale field operation of carbon dioxide sequestration, at Sleipner\u000a      in the North Sea. The data analysis also allowed the determination of the\u000a      time at which the input carbon dioxide first encountered the upper\u000a      horizons of the field, in some instances up to three years after initial\u000a      injection. The Group also compared the data collected at the Otway Project\u000a      in Australia in 2008\/9 with its own model which allows for flow up a\u000a      slope, as is the case for the Australia situation.\u000a    Research was also conducted into the leakage that might occur if there\u000a      was either a point or line fracture through which the carbon dioxide could\u000a      escape. Further, a series of models was developed to analyse the\u000a      mechanisms by which carbon dioxide can dissolve in the surrounding brine\u000a      and produce vigorous convection due to the fact that the mixture is\u000a      heavier than either of the initial fluids. By this mechanism the carbon\u000a      dioxide is gradually stored permanently at the base of the confining\u000a      aquifer. Since the Sleipner project commenced in 1996, 1 million tonnes of\u000a      carbon dioxide have been input annually with 100,000 tonnes permanently\u000a      stored by this mechanism each year. Using linear extrapolation, then, to\u000a      gain approximate timescales, these models show that if the supply was\u000a      curtailed now (after 16 years of input), it would take 160 years for the\u000a      carbon dioxide to be at such a relatively heavy state that it would lie at\u000a      the bottom of the aquifer and so be stored safely and permanently.\u000a    "},{"CaseStudyId":"23933","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":[],"ImpactDetails":"\u000a    The research has had an economic impact for the companies commercialising\u000a      uplift modelling\u000a      through software sales and consultancy, and on their customers who have\u000a      improved the cost-effectiveness\u000a      of their marketing investments. Uplift modelling has been a core part of\u000a      the\u000a      consulting and software solutions marketed by Quadstone Limited from c.\u000a      2000 onwards.\u000a      Quadstone was acquired by Portrait Software, in December 2005 for &#163;3.5M.\u000a      Pitney Bowes\u000a      Software then acquired Portrait Software (including Quadstone) in 2010 for\u000a      &#163;44M. The Uplift\u000a      Software continues to be a key part of the analytical software and\u000a      services delivered by Pitney\u000a      Bowes today, now marketed as Portrait Uplift [6-7]. While the precise\u000a      impact and results of uplift\u000a      modelling are in many cases not shared publicly by Quadstone's customers,\u000a      in some cases they\u000a      are. For example, US Bank (the fifth largest commercial bank in USA as of\u000a      2010) and Telenor (the\u000a      world's 7th largest mobile phone operator) have both published\u000a      case studies discussing the results\u000a      in some detail. These are:\u000a    US Bank. The bank traditionally used `straight response modelling'\u000a      to target sales of various\u000a      products including HELOCS (Home Equity Line of Credits, i.e.\u000a      mortgage-backed loans).\u000a      Traditional response models performed so poorly, in some cases, that with\u000a      the typical 30% cutoff,\u000a      they achieved no incremental sales (compared with the control group) at\u000a      all or a small negative\u000a      uplift. [text removed for publication]. Work presented at Predictive\u000a      Analytics World showed large\u000a      improvement when uplift modelling was used. [text removed for publication]\u000a    Telenor. The published case study [10] shows how, by using uplift\u000a      modelling for its customer\u000a      retention programme, Telenor reduced the rate of customer defection. [text\u000a      removed for\u000a      publication].\u000a    Uplift modelling software from (now) Pitney Bowes is used by\u000a      dozens of financial services,\u000a      telecommunications and other major companies in the US, UK and mainland\u000a      Europe. See [11] for\u000a      further details. An example is T-Mobile Austria who have been\u000a      using the software since 2009. A\u000a      senior expert in their Consumer-Customer Insights division made the\u000a      statement: `with the use of\u000a      the uplift modelling approach T-Mobile Austria has successfully optimized\u000a      big retention campaigns;\u000a      this has not only reduced communication costs in direct marketing\u000a      activities but also had a\u000a      significant uplift in contribution margin as an effect of targeting only\u000a      segments which should be\u000a      \"moved\" by simultaneously avoiding common side effects in pro-active\u000a      targeting customers.' [12]\u000a    Other applications. In addition to these direct impacts of the\u000a      research through Quadstone Limited\u000a      and Pitney Bowes, Uplift Modelling has, after a slow gestation, started to\u000a      be recognised more\u000a      widely as a powerful method for increasing marketing efficiency in areas\u000a      such as demand\u000a      generation (cross-selling, up-selling, deep-selling) and customer\u000a      retention (where campaigns with\u000a      significant negative effects are not uncommon). The following examples\u000a      illustrate this. SAS (the\u000a      world's largest private software company, and the leading provider of\u000a      statistical software) now\u000a      includes an Incremental Response Node in its Enterprise Miner 7.1 product,\u000a      which implements\u000a      some form of uplift modelling. Similarly KXEN, another analytics\u000a      company, lists uplift modelling as\u000a      a capability of its InfiniteInsight Explorer [13]. The ideas underlying\u000a      uplift modelling have now been\u000a      diffused broadly and adopted in models that, although not directly\u000a      traceable to the original\u000a      research, have most likely been influenced by it. An example is the 2012\u000a      Obama campaign which\u000a      used `persuasion' models for each state (equivalent to uplift models) to\u000a      decide who to target [14].\u000a    ","ImpactSummary":"\u000a    Research at the Maxwell Institute led by Radcliffe from 1996 onwards has\u000a      developed new\u000a      statistical models of the response of customers to targeted marketing.\u000a      Traditional customer\u000a      targeting misallocates resources by failing to estimate the change in the\u000a      probability of customer\u000a      behaviour that results from a given marketing action. This results in\u000a      three kinds of waste: treating\u000a      customers for whom intervention is ineffective, failing to treat customers\u000a      for whom it would be\u000a      effective, and treating customers for whom the intervention is\u000a      counterproductive. The new models,\u000a      known as uplift models, predict the change in behaviour, allowing lower\u000a      target volumes, larger\u000a      changes in customer behaviour, and suppressing counterproductive\u000a      interventions. Uplift modelling\u000a      has been commercialised in the form of software and consulting services\u000a      from 2000: it is the core\u000a      of the software Portrait Uplift sold by Pitney Bowes since 2010. The\u000a      research has therefore had a\u000a      major economic impact on Pitney Bowes and earlier companies selling uplift\u000a      software and\u000a      services, and on their customers which include US Bank and phone operators\u000a      T-Mobile Austria\u000a      and Telenor.\u000a    ","ImpactType":"Economic","Institution":"\u000a    The University of Edinburgh\/Heriot-Watt University (Maxwell Institute)\u000a    ","Institutions":[{"AlternativeName":"Edinburgh (University of)","InstitutionName":"University of Edinburgh","PeerGroup":"A","Region":"Scotland","UKPRN":10007790},{"AlternativeName":"Heriot-Watt University","InstitutionName":"Heriot-Watt University","PeerGroup":"B","Region":"Scotland","UKPRN":10007764}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Although information on and results from uplift modelling have been\u000a      published in industry-relevant\u000a      publications and conferences, until recently the details of the algorithms\u000a      were considered\u000a      commercially confidential by Quadstone Limited. Full details of the core\u000a      algorithm have only\u000a      recently been published in [1]. Further details on implementation methods\u000a      and performance\u000a      measures are reported in MSc theses supervised by Radcliffe [3,4].\u000a    References marked (*) best indicate the quality of the research.\u000a    \u000a[1]* Radcliffe, N. J. and Surry, P. D., Real-World Uplift Modelling with\u000a      Significance-Based Uplift\u000a      Trees, submitted to Data Mining and Knowledge Discovery (2012).\u000a      Available at\u000a      http:\/\/stochasticsolutions.com\/pdf\/sig-based-up-trees.pdf.\u000a    \u000a\u000a[2] Radcliffe, N. J. and Surry, P. D., Differential Response Analysis:\u000a      Modeling True Responses by\u000a      Isolating the Effect of a Single Action, Credit Scoring and Credit\u000a        Control IV (1999).\u000a      http:\/\/www.maths.ed.ac.uk\/~mthdat25\/uplift\/cscc99-1\u000a    \u000a\u000a[3] Hofmeyr, D., An Application of Genetic Algorithms to Uplift\u000a      Modelling. M.Sc. Thesis,\u000a        Department of Mathematics and Statistics, University of Edinburgh\u000a      (2011).\u000a      http:\/\/www.maths.ed.ac.uk\/~mthdat25\/uplift\/HofmeyrDavid-1\u000a    \u000a\u000a[4] Radcliffe, N. J., Using control groups to target on predicted lift:\u000a      Building and assessing uplift\u000a      model, Direct Marketing Analytics Journal, Direct Marketing\u000a      Association Analytics Council, 14-21\u000a      (2007). http:\/\/www.maths.ed.ac.uk\/~mthdat25\/uplift\/dma2006-3-1\u000a    \u000a\u000a[5] Mesalles Narajo, O., Testing a New Metric for Uplift Models, M.Sc.\u000a        Thesis, Department of\u000a        Mathematics and Statistics, University of Edinburgh (2012).\u000a      http:\/\/www.maths.ed.ac.uk\/~mthdat25\/uplift\/MesallesNaranjoOscar-1\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"15","Level2":"2","Subject":"Banking, Finance and Investment"}],"Sources":"\u000a    [6] See http:\/\/www.portraitsoftware.com\/products\/portrait-uplift-optimizer\u000a      for a description of the\u000a      Portrait Uplift Software.\u000a    [7] The crucial importance of the research to this product can be\u000a      confirmed by a former Vice\u000a      President at Pitney Bowes.\u000a    [8] See http:\/\/www.portraitsoftware.com\/newsandevents\/press-releases\/portrait-software-and-us-bank-present-predictive-analytics-world-2009\u000a      for a report on the presentation.\u000a    [9] The impact of Uplift Modelling at US Bank can be confirmed by Vice\u000a      President of Marketing\u000a      Analytics at US Bank.\u000a    [10] See http:\/\/www.pbinsight.com\/assets_microsite\/resources\/files\/telenor-cs.pdf\u000a      for a report on\u000a      the benefits of Uplift Modelling for Telenor.\u000a    [11] The page http:\/\/www.portraitsoftware.com\/uplift-modeling\/who-uses-uplift-modeling\u000a      describes\u000a      some users of Portrait Uplift.\u000a    [12] The use of Uplift Modelling by T-Mobile Austria and the statement\u000a      can be confirmed by a\u000a      Senior Expert, Consumer-Customer Insight, T-Mobile Austria.\u000a    [13] A description of KXEN's use of Uplift Modelling is given at\u000a      http:\/\/www.kxen.com\/blog\/2012\/01\/uplift-modeling-with-kxens-infiniteinsight\/\u000a    [14] The page http:\/\/www.thefiscaltimes.com\/Articles\/2013\/01\/21\/The-Real-Story-Behind-Obamas-Election-Victory.aspx\u000a      describes the `persuasion' model used in the 2012 Obama campaign.\u000a    \u000a    ","Title":"\u000a    Uplift modelling for improved customer targeting\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Background. Traditional approaches to targeted marketing have\u000a      applied statistical and machine-learning\u000a      methods in a rather simplistic manner, resulting in suboptimal (and\u000a      sometimes counter-productive)\u000a      performance. The basis for most targeted marketing is a predictive model\u000a      which is\u000a      fitted to data on a population that has been subject to a marketing\u000a      intervention (a `treatment'). The\u000a      model attempts to classify population members as either responders or\u000a      non-responders, or in\u000a      some cases to predict the size of response. Typical responses are\u000a      purchases, renewals or (in the\u000a      negative) account closures. Typical models of this form consider the\u000a      output O (e.g. purchase\/non-purchase)\u000a      as a binary event and given treatment T and customer covariates x,\u000a      typically combining\u000a      geodemographic information with behavioural characteristics, to model the\u000a      probability Pr(O | T; x).\u000a      Similarly, for continuous-valued response S, the conditional\u000a      expectation E(S | T; x) is modelled.\u000a      Traditional targeting methods usually select customers with high values of\u000a      Pr(O | T; x) or expected\u000a      response size E(S | T; x), where the threshold may\u000a      be simply determined by volume (e.g. target\u000a      the top 30%) or may be selected to maximise the expected return on\u000a      investment.\u000a    Uplift modelling. In contrast, uplift modelling focuses on the\u000a      incremental impact of marketing by\u000a      modelling quantities that reflect change in behaviour, such as P(O\u000a      | T; x) &#8212; P(O | T '; x) or E(S\u000a      | T;\u000a      x) &#8212; E(S| T '; x) where T ' denotes non\u000a      treatment. Although it had been recognised best practice to\u000a      maintain a control group to allow assessment of the incremental\u000a      impact of a marketing initiative,\u000a      prior to uplift modelling, we are aware of no attempts to target on the\u000a      basis of modelled incremental\u000a      impact.\u000a    The research programme led by Radcliffe from 1996 onwards applied a range\u000a      of statistical\u000a      approaches including generalised linear models and generalised additive\u000a      models along with\u000a      approaches from machine learning such as decision trees (e.g. CART, CHAID\u000a      or C5). Key\u000a      developments included identifying and correcting the traditional\u000a      mis-formulation of the targeted\u000a      marketing problem and developing a suite of increasingly sophisticated\u000a      methods for building uplift\u000a      models, which directly model the change in behaviour exhibited by\u000a      individuals in the treatment and\u000a      control groups. The core of the current method, the significance-based\u000a        uplift tree [1-3], for the case\u000a      of a binary outcome, models the probability of conversion as pij\u000a      = &#181; + &#945;i + 03b2j\u000a      + 03b3ij where &#945; quantifies\u000a      the effect of the treatment, 03b2 quantifies the effect of the\u000a      split and 03b3 is the interaction term. This is\u000a      solved with a regression using as a split criterion (for a greedy binary\u000a      decision tree) the square of a\u000a      t-statistic for the significance of the 03b3TR\u000a      parameter, corresponding (without loss of generality) to the\u000a      strength of the interaction between the treatment (T[reated]) and the\u000a      split side (R[ight]).\u000a    As well as constructing models, the team has developed measures of\u000a      performance of uplift\u000a      models; in particular, the qini measure [4] is a rank-based statistic\u000a      formed by generalizing of the\u000a      widely used gini coefficient to the case of uplift. More recently a family\u000a      of moment of uplift\u000a      measures have been developed by Radcliffe and Mesalles Narajo [5]. Models\u000a      have been built for\u000a      particular customer data sets provided by companies and used to guide the\u000a      targeting of future\u000a      campaigns, in which effective performance has been verified.\u000a    Attribution. N. J. Radcliffe was with the Maxwell Institute (MI)\u000a      from 1995 to 1998 and has\u000a      remained a MI visiting professor since; he was also a director of\u000a      Quadstone Limited &#8212; a spin-out\u000a      company from the University of Edinburgh (UoE) &#8212; from 1995-2008; since\u000a      2008, he has run\u000a      Stochastic Solutions Limited. Several members of Quadstone staff also made\u000a      significant\u000a      contributions to the work, including D. Signorini, T. Harding and P.\u000a      Surry. UoE postgraduates who\u000a      worked on Uplift Modelling under Radcliffe's supervision include P. Surry\u000a      (Ph.D. student,\u000a      graduated 1998), D. Hofmeyr (M.Sc. student, 2010-11), O. Mesalles Narajo\u000a      (M.Sc. student, 2011-12).\u000a    "},{"CaseStudyId":"23934","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Engineering and Physical Sciences Research Council","Science and Technology Facilities Council"],"ImpactDetails":"\u000d\u000a    Improved commercial software products for molecular dynamics. The\u000d\u000a      enhanced value offered\u000d\u000a      by the NHL thermostats in comparison to existing approaches was\u000d\u000a      immediately recognised by the\u000d\u000a      commercial software company Accelrys who incorporated the techniques in\u000d\u000a      their commercial code\u000d\u000a      Materials Studio. The impact on Accelrys was achieved through an\u000d\u000a      extended period of interactions\u000d\u000a      from which led to a solution to the problem of `ringing' observed with the\u000d\u000a      Nos&#233;-Hoover thermostat;\u000d\u000a      this solution was subsequently used in Accelrys's software. Ringing arises\u000d\u000a      when the system is\u000d\u000a      initialised with data far from a correct equilibrated state. This leads to\u000d\u000a      a severe oscillation in kinetic\u000d\u000a      energy with poor simulation results as a consequence. Although ringing\u000d\u000a      could sometimes be\u000d\u000a      addressed by ad hoc approaches these were inefficient and time consuming;\u000d\u000a      code developers\u000d\u000a      needed a robust, systematic solution. The results published by Leimkuhler\u000d\u000a      and his collaborators in\u000d\u000a      [1-3] suggested that the NHL method would robustly sample the canonical\u000d\u000a      distribution over a much\u000d\u000a      wider range of parameters and this was subsequently verified by Matthews,\u000d\u000a      Akkermans (Accelrys)\u000d\u000a      and Leimkuhler who demonstrated that a working NHL implementation\u000d\u000a      dramatically resolves the\u000d\u000a      ringing problem in simulations of several complex molecules (a silicon\u000d\u000a      system and a substantial\u000d\u000a      organic molecule). As a result, Accelrys implemented NHL in its Materials\u000d\u000a        Studio software. This\u000d\u000a      implementation, carried out in collaboration with Leimkuhler and Matthews,\u000d\u000a      was released in version\u000d\u000a      6.0 of Material Studio (Nov. 2011) and was highlighted as a\u000d\u000a      valuable new feature [6]. Matthews\u000d\u000a      and Leimkuhler drafted the documentation of the new method. The more\u000d\u000a      recent versions of\u000d\u000a      Material Studio continue to rely on the NHL thermostat. A quote from a\u000d\u000a      group manager at Accelrys\u000d\u000a      confirms the importance of the NHL thermostat for their product: `During\u000d\u000a      2011 Accelrys worked\u000d\u000a      closely with Leimkuhler to implement the Nos&#233;-Hoover-Langevin thermostat\u000d\u000a      within Forcite, the\u000d\u000a      molecular dynamics module of Materials Studio. This thermostat was a key\u000d\u000a      contribution to the\u000d\u000a      product because it eliminated a widespread problem experienced by Accelrys\u000d\u000a      customers,\u000d\u000a      specifically the excessive time required to equilibrate a system' [7].\u000d\u000a    Accelrys's Materials Studio is the world's leading commercial\u000d\u000a      software package for molecular\u000d\u000a      simulation of materials. Accelrys had 2012 revenues of $162M, most of\u000d\u000a      which comes from software\u000d\u000a      licenses. Detailed breakdown of sales figures is not available, but Materials\u000d\u000a        Studio is one of\u000d\u000a      Accelrys's two primary software products and is widely used within the\u000d\u000a      commercial materials\u000d\u000a      sector, with thousands of installations worldwide.\u000d\u000a    Enhancements to public domain software. The majority of industrial\u000d\u000a      users of MD simulation\u000d\u000a      tools make use of public domain software developed by government-academic\u000d\u000a      partnerships and\u000d\u000a      wide impact has been achieved through the implementation of the methods in\u000d\u000a      several such\u000d\u000a      resources. In parallel with the Accelrys implementation, the NHL method\u000d\u000a      was implemented in the\u000d\u000a      DL-Poly 4.0 code (STFC Daresbury laboratory, [8]) and into AMBER, a major\u000d\u000a      NSF-funded\u000d\u000a      molecular software package [9]. Confirmed industrial users of these\u000d\u000a      software include Sony,\u000d\u000a      Samsung (DL-Poly, [8]), Pfizer, Novartis, Takeda and Dart Neuroscience\u000d\u000a      (AMBER, [9]). Further\u000d\u000a      implementations were carried out by Bernstein (Center for Computational\u000d\u000a      Materials Science. Naval\u000d\u000a      Research Laboratory, Washington, DC, USA), by Gabor Csanyi (Engineering,\u000d\u000a      Cambridge\u000d\u000a      University) as part of a QM\/MM code, and as part of the NAMD code project\u000d\u000a      [10].\u000d\u000a    This research has impacted widely on the extensive community of MD code\u000d\u000a      users by reducing\u000d\u000a      computational requirements. Because MD simulations use vast computational\u000d\u000a      resources (30% of\u000d\u000a      all CPU cycles on the NSF TeraGrid (now XSEDE) HPC system [11] and over\u000d\u000a      40% of the usage of\u000d\u000a      the UK National HPC Service HECToR relate to molecular simulation [12]),\u000d\u000a      algorithmic\u000d\u000a      improvements lead to large gains in net computing time, with clear\u000d\u000a      benefits for the accuracy,\u000d\u000a      reliability and cost-effectiveness of simulations.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Molecular dynamics (MD) simulations are used extensively in chemistry,\u000d\u000a      biology and material\u000d\u000a      sciences, placing huge demands on computer resources. Because these\u000d\u000a      simulations explore the\u000d\u000a      behaviour of molecules at defined ambient temperature, temperature control\u000d\u000a      (thermostatting) is an\u000d\u000a      essential element of MD algorithms. In a series of papers published from\u000d\u000a      2009 on, Leimkuhler\u000d\u000a      (Maxwell Institute) and his collaborators developed improved numerical\u000d\u000a      methods for temperature\u000d\u000a      control. They proposed new algorithms and analysed their properties (such\u000d\u000a      as fidelity to the\u000d\u000a      dynamical model, efficiency and stability). The new algorithms have since\u000d\u000a      been implemented in the\u000d\u000a      world's leading MD software packages including DL-Poly, AMBER, NAMD and\u000d\u000a      Accelrys's Material\u000d\u000a      Studio. The research has had clear economic impact on the commercial\u000d\u000a      company Accelrys by\u000d\u000a      improving its product, and more broadly on the community of MD code users\u000d\u000a      worldwide by\u000d\u000a      providing improved simulation tools.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    The University of Edinburgh\/Heriot-Watt University (Maxwell Institute)\u000d\u000a    ","Institutions":[{"AlternativeName":"Edinburgh (University of)","InstitutionName":"University of Edinburgh","PeerGroup":"A","Region":"Scotland","UKPRN":10007790},{"AlternativeName":"Heriot-Watt University","InstitutionName":"Heriot-Watt University","PeerGroup":"B","Region":"Scotland","UKPRN":10007764}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    References marked with a * best indicate the quality of the research.\u000d\u000a    \u000a[1]* Leimkuhler, B., Noorizadeh, E. and Theil, F., A gentle stochastic\u000d\u000a      thermostat for molecular\u000d\u000a      dynamics, J. Stat. Phys., 135, 261-277 (2009). http:\/\/dx.doi.org\/10.1007\/s10955-009-9734-0\u000d\u000a    \u000a\u000a[2]* Leimkuhler, B., Noorizadeh, E. and Penrose, O., Comparing the\u000d\u000a      efficiencies of stochastic\u000d\u000a      isothermal molecular dynamics method, J. Stat. Phys., 143,\u000d\u000a      921-942 (2011).\u000d\u000a      http:\/\/dx.doi.org\/10.1007\/s10955-011-0210-2\u000d\u000a    \u000a\u000a[3] Jones, A. and Leimkuhler, B., Adaptive stochastic methods for\u000d\u000a      sampling driven molecular\u000d\u000a      systems, J. Chem. Phys., 135, 084125, (2011). http:\/\/dx.doi.org\/10.1063\/1.3626941ht\u000d\u000a    \u000a\u000a[4]* Leimkuhler, B. and Matthews, C., Rational construction of stochastic\u000d\u000a      numerical methods for\u000d\u000a      molecular sampling, Appl. Math. Res. Express, 2013, 34-56,\u000d\u000a      (2013).\u000d\u000a      http:\/\/dx.doi.org\/10.1093\/amrx\/abs010\u000d\u000a    \u000a\u000a[5] Leimkuhler, B. and Matthews, C., Robust and efficient configurational\u000d\u000a      molecular sampling via\u000d\u000a      Langevin dynamics, J. Chem. Phys., 138, 174102, (2013). http:\/\/dx.doi.org\/10.1063\/1.4802990\u000d\u000a    \u000aGrants:\u000d\u000a    EP\/K039512\/1 SI2-CHE: ExTASY: Extensible Tools for Advanced Sampling and\u000d\u000a      analysis, value\u000d\u000a      &#163;550K (one part of 6 linked US and UK projects worth around &#163;2M),\u000d\u000a      2013-2016.\u000d\u000a    EP\/G036136\/1: Numerical Algorithms and Intelligent Software for the\u000d\u000a      Evolving HPC Platform,\u000d\u000a      value &#163;4.5M, 2009-2014.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"3","Level2":"7","Subject":"Theoretical and Computational Chemistry"}],"Sources":"\u000d\u000a    [6] What's New in Materials Studio 6.0 &gt; Big Impact with Small Science\u000d\u000a      &gt; `Improve temperature\u000d\u000a      stability in molecular dynamics calculations with new thermostats in\u000d\u000a      Forcite'; mentions NHL\u000d\u000a      thermostat. http:\/\/www.maths.ed.ac.uk\/~mthdat25\/thermostat\/whats-new\u000d\u000a    [7] The implementation of NHL thermostat in Accelrys' Materials Studio\u000d\u000a      Software can be confirmed\u000d\u000a      by an Accelrys Group Manager.\u000d\u000a    [8] The implementation of NHL thermostat in DL-Poly 4.0 can be confirmed\u000d\u000a      by a member of the\u000d\u000a      Computational Science and Engineering Department, Science and Technology\u000d\u000a      Facilities\u000d\u000a      Council. See also DL-Poly 4 User Manual (Sec. 3.4.6 describes the\u000d\u000a      implementation of the NHL\u000d\u000a      method).\u000d\u000a    [9] The implementation of adaptive thermostat within AMBER can be\u000d\u000a      confirmed by a Professor at\u000d\u000a      the San Diego Supercomputer Center, University of California San Diego.\u000d\u000a    [10] The implementation of Langevin thermostat within NAMD can be\u000d\u000a      confirmed by a member of\u000d\u000a      the Theoretical and Computational Biophysics Group, University of\u000d\u000a      Illinois. See also\u000d\u000a      http:\/\/www.ks.uiuc.edu\/Research\/namd\/\u000d\u000a    [11] Statistics and examples of MD computations carried out on NSF\u000d\u000a      TeraGrid are reported in\u000d\u000a      http:\/\/www.teragridforum.org\/mediawiki\/images\/d\/d8\/DEISA-PRACE-May2009-Towns.pdf\u000d\u000a    [12] Statistics and examples of MD computations carried out on HECToR are\u000d\u000a      reported in\u000d\u000a      http:\/\/www.hector.ac.uk\/about-us\/reports\/annual\/2011.pdf\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    New thermostatic controls adopted by molecular dynamics software\u000d\u000a        providers\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    In MD simulation, the system size is limited by computational\u000d\u000a      considerations, yet one would like\u000d\u000a      simulation parameters such as temperature and pressure to be strictly\u000d\u000a      regulated so that the\u000d\u000a      molecular model is relevant to the experimental conditions it is meant to\u000d\u000a      mimic. This motivates the\u000d\u000a      introduction in MD codes of thermal regulation mechanisms &#8212; thermostats.\u000d\u000a      These are perturbations\u000d\u000a      of the underlying Newtonian dynamics that enable the simulated system to\u000d\u000a      sample state space (all\u000d\u000a      accessible configurations of the different atoms in a protein or drug\u000d\u000a      molecule, for example) in a\u000d\u000a      manner that approximates experimental conditions. The shortcomings of\u000d\u000a      existing thermostats led\u000d\u000a      Leimkuhler (Maxwell Institute, MI) to develop stochastic-dynamical schemes\u000d\u000a      which are both\u000d\u000a      rigorously ergodic (meaning that they sample the entire accessible phase\u000d\u000a      space) and robust.\u000d\u000a      Previous investigation of the widely used Nos&#233;-Hoover dynamics, for\u000d\u000a      instance, had demonstrated\u000d\u000a      that this method is not ergodic (Legoll et al., Nonlinearity,\u000d\u000a      22, 1673, 2009). This motivated\u000d\u000a      Leimkuhler to characterize the performances of different approaches used\u000d\u000a      in this problem and led\u000d\u000a      him to develop new efficient algorithms.\u000d\u000a    New thermostatic control. In joint work with his PhD student\u000d\u000a      Noorizadeh (MI) and with Theil\u000d\u000a      (Warwick), Leimkuhler showed that ergodic sampling is possible using a new\u000d\u000a      thermostat\u000d\u000a      mechanism that combines Nos&#233;-Hoover dynamics with a highly degenerate\u000d\u000a      (scalar) stochastic\u000d\u000a      process [1]. The proof, which relies on a result of Fields medallist Lars\u000d\u000a      H&#246;rmander, establishes the\u000d\u000a      regularity of the Fokker-Planck operator corresponding to the degenerate\u000d\u000a      diffusion. This involves\u000d\u000a      understanding the effective interactions of the stochastic process with\u000d\u000a      the many physical degrees\u000d\u000a      of freedom to show that the noisy process propagates into all directions\u000d\u000a      and that ergodicity ensues.\u000d\u000a      Subsequently, Leimkuhler and Noorizadeh joined with O. Penrose (MI) to\u000d\u000a      study the `gentleness' of\u000d\u000a      various thermostats in terms of their perturbation of dynamics introduced\u000d\u000a      as measured by the rate\u000d\u000a      of convergence of the kinetic energy [2]. This concept was entirely\u000d\u000a      undeveloped in the\u000d\u000a      mathematical context and made coherent a notion which had only just been\u000d\u000a      suggested by\u000d\u000a      physicists (Bussi et al., J. Chem. Phys., 126,\u000d\u000a      014101, 2007). The gentle thermostats proposed by\u000d\u000a      Leimkuhler and co-workers, termed Nos&#233;-Hoover-Langevin (NHL), are valuable\u000d\u000a      wherever\u000d\u000a      measures of dynamic mobility (e.g. diffusion constants) or time-constants\u000d\u000a      must be recovered from\u000d\u000a      the molecular trajectories. A follow-on project (with A. Jones, Edinburgh)\u000d\u000a      has addressed the\u000d\u000a      treatment of driven systems using adaptive variants of the gentle\u000d\u000a      thermostats as well as Langevin\u000d\u000a      dynamics [3]; this concept is relevant for molecular modelling of material\u000d\u000a      defects and in connection\u000d\u000a      with QM\/MM algorithms which introduce artificial heating along the\u000d\u000a      interface between classical and\u000d\u000a      quantum models. Most recently, attention has turned to Langevin dynamics\u000d\u000a      methods for use in\u000d\u000a      configurational sampling, for which new integration algorithms have been\u000d\u000a      obtained with high\u000d\u000a      sampling accuracy [4] and these have been demonstrated to be effective for\u000d\u000a      biomolecular\u000d\u000a      simulation [5].\u000d\u000a    Implementation. These methods have been implemented in major\u000d\u000a      software packages Materials\u000d\u000a      Studio, DL-Poly, AMBER and NAMD (see section 4 below). The implementation\u000d\u000a      in Materials Studio\u000d\u000a      was partly carried out by Leimkuhler's PhD student Matthews who spent a\u000d\u000a      residency (2012) at the\u000d\u000a      Cambridge headquarters of Accelrys, the company licensing Materials\u000d\u000a      Studio. Most of the methods\u000d\u000a      were developed with funding from the Science and Innovation Centre for\u000d\u000a      Numerical Algorithms and\u000d\u000a      Intelligent Software (NAIS). The thermostatting methods (including the NHL\u000d\u000a      methods and large\u000d\u000a      stepsize isokinetic discretizations) and generalizations are also being\u000d\u000a      implemented by Leimkuhler\u000d\u000a      and his team as part of the MIST (Molecular Integration Software Toolkit)\u000d\u000a      within the ExTASY\u000d\u000a      (Extensible Tools for Advanced Sampling and analYsis) framework. This is a\u000d\u000a      major (&#163;2M)\u000d\u000a      software initiative funded under the bi-national NSF-EPSRC Software\u000d\u000a      Infrastructure for Sustained\u000d\u000a      Innovation programme.\u000d\u000a    Attribution. B. Leimkuhler has been Professor of Applied\u000d\u000a      Mathematics at the Maxwell Institute\u000d\u000a      (MI) since 2006. His PhD students at the MI, E. Noorizadeh (graduated in\u000d\u000a      2010) and C. Matthews\u000d\u000a      (graduated in 2013), contributed to the research as did O. Penrose (MI),\u000d\u000a      A. Jones (Edinburgh,\u000d\u000a      Physics &amp; Astronomy) and F. Theil (Warwick).\u000d\u000a    "},{"CaseStudyId":"23935","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000a    The model initially developed in the Maxwell Institute was used by Waugh\u000a      to assess the impact of proposed housing policy on homelessness in\u000a      Scotland. Waugh's results based on what became known as the Waugh\u000a        model gained influence in the Scottish Government [4-6]: it was\u000a      included in a paper to the then Minister for Communities and Support\u000a      (Stewart Maxwell) in 2008 and has been used since to provide advice to the\u000a      Scottish Government on the interpretation of homelessness statistics and\u000a      on the impact of policy changes.\u000a    Policy impact. The model has had a clear and ongoing impact on the\u000a      Scottish Government's housing policy by providing a quantitative\u000a      underpinning to proposed policies. By characterising and quantifying the\u000a      interaction between the availability of public-sector housing and\u000a      homelessness levels the model has motivated several changes in policy [6].\u000a      These include a change in the Scottish Government's Affordable Housing\u000a      Investment Programme, massively increasing public-sector house build; the\u000a      removal of right to buy for new lets, and the targeted use of\u000a      private-sector rented accommodation. The following impacts have been\u000a      achieved.\u000a    \u000a      Through its continuing use of the model the Scottish Government now\u000a        assesses &#8212; on a systematic, quantitative basis &#8212; the implications of key\u000a        aspects of the implementation of homelessness policy for each of\u000a        Scotland's 32 local authorities.\u000a      The work influenced the distribution of &#163;644M of funding allocated by\u000a        the Affordable Housing Investment Programme in 2009\/10 [7,8].\u000a      The model informed the analysis by the Scottish Government policy\u000a        team, senior officials and Ministers of the relative impact of different\u000a        policy levers on homelessness and influenced the Government's response\u000a        to dealing with the problems of the size of homeless populations and the\u000a        limited ability of local authorities to respond to the need. In\u000a        particular it had a significant influence on the design of policies\u000a        relating to: (a) the right to buy (RTB); (b) the role of the private\u000a        rented sector (PRS); (c) interaction with the Scottish Housing Quality\u000a        Standard; (d) the potential role of housing associations in meeting\u000a        homelessness need.\u000a\u0009\u0009\u000a      For (3a) the model highlighted the long-term impact of RTB on eroding\u000a        the supply of social lets. In particular, the model in [2] quantified\u000a        the long-term adverse effects on homelessness levels and waiting-list\u000a        sizes of reducing public housing stock, suggesting that a reduction in\u000a        RTB sales would secure the future supply of social housing and encourage\u000a        building by local authorities. Consequently, RTB was removed from all\u000a        new tenancies and new-build properties on 11 March 2011, under the\u000a        Housing (Scotland) Act 2010 [9] which overturned a major piece of 1980s\u000a        legislation. As a result of this change of policy, new build by local\u000a        authorities has increased from 6 units in 2006\/07 to 583 units in\u000a        2010\/11 [10].\u000a      Regarding (3b), the model was instrumental in identifying areas of\u000a        Scotland where a significant private rented sector enabled an effective\u000a        use of PRS to house homeless households, and areas where the PRS was too\u000a        small to make a difference. Whilst the model predictions dispelled the\u000a        view that the PRS would be a `magic bullet' to address homelessness\u000a        across Scotland, they demonstrated its potential value in some areas.\u000a        Consequently, greater use of the PRS was enabled though The Homeless\u000a        Persons (Provision of Non-permanent Accommodation) (Scotland)\u000a        Regulations 2010 [11]. As a result of the policies introduced with the\u000a        support of the model, the number of cases assessed as homeless or\u000a        threatened with homelessness in Scotland has decreased by 10,000 (24%)\u000a        between 2008\/09 and 2012\/13 [12].\u000a\u0009\u0009\u000a      The model has been applied to understand the potential impact of a key\u000a        policy initiative &#8212; the Scottish Housing Quality Standard &#8212; on the 2012\u000a        Homelessness commitment. As local authorities work towards meeting the\u000a        Scottish Housing Quality Standard by April 2015, properties without any\u000a        long-term future are earmarked for demolition and a need arises to\u000a        re-house tenants of the demolition stock within the local authority's\u000a        core stock. Consequently, fewer lets are available to other households\u000a        on the waiting list, including homeless households. Moreover, demolition\u000a        stock is most frequently located in higher turnover areas. Demolishing\u000a        these properties often results in a disproportionate reduction in\u000a        available lets. These impacts on supply can potentially make meeting the\u000a        Scottish Government's 2012 homelessness commitment more difficult. The\u000a        aim of the 2012 homelessness commitment is to give all unintentionally\u000a        homeless households permanent accommodation, typically a let from a\u000a        local authority or housing association landlord, and this may lead to\u000a        increased pressure for social housing in some areas. The model has been\u000a        adapted to investigate the supply of social lets in each local authority\u000a        area, taking account of demolition programmes, and highlighted the\u000a        reduction in supply of social lets that can occur.\u000a    \u000a    Other developments. Local authorities have a duty to provide\u000a      temporary accommodation until permanent accommodation is secured.\u000a      Continuing work within the Scottish Government, is using the model to\u000a      identify whether there exists a sufficient supply of social housing in\u000a      each local-authority area in order for: lets to homeless households to\u000a      account for no more than say, a certain proportion of all available lets;\u000a      and the number of households in temporary accommodation does not increase\u000a      from its current level. Where supply is deemed to be insufficient to meet\u000a      these criteria, the minimum amount of new build is calculated to ensure\u000a      that both are met. The model has therefore enabled local authorities to\u000a      reduce the risk of a financial shortfall through having substantial\u000a      numbers of homeless households in more expensive forms of temporary\u000a      accommodation. The model is also attracting international attention, with\u000a      Waugh presenting his results at a conference on homelessness hosted by the\u000a      EU Committee of Regions [13].\u000a    ","ImpactSummary":"\u000a    In 1996 Byatt-Smith, Lacey and Parker (all Maxwell Institute, MI) and\u000a      co-workers developed a mathematical model of housing allocation to examine\u000a      the impact of housing policies on homelessness in England and Wales. The\u000a      model was subsequently adapted to the Scottish context by Lacey and Waugh\u000a      (MI). Since 2008, it has been used by the Scottish Government to help\u000a      inform its housing policy, enabling it to target development funding for\u000a      new build to areas of greatest homelessness need and meet its 2012\u000a      homelessness commitment. The model has provided quantitative underpinning\u000a      for major policy changes enacted in Scotland during the period from 2008:\u000a      the right to buy public-sector housing has been limited, and regions where\u000a      private rented sector housing has the potential to provide housing for\u000a      homeless households have been identified. This has resulted in a marked\u000a      increase of public-sector house builds between 2005\/06 (6 starts) and\u000a      2009\/10 (538 starts). The research informed the allocation of &#163;644M in\u000a      2009\/10 contributing to a 14% reduction in homelessness in Scotland\u000a      between 2008\/09 and 2012\/13.\u000a    ","ImpactType":"Political","Institution":"\u000a    The University of Edinburgh\/Heriot-Watt University\u000a    ","Institutions":[{"AlternativeName":"Edinburgh (University of)","InstitutionName":"University of Edinburgh","PeerGroup":"A","Region":"Scotland","UKPRN":10007790},{"AlternativeName":"Heriot-Watt University","InstitutionName":"Heriot-Watt University","PeerGroup":"B","Region":"Scotland","UKPRN":10007764}],"Panel":"B         ","PlaceName":[],"References":"\u000a    References marked (*) best indicate the quality of the research.\u000a    \u000a[1]* Byatt-Smith J.G. et al., Homeless Populations (Final report,\u000a      29th European Study group with Industry) Oxford, March 1996. \u000a        http:\/\/www.maths.ed.ac.uk\/~mthdat25\/housing\/Homeless-\u000a        populations-final-report-29th-European-Study-group-with-industry\u000a    \u000a\u000a[2]* Byatt-Smith, J.G., Lacey, A.A., Parker, D.F., Simpson, D. Smith W.R.\u000a      and Wattis J.A.D., Mathematical Modelling of Homeless Population, Math.\u000a        Scientist, 28, 1-12, (2003).\u000a      http:\/\/www.maths.ed.ac.uk\/~mthdat25\/housing\/Mathematical-modelling-of-homeless-population\u000a    \u000a\u000a[3]* Waugh, A.J., The Mathematical Modelling of Council House\u000a        Allocation and its Effect on Homeless Applicants, Doctoral Thesis,\u000a      Heriot-Watt University, (2001).\u000a      http:\/\/www.maths.ed.ac.uk\/~mthdat25\/housing\/The-mathematical-modelling-of-council-house-\u000a        allocation-and-its-effect-on-homeless-applicants\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"16","Level2":"5","Subject":"Policy and Administration"},{"Level1":"16","Level2":"8","Subject":"Sociology"}],"Sources":"\u000a    [4] The use of the model by the Scottish Government is described in the\u000a      report Background to the Homelessness Capacity Model (The Waugh\u000a      Model) Communities Analytical Services, February (2009), http:\/\/www.scotland.gov.uk\/Resource\/Doc\/1033\/0084078.doc\u000a    [5] Homelessness and Affordable Housing Need is a 2009\u000a      presentation by the Scottish Government's Communities Analytics Services\u000a      that makes extensive use of the prediction of the Waugh model,\u000a      www.scotland.gov.uk\/Resource\/Doc\/1125\/0076878.ppt\u000a    [6] The impact of the model on decision making in the Scottish Government\u000a      can be confirmed by a senior economist and a former Senior Statistician of\u000a      the Communities Analytical Services.\u000a    [7] The Scottish Government Affordable Housing Investment Programme is\u000a      described at\u000a      http:\/\/www.scotland.gov.uk\/Topics\/Built-Environment\/Housing\/investment1\/ahip\u000a    [8] The funding allocation in 2007-10 is detailed at http:\/\/www.scotland.gov.uk\/Topics\/Built-\u000a        Environment\/Housing\/investment\/ahip\/ahip-2009-10\u000a    [9] The changes to the RTB enacted in the Housing (Scotland) Act 2010 are\u000a      described at\u000a      http:\/\/www.scotland.gov.uk\/Publications\/2011\/02\/02164100\/2\u000a    [10] Statistics on local authority new build housing are available at\u000a      http:\/\/www.scotland.gov.uk\/Topics\/Statistics\/Browse\/Housing-Regeneration\/HSfS\/NewBuildLA\u000a    [11] The Homeless Persons (Provision of Non-permanent Accommodation)\u000a      (Scotland) Regulations 2010 is available at http:\/\/www.legislation.gov.uk\/ssi\/2010\/2\/contents\/made\u000a    [12] Statistics on the number of homeless are in Table 8 of\u000a      http:\/\/www.scotland.gov.uk\/Topics\/Statistics\/Browse\/Housing-Regeneration\/RefTables\/adhoc-\u000a        analysis\/annualreferencetables201213\u000a    [13] Details of the EU conference Homelessness in public and private\u000a        spaces: Mind the policy gap! (2010) are at http:\/\/www.feantsa.org\/spip.php?article164&amp;lang=en\u000a        \u000a    ","Title":"\u000a    Impact of a mathematical model of housing allocation on governmental\u000a        policies to ameliorate homelessness\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"},{"GeoNamesId":"2638360","Name":"Scotland"},{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Mathematical model. Following a court ruling (the Awua judgement,\u000a      1995), the duty of local authorities in England and Wales towards housing\u000a      families deemed to be homeless was reduced. It became imperative to\u000a      investigate the impact of these changes on homeless families and others on\u000a      local authorities' waiting lists. The problem was brought to the attention\u000a      of a European Study Group with Industry in 1996 by Don Simpson, a housing\u000a      consultant and trustee of the housing charity Shelter. It was tackled by\u000a      Byatt-Smith, Lacey, Parker (all Maxwell Institute) and others [1]. They\u000a      developed a mathematical model in which the population of a\u000a      local-authority area in private-or public-sector accommodation, whether on\u000a      a housing waiting list or not, and the homeless is modelled by a\u000a      deterministic system of five ODEs. Analytical and numerical solutions were\u000a      obtained after using decoupling to reduce the model to three differential\u000a      equations [2]. These solutions predicted that reducing the relative\u000a      priority given to re-housing homeless households would have little effect\u000a      on other households on the housing list, but would have a major impact on\u000a      the number of homeless families in temporary accommodation. Moreover,\u000a      because of the multiple time scales exhibited by its dynamics &#8212; including\u000a      one of around 30 years &#8212; the model predicted that changes in policy could\u000a      take decades, or several electoral cycles, to have full effect. The model\u000a      also highlighted the sensitivity of levels of homelessness to availability\u000a      of public-sector housing, showing that any decrease in supply, say through\u000a      right-to-buy, can lead to markedly increased housing lists and\u000a      homelessness.\u000a    Extended models. The original model was extended by Lacey and his\u000a      MSc and PhD student Waugh in order to represent the housing allocation and\u000a      other relevant administration procedures that operate in Edinburgh and\u000a      Glasgow. Edinburgh Council operated a points-based allocation system to\u000a      determine the priority of individuals on its waiting list, with the number\u000a      of points determined in part by the time spent on the list. It proved\u000a      therefore necessary to use a first-order hyperbolic PDE in combination\u000a      with the ODE-based population model in order to represent the system\u000a      adequately. In the case of Glasgow a purely ODE model, with rate laws\u000a      based on `goodness of fit' between applicants and available housing, was\u000a      sufficient. These extensions are documented in Waugh's PhD thesis [3]. The\u000a      model was further developed by Waugh while he worked for a housing charity\u000a      and a housing consultancy, and while on secondment to the Scottish\u000a      Government in 2007\/08.\u000a    Attribution. A. A. Lacey (MI since 1982), D. F. Parker and J. G.\u000a      Byatt-Smith were Professors and Senior Lecturer at the Maxwell Institute\u000a      during their contributions to the research. A. J. Waugh was an MSc then\u000a      PhD student at the Maxwell Institute until his graduation in 2001. He\u000a      continued to develop the model in his subsequent employments in a charity,\u000a      a consultancy and the Scottish Government.\u000a    "},{"CaseStudyId":"23936","Continent":[{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Control of C. difficile. The immediate impact of\u000d\u000a      the research was to inform changes to healthcare practices in NHS\u000d\u000a        Lothian medicine-for-the-elderly wards which served to reduce the\u000d\u000a      incidence of C. difficile, with consequent improvements in patient\u000d\u000a      outcomes. A control strategy, based on control of broad-spectrum\u000d\u000a      antibiotic prescribing and informed by the mathematical models, was\u000d\u000a      identified and implemented in a 12-month pilot study from November 2007\u000d\u000a      across seven medicine-for-the-elderly wards in Edinburgh hospitals. During\u000d\u000a      the pilot study there were a total of 60 cases of C. difficile\u000d\u000a      infection in the wards, compared to 120 during the previous 12 months.\u000d\u000a      Following this initial success, and other pilot studies, a tool-kit of\u000d\u000a      measures was rolled out across all acute wards in Lothian (the 2nd\u000d\u000a      largest NHS region in Scotland) in September 2008. The NHS Lothian\u000d\u000a      consultant who led the pilot study has indicated that `the mathematical\u000d\u000a      and statistical models were key to providing the quantitative evidence\u000d\u000a      necessary to justify the approach taken in the pilot studies' [7].\u000d\u000a    Following the introduction of the toolkit of measures, the incidence of C.\u000a        difficile declined considerably. Monthly median incidence in Lothian\u000d\u000a      during the 15 months from Jan 2007 &#8212; March 2008, was 106 cases per month.\u000d\u000a      During the 24 months from April 2008 &#8212; March 2010, the monthly median\u000d\u000a      incidence was 61 cases per month [9,10]. A more recent estimate of the\u000d\u000a      impact of the control measures can be found in the summary document NHS\u000d\u000a      Lothian at a Glance HEAT 2011-12 Target Performance [11] which notes the\u000d\u000a      `significant progress in reducing the rate of C. difficile\u000d\u000a      infections' and that `Year-end rates for new cases represented a reduction\u000d\u000a      of 74% compared to 2007-8'.\u000d\u000a    As well as the beneficial changes to clinical practices, disease\u000d\u000a      prevention, and patient health outcome, reduction in mortality and\u000d\u000a      morbidity, the research has contributed to substantial economic benefits.\u000d\u000a      The management of each clinical case of C. difficile infection was\u000d\u000a      estimated to cost &#163;4k in 2000. Thus, a 74% reduction from monthly median\u000d\u000a      levels of the order of 100 cases per month represents &#8212; not accounting for\u000d\u000a      cost inflation &#8212; a reduction of monthly costs of the order of &#163;300k\u000d\u000a      translating to annual reductions of around &#163;3.5M. Moreover, there is the\u000d\u000a      `opportunity-cost' benefit that increased infection-control resources can\u000d\u000a      now be targeted at other pathogens, such as norovirus, or other healthcare\u000d\u000a      issues. The impact been extended from Lothian to the whole of Scotland\u000d\u000a      with the adoption of measures to control antimicrobial prescribing by NHS\u000d\u000a      Scotland [8] and a proportionate reduction in C. difficile\u000d\u000a      infection levels has been seen.\u000d\u000a    Control of arboreal pathogens. The spatio-temporal\u000d\u000a      parameter-estimation techniques [4-6] were adopted by the epidemiology\u000d\u000a      group at Cambridge in a major initiative to develop a spatio-temporal\u000d\u000a      modelling toolbox that could be applied to a wide variety of plant\u000d\u000a      diseases [12]. This group was approached by Defra and US\u000d\u000a        Department of Agriculture (USDA) to apply the toolbox to a range of\u000d\u000a      emerging plant pathogens, including sudden oak death, a devastating\u000d\u000a      disease that is threatening woodlands in California and woodlands and\u000d\u000a      heathlands in the UK, and whose host range includes more than 100\u000d\u000a      economically and ecologically important woody hosts. In the Californian\u000d\u000a      context, models parameterised using the methods [4-6] were employed to\u000d\u000a      demonstrate the value of early action in detecting and controlling\u000d\u000a      disease, to determine the regions of the state at greatest risk up to\u000d\u000a      2030, and to demonstrate that creating barriers by removing large areas of\u000d\u000a      vegetation is unlikely to work. More widely, they have been used to inform\u000d\u000a      US policy advisers and policy makers about the risks of spread of sudden\u000d\u000a      oak death in Eastern states of the US [13].\u000d\u000a    In the UK, The Forestry Commission and Defra are using\u000d\u000a      the models to inform, adjust and implement sampling and disease control\u000d\u000a      policies for sudden oak death throughout England and Wales as part of a\u000d\u000a      &#163;25M eradication and control scheme launched in 2010 [14]. Specifically\u000d\u000a      the models formulated by the Cambridge group since 2008 allow comparison\u000d\u000a      of different `what-if' scenarios about the likelihood of disease spread\u000d\u000a      and are providing policy makers with information about the likely efficacy\u000d\u000a      of different culling distances and sampling frequencies, and to guide\u000d\u000a      aerial and ground surveys for the disease, using `hazard maps' predicted\u000d\u000a      by the parameterised models. The insights on sudden oak dynamics from the\u000d\u000a      US setting &#8212; such as the structure of appropriate models for pathogen\u000d\u000a      dispersal and the parameter ranges &#8212; obtained using the parameter\u000d\u000a      estimation method of [4-6], were utilised directly in the initial\u000d\u000a      modelling studies of the UK sudden oak death epidemic, and helped to\u000d\u000a      underpin the subsequent advice to policy makers [14].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    In a series of papers from 2003, Gibson (Maxwell Institute) and\u000d\u000a      collaborators developed Bayesian computational methods for fitting\u000d\u000a      stochastic models for epidemic dynamics. These were subsequently applied\u000d\u000a      to the design of control programmes for pathogens of humans and plants. A\u000d\u000a      first application concerns the bacterial infection Clostridium\u000d\u000a        difficile in hospital wards. A stochastic model was developed which\u000d\u000a      was instrumental in designing control measures, rolled out in 2008 across\u000d\u000a      NHS Lothian region, and subsequently adopted across NHS Scotland.\u000d\u000a      Incidence in Lothian reduced by around 65%, saving an estimated &#163;3.5M per\u000d\u000a      annum in treatment and other costs, reducing mortality and improving\u000d\u000a      patient outcomes, with similar impacts elsewhere in Scotland. A second\u000d\u000a      application concerns the spread of epidemics of plant disease in\u000d\u000a      agricultural, horticultural and natural environments. Models developed in\u000d\u000a      collaboration with plant scientists from Cambridge have been exploited by\u000d\u000a      the Department for Environment, Food and Rural Affairs (Defra) and the\u000d\u000a      Forestry Commission under a &#163;25M scheme, initiated in 2009, to control\u000d\u000a      sudden oak death in the UK, and by the United States Department of\u000d\u000a      Agriculture to control sudden oak death in the USA.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    The University of Edinburgh\/Heriot-Watt University (Maxwell Institute)\u000d\u000a    ","Institutions":[{"AlternativeName":"Edinburgh (University of)","InstitutionName":"University of Edinburgh","PeerGroup":"A","Region":"Scotland","UKPRN":10007790},{"AlternativeName":"Heriot-Watt University","InstitutionName":"Heriot-Watt University","PeerGroup":"B","Region":"Scotland","UKPRN":10007764}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"5332921","Name":"California"}],"References":"\u000d\u000a    References marked (*) best indicate the quality of the research.\u000d\u000a    \u000a[1] Starr, J.M., Martin, H., McCoubrey, J., Gibson, G. and Poxton, I. R.,\u000d\u000a      Risk factors for Clostridium difficilecolonisation and toxin\u000d\u000a      production, Age and Ageing, 32, 657-660 (2003).\u000d\u000a      http:\/\/dx.doi.org\/10.1093\/ageing\/afg112\u000d\u000a    \u000a\u000a[2]* Forrester, M.L., Pettitt, A.N. and Gibson, G.J., Bayesian inference\u000d\u000a      of hospital-acquired infectious diseases and control measures given\u000d\u000a      imperfect surveillance data, Biostatistics, 8, 383-401\u000d\u000a      (2007). http:\/\/dx.doi.org\/10.1093\/biostatistics\/kxl017\u000d\u000a    \u000a\u000a[3]* Starr, J.M., Campbell, A., Renshaw, E., Poxton, I.R. and Gibson,\u000d\u000a      G.J., Spatio-temporal stochastic modelling of Clostridium difficile.\u000d\u000a      Journal of Hospital Infections, 71, 49-56 (2009). http:\/\/dx.doi.org\/10.1016\/j.jhin.2008.09.013\u000d\u000a    \u000a\u000a[4] Gibson, G.J., Otten, W, Filipe, J.N.F., Cook, A.R., Marion, G. and\u000d\u000a      Gilligan, C.A., Bayesian estimation for percolation models of disease\u000d\u000a      spread in plant communities, Statistics &amp; Computing 16,\u000d\u000a      391-402 (2006). http:\/\/dx.doi.org\/10.1007\/s11222-006-0019-z\u000d\u000a    \u000a\u000a[5]* Cook, A.R., Otten, W., Marion, G., Gibson, G.J. and Gilligan, C. A.,\u000d\u000a      Estimation of multiple transmission rates for epidemics in heterogeneous\u000d\u000a      populations, Proc. Nat. Acad. Sciences, 104, 20392-20397\u000d\u000a      (2007). http:\/\/dx.doi.org\/10.1073\/pnas.0706461104\u000d\u000a    \u000a\u000a[6]* Cook, A.R., Gibson, G.J., Gottwald, T.R. and Gilligan, C.A.,\u000d\u000a      Constructing the effect of alternative intervention strategies on historic\u000d\u000a      epidemics. J. Roy. Soc. Interface 5: 1203-1213 (2008). http:\/\/dx.doi.org\/10.1098\/rsif.2008.0030\u000d\u000a    \u000aFunding\u000d\u000a    Research Grants: BB\/C007263\/1. Experimental design for stochastic\u000d\u000a      dynamical models in the life sciences, Gibson (PI), C. A. Gilligan (Co-I),\u000d\u000a      2005-9 (&#163;183k).\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    Control of C. difficile\u000d\u000a    [7] Through their position as Consultant at NHS Lothian: for\u000d\u000a      corroboration of the importance of the inference and predictive modelling\u000d\u000a      in designing the control strategy.\u000d\u000a    [8] Senior Manager, NHS Lothian: for corroboration of the effectiveness\u000d\u000a      of controlling antimicrobial prescribing on levels of C. difficile\u000d\u000a      in NHS Lothian and of the wider impact across NHS Scotland.\u000d\u000a    [9] Guthrie et al., Reduction of Clostridium difficile in\u000d\u000a      NHS Lothian using a toolkit approach, Poster, NHS Scotland Event &#8212; Sharing\u000d\u000a      the learning, 2010.\u000d\u000a      http:\/\/www.knowledge.scot.nhs.uk\/media\/CLT\/ResourceUploads\/21710\/CE21.pdf\u000d\u000a    [10] Media articles citing effectiveness of control measures for C.\u000d\u000a        difficile in NHS Lothian region include: http:\/\/news.stv.tv\/east-central\/85809-decrease-in-levels-of-cdiff-in-lothian\/\u000d\u000a    [11] NHS Lothian at a Glance, HEAT 2011-12, Target Performance.\u000d\u000a      http:\/\/www.nhslothian.scot.nhs.uk\/OurOrganisation\/KeyDocuments\/AnnualReviews\/NHS%20Lo\u000a        thian%202012%20Performance%20Handout.pdf\u000d\u000a    Control of arboreal pathogens\u000d\u000a    [12] Senior Academic, Department of Plant Sciences, Cambridge University:\u000d\u000a      for corroboration of the role of the inferential techniques for\u000d\u000a      parameterisation of models within the toolkit.\u000d\u000a    [13] Senior Scientist, US Department of Agriculture, Florida: to provide\u000d\u000a      evidence of impact of models in practical disease control for tree\u000d\u000a      diseases in the US.\u000d\u000a    [14] Senior Scientist, Forestry Commission: for evidence of the use of\u000d\u000a      the models to inform Forestry Commission Policy on practical control\u000d\u000a      decisions about where and how to control sudden oak death on Forestry\u000d\u000a      Commission land. \u000d\u000a    ","Title":"\u000d\u000a    Statistical methods are helping to control of the spread of epidemics\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2650225","Name":"Edinburgh"}],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"},{"GeoNamesId":"2634895","Name":"Wales"},{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Statistical methods for parameter estimation. Research carried out\u000d\u000a      by Gibson (Maxwell Institute) and collaborators formulated and tested\u000d\u000a      Bayesian methods for estimating parameters in stochastic epidemics models\u000d\u000a      that could take account of the incomplete nature of observations typically\u000d\u000a      available in real-world settings. The methods drew extensively on modern\u000d\u000a      computational approaches including data augmentation and Markov chain\u000d\u000a      Monte Carlo and were subsequently applied in interdisciplinary studies on\u000d\u000a      a) hospital acquired infections and b) the spatio-temporal spread of plant\u000d\u000a      pathogens.\u000d\u000a    Hospital-acquired infections. Gibson in collaboration with Renshaw\u000d\u000a      (Strathclyde), PhD student Campbell (Strathclyde), and Starr (NHS)\u000d\u000a      formulated models for the dynamics of C. difficile in hospital\u000d\u000a      wards. Model development was informed by data from two\u000d\u000a      medicine-for-the-elderly wards, collected over a 17-month period under an\u000d\u000a      earlier project (PIs: Gibson, Starr and Poxton (University of Edinburgh),\u000d\u000a      1998-2001), which recorded the infection status of individuals and\u000d\u000a      incidence of clinical cases of C. difficile. Analysis of these\u000d\u000a      data highlighted the potential importance of, for example, the use of\u000d\u000a      certain antibiotics, as risk factors [1]. The main analysis of the data,\u000d\u000a      published in final form in [3] presented a stochastic compartment model\u000d\u000a      for C. difficile dynamics which allowed for potentially differing\u000d\u000a      transmission rates between patients in the same and different rooms within\u000d\u000a      wards, various resistant and susceptible states and the possible\u000d\u000a      transitions between them, and the random nature of C. difficile\u000d\u000a      infection. As the data had been collected in the course of day-to-day\u000d\u000a      running of the wards, they did not constitute an exhaustive census of the\u000d\u000a      population. The Bayesian data-augmentation and computational techniques\u000d\u000a      used in [3] were therefore instrumental in allowing the stochastic models\u000d\u000a      to be parameterised in a statistically sound fashion, and to be used to\u000d\u000a      predict outcomes and inform the control strategies. The model was applied\u000d\u000a      to predict the potential reduction in disease incidence that might be\u000d\u000a      achieved by controlling several factors associated with C. difficile\u000d\u000a      infection. These results, together with the earlier risk analysis [1],\u000d\u000a      provided quantitative evidence that reducing the rate of transition of\u000d\u000a      patients to the susceptible compartment could be highly effective. They\u000d\u000a      supported the implementation of a control strategy for C. difficile\u000d\u000a      based on antibiotic control across Lothian. The Bayesian data-augmentation\u000d\u000a      techniques were also refined and applied by Gibson (2003-2006) in\u000d\u000a      collaboration with Pettitt and Forester (Queensland University of\u000d\u000a      Technology) to characterise the dynamics of meticillin-resistant Staphylococcus\u000a      aureusis (MRSA) in intensive care units [2].\u000d\u000a    Spatio-temporal spread of plant pathogens. Gibson and Cook\u000d\u000a      (Maxwell Institute) and Gilligan and colleagues (Plant Sciences,\u000d\u000a      Cambridge) tailored the Bayesian data-augmentation approach to fitting\u000d\u000a      spatio-temporal models of plant pathogens, for which data typically record\u000d\u000a      only snapshots of the infected population at sparse sampling times.\u000d\u000a      Algorithms were initially developed using data from epidemics in\u000d\u000a      laboratory microcosms on the fungal pathogen Rhizoctonia solani\u000d\u000a      [4, 5] before being applied to the larger-scale Miami citrus canker\u000d\u000a      epidemic [6]. The methods enabled estimation of key parameters controlling\u000d\u000a      the spatial and temporal dynamics of dispersal of pathogens, the\u000d\u000a      prediction of pathogen spread, and assessment of the likely efficacy of\u000d\u000a      putative control measures. Cook (Maxwell Institute) worked with the\u000d\u000a      epidemiology group, Plant Sciences, Cambridge, during 2007-8 on\u000d\u000a      host-pathogen systems for which spatio-temporal models were required, and\u000d\u000a      subsequently applied the methods developed in [4-6] to data on sudden oak\u000d\u000a      death. This work has had a direct bearing on policy for controlling the\u000d\u000a      spread of this disease. Further collaborative studies involving Gibson\u000d\u000a      have employed the methods to estimate dispersal characteristics of citrus\u000d\u000a      canker and citrus greening.\u000d\u000a    Attribution. G.J. Gibson has been Professor of Statistics at the\u000d\u000a      Maxwell Institute since 2000. His collaborators were from the University\u000d\u000a      of Strathclyde (A. Campbell, E. Renshaw), NHS Lothian (J. M. Starr),\u000d\u000a      University of Edinburgh (I. Poxton, H. Martin, J. McCoubrey), University\u000d\u000a      of Cambridge (C. A. Gilligan, W. Otten) and Queensland University of\u000d\u000a      Technology (M. L. Forrester, A. N. Pettitt). A. Cook was a PhD student in\u000d\u000a      the Maxwell Institute (2002-6) and an RA in the Maxwell Institute on\u000d\u000a      BB\/C007263\/1 (2005-8).\u000d\u000a    "},{"CaseStudyId":"23937","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000a    The methodology developed in the research has been adopted by Barrie\u000a      &amp; Hibbert (B&amp;H) and led to the economic impact described below.\u000a      B&amp;H, a part of Moody's Analytics, is a provider of financial risk\u000a      models to the financial services sector; over $25 trillion of assets and\u000a      liabilities are valued and managed with B&amp;H support; over 60% of the\u000a      insurers in the Global Fortune 500 are B&amp;H clients. The company has\u000a      benefitted from the research in three ways.\u000a    New products and services. The research has allowed B&amp;H to\u000a      develop a LSMC solution for projecting the value of complex liabilities in\u000a      a computationally efficient way. This projection is a key part of the\u000a      Solvency II calculation for firms adopting an internal model approach.\u000a      B&amp;H already sell a leading economic scenario generator (ESG) and the\u000a      LSMC solution uses this platform and allows them to sell a new set of\u000a      services to their existing international clients and to gain new clients\u000a      for their ESG.\u000a    A key part of the credibility of the LSMC solution is the fact that it is\u000a      backed by extensive research and that the work carried out in Cathcart's\u000a      thesis has given clear answers to the implementation issues mentioned\u000a      above. In particular it provides reassurance to clients that the method\u000a      produces good results and that the technical choices made by B&amp;H are\u000a      backed by extensively documented investigations.\u000a    According to a senior executive of B&amp;H, `The commercialisation of\u000a      this research effort has been one of the most successful projects that\u000a      B&amp;H has undertaken' [7]. Since 2011 B&amp;H has taken this research\u000a      and seen commercial benefit in a number of areas:\u000a    \u000a      Supporting B&amp;H Brand &#8212; The technical aspects of these techniques\u000a        and the associated promotion and marketing have helped to further\u000a        support the association of the B&amp;H brand with technical excellence\u000a        in Monte Carlo simulation.\u000a      The work has led to 12 consulting projects generating &#163;0.75M ($1.2M)\u000a        in revenue.\u000a      It has led to 5 new product implementations generating &#163;0.8M ($1.3M)\u000a        in revenue.\u000a      The pipeline for the balance of 2013 foresees circa &#163;0.95M ($1.5M) in\u000a        revenue for existing products.\u000a      Estimated revenue over the next 5 years is circa &#163;6M ($10M)\u000a        representing 12% of total B&amp;H revenue.\u000a    \u000a    Provision of research-based consultancy or training. The services\u000a      provided by B&amp;H include consultancy and training for clients. There is\u000a      a demand for this as the LSMC approach is a relatively sophisticated\u000a      approach that requires specialist expertise that many insurers cannot\u000a      routinely access. The research project has provided an extensive set of\u000a      examples and training materials and has supported 6 commercial consultancy\u000a      projects.\u000a    Improved risk assessment and management. For firms purchasing the\u000a      LSMC solution the end impact of the research is improved risk assessment.\u000a      Clients and their regulators have much more confidence in the estimates of\u000a      the distribution of future values of complex liabilities and the capital\u000a      calculations that are based on these [7].\u000a    ","ImpactSummary":"\u000a    Research by Cathcart, McNeil (both Maxwell Institute) and Morrison\u000a      (Barrie &amp; Hibbert) during the period 2008-2012 has developed a\u000a      methodology based on least squares Monte Carlo to value complex insurance\u000a      liabilities and manage their risks. This methodology has been adopted by\u000a      Barrie &amp; Hibbert (B&amp;H, part of Moody's Analytics) and has enabled\u000a      the company to develop an internationally leading proposition for valuing\u000a      insurance products. This has generated &#163;2.5M in revenue since 2011,\u000a      through implementation in 5 new products and use in 12 new consulting\u000a      projects.\u000a    ","ImpactType":"Economic","Institution":"\u000a    The University of Edinburgh\/Heriot-Watt University (Maxwell Institute)\u000a    ","Institutions":[{"AlternativeName":"Edinburgh (University of)","InstitutionName":"University of Edinburgh","PeerGroup":"A","Region":"Scotland","UKPRN":10007790},{"AlternativeName":"Heriot-Watt University","InstitutionName":"Heriot-Watt University","PeerGroup":"B","Region":"Scotland","UKPRN":10007764}],"Panel":"B         ","PlaceName":[],"References":"\u000a    References marked with a * best indicate the quality of the research.\u000a    \u000a[1] Kretzschmar G., McNeil A.J. and Kirchner A., Integrated models of\u000a      capital adequacy - why banks are undercapitalised, Journal of Banking\u000a        and Finance, 34(12), 2838-2850 (2010). http:\/\/dx.doi.org\/\/10.1016\/j.jbankfin.2010.02.028\u000a    \u000a\u000a[2] Cathcart M.J. and Morrison S., Variable annuity economic capital: the\u000a      least squares Monte-Carlo approach, Life and Pensions, 36-40\u000a      (2009). http:\/\/www.maths.ed.ac.uk\/~mthdat25\/insurance\/Variable-annuity-economic-capital-the-least-squares-Monte-Carlo-approach\u000a    \u000a\u000a[3] Cathcart M.J. and Morrison S., Least squares Monte Carlo simulation,\u000a      Contingencies, 23(2), 46-52 (2011).\u000a      http:\/\/www.contingenciesonline.com\/contingenciesonline\/20110304?pg=55#pg49\u000a    \u000a\u000a[4]* Cathcart M.J., Monte Carlo simulation approaches to the\u000a        valuation and risk management of unit-linked insurance products with\u000a        guarantees, Doctoral thesis, Heriot-Watt University (2012).\u000a      http:\/\/www.maths.ed.ac.uk\/~mthdat25\/insurance\/Monte-Carlo-simulation-approaches-to-the-valuation-and-risk-management-of-unit-linked-insurance-products-with-guarantees\u000a    \u000a\u000a[5]* Cathcart M.J., McNeil A.J. and Morrison S., Calculating variable\u000a      annuity liability `Greeks' using Monte Carlo simulation, submitted to the\u000a      ASTIN Bulletin (2013).\u000a      http:\/\/www.maths.ed.ac.uk\/~mthdat25\/insurance\/Calculating-variable-annuity-liability-Greeks-using-Monte-Carlo-simulation\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"15","Level2":"2","Subject":"Banking, Finance and Investment"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000a    [7] A senior executive of B&amp;H will confirm the impact of the research\u000a      on B&amp;H and its products. \u000a    ","Title":"\u000a    Valuing complex insurance liabilities using least squares Monte Carlo\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Under the Solvency II regulatory framework, insurers face the challenge\u000a      of valuing assets and liabilities in a market-consistent way and\u000a      projecting these values over a one-year time horizon to make sure that\u000a      they have adequate capital to remain technically solvent (i.e. the value\u000a      of assets exceeds the value of liabilities with high probability). One of\u000a      the greatest problems in this exercise is projecting the values of complex\u000a      unit-linked liabilities, such as variable annuities. These values depend\u000a      on many uncertain risk factors such as interest rates, stock market prices\u000a      and policyholder behaviour; they are not generally available in a\u000a      closed-form formula. The lack of a simple pricing formula means that, for\u000a      any hypothetical set of future risk factor values, the market- consistent\u000a      value of liabilities must be estimated using Monte Carlo simulation in an\u000a      appropriate pricing model.\u000a    Least-square Monte Carlo. A naive approach to the projection\u000a      problem involves simulations within simulations, or `nested' simulations.\u000a      In this approach, future scenarios for the risk factors are generated from\u000a      a realistic model of the real world (often called an economic scenario\u000a        generator) under assumptions about expected policyholder mortality\u000a      and behaviour. Then, a distribution of liability values is obtained by\u000a      taking each scenario in turn and computing a Monte Carlo estimate of the\u000a      discounted pay-off to policyholders in a market-consistent model (often\u000a      referred to as risk-neutral model). This results in a nested simulation\u000a      since for every `outer' real-world scenario a large number of `inner'\u000a      risk-neutral scenarios are used to compute the Monte Carlo estimate, and\u000a      this is typically costly in terms of time and resources. The least squares\u000a      Monte Carlo (LSMC) approach drastically reduces the computational effort,\u000a      and this is key to measuring and managing the risk on a more frequent and\u000a      active basis. In the LSMC method the Monte Carlo estimates of the value of\u000a      the product are computed from only a handful of risk-neutral scenarios,\u000a      sometimes as few as one or two. Although these estimates are crude, they\u000a      can be regressed on the real-world risk factor scenarios to obtain a\u000a      polynomial function that accurately captures the relationship between risk\u000a      factors and liability values. This can then be used to estimate a\u000a      distribution of future values for the liabilities. Related techniques\u000a      allow the calculation of sensitivities to the risk factors, which is vital\u000a      information for hedging some of the financial market risk associated with\u000a      the product.\u000a    New methodology. The LSMC method was originally proposed by\u000a      Longstaff and Schwartz in 2001 in the context of valuing American options.\u000a      The present case study lies in the area of balance sheet and capital\u000a      modelling using economic scenario generators, an area for which McNeil\u000a      (Maxwell Institute, MI) and co-authors Kretzschmar and Kirchner developed\u000a      a modelling framework in [1]. The research of Cathcart (MI), McNeil and\u000a      Morrison (Barrie &amp; Hibbert) demonstrated that the LSMC method could be\u000a      embedded in this framework and optimized for practical use. It addressed\u000a      successfully the numerous implementation issues that had to be solved in\u000a      order to turn the LSMC approach into a viable production solution for\u000a      unit-linked insurance products. Key advances included: the identification\u000a      of the optimal balance between numbers of outer and inner scenarios when\u000a      calibrating the least squares regression model; the development of metrics\u000a      for the performance of the method; the choice of the order of polynomial\u000a      without overfitting; the optimisation of performances for estimating\u000a      extreme quantiles of the distribution of future values; the implementation\u000a      of the technique for typical variable annuity structures; the computation\u000a      of hedging strategies for these products by estimating the sensitivities\u000a      (also known as `the Greeks'). These issues are documented in Cathcart's\u000a      PhD thesis [4]. In articles written for industry journals [2-3], Cathcart\u000a      and Morrison have explained the insights emerging from the thesis work for\u000a      a practitioner audience. A research paper with Alexander McNeil [5]\u000a      develops accurate Monte Carlo approaches to calculating variable annuity\u000a      sensitivities in the form of first-order (delta) and second-order (gamma)\u000a      Taylor series approximations.\u000a    Attribution. A. J. McNeil has been Maxwell Professor in the\u000a      Maxwell Institute since 2006; his PhD student M.J. Cathcart graduated in\u000a      2012. The research was in collaboration with S. Morrison of Barrie &amp;\u000a      Hibbert.\u000a    "},{"CaseStudyId":"23938","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"798544","Name":"Poland"},{"GeoNamesId":"2017370","Name":"Russia"},{"GeoNamesId":"2661886","Name":"Sweden"},{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"298795","Name":"Turkey"},{"GeoNamesId":"2658434","Name":"Switzerland"}],"Funders":["Economic and Social Research Council","Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Beneficiaries. The beneficiaries of the research are forensic\u000a      science services and law-enforcement agencies worldwide. They can now\u000a      optimise the size of the samples they test and quantify in precise\u000a      Bayesian terms the weight of evidence. This impact on professional\u000a      practice in turn improves the judicial system of the countries relying on\u000a      these services and agencies by enabling the best use of the evidence\u000a      available and ultimately leading to safer verdicts.\u000a    Impact on beneficiaries. The impact started in the late 1990s with\u000a      the initial work leading up to [1]: the procedure was referred by the\u000a      SFSLG to the Crown Office in Scotland which approved the ideas and issued\u000a      guidance to the Scottish forensic science laboratories for the procedure\u000a      to be used in cases in which sampling was desirable [9]. Cases (a)-(c) are\u000a      examples of this early impact which led to cost savings and, in the case\u000a      (b) of sampling of pornographic files, to a reduction of stress-related\u000a      illnesses amongst the law enforcement agents examining the files (prior to\u000a      Aitken's involvement, out of four officers of the Strathclyde Police Force\u000a      who examined all files on certain seized computers in a particular case,\u000a      three had to take sick leave on stress-related grounds).\u000a    The impact of [1-2] has considerably extended since 2008, due in part to\u000a      the publication of high-profile guidance documents published by crime\u000a      enforcement agencies that refer to the work; these include the `Guidance\u000a      for best practice sampling in forensic science' published in 2007 by the\u000a      European Network of Forensic Science Institutes (ENFSI, which represents\u000a      forensic science laboratories throughout Europe including Russia, also\u000a      Turkey and some trans-Caucasian countries), and the `Guidelines on\u000a      representative drug sampling' [7] published in 2009 jointly by the United\u000a      Nations Office on Drugs and Crime and ENSFI. The software implementing the\u000a      sampling method of [1-2] is available on the ENFSI website: http:\/\/www.enfsi.org\u000a      (see [8]). It is used widely in\u000a      Europe (including Sweden, The Netherlands, Poland, Switzerland, UK) and is\u000a      disseminated world-wide.\u000a    We document the adoption of Aitken's methodology for both sample-size\u000a      determination [1-2] and LR for multivariate hierarchical data [3-5] by\u000a      describing three specific examples of applications in laboratories in\u000a      Australia, Sweden, and the Netherlands.\u000a    Australian National University. [text removed for publication].\u000a      The method of [3] was applied by ANU consultants to a high-profile court\u000a      case in Australia to estimate the strength of the evidence of a telephone\u000a      conversation. [text removed for publication].\u000a    Since this case, the LR derived in [3] has been used more broadly in\u000a      cases involving voice comparison. A senior staff member of the Forensic\u000a      Voice Comparison Laboratory (University of New South Wales, Australia) has\u000a      commented that `the work on statistical modelling for numerical\u000a        calculation of the strength of forensic evidence [3] has become a\u000a        standard tool in the field of forensic voice comparison' [11].\u000a    Statens Kriminaltekniska Laboratorium (SKL, Swedish National\u000a      Laboratory of Forensic Science). SKL, practices a framework for sampling\u000a      of drug units that is built on [1]. The paper [1] gave rise to a research\u000a      project within SKL, that led to general rules for sampling of pills;\u000a      according to senior SKL staff, the process `has substantially reduced\u000a        the amount of material that needs to be analysed, still preserving the\u000a        precision needed for legal purposes, and has hence increased\u000a        cost-efficiency' [12]. SKL are in the process of implementing the\u000a      approach described in [3] for the comparison of amphetamine seizures and\u000a      for the strengthening of glass evidence by the use of composition\u000a      measurements.\u000a    Netherlands Forensic Institute. The glass experts at the\u000a      Netherlands Forensic Institute now use the method developed in [3] in\u000a      every case as a support to earlier analyses. The verbal statements of the\u000a      value of the evidence that they issue to the court are on both methods and\u000a      on graphical displays. A senior forensic statistician at the Netherlands\u000a      Forensic Institute has commented that `the ground breaking work of\u000a        Aitken and others has transformed the way we evaluate forensic evidence'\u000a      and `the LR method is the next step in the evolution from forensic\u000a        craft to forensic science [13]'.\u000a    ","ImpactSummary":"\u000a    In a series of papers published from 1999 on, Aitken (Maxwell Institute)\u000a      and collaborators applied Bayesian statistics to develop a methodology for\u000a      the quantification of judicial evidence derived from forensic analyses.\u000a      They proposed and implemented procedures for (i) determining the optimal\u000a      size of samples that should be taken from potentially incriminating\u000a      material (such as drugs seized); and (ii) the estimation of likelihood\u000a      ratios characterising evidence provided by multivariate hierarchical data\u000a      (such as the chemical composition of crime-scene samples). Their\u000a      procedures have been recommended in international guideline documents\u000a      (including a 2009 publication by the United Nations Office on Drugs and\u000a      Crime) and have been routinely used by forensic science laboratories\u000a      worldwide since 2008. The research has therefore had an impact on the\u000a      administration of justice, leading to a better use of evidence and\u000a      accompanying judicial and economic benefits. Examples are given from\u000a      laboratories in Australia, Sweden and The Netherlands.\u000a    ","ImpactType":"Political","Institution":"\u000a    The University of Edinburgh\/Heriot-Watt University (Maxwell Institute)\u000a    ","Institutions":[{"AlternativeName":"Edinburgh (University of)","InstitutionName":"University of Edinburgh","PeerGroup":"A","Region":"Scotland","UKPRN":10007790},{"AlternativeName":"Heriot-Watt University","InstitutionName":"Heriot-Watt University","PeerGroup":"B","Region":"Scotland","UKPRN":10007764}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Those marked with a * best indicate the quality of the research\u000a    \u000a[1]* Aitken, C.G.G., Sampling &#8212; how big a sample? Journal of Forensic\u000a        Sciences, 44, 750-760 (1999). http:\/\/www.maths.ed.ac.uk\/~mthdat25\/forensic\/Sampling-how-big-a-sample\u000a    \u000a\u000a[2] Aitken, C. G. G. and Lucy, D., Estimation of the quantity of a drug\u000a      in a consignment from measurements on a sample, Journal of Forensic\u000a        Sciences, 47, 968-975 (2002). http:\/\/www.maths.ed.ac.uk\/~mthdat25\/forensic\/Estimation-of-the-quantity-of-a-drug-in-a-consignment-from-measurements-on-a-sample\u000a    \u000a\u000a[3]* Aitken, C.G.G. and Lucy, D., Evaluation of trace evidence in the\u000a      form of multivariate data. Applied Statistics, 53,\u000a      109-122, with corrigendum 665-666 (2004).\u000a      http:\/\/www.maths.ed.ac.uk\/~mthdat25\/forensic\/Evaluation-of-trace-evidence-in-the-form-of-multivariate-data-plus-corrigendum\u000a    \u000a\u000a[4] Aitken, C.G.G., Zadora, G. and Lucy, D., A two-level model for\u000a      evidence evaluation. Journal of Forensic Sciences, 52,\u000a      412-419 (2007). http:\/\/dx.doi.org\/10.1111\/j.1556-4029.2006.00358.x\u000a    \u000a\u000a[5] Aitken, C.G.G., Lucy, D., Zadora, G. and Curran, J.M., Evaluation of\u000a      trace evidence for three-level multivariate data with the use of graphical\u000a      models, Computational Statistics and Data Analysis, 50,\u000a      2571-2588 (2006). http:\/\/dx.doi.org\/10.1016\/j.csda.2005.04.005\u000a    \u000a\u000a[6]* Aitken, C.G.G. and Taroni, F., Statistics and the evaluation of\u000a        evidence for forensic scientists, John Wiley and Sons Ltd (2004, 2nd\u000a      edition).\u000a    \u000aGrants. Aitken's research on been funded by a series of research\u000a      grants:\u000a      SHEFC (01.03.01-31.07.04), value: &#163;338,366.\u000a      ESRC RES-000-23-0729 (01.10.04-31.03.08), value: &#163;205,292.\u000a      EPSRC GR\/S98603\/01 (01.12.04-31.03.07), value: &#163;90,598.\u000a      EPSRC EP\/C532627 (01.08.2006-31.07.2008), value: &#163;95,538.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000a    [7] United Nations Office on Drugs and Crime Guidelines on representative\u000a      drug sampling. UNITED NATIONS PUBLICATION; Sales No. E.09.XI.13\u000a      ISBN 978-92-1-148241-6 (2009). See http:\/\/www.maths.ed.ac.uk\/~mthdat25\/forensic\/UN-Office-on-Drugs-and-Crime-Drugs-Sampling-Guidelines\u000a      or [8].\u000a    [8] ENFSI publications may be found on the website: http:\/\/www.enfsi.org\u000a      Click on `Documents' then `External Publications'. Three are of relevance:\u000a      a. Validation of the `Guidelines on representative\u000a      sampling_DWG-SLG-001-vers002.\u000a      b. Drugs Sampling Guideline UNODC-ENFSI.\u000a      c. ENFSI DWG Calculator for Qualitative Sampling of seized drugs (2012)\u000a      (Software).\u000a    Confirmation of the benefits of the research to forensic science can be\u000a      obtained from:\u000a    [9] Senior manager of the Forensic Science Services, Scottish Police\u000a      Services Authority.\u000a    [10] Senior member of the Forensic Speech Science Committee, Australasian\u000a      Speech Science.\u000a    [11] Senior member of the Forensic Voice Comparison Laboratory, School of\u000a      Electrical Engineering &amp; Telecommunications, University of New South\u000a      Wales, Sydney, New South Wales, Australia.\u000a    [12] Senior statistician at SKL.\u000a    [13] Senior statistician the Netherlands Forensic Institute.\u000a    ","Title":"\u000a    Bayesian statistical methods applied to the quantification of forensic\u000a        evidence\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Our judicial system increasingly relies on the quantification of the\u000a      value of evidence presented in court. As a result, advanced statistical\u000a      methods have a strong impact on the administration of justice. The key\u000a      research insight in this area is the recognition that the Bayesian\u000a      framework provides the tools needed for the interpretation of forensic\u000a      evidence. This has led to the development of increasingly sophisticated\u000a      statistical analyses driven by new measuring equipment for the examination\u000a      of trace evidence and by the increase in computing power that enables the\u000a      lengthy calculations required to be performed efficiently. In papers\u000a      published from 1999 on Aitken (Maxwell Institute, MI) and co-workers have\u000a      contributed to this development and tackled two important problems: the\u000a      determination of the optimal size of samples to be taken from seized\u000a      material and the treatment of multivariate, hierarchical evidence data.\u000a      The methodology issued from his research has since been adopted by\u000a      forensic laboratories worldwide.\u000a    Optimal sample size. When large quantities of potentially\u000a      incriminating material are seized, it is difficult to determine what\u000a      fraction should be used for forensic testing: small samples are open to\u000a      challenge as providing too little information; large samples are costly. A\u000a      procedure that determines optimal sample sizes in terms of clearly\u000a      expressed criteria is therefore of obvious benefit for the administration\u000a      of justice. This led the Scottish Forensic Science Liaison Group (SFSLG)\u000a      to approach Aitken in the late 1990s to resolve the problem of the lack of\u000a      criteria for the choice of sample size. Motivated by this, Aitken\u000a      developed a Bayesian procedure and published the underpinning statistical\u000a      research in 1999 [1]. The theory applies when the sampling unit may be\u000a      classified into two or more possible categories (e.g., licit or illicit).\u000a      As examples we cite cases about which Aitken was directly consulted: (a)\u000a      the sampling of drug tablets from consignments; (b) the sampling of\u000a      computer files for evidence of child pornography; and (c) the sampling of\u000a      CDs for evidence of piracy. In such cases, the theory provides an estimate\u000a      of the number of tablets, computer files or CDs that need to be inspected\u000a      to obtain reliable evidence, potentially sufficient for a prosecution.\u000a      Further work by Aitken and collaborators (see [2] and references therein)\u000a      considered the estimation of the quantity of drugs in a consignment and\u000a      provided the probability distribution for the amount of illicit material\u000a      as a function of the sample size.\u000a    Likelihood ratios for multivariate hierarchical data. When samples\u000a      of material obtained from a crime scene are compared with those obtained\u000a      from a suspect, it is necessary to quantify the support for the\u000a      proposition that they come from the same source. In many cases the data\u000a      characterising the material is multivariate, continuous and hierarchical.\u000a      Examples include the composition of glass taken from fragments of windows,\u000a      or the composition of drugs. The hierarchical nature then arises because\u000a      variations within-source and between-source differ (variation of glass\u000a      composition in a single window pane versus variation between different\u000a      panes, or variation of composition within a drug batch versus variations\u000a      between batches). Research in the MI developed a Bayesian methodology to\u000a      quantify the value of the evidence derived from such multivariate and\u000a      hierarchical data. This overcame the drawbacks of earlier methodologies\u000a      (which often incorrectly assumed the independence of the different\u000a      variables) by providing a likelihood ratio (LR) that can be combined with\u000a      other forms of evidence in an integrated analysis and leads to readily\u000a      interpretable conclusions. The initial work by Lucy and Aitken [3]\u000a      considering a two-level hierarchy of data was extended to a three-level\u000a      hierarchy in [4-5]. The paper [4] also developed an implementation based\u000a      on graphical modelling techniques which is adapted to multivariate data.\u000a    Dissemination. The methodology developed by Aitken and\u000a      collaborators and published in [1-3] has been further disseminated through\u000a      its inclusion in the book [6], a well-cited authority on the role of\u000a      statistics in the evaluation of evidence in forensic science (1740 sales\u000a      to 31st August 2013).\u000a    Software implementing the sampling method of [1-2] has been\u000a      developed and is available on the website http:\/\/www.enfsi.org\u000a      (see [9]). A R package `comparison' computing LRs following [3] has been\u000a      developed by Lucy and is freely available at\u000a      http:\/\/cran.r-project.org\/web\/packages\/comparison\/index.html\u000a    Attribution. C. G. G. Aitken has been with the Maxwell Institute\u000a      since 1979. D. Lucy was a PDRA at the Maxwell Institute from 2001 and\u000a      joined the University of Lancaster in 2006. G. Zadora is at the Institute\u000a      for Forensic Research in Krakow (Poland), J.M. Curran at the University of\u000a      Auckland, New Zealand and F. Taroni at the Institute of Forensic Science\u000a      at the University of Lausanne.\u000a    "},{"CaseStudyId":"23939","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council","Natural Environment Research Council"],"ImpactDetails":"\u000a    The paper [1] received keen media interest leading to dissemination of\u000a      results to the wider public [2-6] and to the now widely accepted view that\u000a      squirrelpox is a key determinant in the replacement of red squirrels by\u000a      greys. The work has greatly improved public understanding of this issue as\u000a      evidenced by many subsequent media articles highlighting the disease\u000a      threat to red squirrels (e.g. there are 74 news articles on the BBC\u000a      website alone since 2008 discussing the threat from squirrelpox to red\u000a      squirrels &#8212; search `pox squirrel').\u000a    The publication has had an impact on environmental policy in Government\u000a      and NGO sectors guiding specific management and planning decisions to\u000a      bring about environmental benefits through the conservation of a key\u000a      threatened native species. The economic impact of the research, though\u000a      difficult to quantify, is not negligible: it is estimated that the\u000a      invasion of grey squirrels have cost the British economy &#163;14 million per\u000a      annum [2]. Large-scale control of grey squirrels (trapping and removal of\u000a      greys) has been implemented in an attempt to reduce grey abundance to a\u000a      level that will not support disease persistence and therefore prevent its\u000a      spread. These procedures which began in 2006 are ongoing and are\u000a      implemented at large scale across the UK to protect remaining red squirrel\u000a      populations. Evidence of these impacts is outlined below.\u000a    Scottish Natural Heritage (SNH) identified key sites where\u000a      management to benefit red squirrels should be prioritised with the central\u000a      aim `to maintain viable self-sustaining populations of red squirrels in\u000a      the future' [7]. The report states that `Tompkins et al. (2003)\u000a      [1] have demonstrated that, even at low levels of infection in grey\u000a      squirrels, squirrelpox virus accelerates the process of replacement and\u000a      has therefore been a crucial factor in the decline of red squirrels in the\u000a      UK.' The impact of the Maxwell Institute's research [1] was therefore to\u000a      highlight potential risks associated with the infection. Its findings were\u000a      incorporated in the `Scottish Red Squirrel Action Plan 2006-2011' [8],\u000a      launched by SNH, Forestry Commission Scotland (FCS) and the Scottish\u000a        Government, one of the aims of which is to halt the decline of red\u000a      squirrel populations and slow down or contain the spread of deadly\u000a      squirrelpox. Control strategies were implemented on the ground from 2008.\u000a      In practice in regions where disease incidence is reported in greys they\u000a      are trapped and removed with the aim of eradicating squirrelpox (locally).\u000a    This work is ongoing with the Government and NGO sectors coordinating\u000a      their effort through Saving Scotland's Red Squirrel (SSRS) &#8212; http:\/\/www.scottishsquirrels.org.uk\/\u000a        ). A further impact since 2008 was the initiation of targeted\u000a      control of greys aimed at slowing and containing the spread of squirrelpox\u000a      and the establishment of 18 red squirrel strongholds, totalling\u000a      approximately 100,000 hectares, throughout Scotland, in which greys and\u000a      squirrelpox can be excluded. The SSRS budget to implement squirrelpox\u000a      control is approximately &#163;400K per year (similar schemes run in England\u000a      and Wales). Therefore as a result of [1] disease management is at the\u000a      forefront of red squirrel conservation strategies [8-10] with red squirrel\u000a      stronghold locations chosen to minimise infection risk.\u000a    In 2013 White entered a partnership agreement with SNH to provide `A\u000a      Modelling Assessment of Control Strategies to Prevent\/Reduce Squirrelpox\u000a      Spread'. The modelling assessment extended the framework of [1] and\u000a      assessed current grey squirrel control efforts indicating that they had\u000a      reduced the rate of spread of squirrelpox but would not prevent the\u000a      disease from expanding to occupy most regions of Southern Scotland over\u000a      the next 5-10 years. The work highlighted how the spread of disease to the\u000a      currently disease-free high density populations of Central Scotland may be\u000a      prevented by targeting grey control to specific dispersal routes\u000a      (pinch-points). The model study also assessed the impact of potential grey\u000a      squirrel control beyond the Central Lowlands of Scotland that will act as\u000a      a contingency should squirrelpox reach the Central Scotland populations,\u000a      the impact of squirrelpox at the interface between red and grey\u000a      populations in Highland Scotland and the viability of control measures at\u000a      regional strongholds that support red squirrel populations. The model\u000a      results were presented at the SSRS Steering Group meetings in 2013 and\u000a      were fundamental in shaping an options paper [10] that outlined the\u000a      proposed strategies for protecting red squirrels from 2014. The options\u000a      paper reports that `Grey squirrel control is responding to the modelling\u000a      work ... where control in the identified pinch-point areas is already\u000a      taking place', and the SSRS preferred option intends to `increase\u000a      resourcing to allow greater coverage of the remaining pinch-point areas'\u000a      outlined by the model study. Thus, the model results are fundamental in\u000a      shaping current and future policy and for planning the allocation of\u000a      resources.\u000a    A key strategic stronghold for red squirrel conservation is the Isle of\u000a      Arran. In 2012 the Arran Red Squirrel Assessment, FCS [11] stated that\u000a      research should `evaluate the risk of disease spread on Arran through the\u000a      use of modelling approaches. The paper [1] highlighted the threat posed by\u000a      squirrelpox on the population abundance of red squirrels. They showed that\u000a      a squirrelpox epidemic could lead to the rapid reduction in red\u000a      abundance.' The Maxwell Institute is assisting FCS with the development of\u000a      modelling strategies to assess the disease risk posed to red squirrel\u000a      populations on Arran. The investigation extends the mathematical framework\u000a      developed in [1] and designs strategies that limit the spread and severity\u000a      of squirrelpox outbreaks.\u000a    ","ImpactSummary":"\u000a    Mathematical modelling of squirrel populations published in 2003 by White\u000a      (Maxwell Institute), Tompkins and Boots (Stirling) highlighted how\u000a      squirrelpox virus transmitted by invasive grey squirrels to reds is a\u000a      critical factor in the decline of UK red squirrels. As a consequence of\u000a      this research the role of squirrelpox is now universally accepted. This\u000a      has had an impact on policy and practice since 2006 with priority given to\u000a      the control of grey squirrel numbers in order to prevent the spread of\u000a      squirrelpox. The modelling framework developed at the Maxwell Institute\u000a      was reported to the Saving Scotland's Red Squirrel steering group and has\u000a      been used to design the conservation strategies currently applied\u000a      throughout Scotland. The research has therefore had an impact on the\u000a      environment, contributing to the mitigation of a problem that is estimated\u000a      to cost &#163;14M\/year to the UK economy.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    The University of Edinburgh\/Heriot-Watt University (Maxwell Institute)\u000a    ","Institutions":[{"AlternativeName":"Edinburgh (University of)","InstitutionName":"University of Edinburgh","PeerGroup":"A","Region":"Scotland","UKPRN":10007790},{"AlternativeName":"Heriot-Watt University","InstitutionName":"Heriot-Watt University","PeerGroup":"B","Region":"Scotland","UKPRN":10007764}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] Tompkins D.M., White A. R. and Boots, M., Ecological replacement of\u000a      native red squirrels by invasive greys driven by disease. Ecology\u000a        Letters. 6, 189-196 (2003). http:\/\/dx.doi.org\/10.1046\/j.1461-0248.2003.00417.x\u000a    \u000aThis mathematical study was published in Ecology Letters (the top journal\u000a      in the field) to ensure it received maximum exposure to a biological\u000a      audience and to conservation practitioners. (Cited by 156; Ecology Letters\u000a      Impact Factor &#8212; 17.557, Ranked 1\/131 in Ecology.)\u000a    Grant\u000a    EPSRC-NERC Environmental Mathematics and Statistics Fellowship\u000a      (NER\/T\/S\/2002\/00162), 'Spatial modelling techniques applied to ecological\u000a      and laboratory systems' (2003). Value &#163;100K.\u000a    ","ResearchSubjectAreas":[{"Level1":"5","Level2":"2","Subject":"Environmental Science and Management"},{"Level1":"6","Level2":"3","Subject":"Evolutionary Biology"},{"Level1":"6","Level2":"2","Subject":"Ecology"}],"Sources":"\u000a    [2] Williams, F., et al., The Economic Cost of Invasive\u000a      Non-Native Species on Great Britain, CABI Report, (2010).\u000a      http:\/\/www.maths.ed.ac.uk\/~mthdat25\/conservation\/The_Economic_Cost_of_Invasive_Non-Native_Species_to_Great_Britain\u000a    [3] `UK Red Squirrel Drop: Are Gray Squirrels to blame.' National\u000a        Geographic, May 2 (2003). http:\/\/news.nationalgeographic.co.uk\/news\/2003\/05\/0502_030502_redsquirrels.html\u000a    [4] `Conquest by Disease Carriers.' Editor's research highlights, Science,\u000a      299, 1947 (2003). http:\/\/dx.doi.org\/10.1126\/science.299.5615.1947c\u000a    [5] `Counting on squirrels for help.' The Scotsman, January 14,\u000a      page 42 (2005). http:\/\/www.scotsman.com\/news\/sci-tech\/counting-on-squirrels-for-help-1-671374\u000a    [6] `New research may be able to save the declining red squirrel\u000a      population &#8212; and it's all done with maths.' The Guardian, 22\u000a      February, (2005).\u000a      http:\/\/www.theguardian.com\/education\/2005\/feb\/22\/highereducation.workinprogress\u000a    [7] Poulsom, L., Griffiths, M., Broome, A. and Mayle, B, Identification\u000a      of priority woodlands for red squirrel conservation in North and Central\u000a      Scotland: a preliminary analysis, Scottish Natural Heritage\u000a        Commissioned Report No. 089 (ROAME No. F02AC334) (2005).\u000a      http:\/\/www.snh.org.uk\/pdfs\/publications\/commissioned_reports\/f02ac334.pdf\u000a    [8] Scottish Red Squirrel Action Plan 2006-2011. Forestry Commission\u000a      Scotland, Scottish Executive, Scottish Natural Heritage. http:\/\/www.snh.gov.uk\/docs\/A40765.pdf\u000a    [9] A Modelling Assessment of Control Strategies to Prevent\/Reduce\u000a      Squirrelpox Spread. This is a partnership project between Heriot-Watt\u000a      University and Scottish Natural Heritage. Copies available from SNH\u000a      employee who can also corroborate the factual claims of the impact of this\u000a      modelling work on policy.\u000a    [10] Project Manager, SWT can provide details of the options paper\u000a      presented at the SSRS steering group meetings in 2013 and corroborate how\u000a      the mathematical modelling results are being used inform current policy\u000a      and practice in Scotland.\u000a    [11] Arran Red Squirrel Assessment. 2012. Forestry Commission Scotland.\u000a      Available from Ecological Consultant to the Forestry Commission Scotland\u000a      and\/or District Forrester, Forestry Commission Scotland. \u000a    ","Title":"\u000a    Mathematical modelling drives conservation efforts and policy to\u000a        prevent squirrelpox spread and the replacement of red squirrels by\u000a        invasive greys in the UK\u000a    ","UKLocation":[{"GeoNamesId":"2636910","Name":"Stirling"}],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"},{"GeoNamesId":"6269131","Name":"England"},{"GeoNamesId":"2634895","Name":"Wales"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Since its introduction into the UK, the grey squirrel has `replaced' the\u000a      native red squirrel throughout most of England and Wales, and in parts of\u000a      Scotland and Ireland. There are now only certain regions in which the red\u000a      squirrel survives and maintaining these populations is a conservation\u000a      priority. As such the UK red squirrel is a protected species (Nature\u000a      Conservation Act 2004) and was one of the first species identified for\u000a      conservation under the UK Biodiversity Action Plan (DEFRA 2007).\u000a    Mathematical model. The focal publication by White (Maxwell\u000a      Institute), who led the mathematical model development and analysis,\u000a      Tompkins and Boots [1] employs mathematical modelling techniques to\u000a      provide evidence that squirrelpox infection was a key driver of the rapid\u000a      replacement of red squirrels by greys in the UK. The modelling framework\u000a      and mathematical analysis were developed by White as part of an EPSRC-NERC\u000a      Environmental Mathematics and Statistics Fellowship (NER\/T\/S\/2002\/00162).\u000a      The model consists of a system of five nonlinear ODEs that represent\u000a      competition for resources and transmission of squirrelpox virus that is\u000a      carried by, but avirulent to, the grey squirrels yet lethal to reds. The\u000a      occurrence of squirrelpox virus was known prior to this paper but this\u000a      study was the first to show that squirrelpox accelerates the process of\u000a      replacement and therefore that disease was a causative factor in the\u000a      decline of red squirrels in the UK. Moreover, the findings predicted that\u000a      the instantaneous number of infections was low even though the effect on\u000a      the population dynamics was marked, indicating that low visibility did not\u000a      equate to low importance of the disease. This explained the difficulty in\u000a      observing such infection in the field and why squirrelpox was previously\u000a      overlooked as a key determinant of red replacement.\u000a    In addition to outlining the underlying mechanisms that drive the\u000a      red\/grey\/squirrelpox interaction the publication [1] broadened our\u000a      understanding of the processes underlying ecological invasions in general\u000a      with the model framework becoming a textbook theoretical example from\u000a      which to understand the importance of parasites introduced with invading\u000a      competitive species (Keeling and Rohani, Modelling infectious diseases\u000a        in humans and animals, 2008). The specific methods are applicable\u000a      for studying the replacement of red squirrels by greys across Europe\u000a      (greys pose a threat in Italy, France, Switzerland and Germany). Moreover,\u000a      disease-mediated invasion is a global widespread phenomenon that effects a\u000a      wide range of taxa (Strauss, A. et al., Functional Ecology,\u000a      26, 1249-1261, 2012) and mathematical techniques outlined in [1]\u000a      can be applied to a wide range of other ecologically important systems in\u000a      which disease carrying invaders threaten native species (e.g. in an\u000a      analogous manner native crayfish populations are threatened at the global\u000a      scale from the spread of crayfish plague carried by the introduced signal\u000a      crayfish).\u000a    Attribution. A. R. White has been with the Maxwell Institute since\u000a      1999. D. M. Tompkins and M. Boots were at the University of Stirling\u000a      during the period of the underpinning research.\u000a    "},{"CaseStudyId":"23940","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Linear optimization and quadratic optimization are at the heart of\u000a      numerous industrial and commercial applications of mathematics. These\u000a      arise in very different problems and in broad variety of sectors of the\u000a      economy &#8212; finance, energy, telecommunications, transport, manufacturing,\u000a      to mention but a few. Although many such problems involve some form of\u000a      nonlinearity, the need to find a compromise between modelling accuracy and\u000a      ability to solve the resulting optimization problem often necessitates\u000a      using simplifying assumptions and employing the modelling based on linear\u000a      and quadratic optimization. As a result, millions of optimization problems\u000a      are solved every day and the vast majority of them, 90-95%, rely on linear\u000a      or quadratic optimization models.\u000a    The ability to solve such problems efficiently is therefore crucial. In\u000a      many situations, optimization problems have to be solved in real-time;\u000a      this is for example the case when optimizing portfolios in high-frequency\u000a      trading, when optimizing the usage of electricity transmission networks\u000a      following the changes in energy demand, assigning frequencies to mobile\u000a      phone communications, scheduling crew for sea or air transport, etc. The\u000a      corresponding businesses are worth billions of pounds and any improvement\u000a      in their operations achieved by optimization is of great value to the\u000a      companies and to society. An indication of the size of the optimization\u000a      software market is provided by the investment made in recent years by\u000a      major companies: in 2009, IBM (market capital $220B) acquired ILOG, then\u000a      owner of Cplex software for $340M (see http:\/\/www-03.ibm.com\/press\/us\/en\/pressrelease\/26403.wss);\u000ain\u000a      2008 Fair Isaac (FICO, market capital $1.7B) acquired DASH, owner of the\u000a      software Xpress (see http:\/\/www.fico.com\/en\/Company\/News\/Pages\/01-22-2008.aspx).\u000a    Fast optimization solvers that can deliver reliable answers to\u000a      challenging problems in acceptable time frames are the key to the\u000a      successes of Cplex, DASH and others. The methods developed by Gondzio,\u000a      specifically the technique of multiple centrality corrections [1], special\u000a      preconditioners for KKT systems [2] and warmstarting techniques [3-4] have\u000a      influenced the design of these modern optimization software. Multiple\u000a      centrality corrections, in particular, have been implemented in all major\u000a      commercial optimization solvers: IBM-Cplex, Gurobi, MOSEK, FICO Xpress,\u000a      and in numerous academic optimization solvers: PCx, BPMPD, HOPDM, OOQP and\u000a      OOPS [5-9]. Their use contributes to these codes' ability to solve\u000a      difficult optimization problems. In some cases their use is essential to\u000a      ensure that a problem can be solved at all. Their importance is reflected\u000a      in a remark by a senior executive from Gurobi Optimization who stated that\u000a      `it is not possible to build a competitive interior point code without\u000a      centrality correctors' [6].\u000a    The following optimization solvers, which together dominate the\u000a      optimization market, use multiple centrality corrections:\u000a    \u000a      IBM ILOG CPLEX Optimizer [5]\u000a        http:\/\/www-01.ibm.com\/software\/integration\/optimization\/cplex-optimizer\/\u000a\u000a      Gurobi Optimization [6]\u000a        http:\/\/www.gurobi.com\/\u000a\u000a      MOSEK [7]\u000a        http:\/\/www.mosek.com\/\u000a\u000a      FICO Xpress Optimization Suite [8]\u000a        http:\/\/www.fico.com\/en\/Products\/DMTools\/Pages\/FICO-Xpress-Optimization-Suite.aspx\u000a\u000a      PCx [9]\u000a        http:\/\/pages.cs.wisc.edu\/~swright\/PCx\/\u000a\u000a      OOQP [9]\u000a        http:\/\/pages.cs.wisc.edu\/~swright\/ooqp\/reference-manual\/classGondzioSolver.html\u000a\u000a    \u000a    Gondzio's research has therefore had an economic impact on the companies\u000a      commercialising these solvers, and on their numerous customers across the\u000a      world.\u000a    ","ImpactSummary":"\u000a    Research by Gondzio (Maxwell Institute) on algorithms for large-scale\u000a      optimization has led to major advances in the design of interior point\u000a      methods (IPMs). The advances include new ways of exploiting centrality\u000a      (1996-2008) as well as special preconditioning (2004) and warmstarting\u000a      (2003, 2008) techniques. These techniques make it possible to solve more\u000a      difficult optimization problems more quickly. Some of these have been\u000a      implemented by all major commercial providers of optimization software\u000a      including IBM, Gurobi, Mosek and FICO. The techniques have therefore had\u000a      an economic impact on these companies and on thousands of their customers\u000a      worldwide who now benefit from faster, more reliable methods to solve\u000a      their challenging optimization tasks.\u000a    ","ImpactType":"Technological","Institution":"\u000a    The University of Edinburgh\/Heriot-Watt University (Maxwell Institute)\u000a    ","Institutions":[{"AlternativeName":"Edinburgh (University of)","InstitutionName":"University of Edinburgh","PeerGroup":"A","Region":"Scotland","UKPRN":10007790},{"AlternativeName":"Heriot-Watt University","InstitutionName":"Heriot-Watt University","PeerGroup":"B","Region":"Scotland","UKPRN":10007764}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Those marked with a * best indicate the quality of the research.\u000a    \u000a[1]* Colombo, M. and Gondzio, J., Further development of multiple\u000a      centrality correctors for interior point methods, Computational\u000a        Optimization and Applications, 41, 277-305 (2008). http:\/\/dx.doi.org\/10.1007\/s10589-007-9106-0\u000a    \u000a\u000a[2]* Bergamaschi, L., Gondzio, J. and Zilli, G., Preconditioning\u000a      indefinite systems in interior point methods for optimization, Computational\u000a        Optimization and Applications, 28, 149-171 (2004). http:\/\/dx.doi.org\/10.1023\/B:COAP.0000026882.34332.1b\u000a    \u000a\u000a[3] Gondzio, J. and Grothey, A., Reoptimization with the primal-dual\u000a      interior point method, SIAM Journal on Optimization, 13,\u000a      842-864 (2003). http:\/\/dx.doi.org\/10.1137\/S1052623401393141\u000a    \u000a\u000a[4]* Gondzio, J. and Grothey, A., A new unblocking technique to warmstart\u000a      interior point methods based on sensitivity analysis, SIAM Journal on\u000a        Optimization, 19, 1184-1210 (2008). http:\/\/dx.doi.org\/10.1137\/060678129\u000a    \u000aGrants\u000a    EPSRC grant GR\/M68169, Parallel solutions of structured linear programs\u000a      with interior point methods, &#163;51K (1999).\u000a    EPSRC grant GR\/R99683\/01, Parallel solution of large scale structured\u000a      nonlinear programs with interior point methods, &#163;145K (2005).\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000a    [5] Manager in the Optimization and Mathematical Software Business\u000a      Analytics and Mathematical Science group of IBM Research USA will confirm\u000a      that the centrality correctors are used in IBM-Cplex barrier.\u000a    [6] Senior executive of Gurobi Optimization will confirm that it is not\u000a      possible to build a competitive interior point code without centrality\u000a      correctors.\u000a    [7] Senior executive of MOSEK Optimization will confirm that the MOSEK\u000a      optimization software employs a modified version of the centrality\u000a      correctors.\u000a    [8] Senior software engineer at FICO Xpress will confirm that the\u000a      underpinning research was useful to developing their software.\u000a    [9] Professor at University Wisconsin-Madison will confirm the benefits\u000a      of using the centrality correctors in research optimization software. \u000a    ","Title":"\u000a    Interior point methods adopted by leading optimization software\u000a        providers\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Interior point methods. IPMs are an essential component of the\u000a      optimization software used throughout the world. Although they were\u000a      initially only applied to solve linear optimization problems, their\u000a      potential advantages in other contexts were quickly recognised and they\u000a      have been extended to other classes of problems: quadratic, nonlinear,\u000a      conic, and semi-definite. IPMs reached maturity in the 1990s and since\u000a      then they have been included in all the major commercial optimization\u000a      solvers. It is widely accepted that IPMs outperform all earlier\u000a      optimization methods on a broad range of complex problems, especially when\u000a      instances are very large.\u000a    Multiple centrality correctors. Since the mid-1990s, Gondzio and\u000a      co-workers have been developing new methods for IPMs. A key insight of\u000a      their research was the recognition of the role of centrality in the\u000a      practical behaviour of IPMs. This led them to design the first successful\u000a      practical method exploiting centrality (Gondzio, Computational\u000a        Optimization and Applications, 6, 137-156, 1996). The\u000a      technique of multiple centrality correctors of Gondzio was refined in\u000a      collaboration with Colombo, then a PhD student of Gondzio at the Maxwell\u000a      Institute [1]. The method was shown to outperform the standard algorithms\u000a      used at the time for solving difficult optimization problems. Special\u000a      preconditioners for iterative (Krylov subspace) solvers in IPMs were\u000a      subsequently developed by Gondzio in collaboration with Bergamaschi and\u000a      Zilli [2]. The use of these preconditioners was shown to be particularly\u000a      attractive when solving some difficult quadratic optimization problems.\u000a      IPMs owe their success to the way in which they deal with the\u000a      complementarity condition xi sj\u000a      = 0 for all j. IPMs perturb this condition and replace it with xi\u000a        sj = &#181;, where the parameter &#181; is driven to zero.\u000a      To avoid guessing the partitioning into active and inactive inequality\u000a      constraints, the algorithm forces a systematic reduction of &#181;. The\u000a      partition of vectors x and s into zero and nonzero\u000a      elements is gradually revealed as the algorithm progresses. The theory of\u000a      IPMs requires that all complementarity products remain close to &#181; or at\u000a      least display the same magnitude. This feature is commonly known as a good\u000a      `centrality' property. However, standard steps in the Newton direction may\u000a      destroy the centrality of the iterates. The technique of centrality\u000a      correctors remedies this drawback by using a sequence of corrective\u000a      actions which re-centre all complementarity products.\u000a    Warmstarting techniques. Gondzio and Grothey (also Maxwell\u000a      Institute) developed new approaches to warmstarting (reoptimization) of\u000a      interior point methods [3-4]. These are relevant to numerous situations\u000a      when a series of somewhat similar problems needs to be solved. The\u000a      warmstarting techniques were demonstrated to be particularly attractive in\u000a      the context of portfolio optimization. Taken together, the techniques\u000a      developed by Gondzio and collaborators have led to recognised improvements\u000a      of IPMs, making them faster and able to solve larger problems.\u000a    Attribution. The research was led by J. Gondzio at the Maxwell\u000a      Institute (MI) from 1998 on. A. Grothey was a PDRA of Gondzio on two EPSRC\u000a      projects (starting in 1999) see below) before taking up an academic\u000a      position in the MI (from 2005). M. Colombo, was a PhD student of Gondzio\u000a      in the MI (graduated in 2007). External collaborators L. Bergamaschi and\u000a      G. Zilli are at the University of Padova.\u000a    "},{"CaseStudyId":"23941","Continent":[{"GeoNamesId":"6255146","Name":"Africa"}],"Country":[{"GeoNamesId":"953987","Name":"South Africa"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    Projections Toolkit. Significant impact has been achieved in the\u000d\u000a      form of increased turnover for Longevitas, an SME formed in 2006, which\u000d\u000a      offers its customers leading-edge software for modelling and analysing\u000d\u000a      demographic risks, including mortality, longevity and critical illness.\u000d\u000a      The increased turnover results from the licensing and marketing of a\u000d\u000a      software application &#8212; Projections Toolkit &#8212; which was developed\u000d\u000a      by Longevitas staff from code originally written by Currie. The Projections\u000a        Toolkit is an important tool in the service provided by Longevitas\u000d\u000a      to the pensions and insurance industry and embodies a wide range of\u000d\u000a      forecasting models &#8212; specifically the 2- dimensional models [1-3, 5] and\u000d\u000a      the Lee-Carter models [4]. [text removed for publication.]\u000d\u000a    Impact on practices of advisory agencies. The research has\u000d\u000a      generated further impact through the uptake of the smoothing and\u000d\u000a      forecasting methods described in Section 2 by two bodies, the Office for\u000d\u000a      National Statistics and the Continuous Mortality Investigation, whose\u000d\u000a      respective remits includes the provision of statistical analyses for use\u000d\u000a      by government and by the pensions and insurance industries.\u000d\u000a    The Office for National Statistics (ONS) provides official\u000d\u000a      statistics on behalf of the UK government including the National\u000d\u000a      Population Projections (NPP) which impact strongly on government policy on\u000d\u000a      pensions, social care and health. Since 2006 the ONS has used the 2-\u000d\u000a      dimensional smoothing model to remove fluctuations from age to age and\u000d\u000a      year to year in formulating smoothed mortality surfaces for each gender\u000d\u000a      [8]. During development of the new methodology, Currie provided direct\u000d\u000a      support to the Principal Methodologist involved at ONS. Our contribution\u000d\u000a      to the work of ONS is described in [9] which states (p4) that \"Mortality\u000d\u000a      rates for the UK in each calendar year in the period 1961 to 2009 have\u000d\u000a      been smoothed to remove fluctuations from age to age and year to year,\u000d\u000a      using a new methodology\" &#8212; a direct reference to their use of our 2-\u000d\u000a      dimensional smoothing model as described by them on p5: \"A p-spline model\u000d\u000a      was then applied to the resulting crude mortality rates to produce a\u000d\u000a      fitted, smoothed mortality surface to the historical data for each\u000d\u000a      gender.\"\u000d\u000a    The Continuous Mortality Investigation (CMI) is funded by the\u000d\u000a      insurance industry and actuarial consultancies and has two main functions:\u000d\u000a      (i) the collection of claims and exposed-to-risk data from across the\u000d\u000a      insurance industry and pension schemes, the collation of these data by\u000d\u000a      source (for example, by type of insurance), and the creation of a\u000d\u000a      repository of the resulting mortality data sets; (ii) the modelling and\u000d\u000a      forecasting of the data in (i).\u000d\u000a    The underpinning research, which has been partially supported by the CMI\u000d\u000a      (see section 2), is now incorporated into their forecasting methodology\u000d\u000a      [10]. Specifically, the research has provided the methods used in the CMI\u000d\u000a      model for smoothing the data by fitting a 2-dimensional P-spline age-cohort\u000d\u000a      model, as described in [3]; see the CMI 2010 Working paper [11].\u000d\u000a      The final model is used extensively by many insurance companies to assess\u000d\u000a      pricing and reserving for various classes of assurance and annuities, and\u000d\u000a      by pension schemes in valuing their liabilities for funding or risk\u000d\u000a      transfer assessments.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research carried out from 2003 by Currie (Maxwell Institute) and his PhD\u000d\u000a      students Djeundje, Kirkby and Richards (also Longevitas), and\u000d\u000a      international collaborators Eilers and Durban, created new, flexible\u000d\u000a      smoothing and forecasting methods. These methods are now widely used by\u000d\u000a      insurance and pension providers to forecast mortality when determining\u000d\u000a      pricing and reserving strategy for pensions. The methods were incorporated\u000d\u000a      by the SME Longevitas in its forecasting package Projections Toolkit\u000d\u000a      launched in 2009. This generated impact in the form of &#163;400K turnover for\u000d\u000a      Longevitas in licensing and consultancy fees, with further impact on the\u000d\u000a      pricing and reserving strategies on Longevitas's customers. Since 2010 the\u000d\u000a      methods have been adopted by the Office for National Statistics (ONS) to\u000d\u000a      make the forecasts required to underpin public policy in pensions, social\u000d\u000a      care and health and by The Continuous Mortality Investigation (CMI) to\u000d\u000a      model and provide forecasts on mortality to the pensions and insurance\u000d\u000a      industries. As a result, the research has changed practices in these\u000d\u000a      advisory agencies and in the insurance industry.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    The University of Edinburgh\/Heriot-Watt University (Maxwell Institute)\u000d\u000a    ","Institutions":[{"AlternativeName":"Edinburgh (University of)","InstitutionName":"University of Edinburgh","PeerGroup":"A","Region":"Scotland","UKPRN":10007790},{"AlternativeName":"Heriot-Watt University","InstitutionName":"Heriot-Watt University","PeerGroup":"B","Region":"Scotland","UKPRN":10007764}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"1007311","Name":"Durban"}],"References":"\u000d\u000a    References marked with a * best indicate the quality of the research.\u000d\u000a    \u000a[1]* Currie I.D., Durban, M. and Eilers, P.H.C., Smoothing and\u000d\u000a      forecasting mortality rates, Statistical Modelling, 4,\u000d\u000a      279-298 (2004). http:\/\/dx.doi.org\/10.1191\/1471082X04st080oa\u000d\u000a    \u000a\u000a[2]* Currie, I.D., Durban, M. and Eilers, P.H.C., Generalized linear\u000d\u000a      array models with applications to multidimensional smoothing, Journal\u000d\u000a        of the Royal Statistical Society, Series B, 68, 259-280\u000d\u000a      (2006). http:\/\/dx.doi.org\/10.1111\/j.1467-9868.2006.00543.x\u000d\u000a    \u000a\u000a[3] Richards, S. J., Kirkby, J.G. and Currie, I.D., The importance of\u000d\u000a      year of birth in two-dimensional mortality data (with Discussion), British\u000a        Actuarial Journal, 12, 5-61 (2006). http:\/\/dx.doi.org\/10.1017\/S1357321700004682\u000d\u000a    \u000a\u000a[4] Richards, S. J. and Currie, I. D., Longevity risk and annuity pricing\u000d\u000a      with the Lee-Carter model, British Actuarial Journal, 15,\u000d\u000a      317-365 (2009). http:\/\/dx.doi.org\/10.1017\/S1357321700005675\u000d\u000a    \u000a\u000a[5] Djeundje, V. A. B. and Currie, I. D., Smoothing dispersed counts with\u000d\u000a      applications to mortality data, Annals of Actuarial Science, 5,\u000d\u000a      33-52 (2011). http:\/\/dx.doi.org\/10.1017\/S1748499510000047\u000d\u000a    \u000a\u000a[6] Richards, S.J., Currie, I.D. and Ritchie, G.P., A value-at-risk\u000d\u000a      framework for longevity trend risk. (with Discussion) Read before the\u000d\u000a      Institute and Faculty of Actuaries in Edinburgh and London (November\u000d\u000a      2012), British Actuarial Journal, Available on CJO 2013 http:\/\/dx.doi.org\/10.1017\/S1357321712000451\u000d\u000a    \u000aGrants\u000d\u000a    The work was supported by two PhD studentships (J. Kirkby, 2003-2006 and\u000d\u000a      V. Djeundje Biatat, 2008-2011) both jointly funded by the EPSRC and the\u000d\u000a      CMI (total funding &#163;156K); EPSRC grant numbers GR\/P02912 and EP\/P504570\/1\u000d\u000a      respectively. The work of Iain Currie was supported by four grants from\u000d\u000a      the Actuarial Profession during 2003-09, total value: &#163;25K.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a    [7] Projections Toolkit: Board Member, Longevitas, will confirm the\u000d\u000a      central role played by the research in [1-6] in Projections Toolkit.\u000d\u000a      www.longevitas.co.uk\/site\/ourservices\/projectionstoolkit\u000d\u000a    [8] Senior Methodologist, Office for National Statistics: will confirm\u000d\u000a      the use of the methodology by ONS.\u000d\u000a    [9] National Population Projections, 2010-based reference volume: Series\u000d\u000a      PP2, published by the ONS. See Chapter 4, `Mortality' for a description of\u000d\u000a      how the research is used by the ONS. http:\/\/www.ons.gov.uk\/ons\/rel\/npp\/national-population-projections\/2010-based-reference-volume--series-pp2\/mortality.html\u000d\u000a    [10] Continuous Mortality Investigation: Senior actuary with Barnett\u000d\u000a      Waddingham will confirm the use of the methodology by CMI.\u000d\u000a    [11] Continuous Mortality Investigation , The CMI mortality projections\u000d\u000a      model, `CMI 2010', Working Paper 49, November (2010).\u000d\u000a      www.actuaries.org.uk\/research-and-resources\/pages\/continuous-mortality-investigation\u000d\u000a    See under CMI working papers on the CMI web site above; in particular,\u000d\u000a      section 5, p20. \u000d\u000a    ","Title":"\u000d\u000a    Statistical models of mortality impact on the pricing and reserving of\u000d\u000a        pensions\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2645313","Name":"Kirkby"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Pensions and annuities are future liabilities that depend on the future\u000d\u000a      course of mortality. Research by a team led by Currie (Maxwell Institute,\u000d\u000a      MI) has provided a suite of solutions to several of the modelling\u000d\u000a      challenges facing the insurance and pensions industry in their forecasting\u000d\u000a      of mortality.\u000d\u000a    Statistical research. In 2004, Currie, Durban (Carlos III Madrid)\u000d\u000a      and Eilers (Leiden) proposed a new approach to forecasting mortality that\u000d\u000a      uses a 2-dimensional smoothing model to reveal the underlying pattern in\u000d\u000a      mortality by both age and time, and to forecast this pattern in time [1].\u000d\u000a      Fitting the 2-dimensional model in [1] is computationally intensive and in\u000d\u000a      2006 the authors formulated an algorithm that improves computational time\u000d\u000a      by orders of magnitude [2]. Mortality data are usually indexed by age of\u000d\u000a      death and year of death, and the methods described in [1] and [2] provided\u000d\u000a      an immediate modelling solution to this scenario. Further research by\u000d\u000a      Richards (Longevitas), Kirkby (MI) and Currie showed how the 2-dimensional\u000d\u000a      model could also be applied when data are indexed by age at death and year\u000d\u000a      of birth and demonstrated the importance of cohort effects in the\u000d\u000a      modelling and forecasting of mortality rates [3]. In another strand of\u000d\u000a      research, Richards and Currie [4] in 2009 showed how to produce more\u000d\u000a      regular forecasts with the Lee-Carter model &#8212; a benchmark model widely\u000d\u000a      used to forecast mortality &#8212; improving its actuarial performance. A common\u000d\u000a      feature of mortality data is overdispersion (where the observed mortality\u000d\u000a      rates are more variable in age and time than predicted by a simple random\u000d\u000a      model). Djeundje (MI) and Currie [5] showed how the techniques developed\u000d\u000a      in [1] and [2] could be extended to accommodate overdispersion. The\u000d\u000a      forecasting of the mortality of the very old is of particular interest to\u000d\u000a      annuity and pension providers but presents additional challenges since\u000d\u000a      data quality can be poor at such ages. Currie formulated a new solution to\u000d\u000a      this problem using the approach and computational methods in [1] and [2].\u000d\u000a    Implementation for users. All the techniques described above have\u000d\u000a      been incorporated in the commercial package Projections Toolkit\u000d\u000a      developed and licensed by Longevitas, a software and consulting company.\u000d\u000a      The techniques proposed in [1-3] have also been implemented in the freely\u000d\u000a      available R package MortalitySmooth, developed within the Max\u000d\u000a      Planck Institute for Demographic Research.\u000d\u000a    Attribution. I. D. Currie has been a member of the Maxwell\u000d\u000a      Institute since 1973. J. Kirkby (graduated in 2009) and V. Djeundje Biatat\u000d\u000a      (graduated in 2011) were PhD students, both supervised by I. D. Currie and\u000d\u000a      funded by EPSRC and CMI. S. J. Richards completed a part time PhD at HWU\u000d\u000a      in 2012. M. Durban was at Carlos III Madrid and P. H. C. Eilers at Leiden.\u000d\u000a    "},{"CaseStudyId":"23942","Continent":[{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2077456","Name":"Australia"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Nature and reach of impact. Longevity risk leads to significant\u000d\u000a      financial risk in pension schemes, which in turn increases volatility in\u000d\u000a      balance sheets of companies sponsoring pension schemes. Many companies,\u000d\u000a      especially those with long-established pension schemes, choose to hedge\u000d\u000a      the impact of this risk through use of longevity swaps and other financial\u000d\u000a      structures which transfer the risk to financial institutions such as\u000d\u000a      multinational reinsurers. As a result, the pension industry has developed\u000d\u000a      into a major global industry, encompassing major financial institutions as\u000d\u000a      well as pension schemes and life insurers. The CBD model, its descendants\u000d\u000a      and the R package written by Cairns have provided this industry with\u000d\u000a      crucial tools to assess longevity risk and price their products. These\u000d\u000a      tools were crucial in educating the market in the early days, in winning\u000d\u000a      over the actuarial profession, and persuading investors to take longevity\u000d\u000a      investments seriously. They have since been adopted by a broad range of\u000d\u000a      stakeholders including major US, UK and multinational institutions to\u000d\u000a      inform transactions worth several billions of pounds. We list below\u000d\u000a      several of these stakeholders and, for each, detail the impact of the CBD\u000d\u000a      model and its descendants.\u000d\u000a    [text removed for publication] Collaboration between Cairns, Blake and\u000d\u000a      Dowd and [text removed for publication] staff of a multinational company\u000d\u000a      developed insightful new models, and a comprehensive methodology for the\u000d\u000a      comparison of different models and the assessment of model risk. It helped\u000d\u000a      leverage the resources of the [text removed for publication] company's\u000d\u000a      longevity team; exploited complementary skill sets to develop rigorous and\u000d\u000a      practical solutions; helped provide visibility and build the company's\u000d\u000a      reputation [text removed for publication] as a market leader; and helped\u000d\u000a      persuade clients to agree to do substantial deals with the company [text\u000d\u000a      removed for publication] [6].\u000d\u000a    [text removed for publication] A very significant US-based buyer of\u000d\u000a      longevity risk from pension schemes and insurers, [text removed for\u000d\u000a      publication] is a major user of the results in [2]. They have adopted the\u000d\u000a      model comparison methodology of [2] as the foundation of their efforts to\u000d\u000a      understand and navigate their way through a diverse array of model\u000d\u000a      choices. They most often base their assessments of risk and decisions on\u000d\u000a      `second-generation' CBD models (i.e. M6, M7 in [2]). Since 2010, they have\u000d\u000a      used the methodology in their assessment of [text removed for publication]\u000d\u000a      pension liabilities and have subsequently executed substantial\u000d\u000a      transactions in the UK [text removed for publication] [7].\u000d\u000a    [text removed for publication] A rapidly-expanding, specialised\u000d\u000a      consultancy that provides capital markets and actuarial advice to pension\u000d\u000a      schemes and insurers considered a number of variants of the CBD models and\u000d\u000a      of the earlier Lee-Carter model in their development of an in-house\u000d\u000a      longevity model, concluding that the 2-factor CBD model [1] was best for\u000d\u000a      their pensions client work. CBD is used for assessing risk and developing\u000d\u000a      strategies for reducing risk, including the management of risk-based\u000d\u000a      capital requirements [8].\u000d\u000a    [text removed for publication]\u000d\u000a    [text removed for publication] A provider of specialist software for\u000d\u000a      modelling past and future mortality rates used by insurance companies\u000d\u000a      [text removed for publication] incorporated CBD into its [text removed for\u000d\u000a      publication] software from outset [text removed for publication] The\u000d\u000a      software is in regular use by clients, [text removed for publication]\u000d\u000a      finding particular application in the calculation of capital requirements\u000d\u000a      for regulatory purposes [10].\u000d\u000a    UK Pension Protection Fund. The PPF receives a levy from pension\u000d\u000a      schemes as insurance against possible bankruptcy of the sponsoring\u000d\u000a      companies, leaving the scheme in deficit. In such circumstances, the PPF\u000d\u000a      takes over the distressed scheme and assumes responsibility for paying\u000d\u000a      scheme pensions. Assets and liabilities at July 2013 were &#163;20 billion,\u000d\u000a      with both expected to increase rapidly over coming years. The PPF has\u000d\u000a      developed an internal, long-term risk model (LTRM) that covers all of its\u000d\u000a      major risk categories including future mortality improvements. Mortality\u000d\u000a      is modelled using the `M7' second-generation CBD model in [2]. M7's use\u000d\u000a      forms an important element of the PPF's overall programme of risk\u000d\u000a      measurement, monitoring and management, including setting its funding\u000d\u000a      strategy. M7 also influences setting of scheme levies (2013\/14: &#163;630\u000d\u000a      million). As the PPF matures the use of M7 will become even more important\u000d\u000a      [11-12].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research carried out by Cairns (Maxwell Institute), Blake (Cass Business\u000d\u000a      School) and Dowd (Nottingham, now Durham) in 2006 produced the `CBD' model\u000d\u000a      for predicting future life expectancy. The CBD model and its extensions\u000d\u000a      developed in 2009 by Cairns and collaborators have had a major impact on\u000d\u000a      pensions and life industry risk management practices: multinational\u000d\u000a      financial institutions [text removed for publication] and other\u000d\u000a      stakeholders have relied on the CBD model to risk assess, price and\u000d\u000a      execute financial deals [text removed for publication] since 2010. CBD is\u000d\u000a      also used by risk management consultants to advise clients, is embedded in\u000d\u000a      both open-source and commercial software, and is used by the UK's Pension\u000d\u000a      Protection Fund to measure and manage longevity risk.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    The University of Edinburgh\/Heriot Watt University (Maxwell Institute)\u000d\u000a    ","Institutions":[{"AlternativeName":"Edinburgh (University of)","InstitutionName":"University of Edinburgh","PeerGroup":"A","Region":"Scotland","UKPRN":10007790},{"AlternativeName":"Heriot-Watt University","InstitutionName":"Heriot-Watt University","PeerGroup":"B","Region":"Scotland","UKPRN":10007764}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2172797","Name":"Cairns"}],"References":"\u000d\u000a    References marked with a * best indicate the quality of the research.\u000d\u000a    \u000a[1]* Cairns, A.J.G., Blake, D. and Dowd, K., A Two-Factor Model For\u000d\u000a      Stochastic Mortality With Parameter Uncertainty: Theory And Calibration, Journal\u000d\u000a        of Risk and Insurance, 73, 687-718 (2006). http:\/\/dx.doi.org\/10.1111\/j.1539-6975.2006.00195.x\u000d\u000a    \u000a\u000a[2]* Cairns, A. J. G., David, B., Dowd, K., Coughlan, G. D., Epstein, D.,\u000d\u000a      Ong, A. and Balevich, I. A, Quantitative Comparison Of Stochastic\u000d\u000a      Mortality Models Using Data From England And Wales And The United States,\u000d\u000a      North American Actuarial Journal, 13, 1-35 (2009). (Awarded\u000d\u000a      the 2009 Society of Actuaries Prize). http:\/\/dx.doi.org\/10.1080\/10920277.2009.10597538\u000d\u000a    \u000a\u000a[3] Dowd, K., Cairns, A.J.G., Blake, D., Coughlan, G.D., Epstein, D. and\u000d\u000a      Khalaf-Allah, M., Evaluating the Goodness of Fit of Stochastic Mortality\u000d\u000a      Models, Insurance: Mathematics and Economics, 47, 255-265\u000d\u000a      (2010). http:\/\/dx.doi.org\/10.1016\/j.insmatheco.2010.06.006\u000d\u000a    \u000a\u000a[4]* Cairns, A.J.G., Blake, D., Dowd, K., Coughlan, G.D., Epstein, D. and\u000d\u000a      Khalaf-Allah, M., Mortality Density Forecasts: An Analysis Of Six\u000d\u000a      Stochastic Mortality Models, Insurance: Mathematics and Economics,\u000d\u000a      48, 355-367 (2011).\u000d\u000a      http:\/\/dx.doi.org\/10.1016\/j.insmatheco.2010.12.005\u000d\u000a    \u000a\u000a[5] Dowd, K., Cairns, A.J.G., Blake, D., Coughlan, G.D., Epstein, D. and\u000d\u000a      Khalaf-Allah, M., Backtesting Stochastic Mortality Models: An Ex-Post\u000d\u000a      Evaluation of Multi-Period-Ahead Density Forecasts, North American\u000d\u000a        Actuarial Journal, 14, 281-298 (2011).\u000d\u000a      http:\/\/dx.doi.org\/10.1080\/10920277.2010.10597592\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"15","Level2":"2","Subject":"Banking, Finance and Investment"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    [6] [text removed for publication]\u000d\u000a    [7] [text removed for publication]\u000d\u000a    [8] [text removed for publication]\u000d\u000a    [9] [text removed for publication]\u000d\u000a    [10] [text removed for publication]\u000d\u000a    [11] [text removed for publication]\u000d\u000a    [12] Use of the CBD model M7 in the Pension Protection Fund is documented\u000d\u000a      at\u000d\u000a      http:\/\/www.actuaries.org.uk\/research-and-resources\/documents\/financial-management-uk-pensions-protection-fund\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Stochastic models of longevity risk adopted by the pension industry\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2650628","Name":"Durham"},{"GeoNamesId":"2641170","Name":"Nottingham"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The papers of Cairns, Blake and Dowd (2006, [1]) and Cairns et al.\u000d\u000a      (2009, [2]) concern the development and application of new stochastic\u000d\u000a      models &#8212; the CBD model and its extensions &#8212; for the forecasting of\u000d\u000a      mortality rates. These are part of a broader programme of longevity risk\u000d\u000a      research which continues to the present day.\u000d\u000a    CBD model. The last 100 years have seen large decreases in rates\u000d\u000a      of mortality at all ages resulting from advances in medicine, technology\u000d\u000a      and public health policy. However, the pattern of improvements has been\u000d\u000a      unpredictable, and this results in considerable uncertainty over what\u000d\u000a      might happen in the future. The motivation for the work in [1] came from\u000d\u000a      the pensions and life insurance industry where future mortality trends and\u000d\u000a      the uncertainty around them are of obvious importance. The work responded\u000d\u000a      to the industry's need to estimate the `longevity risk', that is, the risk\u000d\u000a      that people live, in aggregate, longer than anticipated, a risk that\u000d\u000a      causes pension schemes and annuity providers to incur financial losses.\u000d\u000a    Compared with earlier work in this field, [1] emphasized (a) the need for\u000d\u000a      models of underlying mortality improvements to be driven by more than one\u000d\u000a      source of randomness, (b) that, at higher ages, the mortality curve can be\u000d\u000a      approximated by a simple parametric form, and (c) that forecasts going\u000d\u000a      beyond a 10-year time horizon need to take account of parameter\u000d\u000a      uncertainty. The relative simplicity of the basic CBD model, and its\u000d\u000a      ability to capture the big picture, meant that it has risen rapidly as a\u000d\u000a      highly-cited and robust benchmark model with a number of important\u000d\u000a      descendants.\u000d\u000a    Extensions. In 2007, [text removed for publication] Cairns, Blake\u000d\u000a      and Dowd were engaged as research consultants as a direct result of their\u000d\u000a      work in [1]. The commission was to develop a body of published research on\u000d\u000a      longevity modelling that would: (i) help practitioners measure and\u000d\u000a      understand their exposures to longevity risk; and (ii) give them the\u000d\u000a      confidence to make active decisions on how to manage this risk. The\u000d\u000a      collaboration lasted for 4 years resulting in 7 peer-reviewed publications\u000d\u000a      (including [2-5]) in international journals. The initial phase of work had\u000d\u000a      two objectives: to develop new mortality models that built on the\u000d\u000a      advantages and disadvantages of previously published models; and to\u000d\u000a      conduct the first comprehensive comparison of all important stochastic\u000d\u000a      mortality models (five existing models `M1' to `M5' and three new models\u000d\u000a      `M6' to `M8' that built on the team's knowledge of the advantages and\u000d\u000a      disadvantages of models M1 to M5). This produced four out of the seven\u000d\u000a      papers (the most influential being [2] which was awarded the 2009 Society\u000d\u000a      of Actuaries Prize). Apart from the development of new models, the team\u000d\u000a      pioneered a forensic approach to analysis of individual models and groups\u000d\u000a      of models, setting further benchmarks for model selection criteria,\u000d\u000a      communication of risk, and assessment of model risk. This initial phase\u000d\u000a      also resulted in the production of a suite of open-source software in R\u000d\u000a      (written by Cairns) for fitting models M1-M3 and M5-M8, and simulation\u000d\u000a      models for M1 and M5 [text removed for publication]. The research papers\u000d\u000a      were written in a style that ensured accessibility of their methodology to\u000d\u000a      stakeholders in the developing longevity market. Their influence is\u000d\u000a      attested by the fact that the names M1 to M8, labelling various models, as\u000d\u000a      well as the name `CBD' are now in common usage amongst longevity-risk\u000d\u000a      experts.\u000d\u000a    Attribution. A. J. G. Cairns has been a Professor of Financial\u000d\u000a      Mathematics in the Maxwell Institute since 1992. His co-authors were with\u000d\u000a      the Cass Business School (D. Blake), Nottingham University Business School\u000d\u000a      (K. Dowd) [text removed for publication] during the period of the\u000d\u000a      underpinning research.\u000d\u000a    "},{"CaseStudyId":"26119","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Biotechnology and Biological Sciences Research Council"],"ImpactDetails":"\u000d\u000a    \u000d\u000a      \u000aReach: Benefits veterinarians, dog owners and companion dogs in\u000d\u000a        the UK and Europe;\u000d\u000a        animal drug development industry R&amp;D.\u000d\u000a      \u000aSignificance: Offers the first set of validated tools for vets\u000d\u000a        and dog owners to assess acute\u000d\u000a        and chronic pain, allowing more timely, appropriate and compassionate\u000d\u000a        treatment.\u000d\u000a    \u000d\u000a    Pain assessment in animals presents a challenge for clinicians because of\u000d\u000a      the inherent\u000d\u000a      communication difficulties but also because different animals and species\u000d\u000a      respond differently to\u000d\u000a      pain. The CMPS was the first statistically constructed and validated acute\u000d\u000a      pain scale for dogs in\u000d\u000a      surgical and clinical settings, incorporating a structured and objective\u000d\u000a      approach to pain\u000d\u000a      assessment. It has been used in clinical trials, was subsequently\u000d\u000a      streamlined into the CMPS Short\u000d\u000a      Form (CMPS-SF), and includes an intervention level for the administration\u000d\u000a      of analgesic relief. The\u000d\u000a      CMPS was first circulated through a comprehensive programme of CPD\u000d\u000a      conducted by Reid and\u000d\u000a      Nolan, but was later made available as an online tool. The CMPS acute pain\u000d\u000a      scale is available for\u000d\u000a      open-access download from the University of Glasgow website [a]. Since\u000d\u000a      2008, the CMPS has\u000d\u000a      been downloaded by 3266 non-academic users worldwide. Once downloaded the\u000d\u000a      CMPS can be\u000d\u000a      copied (thus download data are likely to underestimate usage by this\u000d\u000a      route). It has been translated\u000d\u000a      into Italian (2010) for use in clinical trials, as validated pain\u000d\u000a      measurement methods are required for\u000d\u000a      the development and licensing of new animal analgesic drugs.\u000d\u000a    The unique means by which the CMPS can define an intervention threshold\u000d\u000a      for administering pain\u000d\u000a      relief has resulted in its inclusion in a number of textbooks,\u000d\u000a      establishing it as a standard reference\u000d\u000a      for measuring pain for both teaching and veterinary practice. It has also\u000d\u000a      featured in key pain\u000d\u000a      management handbooks for practitioners such as the Handbook of\u000d\u000a        Veterinary Pain Management\u000d\u000a      (2008) and Anaesthesia for Veterinary Technicians (2010) [b]. In\u000d\u000a      2009, the Glasgow Pain and\u000d\u000a      Welfare Research Group (Scott, Nolan and Reid) won the Universities\u000d\u000a      Federation for Animal\u000d\u000a      Welfare inaugural Companion Animal Welfare Award [c], which recognises\u000d\u000a      `significant innovations\u000d\u000a      or advances for the welfare of companion animals'.\u000d\u000a    The CMPS acute pain scale has been used by many of the market-leading\u000d\u000a      veterinary healthcare\u000d\u000a      companies in clinical trials and regulatory application for novel\u000d\u000a      analgesics. This included Merial,\u000d\u000a      Novartis Animal Health Inc., Vetoquinol, Dechra and Animalcare. The\u000d\u000a      following applications have\u000d\u000a      gained regulatory approval based on data obtained with the CMPS:\u000d\u000a    \u000d\u000a      2008, Merial Ltd.&#8212; approval from the US Food and Drug Administration\u000d\u000a        (FDA) for `previcox' in\u000d\u000a        dogs following orthopaedic surgery.\u000d\u000a      2011 &#8212; approval from the European Medicines Agency for `Recuvyra' in\u000d\u000a        dogs following\u000d\u000a        orthopaedic surgery.\u000d\u000a      2011 Novartis Animal Health US Inc. &#8212; approval from the FDA for\u000d\u000a        `DERAMAXX' in dogs\u000d\u000a        following dental surgery.\u000d\u000a      2012, Nexcyon Pharmaceuticals Inc. &#8212; approval from the FDA for\u000d\u000a        `Recuvyra' in dogs following\u000d\u000a        general surgery.\u000d\u000a    \u000d\u000a    Details are provided in [d].\u000d\u000a    Vetoquinol is the 10th largest veterinary pharmaceutical\u000d\u000a      company in the world. Since mid-2012, its\u000d\u000a      UK subsidiary has been using the CMPS acute pain scale as marketing\u000d\u000a      support for their range of\u000d\u000a      pain medications, including Cimalgex&#174;. Vetoquinol specifically\u000d\u000a      chose the CMPS acute pain scale\u000d\u000a      because:\u000d\u000a    it is scientifically validated and well known in the UK whilst being\u000d\u000a        simple to use....it was\u000d\u000a        recommended to us by various veterinary surgeons throughout the UK.\u000d\u000a    The Vetoquinol CMPS acute pain scale marketing support materials were a\u000d\u000a      major focus of their\u000d\u000a      Cimalgex&#174; promotional activities at the London Vet Show (November\u000d\u000a      2012) and British Small\u000d\u000a      Animal Veterinary Association Congress exhibition (April 2013).\u000d\u000a    The success of CMPS is reflected in its use as a standard tool in wider\u000d\u000a      developments. CMPS itself\u000d\u000a      is now being used in a wider range of disease conditions, including\u000d\u000a      obesity [A] and cancer\u000d\u000a      (publication in preparation). It has now also led to the creation of\u000d\u000a      similar practical tools for welfare\u000d\u000a      assessment in farm animals (cows and pigs), for use by owners or stockmen\u000d\u000a      [B]. New research,\u000d\u000a      funded by Pfizer, has also created an acute pain instrument for cats\u000d\u000a      which, for the first time in this\u000d\u000a      species, includes facial shape and expression, based on an in-depth\u000d\u000a      landmark study of cats' faces.\u000d\u000a    The GUVQuest tool for the assessment of chronic pain in dogs can be used\u000d\u000a      by non-clinicians such\u000d\u000a      as animal owners and is informing decisions on euthanasia, treatment or\u000d\u000a      cancer therapy. It has\u000d\u000a      been exploited commercially by Newmetrica Ltd, through a University of\u000d\u000a      Glasgow Easy Access IP\u000d\u000a      agreement, for provision as an online tool which provides users with\u000d\u000a      automated output. This online\u000d\u000a      version of the tool has been licensed to a number of individuals and\u000d\u000a      companies for use in obesity,\u000d\u000a      cancer and arthritis clinical trials.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Statistical research has played a leading role in the creation of two\u000d\u000a      pain measurement tools for\u000d\u000a      dogs and a welfare tool for farm animals. The Glasgow Composite Measure\u000d\u000a      Pain Scale (CMPS)\u000d\u000a      was the first validated, multidimensional tool for assessing acute pain in\u000d\u000a      domestic animals in\u000d\u000a      surgical and clinical settings, leading to improved treatment. The tool is\u000d\u000a      based on a statistical\u000d\u000a      model and is unique in providing an intervention threshold for pain\u000d\u000a      relief. Since its launch in 2008, it\u000d\u000a      has been downloaded by over 3,000 users from both veterinary practice and\u000d\u000a      industry and it has\u000d\u000a      been used by several companies for regulatory approval of novel\u000d\u000a      analgesics. A second pain\u000d\u000a      measurement tool (GUVQuest) was developed for chronic pain in dogs,\u000d\u000a      enabling its impact on\u000d\u000a      quality of life to be assessed. This tool is suitable for animal owners\u000d\u000a      and is being exploited\u000d\u000a      commercially in web-based form. A welfare tool for pigs has also been\u000d\u000a      developed using the same\u000d\u000a      statistical principles and work is underway on the development of similar\u000d\u000a      tools for cows and cats.\u000d\u000a    ","ImpactType":"Political","Institution":"\u000d\u000a    University of Glasgow\u000d\u000a    ","Institutions":[{"AlternativeName":"Glasgow (University of)","InstitutionName":"University of Glasgow","PeerGroup":"A","Region":"Scotland","UKPRN":10007794}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1. Holton, L., Reid,\u000d\u000a        J., Scott,\u000d\u000a        E.M., Pawson,\u000d\u000a        P., and Nolan,\u000d\u000a        A.M. (2001) Development\u000d\u000a        of a\u000d\u000a        behaviour-based scale to measure acute pain in dogs. Veterinary\u000d\u000a          Record, 148 (17). pp. 525-531.\u000d\u000a      ISSN 0042-4900 (doi:10.1136\/vr.148.17.525)\u000d\u000a    \u000a\u000a2. Burnell, M. (2004). The scaling of acute pain in dogs.\u000d\u000a      University of Glasgow PhD thesis in\u000d\u000a      Statistics. [available from HEI]\u000d\u000a    \u000a\u000a3. Morton, C.M., Reid,\u000d\u000a        J., Scott,\u000d\u000a        E.M., Holton, L.L., and Nolan, A.M. (2005) Application\u000d\u000a        of a\u000d\u000a        scaling model to establish and validate an interval level pain scale for\u000d\u000a        assessment of acute\u000d\u000a        pain in dogs. American\u000d\u000a        Journal of Veterinary Research, 66(12). pp. 2154-2166.\u000d\u000a      (doi:10.2460\/ajvr.2005.66.2154)\u000d\u000a      *\u000d\u000a    \u000a\u000a4. Wiseman-Orr M.L., Scott E.M., Reid J. and Nolan A.M. (2006) Validation\u000d\u000a      of a structured\u000d\u000a      questionnaire as an instrument to measure chronic pain in dogs on the\u000d\u000a      basis of effects on\u000d\u000a      health-related quality of life. American Journal of Veterinary\u000d\u000a        Research 67(11): 1826-1836.\u000d\u000a      (doi:10.2460\/ajvr.67.11.1826)\u000d\u000a      *\u000d\u000a    \u000a\u000a5. Reid, J.,\u000d\u000a      Nolan, A.M.,\u000d\u000a      Hughes, J.M.L., Lascelles, D., Pawson,\u000d\u000a        P., and Scott,\u000d\u000a        E.M. (2007)\u000d\u000a      Development of the short-form\u000d\u000a        Glasgow Composite Measure Pain Scale (CMPS-SF) and\u000d\u000a        derivation of an analgesic intervention score. Animal\u000d\u000a          Welfare, 16. pp. 97-104. ISSN 0962-7286.\u000d\u000a      [available from HEI] *\u000d\u000a    \u000a* best indicators of research quality\u000d\u000a    Key Grants\u000d\u000a      &#8226; 2000: J. Reid, A. Nolan, E.M. Scott. &#163;166k, Pfizer Animal Health. Pain\u000d\u000a        scales for dogs.\u000d\u000a      &#8226; 2001: J. Fitzpatrick, E.M. Scott, A.Nolan, C. Nicol. &#163;258k, BBSRC, A\u000d\u000a        welfare index for dairy\u000d\u000a        cows.\u000d\u000a      &#8226; 2004: A. Nolan, J. Fitzpatrick, E.M. Scott, J. Reid: &#163;195k, BVA Animal\u000d\u000a        Welfare Trust.\u000d\u000a        Lectureship.\u000d\u000a      &#8226; 2006: S. Fleetwood Walker, A. Nolan, J. Russell, A. Lawrence. [Scott,\u000d\u000a        Glasgow CI]. &#163;2.5m,\u000d\u000a        BBSRC; Perinatal programming of stress responses, nociceptive mechanisms\u000d\u000a        and the welfare\u000d\u000a        consequences in pigs. With University of Edinburgh and Scottish\u000d\u000a        Agricultural Colleges.\u000d\u000a      &#8226; 2011: A. Nolan, E.M. Scott. &#163;59k, Pfizer Animal Health, an acute pain\u000d\u000a        scale for cats.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    A. German AJ, Holden SL, Wiseman-Orr ML, Reid J, Nolan AM, Biourge V,\u000d\u000a      Morris PJ, Scott EM\u000d\u000a      (2012). Quality of Life is Reduced in Obese Dogs, but Improves After\u000d\u000a      Successful Weight Loss.\u000d\u000a      The Veterinary Journal 192 (3). pp. 428-434. ISSN 1090-0233 (doi:10.1016\/j.tvjl.2011.09.015)\u000d\u000a      Corroborates use of CMPS as standard tool in wider developments (disease\u000d\u000a      conditions).\u000d\u000a    B. Wiseman-Orr,\u000d\u000a        M.L., Scott,\u000d\u000a        E.M., and Nolan,\u000d\u000a        A.M. (2011). Development\u000d\u000a        and testing of a novel\u000d\u000a        instrument to measure health-related quality of life (HRQL) of farmed\u000d\u000a        pigs and promote welfare\u000d\u000a        enhancement (parts 1 and 2). Animal\u000d\u000a        Welfare, 20. pp. 535-558. ISSN 0962-7286 [available\u000d\u000a      from HEI] Corroborates use of CMPS as standard tool in wider developments\u000d\u000a      (other animals).\u000d\u000a    a. University of Glasgow website to download CMPS [link]\u000d\u000a    b. Textbooks reproducing CMPS:\u000d\u000a    - Wiseman-Orr ML, Reid JA, Nolan AM and Scott EM (2008) Quality of life\u000d\u000a      issues. In Handbook\u000d\u000a        of Veterinary Pain Management, 2nd Edition. Eds James S. Gaynor and\u000d\u000a      William W. Muir, III. St\u000d\u000a      Louis: Mosby Elsevier. ISBN 978-0-323-04679-4 and\u000d\u000a    - Bryant S (2010) Pain Assessment. In Anesthesia for Veterinary\u000d\u000a        Technicians, Wiley-Blackwell.\u000d\u000a      ISBN 978-0-8138-0586-3\u000d\u000a    c. Animal Welfare Award: [link]\u000d\u000a    d. Regulatory approval:\u000d\u000a    - Merial Ltd., FDA approval for `previcox' [PDF\u000d\u000a        link, or available from HEI]\u000d\u000a    - EMA approval for `Recuvyra' [PDF\u000d\u000a        link, or available from HEI]\u000d\u000a    - Novartis Animal Health US Inc., FDA approval for `DERAMAXX' [PDF\u000d\u000a        link, or available from\u000d\u000a      HEI]\u000d\u000a    - Nexcyon Pharmaceuticals Inc., FDA approval for `Recuvyra' [PDF\u000d\u000a        link, or available from HEI]\u000d\u000a    e. Information from Vetoquinol is available on request\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Quantifying animal pain\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Measurement is fundamental to the application of scientific methods in\u000d\u000a      almost any context and\u000d\u000a      statistical methodology provides a principled basis for dealing with the\u000d\u000a      variation, analysis and\u000d\u000a      interpretation of measurement data. However, in some settings observed\u000d\u000a      data are only indirectly\u000d\u000a      related to the underlying attributes of interest and so the construction\u000d\u000a      of meaningful measurements\u000d\u000a      of the underlying process is the fundamental problem. An important example\u000d\u000a      is the phenomenon of\u000d\u000a      pain &#8212; an experience with a strong subjective component which makes\u000d\u000a      quantitative measurement\u000d\u000a      problematic. In human medicine, this has been tackled through structured\u000d\u000a      self-reporting\u000d\u000a      questionnaires, which have provided valid, reliable and responsive tools\u000d\u000a      for the measurement of\u000d\u000a      pain and quality of life, developed using psychometric methods and with a\u000d\u000a      central role played by\u000d\u000a      statistical models. By contrast, in animal care where direct communication\u000d\u000a      is impossible, the\u000d\u000a      absence of a valid, reliable means of quantifying pain presents a major\u000d\u000a      challenge. Traditional\u000d\u000a      approaches based on observer ratings on a one-dimensional scale such as\u000d\u000a      the Numerical Rating\u000d\u000a      Scale have been shown to be unreliable for animals. Without a means of\u000d\u000a      measuring pain, it is\u000d\u000a      difficult to assess the effects of analgesic strategies and therapeutic\u000d\u000a      agents, and so animal welfare\u000d\u000a      is hard to guarantee. The process of developing suitable measurement\u000d\u000a      scales is therefore\u000d\u000a      fundamental to progress.\u000d\u000a    Collaborative research began in the University of Glasgow in 1997 to\u000d\u000a      develop, for the first time, a\u000d\u000a      validated pain scale for the measurement of acute pain in dogs, using\u000d\u000a      sound statistical principles.\u000d\u000a      The project was led by Prof. Marian Scott of the School of Mathematics\u000d\u000a      &amp; Statistics and by Profs.\u000d\u000a      Andrea Nolan and Jackie Reid, then of the School of Veterinary Medicine.\u000d\u000a      The research was\u000d\u000a      interdisciplinary from the beginning, requiring levels of communication\u000d\u000a      and co-operation between\u000d\u000a      the partners which were sufficiently deep to allow the nature of the\u000d\u000a      veterinary issues to be\u000d\u000a      understood clearly and to allow the imaginative and innovative application\u000d\u000a      of statistical methods in\u000d\u000a      reaching an effective solution. The key role of Prof. Scott in this\u000d\u000a      interdisciplinary collaboration is\u000d\u000a      reflected in her role as a principal author of all the scientific papers\u000d\u000a      and a major grantholder.\u000d\u000a    The starting point for the new research was the development of a novel\u000d\u000a      veterinary language of\u000d\u000a      pain, beginning with the generation of a large set of descriptors based on\u000d\u000a      simple words or phrases\u000d\u000a      used by human observers of animal behaviour. These were grouped into seven\u000d\u000a      behavioural\u000d\u000a      categories (vocalisation, attention to painful area, mobility, response to\u000d\u000a      touch, demeanour, posture,\u000d\u000a      activity) by an innovative combination of cluster analysis and Cronbach's\u000d\u000a      alpha as a measure of\u000d\u000a      within-category consistency [1]. In order to generate data on an interval\u000d\u000a      level measurement scale,\u000d\u000a      a sophisticated and powerful framework based on latent class analysis was\u000d\u000a      developed in a\u000d\u000a      University of Glasgow Statistics PhD thesis [2]. In combination with\u000d\u000a      clinical and practical\u000d\u000a      considerations, this led to the selection of a particular form of\u000d\u000a      Thurstone-Rasch pairwise-comparison\u000d\u000a      model [3]. The resulting measurement scale was then validated in different\u000d\u000a      medical\u000d\u000a      and surgical conditions, using statistical methods for optimal design and\u000d\u000a      analysis [3]. These studies\u000d\u000a      demonstrated that the scores produced by the tool corresponded\u000d\u000a      appropriately to independent\u000d\u000a      clinical assessment and quantified the level of inter-observer variation.\u000d\u000a      Multi-centre trials [5] were\u000d\u000a      subsequently designed to identify a suitable intervention level at which\u000d\u000a      pain relief should be\u000d\u000a      administered, using discriminant analysis.\u000d\u000a    This was the first time that such models had been applied in, and adapted\u000d\u000a      to, veterinary pain\u000d\u000a      assessment and the resulting tool became known as the Glasgow Composite\u000d\u000a      Measure Pain Scale\u000d\u000a      (CMPS). A short form (CMPS-SF) was subsequently developed for widespread\u000d\u000a      clinical use [5].\u000d\u000a      After addressing these issues in acute pain, attention moved to the\u000d\u000a      assessment of chronic pain in\u000d\u000a      dogs and its impact on quality of life, including both intensity and\u000d\u000a      affect. This led to one of the first\u000d\u000a      validated quality-of-life tools in veterinary science (GUVQuest),\u000d\u000a      supported by funding from Pfizer\u000d\u000a      Animal Health (&#163;166k, 2000) and the BVA Animal Welfare Trust (&#163;195k,\u000d\u000a      2004). The development\u000d\u000a      of GUVQuest was based on adaptation of the statistical methods used in the\u000d\u000a      earlier CMPS tool.\u000d\u000a      This involved the design of interviews, focus groups, and online surveys\u000d\u000a      whose subsequent\u000d\u000a      analysis allowed a language of chronic pain to be identified, using over\u000d\u000a      100 descriptors. A study to\u000d\u000a      provide data for a scoring model was then designed and executed.\u000d\u000a      Statistical modelling based on\u000d\u000a      factor analysis allowed a small number of factors to be identified,\u000d\u000a      producing an informative\u000d\u000a      multivariate profile for each animal [4]. Subsequent research has allowed\u000d\u000a      a short form of the\u000d\u000a      GUVQuest to be developed, based on modelling the contribution of each item\u000d\u000a      to the overall score\u000d\u000a      and characterising the performance of each item as evaluative or\u000d\u000a      discriminatory.\u000d\u000a    "},{"CaseStudyId":"26121","Continent":[{"GeoNamesId":"6255146","Name":"Africa"},{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"953987","Name":"South Africa"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Shell Global Solutions (based in Chester, UK) undertakes a wide variety\u000d\u000a      of projects where statistical and mathematical modelling is required. The\u000d\u000a      GWSDAT software was developed by Shell, based on statistical methodology\u000d\u000a      and interactive software tools from the University of Glasgow. GWSDAT was\u000d\u000a      first released for use by Shell consultants in 2009 and the most recent\u000d\u000a      version was released in 2012. A survey carried out by Shell in 2013\u000d\u000a      estimates there are currently around 200 users worldwide, from countries\u000d\u000a      including, for example, the UK, USA, Australia and South Africa.\u000d\u000a    Shell is responsible for the operation of a huge number of installations,\u000d\u000a      from small petrol stations to extensive refinery sites, and significant\u000d\u000a      attention is paid to monitoring the surrounding environment to ensure the\u000d\u000a      prevention of any adverse effects. Inadvertent release of soluble material\u000d\u000a      can lead to pollution of groundwater, with a risk that contamination is\u000d\u000a      transported beyond the site boundaries. It is standard practice to drill\u000d\u000a      boreholes around installations, from which samples can be regularly drawn\u000d\u000a      and analysed, in order to identify and monitor the presence of any\u000d\u000a      pollutants.\u000d\u000a    The measurements from boreholes at a particular site generate a dataset\u000d\u000a      which records pollutant levels at points in space and time, but there are\u000d\u000a      issues with the practicalities and costs of sampling which restrict the\u000d\u000a      number of measurements made. Also, the consultants responsible for\u000d\u000a      interpreting these measurements generally have a strong scientific or\u000d\u000a      engineering background but may not have extensive training in statistical\u000d\u000a      methods. Using data from boreholes to construct reliable estimates of the\u000d\u000a      underlying pollution patterns across (i) the whole site and (ii) the\u000d\u000a      entire time period requires specialist statistical methods and concepts.\u000d\u000a      However, referring results to others with the necessary expertise for\u000d\u000a      statistical analysis would potentially lead to significant delays.\u000d\u000a    The GWSDAT directly addresses these needs. Shell has identified the\u000d\u000a      benefits of the software to include:\u000d\u000a    \u000d\u000a      The early identification of rising contaminant concentration trends,\u000d\u000a        leading to reduced response time in the event of leaks or spills. There\u000d\u000a        has been more than one instance of unknown leaks being identified by the\u000d\u000a        use of the software tool.\u000d\u000a      Improved data transparency leading to better designed monitoring\u000d\u000a        networks and more robust conceptual site models, thereby avoiding\u000d\u000a        collection of redundant data.\u000d\u000a      Clarity on the relationship between dissolved solute concentrations\u000d\u000a        and groundwater elevation and flow direction, for improved plume control\u000d\u000a        measures and fit-for-purpose remediation system design.\u000d\u000a      Significantly reduced time and resource expenditure on the analysis\u000d\u000a        and reporting of monitoring data, particularly in the case of smaller\u000d\u000a        sites where insufficient data is available to justify the cost of using\u000d\u000a        geographic information systems or transport simulations.\u000d\u000a      Significant cost savings. A recent survey undertaken by the company\u000d\u000a        indicates that the software package has led to savings in the region of\u000d\u000a        $10m in the last 3 years.\u000d\u000a    \u000d\u000a    Users view an Excel spreadsheet and explore, graphically and\u000d\u000a      interactively, the data it contains. Importantly, the underlying\u000d\u000a      statistical model constructs an estimate of the pollution surface across\u000d\u000a      the entire spatial region of interest and a slider can be used to view how\u000d\u000a      this changes over time. Different solutes can be selected or the data from\u000d\u000a      particular wells viewed in greater detail, with estimates of trend\u000d\u000a      superimposed. Large datasets (more than 50,000 rows) can be modelled in a\u000d\u000a      matter of seconds. The added value and flexibility of the system has been\u000d\u000a      very well received by users.\u000d\u000a    Key staff responsible for the GWSDAT software application within Shell\u000d\u000a      commented that:\u000d\u000a    Data analysis and reporting that previously required weeks of effort\u000d\u000a        can now be completed automatically, at the click of a mouse. The GWSDAT\u000d\u000a        spatiotemporal modelling research programme has yielded total savings\u000d\u000a        for Shell in excess of $10M over the past 3 years.\u000d\u000a    Although developed specifically for use by Shell, the package is also\u000d\u000a      being made available to external users, including environmental\u000d\u000a      regulators. It is currently being submitted to an industry website from\u000d\u000a      which it will be freely available to other industrial users. Shell also\u000d\u000a      regularly publicises the package at national and international\u000d\u000a      conferences.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    With global demand for energy ever increasing, environmental impact has\u000d\u000a      become a major priority for the oil industry. A collaboration between\u000d\u000a      researchers at the University of Glasgow and Shell Global Solutions has\u000d\u000a      developed GWSDAT (GroundWater Spatiotemporal Data Analysis Tool). This\u000d\u000a      easy-to-use interactive software tool allows users to process and analyse\u000d\u000a      groundwater pollution monitoring data efficiently, enabling Shell to\u000d\u000a      respond quickly to detect and evaluate the effect of a leak or spill.\u000d\u000a      Shell estimates that the savings gained by use of the monitoring tool\u000d\u000a      exceed $10m over the last three years. GWSDAT is currently being used by\u000d\u000a      around 200 consultants across many countries (including the UK, US,\u000d\u000a      Australia and South Africa) with potentially significant impacts on the\u000d\u000a      environment worldwide.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Glasgow\u000d\u000a    ","Institutions":[{"AlternativeName":"Glasgow (University of)","InstitutionName":"University of Glasgow","PeerGroup":"A","Region":"Scotland","UKPRN":10007794}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a&#8226; Bowman, A.W. &amp; Azzalini, A. (1997). Applied Smoothing\u000d\u000a        Techniques for Data Analysis. Oxford University Press. ISBN\u000a        9780198523963 [available from HEI]\u000d\u000a    \u000a\u000a&#8226; Bowman, A.W., Crawford, E., Alexander, G. &amp; Bowman, R.W. (2006a).\u000a        rpanel: simple interactive controls for R functions using the tcltk\u000d\u000a        package. Journal of Statistical Software 17, issue 9.*\u000d\u000a    \u000a\u000a&#8226; Bowman, A.W., Crawford, E. Bowman, R.W. (2006b). rpanel: making graphs\u000d\u000a      move with tcltk. R News; 6, issue 4. (pdf;\u000d\u000a      software)\u000d\u000a    \u000a\u000a&#8226; Bowman, A.W., Giannitrapani, M., &amp; Scott, E.M. (2009).\u000d\u000a      Spatiotemporal smoothing and sulphur dioxide trends over Europe. Applied\u000d\u000a      Statistics, 58, 5, 737-752. doi:10.1111\/j.1467-9876.2009.00671.x\u000d\u000a      *\u000d\u000a    \u000a\u000a&#8226; Bowman, A.W., Evers, L., Molinari, D., Jones, W. &amp; Spence, M.J.\u000d\u000a      (2013). Bayesian\u000asmoothing\u000a        of spatiotemporal data with applications to groundwater monitoring.\u000d\u000a      Submitted for publication. (arxiv:1310.7815) *\u000d\u000a    \u000a\u000a&#8226; Jones, W., Spence, M.J., Bowman, A.W., Evers, L., Molinari, D. (2013)\u000d\u000a      GWSDAT &#8212; Groundwater Spatiotemporal Data Analysis Tool. Submitted for\u000d\u000a      publication.\u000d\u000a      (arxiv:1310.8158)\u000d\u000a    \u000a* best indicators of research quality\u000d\u000a    Full details on the rpanel package can be found at;\u000d\u000a    &#8226; http:\/\/cran.r-project.org\/web\/packages\/rpanel\/\u000d\u000a    &#8226; http:\/\/www.stats.gla.ac.uk\/~adrian\/rpanel\/\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    Shell have presented GWSDAT at the following conferences:\u000d\u000a    \u000d\u000a      AquaConSoil 2013 (12th International UFZ-Deltares Conference on\u000d\u000a        Groundwater-Soil-Systems and Water Resource Management), Barcelona,\u000d\u000a        Spain, April 2013\u000d\u000a      ENBIS (European Network for Business and Industrial Statistics),\u000d\u000a        Coimbra, Portugal, Sep 2011\u000d\u000a      UseR! (The R User Conference), Warwick, UK, Aug 2011\u000d\u000a      NICOLE (Network for Industrially Contaminated Land in Europe) Network\u000d\u000a        Conference, Copenhagen, Denmark, May 2011. Workshop on Innovative Site\u000d\u000a        Characterisation Tools.\u000d\u000a      European Geosciences Union (EGU), Vienna, Austria, Apr 2011\u000d\u000a      Environmental Protection Agency National Tanks Conference, Boston,\u000d\u000a        USA, Sep 2010\u000d\u000a      UseR! (The R User Conference), Rennes, France, Jul 2009\u000d\u000a    \u000d\u000a    Testimonials:\u000d\u000a    \u000d\u000a      Statistical Consultant, Shell Global Solutions UK (details of software\u000d\u000a        application within Shell, confirmation of improved data analysis and\u000d\u000a        savings)\u000d\u000a      Environmental Geochemist, Shell Global Solutions UK (details of\u000d\u000a        software application within Shell, confirmation of improved data\u000d\u000a        analysis and savings)\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Development of an innovative data analysis tool to monitor groundwater\u000d\u000a        pollution and environmental impact\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2648579","Name":"Glasgow"},{"GeoNamesId":"2653228","Name":"Chester"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"},{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    `Flexible regression' has been a major statistical research theme at the\u000d\u000a      University of Glasgow for over 20 years. The main aim of this research is\u000d\u000a      to use observed data to estimate an underlying regression function,\u000d\u000a      describing the relationship between a response and one or more covariates,\u000d\u000a      without constraining the function to follow any particular shape, apart\u000d\u000a      from requiring it to be smooth. Professor Adrian Bowman (Professor of\u000d\u000a      Statistics since 1995) has made extensive contributions to this topic,\u000d\u000a      including a book (Bowman &amp; Azzalini, 1997) which provided methods for\u000d\u000a      a wide variety of data structures and placed a strong emphasis on the\u000d\u000a      inferences which could be drawn from real applications.\u000d\u000a    Since 2002, research has been directed towards spatiotemporal models for\u000d\u000a      data which have been collected over both space and time. There are many\u000d\u000a      environmental applications for models of this type. A particular example\u000d\u000a      is the modeling of sulphur dioxide air pollution over Europe (Bowman et\u000d\u000a      al., 2009) using data from an extensive network of monitoring stations.\u000d\u000a      The underlying methodology represented one of the first developments of\u000d\u000a      flexible regression in a spatiotemporal setting, allowing complex trends\u000d\u000a      and interactions to be explored, and both theory and computational methods\u000d\u000a      were described.\u000d\u000a    In 2006, Shell Global Solutions began to investigate the use of these\u000d\u000a      methods in the context of groundwater monitoring, prompted in part by the\u000d\u000a      appointment of a PhD student in Statistics from the University of Glasgow.\u000d\u000a      The statistical and computing tools developed in Glasgow fed into the\u000d\u000a      creation of new software, which gained extensive use within the company.\u000d\u000a      The software tool allowed responsible staff to view and interpret the\u000d\u000a      extensive groundwater data which is routinely collected, and the company\u000d\u000a      quickly recognised the potential value of the tool within Shell\u000d\u000a      operations. Version 1 of GroundWater Spatiotemporal Data Analysis Tool\u000d\u000a      (GWSDAT) was released for use by Shell consultants in 2009.\u000d\u000a    With interest from Shell, discussions about how the methodology might be\u000d\u000a      developed further were initiated. A PhD studentship, jointly funded by\u000d\u000a      Shell and the University of Glasgow, was created and a student recruited\u000d\u000a      in 2009. The subsequent research agenda placed strong emphasis on (i) the\u000d\u000a      need for very efficient computational methods for large datasets and (ii)\u000d\u000a      the production of very stable estimates which provided good predictions\u000d\u000a      even in regions where the monitoring sites were sparsely located. Dr.\u000d\u000a      Ludger Evers (Lecturer in Statistics, University of Glasgow, 2008-present)\u000d\u000a      joined the research project in 2009, contributing extensive experience in\u000d\u000a      computational and Bayesian methods. The ensuing research used B-spline\u000d\u000a      bases to provide pollution surface estimates which can be represented\u000d\u000a      mathematically in very compact forms, employing a fully Bayesian model to\u000d\u000a      account for all the sources of uncertainty and using a combination of\u000d\u000a      matrix techniques to provide very fast solutions. The combination of these\u000d\u000a      methods allows pollution levels to be estimated in a fully automatic, and\u000d\u000a      very stable, manner. The new model also gives access to useful measures of\u000d\u000a      uncertainty, including credibility intervals and standard errors. These\u000d\u000a      methods have been implemented in Shell software since 2012 but a paper\u000d\u000a      describing the underlying methodology was deferred until full practical\u000d\u000a      evaluation could take place and illustrative examples reported (Bowman et\u000d\u000a      al., 2013).\u000d\u000a    In parallel with this line of research, a statistical software tool for\u000d\u000a      interactive analysis and graphics is under development at the University\u000d\u000a      of Glasgow (School of Mathematics and Statistics). The standard\u000d\u000a      statistical computing environment in academic research and many areas of\u000d\u000a      industry is `R'. In 2005 Bowman began to lead the development of `rpanel',\u000d\u000a      an add-on package for R, which was created to make it as easy and\u000d\u000a      efficient as possible for non-technical users to construct interactive\u000d\u000a      software such as dynamic graphics (Bowman et al., 2006ab). Users can\u000d\u000a      easily create buttons, sliders and other types of graphical control and\u000d\u000a      can interact with plots in ways which considerably enhance exploration of\u000d\u000a      datasets. The package is widely used and continues to be developed in\u000d\u000a      Glasgow, with the most recent version released in 2013.\u000d\u000a    Shell Global Solutions used the `rpanel' package to build the first\u000d\u000a      version of GWSDAT in 2007-2009. The University of Glasgow team worked with\u000d\u000a      the company to add functionality to `rpanel' in response to specific\u000d\u000a      requests. The resulting Shell software, GWSDAT incorporates the\u000d\u000a      statistical methodology described above and uses the rpanel package to\u000d\u000a      provide users with easy to use, interactive controls for reading data,\u000d\u000a      fitting models and exploring the results in graphical form.\u000d\u000a    "},{"CaseStudyId":"26127","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3017382","Name":"France"}],"Funders":["Royal Society"],"ImpactDetails":"\u000d\u000a    Engineering applications that require the investigation of material\u000d\u000a      properties use software packages which make use of `finite element' (FE)\u000d\u000a      methods. These packages are used extensively worldwide and considered\u000d\u000a      indispensible by professional engineers who undertake design calculations\u000d\u000a      and stress analysis, particularly in the automotive and aerospace\u000d\u000a      industries where rubberlike materials are employed in many components.\u000d\u000a      Such materials can deform significantly, and the underlying mathematical\u000d\u000a      models therefore play a crucial role in design decisions which are central\u000d\u000a      to the safe and successful operation of the end products, and associated\u000d\u000a      with substantial financial investments.\u000d\u000a    There are several industry-standard commercially available systems,\u000d\u000a      including Abaqus, ADINA, ALGOR, ANSYS and MARC. All of these systems\u000d\u000a      incorporate Prof. Ogden's models as core standard tools for the analysis\u000d\u000a      of hyperelastic behaviour and the properties of rubberlike materials.\u000d\u000a      Given the widespread use of materials of this type in manufacturing and\u000d\u000a      the very high volume of associated engineering design and analysis\u000d\u000a      activity, the industrial impact of these models is difficult to quantify\u000d\u000a      precisely but is clearly enormous. The methods are key in automotive and\u000d\u000a      aeronautical engineering and, from these industries alone, the economic\u000d\u000a      impact is therefore extremely high.\u000d\u000a    The software designers of Abaqus, ADINA and ANSYS, the three most widely\u000d\u000a      used FE software packages, adopted the Ogden-Roxburgh model from published\u000d\u000a      literature and incorporated it into their systems. (In particular, the\u000d\u000a      Abaqus system incorporates all the Ogden models.) These FE packages are\u000d\u000a      used for calculations involving solid materials in mechanics problems, and\u000d\u000a      the models are essential to these calculations because they give the\u000d\u000a      fundamental constitutive relationships needed to represent the properties\u000d\u000a      of the materials proposed for use in the product being designed. These\u000d\u000a      software packages, all utilising the Ogden models, are standard core tools\u000d\u000a      in industrial design.\u000d\u000a    Abaqus Unified FEA is a suite of simulation software in the SIMULIA brand\u000d\u000a      of Dassault Syst&#232;mes, an international company with over 11,000 employees\u000d\u000a      with headquarters in Paris, France. Dassault Syst&#232;mes is one of the\u000d\u000a      industry leaders in the provision of tools to facilitate realistic\u000d\u000a      simulation in product design and manufacturing, which reduces\u000d\u000a      manufacturing costs by cutting design time and prototype costs. The Abaqus\u000d\u000a      manual gives comprehensive and detailed descriptions of the methods\u000d\u000a      available for implementation. Chapter 18 deals with `Elastic mechanical\u000d\u000a      properties' and section 18.5 with `hyperelasticity'. In section 18.6.1\u000d\u000a      where the `Mullins effect in rubberlike materials' is discussed, the\u000d\u000a      methods described are based on the Ogden-Roxburgh model and [1] is the\u000d\u000a      only referenced paper. Under `Anisotropic hyperelastic behavior' the\u000d\u000a      Holzapfel-Gasser-Ogden model is one of only two specific options listed.\u000d\u000a      Similar evidence of the impact of these models is provided in the manuals\u000d\u000a      of the other major finite element systems. It is therefore clear that\u000d\u000a      professional engineers worldwide are making extremely wide use of these\u000d\u000a      methods to solve a whole variety of problems in a very wide range of\u000d\u000a      application areas.\u000d\u000a    Two quotes from Endurica [1], an industrial consulting and software\u000d\u000a      company:\u000d\u000a    \"Although I had been aware of several alternative approaches to\u000d\u000a        modeling the [Mullins] effect prior to encountering your model, I found\u000d\u000a        all of these to be out of reach for our applications for various\u000d\u000a        reasons. I was immediately impressed by the simplicity of the\u000d\u000a        Ogden-Roxburgh model, its compatibility with approaches we were already\u000d\u000a        investing in, and its effectiveness in accurately representing the main\u000d\u000a        features of the effect. Recognizing these advantages, and what they\u000d\u000a        would mean to modeling efforts in the tire industry and beyond, I was\u000d\u000a        able to persuade my management to fund a project with HKS (now Dassault\u000d\u000a        Systemes) to implement the Ogden-Roxburgh model in the Abaqus Finite\u000d\u000a        Element code.\"\u000d\u000a    \"The Ogden-Roxburgh model is continuing to grow in its impact. I have\u000d\u000a        noticed other workers building various additional effects on top of the\u000d\u000a        Ogden-Roxburgh model (permanent set and cyclic softening, for example).\u000d\u000a        For my part, I have implemented the Ogden-Roxburgh model to represent\u000d\u000a        rubber's cyclic stress-strain behavior in the world's first commercially\u000d\u000a        available fatigue analysis software.\"\u000d\u000a    From Abaqus [2]:\u000d\u000a    \"This [the Ogden-Roxburgh] model was also implemented in Abaqus and\u000d\u000a        serves as the only way to capture this Mullins effect, stress softening\u000d\u000a        behavior, especially noticeable in filled rubber. Capturing this stress\u000d\u000a        softening behavior in elastomers eliminates a key approximation that FEA\u000d\u000a        users have had to make for many years, and leads to a much higher\u000d\u000a        fidelity material model, and thus a much more realistic simulation\u000d\u000a        model.\"\u000d\u000a    From ANSYS [3]:\u000d\u000a    \"Without this material model, accurate representation of the rubber\u000d\u000a        behavior is very difficult. Because of this, it is a popular practice\u000d\u000a        for rubber product design in the ANSYS user community.\"\u000d\u000a    From Abaqus [2] again:\u000d\u000a    \"More recently, another significant contribution from Professor Ogden\u000d\u000a        and colleagues made its way into the commercial Abaqus FEA software.\u000d\u000a        This was a proposed model for capturing the anisotropy in biological\u000d\u000a        soft tissues. This micromechanically based anisotropic strain energy\u000d\u000a        potential (Holzapfel, Gasser, and Ogden, 2000 and also, Gasser, Ogden,\u000d\u000a        and Holzapfel, 2006) is now available in Abaqus for modeling biological\u000d\u000a        tissues and other anisotropic elastomeric materials. This anisotropic\u000d\u000a        strain energy potential is commonly used by life sciences researchers.\"\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Professor Ray Ogden FRS has made fundamental advances in mathematical\u000d\u000a      models for the elastic response of rubber-like materials. These models\u000d\u000a      have been adopted as the standard starting point for the design and\u000d\u000a      analysis of rubber-like solids and they have been incorporated into the\u000d\u000a      industry-standard commercial `finite element' software packages, including\u000d\u000a      Abaqus, ADINA, ALGOR, ANSYS and MARC. These packages are widely used by\u000d\u000a      professional engineers for design calculations and stress analysis,\u000d\u000a      particularly in the aeronautical and automotive industries, playing a\u000d\u000a      crucial role in design decisions associated with enormous financial\u000d\u000a      investments and with the safe and successful operation of the products\u000d\u000a      involved. Models have now also been constructed for the behaviour of soft\u000d\u000a      biological tissue and these are in widespread use in applications in\u000d\u000a      cardiovascular research and the life sciences.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Glasgow\u000d\u000a    ","Institutions":[{"AlternativeName":"Glasgow (University of)","InstitutionName":"University of Glasgow","PeerGroup":"A","Region":"Scotland","UKPRN":10007794}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2988507","Name":"Paris"}],"References":"\u000d\u000a    \u000a[1] R.W. Ogden and D.G. Roxburgh, A pseudo-elastic model for the Mullins\u000d\u000a      effect in filled rubber. Proceedings of the Royal Society of London A 455\u000d\u000a      (1999), 2861-2877.\u000d\u000a      doi.org\/10.1098\/rspa.1999.0431*\u000d\u000a    \u000a\u000a[2] G.A. Holzapfel, T.C. Gasser and R.W. Ogden, A new constitutive\u000d\u000a      framework for arterial wall mechanics and a comparative study of material\u000d\u000a      models. Journal of Elasticity 61 (2000), 1-48. [Reprinted in\u000d\u000a      Cardiovascular Soft Tissue Mechanics, edited by S. C. Cowin and J. D.\u000d\u000a      Humphrey, Kluwer Academic Publishers, Dordrecht (2001).] doi:10.1023\/A:1010835316564*\u000d\u000a    \u000a\u000a[3] T.C. Gasser, R.W. Ogden and G.A. Holzapfel, Hyperelastic modelling of\u000d\u000a      arterial layers with distributed collagen fibre orientations. Journal of\u000d\u000a      Royal Society Interface 3 (2006), 15-35.\u000d\u000a      doi:10.1098\/rsif.2005.0073*\u000d\u000a    \u000a* best indicators of quality\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"12","Subject":"Materials Engineering"},{"Level1":"11","Level2":"16","Subject":"Medical Physiology"},{"Level1":"9","Level2":"3","Subject":"Biomedical Engineering"}],"Sources":"\u000d\u000a    Statements from the following companies [1, 2, 3] are available from HEI:\u000d\u000a    [1] Endurica LLC was founded in 2008 to provide services, technology, and\u000d\u000a      training that accelerate reliable design for elastomer materials and\u000d\u000a      components:\u000d\u000a      www.endurica.com\u000d\u000a      (Statement from President of company)\u000d\u000a    [2] Abaqus Analysis User's Manual (version 6.8). Vol. III: Materials.\u000d\u000a      Dassault Syst&#232;mes Simulia Corp., Providence, RI, USA, 2008:\u000d\u000a      http:\/\/www.3ds.com\/products-services\/simulia\/portfolio\/abaqus\/overview\/\u000d\u000a      (Statement from Senior Sales Manager, EMEA Academia)\u000d\u000a    [3] ANSYS, ANSYS, Inc., Canonsburg, PA, USA:\u000d\u000a      http:\/\/www.ansys.com\/Products\u000d\u000a      (Statement from Lead Product Manager, Mechanical Products) \u000d\u000a    ","Title":"\u000d\u000a    Mathematical models for design and stress analysis in the rubber and\u000d\u000a        automotive industries\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Rubber and other rubber-like materials are ubiquitous in the manufacture\u000d\u000a      of machinery and equipment. Understanding the properties of these\u000d\u000a      materials when they are placed under stress in routine, and indeed\u000d\u000a      abnormal, use is therefore vital in the design and manufacture of\u000d\u000a      machinery and equipment which is able to perform safely to a desired\u000d\u000a      specification. Professor Ray Ogden (Professor, George Sinclair Chair\u000d\u000a      1984-2010, Visiting Professor 2010-2012, George Sinclair Chair of\u000d\u000a      Mathematics 2012-present) has made fundamental advances in modelling the\u000d\u000a      elastic response of rubber-like materials through the development of\u000d\u000a      strain-energy functions, which have been used both in further theoretical\u000d\u000a      developments and in a very wide range of applications.\u000d\u000a    A recent key advance, building on earlier work, has provided models for inelastic\u000a        stress-softening effects in rubber-like materials using a novel\u000d\u000a      theory of pseudo-elasticity [1]. When a rubber test piece is\u000d\u000a      loaded in simple tension from its virgin state, unloaded and then\u000d\u000a      reloaded, the stress required on reloading is less than that on the\u000d\u000a      initial loading. This stress-softening phenomenon is referred to\u000d\u000a      as the Mullins effect. Prof. Ogden developed a model for this, based on\u000d\u000a      the theory of incompressible isotropic elasticity but amended by the\u000d\u000a      incorporation of a single continuous deformation dependent parameter,\u000d\u000a      interpreted as a damage parameter. This parameter controls the material\u000d\u000a      properties in the sense that it governs the material response by a\u000d\u000a      strain-energy function, which switches continuously from one form to\u000d\u000a      another when loading is switched to unloading and vice versa. For\u000d\u000a      this reason the model is referred to as pseudo-elastic. Energy dissipation\u000d\u000a      in a loading-unloading cycle is measured by a damage function which\u000d\u000a      depends on the damage parameter. A specific form of this function with two\u000d\u000a      adjustable material constants, coupled with standard forms of the\u000d\u000a      strain-energy function, is used to model the qualitative features of the\u000d\u000a      Mullins effect in both simple tension and pure shear. This work has become\u000d\u000a      a standard reference for subsequent models and it has received many\u000d\u000a      citations in the scientific literature (Google Scholar: 283, World of\u000d\u000a      Science: 185).\u000d\u000a    More recently still, and in collaboration with Professor Gerhard\u000d\u000a      Holzapfel (Graz University of Technology, Austria), Prof. Ogden has\u000d\u000a      developed models for the nonlinear elastic behaviour of soft biological\u000d\u000a      tissues [2, 3], in particular for artery wall tissue. For example, some\u000d\u000a      arteries can be modelled as thick-walled nonlinearly elastic circular\u000d\u000a      cylindrical tubes consisting of two layers (corresponding to the media\u000d\u000a      and adventitia). Each layer is treated as a fibre-reinforced\u000d\u000a      material with the fibres corresponding to the collagenous component of the\u000d\u000a      material. A specific form of the model, which requires only three material\u000d\u000a      parameters for each layer, is used to study the response of an artery\u000d\u000a      under combined axial extension, inflation and torsion. The characteristic\u000d\u000a      and very important residual stress in an artery in vivo is\u000d\u000a      accounted for by assuming that the natural (unstressed and unstrained)\u000d\u000a      configuration of the material corresponds to an open sector of a tube,\u000d\u000a      which is then closed by an initial bending to form a load-free, but\u000d\u000a      stressed, circular cylindrical configuration prior to application of the\u000d\u000a      extension, inflation and torsion. The effect of residual stress on the\u000d\u000a      stress distribution through the deformed arterial wall can then be\u000d\u000a      described. These models have been used extensively for the analysis of the\u000d\u000a      mechanical behaviour of a variety of soft tissues and they have provided\u000d\u000a      an important springboard for further modelling developments. The\u000d\u000a      scientific importance of this work is reflected in the very high level of\u000d\u000a      citations for [2] (Google Scholar: 1206, Web of Science: 752) and [3]\u000d\u000a      (Google Scholar: 502, Web of Science: 284) in the literature.\u000d\u000a    "},{"CaseStudyId":"26129","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000d\u000a    Evidence base for medical practice\u000d\u000a      The results of the WOSCOPS study represented the first demonstration of\u000d\u000a      the benefits of statin therapy in a male population with no history of\u000d\u000a      myocardial infarction, showing a 31% reduction in coronary events and a\u000d\u000a      28% reduction in deaths from coronary heart disease in patients taking\u000d\u000a      pravastatin. A further major study, PROSPER (Lancet 2002;\u000d\u000a      23;360(9346):1623-30), also conducted by the RCB, established that taking\u000d\u000a      pravastatin for an average of 3 years reduced the risk in an older\u000d\u000a      population (70-82 years) where the epidemiological association between\u000d\u000a      elevated cholesterol and increased risk is less evident. These studies\u000d\u000a      therefore clearly established the evidence for statins as a primary\u000d\u000a      prevention mechanism for reducing cardiovascular risk. The landmark nature\u000d\u000a      of the WOSCOPS results is reflected in the subsequent very large number of\u000d\u000a      medical citations.\u000d\u000a    Impact on health and patient care\u000d\u000a      Statins are now very widely used in primary prevention of coronary\u000d\u000a      disease, with more than 60 million statin prescriptions dispensed in\u000d\u000a      England alone in 2012. Although there have been other clinical trials\u000d\u000a      which have investigated the use of statins in different patient groups\u000d\u000a      with different risk categories, WOSCOPS remains the definitive study which\u000d\u000a      first established the benefits, and safety, of statins in preventing\u000d\u000a      first-time heart attacks for those with raised cholesterol. Indeed, there\u000d\u000a      would be ethical issues in repeating a study of this type now that the\u000d\u000a      benefits have been identified. WOSCOPS is therefore a primary source in\u000d\u000a      the supporting evidence for national and international guidelines on the\u000d\u000a      management of patients. Examples where WOSCOPS is cited include guidelines\u000d\u000a      for risk estimation and the prevention of cardiovascular disease [a].\u000d\u000a      WOSCOPS is also extensively referenced in clinical aids such as GPnotebook\u000d\u000a      (www.gpnotebook.co.uk), which\u000d\u000a      assist doctors in decisions on suitable medications and management of\u000d\u000a      patients.\u000d\u000a    At the international level, the American Association of Clinical\u000d\u000a      Endocrinologists issued Guidelines for management of dyslipidaemia and\u000d\u000a        prevention of atherosclerosis (2012) which recommends statins as the\u000d\u000a      cholesterol lowering drug of choice, with the in-depth analysis of the\u000d\u000a      treatment recommendations citing WOSCOPS as one of the major randomised\u000d\u000a      controlled trials supporting the use of statins in primary prevention [b].\u000d\u000a      Similarly, the European Society of Cardiology\/European Atherosclerosis\u000d\u000a      Society issued Guidelines for the management of dyslipaemias\u000d\u000a      (2011) which cites WOSCOPS to support the use of statins in patients who\u000d\u000a      have been stratified according to cardiovascular risk and low-density\u000d\u000a      lipoprotein levels [c]. The study has therefore had significant impact on\u000d\u000a      patient care worldwide.\u000d\u000a    Identification of long-term health and economic benefits\u000d\u000a      A follow-up study of WOSCOPS patients 10 years after the end of the trial\u000d\u000a      showed continuing effects in risk reduction and no long-term safety\u000d\u000a      concerns. This study, conducted by the RCB, integrating record linkage,\u000d\u000a      statistical methods and economic analysis, shows that treatment of 1000\u000d\u000a      patients for five years in middle age saves over 1800 days in hospital\u000d\u000a      over the following 15 years, with a consequent saving to the NHS of &#163;710k\u000d\u000a      [d]. As the use of statins has increased internationally, the replication\u000d\u000a      of these effects over large populations clearly represents savings of\u000d\u000a      enormous size, both in financial terms and in the positive improvement in\u000d\u000a      patient health. The use of record linkage has therefore had the double\u000d\u000a      impact of providing a highly effective and cost-saving means of pursuing\u000d\u000a      medical follow-up studies in general, as well as identifying the long-term\u000d\u000a      health and economic impact associated with the original WOSCOPS study in\u000d\u000a      particular.\u000d\u000a    Public understanding and debate\u000d\u000a      The recent, record linkage based, follow-up study of WOSCOPS patients [d]\u000d\u000a      has attracted significant press coverage [e], reflecting the widespread\u000d\u000a      public interest in the associated health and economic issues. The study\u000d\u000a      has therefore contributed significantly to public understanding and public\u000d\u000a      debate of the underlying issues. A recent example was the reference to\u000d\u000a      WOSCOPS long-term follow-up as an excellent illustration of the potential\u000d\u000a      of record linkage in medical research at the national launch of the new\u000d\u000a      Health e-Research Centres (HeRCs).\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Cardiovascular disease is a major worldwide health issue and cholesterol\u000d\u000a      has long been recognised as an important risk factor. The Robertson Centre\u000d\u000a      for Biostatistics (RCB), led by Prof. Ian Ford, has played a central role\u000d\u000a      in establishing for the first time the benefits of statins in preventing\u000d\u000a      first-time heart attacks in men, with subsequent major influence on\u000d\u000a      medical practice and guidelines for patient care. Innovative record\u000d\u000a      linkage techniques used by the RCB have identified the long-term benefits\u000d\u000a      of treatment, confirmed safety, and quantified the economic benefits.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    University of Glasgow\u000d\u000a    ","Institutions":[{"AlternativeName":"Glasgow (University of)","InstitutionName":"University of Glasgow","PeerGroup":"A","Region":"Scotland","UKPRN":10007794}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1) Shepherd J, Cobbe\u000a        SM, Ford\u000a        I, et al. (1995). Prevention of coronary heart disease with\u000d\u000a      pravastatin in men with hypercholesterolemia. New Engl J Med. 333, 1301-7.\u000d\u000a      doi:10.1056\/NEJM199511163332001.\u000d\u000a      Citations: 5128 (InCites), 6098 (Scopus) *\u000d\u000a    \u000a\u000a2) WOSCOPS Study Group (1995). Computerised record linkage: compared with\u000d\u000a      traditional follow-up methods in clinical trials and illustrated in a\u000d\u000a      prospective epidemiological study. Journal of Clinical Epidemiology 48,\u000d\u000a      1441-1452. doi:10.1016\/0895-4356(95)00530-7\u000d\u000a    \u000a\u000a3) McLeod, M. (1995). Record linkage applied to medical and cohort\u000d\u000a        studies. PhD thesis, University of Glasgow. (available from HEI)\u000d\u000a    \u000a\u000a4) Ford, I., Norrie, J. &amp; Ahmadi, S. (1995). Model inconsistency,\u000d\u000a      illustrated by the Cox proportional hazards model. Statistics in Medicine\u000d\u000a      14, 735-746. doi:10.1002\/sim.4780140804\u000d\u000a    \u000a\u000a5) Ford, I. and Norrie, J. (2002). The\u000a        role of covariates in estimating treatment effects and risk in long-term\u000d\u000a        clinical trials. Statistics in Medicine 21 (19), 2899-2908. doi:10.1002\/sim.1294\u000d\u000a      *\u000d\u000a    \u000a\u000a6) Ford\u000d\u000a        I, Murray\u000d\u000a        H, Packard\u000d\u000a        CJ, Shepherd\u000d\u000a        J, Macfarlane\u000d\u000a        PW, Cobbe\u000d\u000a        SM: WOSCOPS\u000d\u000a        Study Group (2007). Long-term follow-up of the West of Scotland\u000d\u000a      Coronary Prevention Study. N Engl J Med., 357(15):1477-86. doi:10.1056\/NEJMoa065994\u000d\u000a      *\u000d\u000a    \u000a* best indicators of research quality\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    Evidencing impact on health and patient care\u000d\u000a      [a] Risk estimation and the prevention of cardiovascular disease, Scot.\u000d\u000a      Inter. Guidelines Network, Guideline 97, 2007. ISBN 1899893997. http:\/\/www.sign.ac.uk\/pdf\/sign97.pdf\u000d\u000a    [b] AACE guideline, 2012 https:\/\/www.aace.com\/files\/lipid-guidelines.pdf\u000d\u000a      ; WOSCOPS, PROSPER: p40 (Tables 18 and 20, major trials in primary\u000d\u000a      prevention), pp. 28 and 29.\u000d\u000a    [c] ECS guideline, 2011 http:\/\/eurheartj.oxfordjournals.org\/content\/32\/14\/1769.full.pdf;\u000d\u000a      WOSCOPS (NEJM 1995, ref 19) and PROSPER (ref 26), Table 3, pg 1780.\u000d\u000a    Evidencing identification of long-term health and economic benefits\u000d\u000a      [d] McConnachie\u000d\u000a      A, Walker\u000d\u000a      A, Robertson\u000d\u000a      M, Marchbank\u000d\u000a      L, Peacock\u000d\u000a      J,\u000d\u000a        Packard CJ, Cobbe\u000d\u000a      SM, Ford\u000d\u000a      I. (2013). Long-term impact on healthcare resource utilization of statin\u000d\u000a      treatment, and cost effectiveness in the primary prevention of\u000d\u000a      cardiovascular disease: a record linkage study. Eur Heart J ; doi:10.1093\/eurheartj\/eht232\u000d\u000a    Evidencing influencing public understanding and debate\u000d\u000a      [e] The\u000a        Herald (9 July 2013): Giving statins to middle-age healthy men saves\u000d\u000a        money\u000d\u000a      The\u000a        Scotsman (10 July 2013): Statins for heart problems could save NHS\u000d\u000a        millions\u000d\u000a      Daily\u000a        Express (10 July 2013): Give statins to healthy Scots now\u000d\u000a      Reuters\u000a        (19 July 2013): Statins for healthy men may save money: study\u000d\u000a    ","Title":"\u000d\u000a    Quantification of the benefits of statins in preventing cardiovascular\u000d\u000a        disease\u000d\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The Robertson Centre for Biostatistics (RCB) in the University of Glasgow\u000d\u000a      was created in 1988 (as the Databases Unit) in order to structure existing\u000d\u000a      research in the methodology and practice of biostatistics and to promote\u000d\u000a      its development. Led by Ian Ford, Professor of Biostatistics, the RCB\u000d\u000a      began in the Department of Statistics (now incorporated into the School of\u000d\u000a      Mathematics &amp; Statistics). In 2009, the RCB was recognised for its\u000d\u000a      fundamental role in interdisciplinary medical research and moved to the\u000d\u000a      Faculty of Medicine.\u000d\u000a    The RCB has been involved in many large and high profile studies but a\u000d\u000a      particularly important example is the West of Scotland Coronary Prevention\u000d\u000a      Study (WOSCOPS). Cardiovascular disease is a major cause of death\u000d\u000a      worldwide and high cholesterol has long been implicated as a risk factor.\u000d\u000a      However, in the late 1980's the effect of reducing cholesterol in patients\u000d\u000a      who have not yet suffered any cardiac event, was unclear; indeed many\u000d\u000a      thought that this could lead to increased risk of cancer and death. The\u000d\u000a      WOSCOPS study sought to answer this question in individuals with raised\u000d\u000a      cholesterol levels. The central role of the RCB and its statistical\u000d\u000a      research are described below.\u000d\u000a    The conduct of the WOSCOPS study\u000d\u000a      As in any large clinical trial, statistical methods and expertise lay at\u000d\u000a      the heart of the WOSCOPS study. This was essential in design, to meet the\u000d\u000a      ethical demands of patient involvement and ensure the highest quality of\u000d\u000a      statistical evidence. It was critical in the collection, collation and\u000d\u000a      management of data to ensure information of the highest quality. It was\u000d\u000a      central to the analysis and interpretation of the results, whose potential\u000d\u000a      implications in terms of future patient treatment guidelines were\u000d\u000a      enormous. The RCB was responsible for all of these aspects of the trial\u000d\u000a      and for all subsequent publications. The complexity of the work involved\u000d\u000a      in supporting trials is reflected in the size of the RCB, with\u000d\u000a      approximately 50 staff, including 12 statisticians. The critical role of\u000d\u000a      statistical methodology is indicated by the position of Ian Ford as one of\u000d\u000a      the principal grant holders for the trials and as one of the principal\u000d\u000a      authors in the subsequent publications in the highest ranking medical\u000d\u000a      journals. Ford was the lead or corresponding author of all of the\u000d\u000a      publications listed below. The adaptation of statistical methods to the\u000d\u000a      complexities and scale of the WOSCOPS study was a fundamental and critical\u000d\u000a      contribution to this interdisciplinary research. The results [2] provided\u000d\u000a      the first definitive evidence of the link between cholesterol reduction\u000d\u000a      and reduced risk of first-time heart attack and, importantly, established\u000d\u000a      the safety of this approach.\u000d\u000a    Methodological issues in the construction and interpretation of\u000d\u000a        statistical models\u000d\u000a      The complexities of large clinical trials need to be supported by\u000d\u000a      methodological innovations. For example, the issue of whether statistical\u000d\u000a      models to assess treatment effects should be adjusted for relevant\u000d\u000a      covariates has been a controversial one in trial design and\u000d\u000a      interpretation. Methodological work by Ford et al. [4] identified,\u000d\u000a      in the context of Cox regression models, that adjusted and unadjusted\u000d\u000a      models may not both be valid and that, even when they are, the model\u000d\u000a      parameters may have different interpretations. This attracted significant\u000d\u000a      attention, as a result of which a second paper [5] was invited by Statistics\u000a        in Medicine. This indicated clearly, in a general setting of\u000d\u000a      non-linear models, that when covariate adjustment takes place then change\u000d\u000a      in the magnitude of treatment effects should be expected. The implications\u000d\u000a      from a regulatory perspective that there should be a prespecified decision\u000d\u000a      on whether inference should be conditional or unconditional with respect\u000d\u000a      to covariates was clearly identified and the issues were specifically\u000d\u000a      linked to the results of the WOSCOPS study.\u000d\u000a    Establishing record linkage as a valid and powerful means of follow-up\u000d\u000a      Clinical trials on the scale of WOSCOPS (involving an initial screening of\u000d\u000a      105,000 individuals) are large and complex operations which can usually be\u000d\u000a      sustained for only a small number of years. The health effects of\u000d\u000a      treatment may operate over much longer timescales and identifying the\u000d\u000a      presence and nature of long-term effects is of major medical importance,\u000d\u000a      but it is a major challenge to do this after the completion of a trial. In\u000d\u000a      addressing this issue, the RCB has been a pioneer in the validation and\u000d\u000a      use of computerised record linkage to identify the subsequent health\u000d\u000a      history of trial subjects. Scotland is exceptionally well placed to\u000d\u000a      exploit this approach as health information for patients across the whole\u000d\u000a      country is collated centrally. Record linkage involves the matching of a\u000d\u000a      unique identifier, or in the case of WOSCOPS patient information (such as\u000d\u000a      name, date of birth, address), across different records, through a\u000d\u000a      probabilistically constructed index. Ian Ford was corresponding author on\u000d\u000a      one of the first validations of this approach [3] and supervisor of a\u000d\u000a      University of Glasgow Statistics PhD thesis [4] which explored the\u000d\u000a      effectiveness of the method in great detail. This established the\u000d\u000a      suitability of health record linkage in Scotland for use in clinical\u000d\u000a      studies and, in particular, it provided a valid mechanism for tracking the\u000d\u000a      longer term health effects of cholesterol reduction in WOSCOPS patients\u000d\u000a      [6].\u000d\u000a    "},{"CaseStudyId":"26674","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    BAE Systems\u000a    The frequency assignment system developed in the research described above\u000a      was presented to leading\u000a      figures in UK military communications research at a seminar in February\u000a      2000. It was clear that the\u000a      approach was a considerable advance on existing military frequency\u000a      assignment systems. The system\u000a      was subsequently selected to form the basis of the frequency assignment\u000a      system in project Bowman\u000a      and its incorporation was eased since Dr. Taplin was a BAE Systems\u000a      employee and returned to work at\u000a      BAE Systems at the end of the project. In application, the system was\u000a      found to be remarkably error free.\u000a      The experience of using the system in Bowman led to its use in a further\u000a      military communication\u000a      system, known as Falcon. BAE Systems developed the Bowman management\u000a      system and leveraged\u000a      this experience to design Falcon's management software.\u000a    Application in Bowman\u000a    The Bowman communication system is a combat net radio system used for\u000a      tactical communications at\u000a      platoon level and upwards by the British Armed Forces. It was phased into\u000a      use starting in 2004 and fully\u000a      rolled out in 2008, replacing the aging Clansman analogue radio system.\u000a      Bowman delivered a step\u000a      change in capability over the Clansman family of radios through its\u000a      security, data capability, reliability\u000a      and resilience against electronic warfare attack. It is a tactical\u000a      communications system integrating digital\u000a      voice and data technology to provide secure radio, telephone, intercom and\u000a      tactical internet services in\u000a      a modular and fully integrated system. The key aspect of Smith's\u000a      algorithms in Bowman is that a large\u000a      number of operators can simultaneously send and receive communications\u000a      (i.e. the capability and\u000a      reliability of the system has been substantially improved). Over the\u000a      assessment period, Bowman has\u000a      been operated by soldiers from every part of the British Army as well as\u000a      specialists from the Royal\u000a      Corps of Signals. Furthermore, the Bowman communication system is\u000a      currently being rolled out across\u000a      elements of the Royal Netherlands Navy.\u000a    Application in Falcon\u000a    The Falcon communication system came into service in 2010, replacing the\u000a      near 30-year old Ptarmigan\u000a      system, with a new generation tactical communications system. It delivers\u000a      secure voice and data over\u000a      an internet protocol system. Falcon is currently being fielded by the\u000a      Royal Signals and the Royal Air\u000a      Force. Falcon replaces aging asynchronous transfer mode equipment used in\u000a      Ptarmigan with a scalable\u000a      application that can be configured rapidly to meet the needs of an\u000a      expeditionary force. It connects with\u000a      other digital communications technologies in the British military and\u000a      coalition allies. Falcon also uses\u000a      manpower more efficiently than Ptarmigan; the highly automated system\u000a      requires 50 to 75 percent\u000a      fewer personnel to operate. Smith's algorithms have made valuable\u000a      contributions in maximizing data\u000a      transfer and reducing interference between users.\u000a    Financial impact\u000a    Those close to the project asked to estimate the financial benefit see\u000a      such requests as intrusions into\u000a      commercial confidentiality. However, all parties recognize that the work\u000a      was essential: Bowman would\u000a      have taken a different path if the spectral use had not been improved.\u000a      Hence the project is viewed as\u000a      highly worthwhile and certainly provided a benefit to the economy which\u000a      should be seen in the light of\u000a      the total costs of the two projects. The Bowman family of digital radios,\u000a      and the associated Combat\u000a      Infrastructure Platform project, constituted a &#163;2.4 billion project while\u000a      Falcon was over &#163;200 million.\u000a      Smith's frequency assignment engine is critical to the operation of\u000a      both Bowman and Falcon. The\u000a      effectiveness of a frequency assignment system that minimizes\u000a      interference, while at the same time\u000a      allowing efficient use of the radio spectrum has huge commercial benefits;\u000a      this is particularly important\u000a      in the light of continuing pressure to release military spectrum for civil\u000a      use. In civil use, the radio\u000a      spectrum now has an enormous commercial value (e.g. the auction of\u000a      spectrum for third generation\u000a      mobile telephones raised over &#163;22 billion for the UK government).\u000a    Reach and significance\u000a    The Bowman communication system is expected to continue in service until\u000a      approximately 2026 and is\u000a      currently fitted to over 15 000 military vehicles, the entire Royal Navy\u000a      fleet and carried by dismounted\u000a      soldiers resulting in close to 50 000 radio sets. Additionally, 75 000\u000a      personnel required training in their\u000a      use. General Dynamics, the radio set manufacturer, has a global\u000a      workforce of almost 100 000 people.\u000a      Smith's work has an extensive reach that will continue for at least the\u000a      next 10-15 years. While it is not\u000a      easy to determine the significance of the work to the entire Bowman and\u000a      Falcon projects, all parties see\u000a      Smith's algorithms as an essential component and the entire project\u000a      would have taken a very different\u000a      route in their absence.\u000a    ","ImpactSummary":"\u000a    Between 1994 and 2000 Prof. Derek Smith developed algorithms that\u000a      in the last 10 years have been\u000a      incorporated into major communication systems used throughout the British\u000a      Armed Forces. Previous\u000a      systems were unable to reliably deal with the huge volume of data provided\u000a      by modern intelligence,\u000a      surveillance and reconnaissance assets, particularly suffering from\u000a      interference between users. Since\u000a      2004, alternative systems (Bowman and Falcon) overcoming these\u000a      deficiencies have been rolled out.\u000a      Approximately 50000 radio sets using this technology have been\u000a      manufactured and fitted to 15000\u000a      military vehicles, including the entire Royal Navy fleet and 75000 people\u000a      required training in its use.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of South Wales\u000a    ","Institutions":[{"AlternativeName":"South Wales (University of)","InstitutionName":"University of South Wales","PeerGroup":"D","Region":"Wales","UKPRN":10007793}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Publications\u000a    These papers motivated Smith's involvement with BAE Systems and QuinetiQ\u000a      or arose from joint\u000a      collaborative activity developing the algorithms that were implemented in\u000a      Bowman &amp; Falcon systems:\u000a    \u000a&#8226; S. Hurley, D.H. Smith and S.U. Thiel, F. A. Soft: A System for Discrete\u000a      Channel Frequency\u000a      Assignment, Radio Science, 32(5), (1997), pp. 1921-1939. doi:\u000a      10.1029\/97RS01866\u000a    \u000a\u000a&#8226; D.H. Smith, R.K. Taplin and S. Hurley, Frequency Assignment with\u000a      Complex Co-Site\u000a      Constraints, IEEE Transaction on Electromagnetic Compatibility, 43(2),\u000a      (May 2001), pp. 210-218.\u000a      (ISSN 0018-9375). doi: 10.1109\/15.925542\u000a    \u000a\u000a&#8226; R. Bradbeer, S. Hurley, D.H. Smith and G. Wyman, Improving Efficiency\u000a      in Frequency\u000a      Assignment Engines, IEEE MILCOM 2001: Communications for Network Centric\u000a      Operations,\u000a      CD-ROM Proceedings, October 2001. (CD-ROM Proceedings, ISBN 0-7803-7227-1,\u000a      Softbound\u000a      Proceedings ISBN 0-7803-7225-5). doi: 10.1109\/MILCOM.2001.985758\u000a    \u000aRelated Grants\u000a    &#8226; Prior to 1999: (prior to BAE Systems\/QinetiQ involvement)\u000a    &#8226; UK Radiocommunications Agency\u000a    &#8226; ComOpt AB (a Swedish software company)\u000a    &#8226; EPSRC\u000a    &#8226; 1999-2000: (BAE Systems\/QinetiQ collaboration)\u000a    &#8226; Postdoctoral Research Assistant funded by BAE Systems\/QinetiQ (under\u000a      Ministry of\u000a      Defence Pathfinder programme), British Aerospace Defence Systems (Land and\u000a      Sea\u000a      Systems) (purchase order M084353). Lead investigator was Smith\u000a      with Professor Hurley\u000a      (University of Cardiff), Mr. G. Wyman for BAE Systems and Mr. R. Bradbeer\u000a      of QinetiQ.\u000a      This grant initiated the development of frequency assignment algorithms\u000a      that have been\u000a      used in Bowman and Falcon communication systems.\u000a    &#8226; 2000-current: (BAE Systems)\u000a    &#8226; 2002-2003 &#8212; Postdoctoral Research Assistant seconded for Frequency\u000a      Assignment\u000a      Methods for Frequency Hoppers (BAE Systems purchase order 47002477). This\u000a      work\u000a      contributed to frequency hopping aspects of Project Bowman.\u000a    &#8226; 2003-2004 &#8212; Postdoctoral Research Assistant seconded for CDMA (BAE\u000a      Systems\u000a      purchase order 46001008). This work contributed to frequency hopping\u000a      aspects of\u000a      Project Bowman.\u000a    &#8226; 2004-2007 &#8212; EPSRC Case studentship. (BAE Systems purchase order\u000a      47003909).\u000a      All three of these projects were supervised by Smith and Perkins.\u000a    ","ResearchSubjectAreas":[{"Level1":"10","Level2":"5","Subject":"Communications Technologies"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"8","Level2":"6","Subject":"Information Systems"}],"Sources":"\u000a    \u000a      Background on Bowman communication system including its use by British\u000a        Armed forces and\u000a        Royal Netherlands Navy:\u000a        http:\/\/www.generaldynamics.uk.com\/solutions-and-capabilities\/bowman-command-battlespace-management\u000a\u000a      Testimonials on how Bowman fared in the combat zone from end-users:\u000a        http:\/\/www.generaldynamics.uk.com\/about-gduk\/customer-success#bowman-quotes-and-video\u000a\u000a      Background on Falcon communication system, including its improvement\u000a        over previously-used\u000a        communication systems:\u000a        http:\/\/www.afcea.org\/signal\/articles\/templates\/SIGNAL_Article_Template.asp?articleid=1031&amp;zoneid=52\u000a\u000a      Further details on Falcon communication system and how it improves on\u000a        previous technologies:\u000a        http:\/\/www.baesystems.com\/product\/BAES_019993\/falcon\u000a\u000a    \u000a    There are five corroborative sources who are able to assess the impact of\u000a      Smith's work on Bowman and\u000a      Falcon comprising past and present employees of BAE Systems and QinetiQ.\u000a    ","Title":"\u000a    Developing frequency assignment techniques for British military\u000a      communication systems\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Overview of problem\u000a    Modern communication systems involve the transmission and reception of\u000a      huge volumes of data. While\u000a      it is always important for data to be uncorrupted between the sender and\u000a      the receiver, it is particularly\u000a      problematic for military communication systems where the infrastructure\u000a      has to be rapidly deployed and\u000a      information sent on the network can be a life-or-death matter. Indeed,\u000a      where there are a large number of\u000a      users in close proximity to each other, for example in a combat situation,\u000a      there is much competition for\u000a      broadcast frequencies and data can undergo interference that renders the\u000a      communication useless.\u000a      Thus intelligent algorithms are required that efficiently utilize the\u000a      range of broadcast frequencies\u000a      available while simultaneously minimizing interference between users. Smith,\u000a      along with others\u000a      including Perkins and industrial partners, developed algorithms\u000a      between 1994 and 2000 that have been\u000a      incorporated into new communication systems utilised throughout the\u000a      British Army, Navy and Air force\u000a      from 2004 onwards.\u000a    General research\u000a    Smith has an extensive history of mathematical research in radio\u000a      frequency assignment methods. In\u000a      particular, in the period 1994-2010 alongside collaborators including\u000a      Prof. S. Hurley (University of\u000a      Cardiff), Perkins and with a number of postdoctoral researchers\u000a      and postgraduate research students,\u000a      considerable attention was focussed on developing algorithms for radio\u000a      frequency assignment based on\u000a      graph theoretic methods and meta-heuristic algorithms. The algorithms\u000a      themselves were underpinned\u000a      by evaluation work in graph theory and mathematical programming. Indeed,\u000a      in excess of 30 papers\u000a      were published on this work alone during the sixteen year period. While\u000a      many of the algorithms\u000a      developed gave leading results on standard benchmark problems, crucially\u000a      they were developed in a\u000a      way that made them suitable for immediate implementation.\u000a    Specific research\u000a    In the late 1990s, the Bowman net combat radio system was being developed\u000a      for the British Army by\u000a      BAE Systems and QinetiQ with funding from the Ministry of Defence\u000a      Pathfinder programme and\u000a      researchers at these companies realised that Smith's work on radio\u000a      frequency assignment would prove\u000a      invaluable to the project. Hence, between February 1999 and March 2000 a\u000a      postdoctoral research\u000a      assistant (Dr Richard Taplin) was employed by BAE Systems and seconded to\u000a      the University of\u000a      Glamorgan (now USW) to develop the algorithms in a way that was suitable\u000a      for Bowman. The initial\u000a      frequency assignment algorithms were improved by the addition of the\u000a      ability to handle issues involving\u000a      radios that were cosited, including handling interference caused by\u000a      intermodulation products, spurious\u000a      emissions and spurious responses. These improved algorithms were both\u000a      published in research papers\u000a      and integrated into the Bowman communication system and the later Falcon\u000a      communication system,\u000a      both as currently used by the British Armed Forces. Further research\u000a      students supervised by Smith and\u000a      Perkins and with funding from BAE Systems have further refined\u000a      aspects of this work in recent years\u000a      and collaborative activity is planned to continue with General Dynamics,\u000a      the manufacturer of the\u000a      Bowman and Falcon radio systems.\u000a    "},{"CaseStudyId":"27041","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    HPC Wales Background\u000a      High Performance Computing Wales (HPC Wales), launched in July 2010, is a\u000a      &#163;44 million five-year project providing a super-computing facility at a\u000a      scale unprecedented in Europe due to its unique \"hub- and-spoke\" design.\u000a      Briefly, a number of \"hubs\" are distributed throughout the nation and are\u000a      joined by high-speed connections. Financial support was provided by ERDF\u000a      and ESF European funds, the Department for Business, Innovation and\u000a      Skills, collaborating academic institutions (including USW), the Welsh\u000a      Assembly Government and the private sector.\u000a    In early 2011, Fujitsu won the procurement bid to provide infrastructure,\u000a      support and services. The nature of the projects, the application areas\u000a      and the necessary equipment was to be shaped by its early adopters since\u000a      they would be providing real-world problems that would test and refine the\u000a      system. Associated with the procurement process, Fujitsu re-invested\u000a      approximately &#163;1.5 million to support projects led by early adopters.\u000a    Wiltshire identified a number of early adopters, including\u000a      Tatarinova, who provided sample problems that shaped current application\u000a      areas, trained HPC Wales staff and introduced further collaborators from\u000a      commercial organisations, thus helping to expand employment and develop\u000a      technical skills. Indeed, HPC Wales employs approximately 30 managerial,\u000a      technical and support staff and it is planned to generate a further 400\u000a      jobs requiring high level skills and technical training.\u000a    Impact on HPC Wales\u000a      Tatarinova's \"test\" project, started in 2011, involved the analysis of\u000a      large genome sequences using a combination of standard software coupled\u000a      with the implementation of novel algorithms. Funded by a &#163;56K award from\u000a      Fujitsu, HPC Wales were able to understand the type of problems they would\u000a      be supporting and how to appropriately manage their hardware and software.\u000a      This work was so influential that in September 2012 HPC Wales rated it as\u000a      one of their top 5 projects.\u000a    The success of this project resulted in two further Tatarinova-led\u000a      projects supported by HPC Wales, each worth &#163;15k, and running in\u000a      collaboration with non-academic partners including the National Botanical\u000a      Gardens of Wales (the Barcode Wales project) and Morvus Technology\u000a      Ltd. investigating a range of problems including analysing strains of Escherichia\u000a        coli.\u000a    Tatarinova and Wiltshire actively promoted HPC Wales by\u000a      presenting at numerous promotional events and gave media interviews.\u000a      Further projects and collaborators, both within and outside of academia,\u000a      arose as a result of these promotional activities (e.g. OSTC Ltd. have two\u000a      projects utilising HPC Wales working with Roach), thus helping to\u000a      secure the future of HPC Wales and its associated employment.\u000a    As of October 2013, and in only three years from its inception, in excess\u000a      of 50 commercial businesses and industrial partners are engaged in over 60\u000a      collaborative projects using HPC Wales; 40% of these involving the life\u000a      sciences and approximately 5% on problems in genomics and bioinformatics.\u000a      A key feature of HPC Wales is its accessibility to businesses, such as\u000a      those brought on board by Wiltshire and Tatarinova, and HPC Wales'\u000a      success in this area was acknowledged through being a runner up in Open\u000a      Data category of the national Next Generation Digital Challenge Awards in\u000a      October 2013.\u000a    Impact on supercomputing and bioinformatics\u000a      As part of Tatarinova's \"test\" project, certain bioinformatics software (cisexpress)\u000a      required redesigning to work on the highly-parallelised architecture of\u000a      HPC Wales. Fujitsu provided the equivalent of approximately &#163;90k in\u000a      consultancy fees to support the redesign of key algorithms that not only\u000a      improved essential software for the analysis of large genome sequences,\u000a      but also resulted in training HPC Wales staff.\u000a    Since early 2013, this revised software, running on HPC Wales, has been\u000a      made available to the world-wide scientific community (http:\/\/glacombio01.comp.glam.ac.uk\/cisExpress\/new\/home.php)\u000a      and as of October 2013, approximately 50% of the total simulations\u000a      performed (numbering several hundred) have been conducted by scientists\u000a      external to USW. This revised software has therefore laid foundations for\u000a      further bioinformatics projects around the world with HPC Wales and USW\u000a      researchers at their core.\u000a    Creating impact legacy\u000a      Tatarinova's work with HPC Wales has been designed to create a lasting\u000a      impact. Barcode Wales has constructed a DNA database for the\u000a      native flowering plants and conifers for Wales and represents the most\u000a      comprehensive sampling of any national flora to date. Thus, future\u000a      environmental surveys need not rely on a taxonomic expert to determine\u000a      flora and can instead use sampled DNA and associated software developed\u000a      from this project. In January 2013, Tatarinova, alongside Prof. Denis\u000a      Murphy (USW), created a spin-out company called myregulome.com ltd\u000a      which is currently marketing software for genomic analysis developed in\u000a      collaboration with Fujitsu and HPC Wales.\u000a    ","ImpactSummary":"\u000a    Prof. Ron Wiltshire and Dr. Tatiana Tatarinova helped to develop\u000a      High Performance Computing Wales (HPC Wales) by providing \"test\" problems\u000a      from the life sciences (bioinformatics), defining key software packages,\u000a      and brought on-board collaborative partners to instigate project areas.\u000a      Established in 2010 with &#163;44 million funding, HPC Wales is Europe's first\u000a      nation-wide \"hub-and-spoke\" super-computing facility. In the three years\u000a      following its conception, HPC Wales has approximately 50 collaborative\u000a      organisations supporting over 60 major projects; 40% in life sciences and\u000a      5% in bioinformatics through Wiltshire's and Tatarinova's\u000a      involvement. Further impact arose through the redesign of software and\u000a      up-skilling of HPC Wales staff.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of South Wales\u000a    ","Institutions":[{"AlternativeName":"South Wales (University of)","InstitutionName":"University of South Wales","PeerGroup":"D","Region":"Wales","UKPRN":10007793}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Research Publications (these publications defined projects and\u000a      software used in HPC Wales; the de Vere et al. paper arose from\u000a      collaborative work that included the utilization of HPC Wales):\u000a      \u000a&#8226; Tatarinova, T.V.; Alexandrov, N.; Bouck, J.; Feldmann, K. (2010). GC3\u000a      biology in corn, rice, sorghum and other grasses. BMC Genomics, 11:\u000a      308. doi: 10.1186\/1471-2164-11-308\u000a      \u000a\u000a&#8226; Sablok, G.; Chandra Nayak, K.; Vazquez, F.; Tararinova, T.V. (2011).\u000a      Synonymous codon usage, GC3, and evolutionary patterns across\u000a      plastomes of three pooid model species: emerging grass genome models for\u000a      monocots. Molecular biotechnology, 49(2): 116-128. doi:\u000a      10.1007\/s12033-011-9383-9.\u000a      \u000a\u000a&#8226; de Vere, N.; Rich, T.C.G.; Ford, C.R.; Trinder, S.A.; Long, C.; Moore,\u000a      C.W.; Satterthwaite, D.; Davies, H.; Allainguillaume, J.; Ronca, S.;\u000a      Tatarinova, T.; Garbett, H.; Walker, K.; Wilkinson, M.J. (2012). DNA\u000a      Barcoding the Native Flowering Plants and Conifers of Wales, PlosOne,\u000a      7(6): e37945. doi:10.1371\/journal.pone.0037945\u000a    \u000aResearch Grants\/Funding (selection of research studentships\u000a      awarded to Tatarinova that are related to impact on HPC Wales):\u000a      &#8226; &#163;56K from Fujitsu &amp; HPC Wales for 1 research studentship (No grant\u000a      number, can provide collaborative agreement on request, but see http:\/\/www.hpcwales.co.uk\/farzana-studentship)\u000a      &#8226; 2 x KESS Research Students Studentships with Morvus Technology Ltd and\u000a      National Botanical Gardens Wales, both in collaboration with HPC Wales\u000a      (Projects \"Development of theoretical cell models for the action of novel\u000a      antiapoptotic proteins\" and \"Development of bioinformatics tools and\u000a      procedures for computational support\", respectively)\u000a      &#8226; &#163;2.5k from Welsh Crucible for \"My GENE Code PEiMB: Public Engagement in\u000a      Molecular Biology\". This Award required selection by the University and\u000a      attendance at numerous workshops throughout Wales engaging with\u000a      multi-disciplinary researchers.\u000a    ","ResearchSubjectAreas":[{"Level1":"6","Level2":"4","Subject":"Genetics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    Shaping HPC Wales:\u000a    \u000a      \u000ahttp:\/\/www.hpcwales.co.uk\/sites\/default\/files\/hpcwales\/HPCWales_NewsByte0112.pdf\u000a        Details of Fujitsu providing facilities and support for HPC Wales;\u000a        Tatarinova defining HPC Wales software.\u000a      \u000ahttp:\/\/www.hpcwales.co.uk\/who-we-are\u000a        Current list of staff employed directly by HPC Wales in management,\u000a        support and technical roles.\u000a      \u000ahttp:\/\/www.bbc.co.uk\/news\/10587005\u000a        Job creation estimates and funding sources for HPC Wales.\u000a      \u000ahttp:\/\/www.fujitsu.com\/downloads\/TC\/sc12\/booth\/hpc-wales-fujitsubp-sc12.pdf\u000a        Priority areas of HPC Wales, including a list of customers and customer\u000a        feedback, demonstrating Tatarinova's influence on shaping HPC Wales.\u000a        Statement that Tatarinova's \"test\" project is one of HPC Wales' top 5\u000a        projects.\u000a      \u000a\u000a          http:\/\/www.welshcrucible.org.uk\/2011-programme\/participants\/welsh-crucible-participant-\u000a          profiles\/western-mail-profiles-2011-series\/western-mail-profiles-dr-tatiana-tatarinova-university-\u000a          of-glamorgan\/\u000a        How HPC Wales' projects have been shaped by Tatarinova.\u000a      \u000ahttp:\/\/www.hpcwales.co.uk\/sites\/default\/files\/hpcwales\/20120905_software_list.pdf\u000a        Details of software installed on HPC Wales including significant\u000a        genomics-related programs. \u000a\u0009\u0009\u000a      Promoting HPC Wales and introducing business partners:\u000a        \u000a      \u000ahttp:\/\/www.bbc.co.uk\/programmes\/b01pw57l\u000a        Radio interview on BBC Radio Wales on Science Cafe, broadcast 15th\u000a        January, 2013, with Tatarinova promoting HPC Wales and explaining how\u000a        she has helped define its application areas.\u000a      \u000ahttp:\/\/www.hpcwales.co.uk\/fighting-cancer\u000a        Video link and case study &#8212; reference to algorithm being redesigned in\u000a        collaboration with Fujitsu, HPC Wales and other software developers.\u000a    \u000a    Impact on bioinformatics:\u000a    \u000a      International Review of Mathematical Sciences in the United Kingdom http:\/\/www.epsrc.ac.uk\/SiteCollectionDocuments\/other\/MathsIR2010EvidenceDocumentsParts1-\u000a          3.pdf (p.173)\u000a        \"The analysis of properties of genes in various plant species by\u000a        Tatarinova (Glamorgan) and colleagues has provided major insight for\u000a        tissue specific and stress response in plants and resulted in patents in\u000a        areas of plant biotechnology\"\u000a    \u000a    Corroborating sources:\u000a    \u000a      HPC Wales\u000a        Can corroborate Wiltshire's and Tatarinova's involvement in\u000a        defining software and projects for HPC Wales.\u000a    \u000a    ","Title":"\u000a    Using genomics to shape high performance computing\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The problem\u000a      The recent construction of numerous genomic databases has resulted in\u000a      lengthy lists of millions or billions of chemical units (proteins) that\u000a      comprise a single organism's genome. Buried in these lists are the\u000a      organism's genes (e.g. it is estimated humans have 20 000-25 000 genes).\u000a      Searching for known genes within such lists and relating them to their\u000a      function allows researchers to predict how a particular organism will fare\u000a      under given conditions (e.g. how a certain person will respond to a\u000a      particular drug or how a given plant will behave when introduced to a\u000a      specific soil-type). Hence, targeted and individual- based treatments are\u000a      possible provided there is sufficient knowledge of the organism's genetic\u000a      components. However, searching for these genes within the genome is a\u000a      computationally intensive task and is the focus of bioinformatics.\u000a    The research\u000a      Locating and identifying known genes in an organism's genome requires a\u000a      range of parametric and non-parametric statistical approaches to compare\u000a      data sets, coupled with the development of novel algorithms involving the\u000a      integration of multiple data types and subsequent efficient processing of\u000a      those data-rich sets. To this end, significant progress has been made by\u000a      Dr. Tatiana Tatarinova at the University of South Wales, focussing on\u000a      partial sequencing of certain plant species with their relatively small\u000a      sequence length. Tatarinova has published in the region of 40 articles and\u000a      books and is co-author of over a dozen patents involving the modification\u000a      of genes in plant crops. Her contribution to the discipline was recognised\u000a      in the 2010 \"International Review of Mathematics in the UK\" and she has\u000a      been rated as one of Wales's 30 most promising Scientists by Welsh\u000a      Crucible.\u000a    Analysing full genome sequences is not feasible using desktop computers\u000a      due to the sheer scale of the data sets. Instead this requires access to\u000a      high-performance computing alongside the development of appropriate\u000a      software exploiting the highly-parallelised architecture of such\u000a      facilities. Thus in 2010 when HPC Wales was launched, Tatarinova's\u000a      research was a natural fit. Indeed, Tatarinova has a history of working\u000a      for and alongside many of the world's leading bio-agricultural companies,\u000a      for example Monsanto Co., and so has a successful track record in\u000a      academic and commercial collaborations.\u000a    The impact area\u000a      HPC Wales, Europe's first nationally-distributed supercomputing network,\u000a      was established in partnership with a number of Welsh universities, the\u000a      Welsh Government and Fujitsu in 2010. The projects it supports and its\u000a      long term direction was influenced by its early adopters. Tatarinova's\u000a      research in genomics required vast amounts of computational time and was a\u000a      natural fit with HPC Wales. Therefore Tatarinova, with support from Wiltshire,\u000a      contributed to the development of HPC Wales by providing important\u000a      research problems that were used to test and refine the system's hardware\u000a      and software. Furthermore, Tatarinova and Wiltshire brought on\u000a      board a number of collaborative organisations through the promotion of\u000a      their research specialities. To promote projects in this area, and as\u000a      evidence of their support for this work, Fujitsu funded a research\u000a      studentship while HPC Wales have since funded two further studentships on\u000a      related projects supervised by Tatarinova.\u000a    "},{"CaseStudyId":"27165","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    The research [2] was conducted by Goldstein and O'Hagan in the\u000d\u000a      context of assessing assets of a regional water company, who sponsored\u000d\u000a      aspects of this work. O'Hagan implemented [2] in the inference\u000d\u000a      programme termed ABLE (Assessment with Bayes Linear Estimation &#8212; as\u000d\u000a      described in [2], which states that ABLE performs all the\u000d\u000a      calculations in that paper), and applied this approach as a consultant,\u000d\u000a      first for various water companies, then more widely.\u000d\u000a    ABLE was applied to the assets of London Underground, through a\u000d\u000a      consultancy with Metronet (which was contracted to maintain nine London\u000d\u000a      Underground lines), to achieve a better understanding of long term\u000d\u000a      investment requirements and the sustainability or otherwise of current\u000d\u000a      investment levels in infrastructure. This lead to the development of\u000d\u000a      ESTEEM (Engineering Strategy for Economic and Efficient Management), which\u000d\u000a      applied the Bayes linear methodology of [1] and [2], based\u000d\u000a      on the ABLE programme, to Metronet's assets, namely the maintainable items\u000d\u000a      in all of the stations, bridges and other structures, that require an\u000d\u000a      estimated &#163;5 billion investment over a 50 year period. The aim of ESTEEM\u000d\u000a      was to provide the company, through asset management estimates of asset\u000d\u000a      degradation, costs, risks and their probabilities for each maintainable\u000d\u000a      item, with a whole-life cost (WLC) strategic planning process for\u000d\u000a      maintenance and renewal of its civil engineering assets, under varying\u000d\u000a      funding constraints, over a 100 year planning horizon. A 2009 audit report1\u000d\u000a      stated that the anticipated benefit of the ESTEEM project was a 20%\u000d\u000a      saving, equalling about &#163;600m (p7 of report). ESTEEM also improved\u000d\u000a      passenger safety by combining safety, business and financial risk factors\u000d\u000a      into a single modelling process (p3). The report (p25) concludes, \"ESTEEM,\u000d\u000a      as so far implemented in Metronet, is a clear demonstration of best\u000d\u000a      practice, leading edge thinking in the areas of civil engineering\u000d\u000a      strategic planning and whole-life cost justification.\" The ESTEEM project\u000d\u000a      continued throughout a restructuring, resulting in the assets being\u000d\u000a      returned to the public sector, to Transport for London (TfL). In November\u000d\u000a      2010, ESTEEM won the prestigious Institution of Engineering and Technology\u000d\u000a      Innovation Award, in the Asset Management category. In its entry\u000d\u000a      submission, London Underground Ltd described the aim of ESTEEM as \"an\u000d\u000a      approach to optimising investment that aids training, promotes culture\u000d\u000a      change and improves decision making\" 2.\u000d\u000a    The ESTEEM protocol was followed in particular for all of the\u000d\u000a      maintainable components in every one of the stations and bridges in that\u000d\u000a      portion of the London Underground network originally controlled by\u000d\u000a      Metronet. This comprised about 2\/3 of the network and many thousands of\u000d\u000a      components, all of whose uncertain characteristics were assessed by\u000d\u000a      experts, leading to a full uncertainty specification and analysis within\u000d\u000a      the ABLE structure. This analysis was used as the basis for developing and\u000d\u000a      comparing whole-life maintenance strategies for all of these assets, as\u000d\u000a      part of the decision support structure outlined in the Esteem documents.\u000d\u000a      The ESTEEM Civil Assets least WLC predictions substantiated a basis for\u000d\u000a      long term investment in the asset base and justified inclusion of\u000d\u000a      preventative maintenance in a new performance contract for maintaining the\u000d\u000a      assets. A particular example of the benefits reaped from this project is\u000d\u000a      in the waterproofing of structures. Prior to ESTEEM, this was thought to\u000d\u000a      be too expensive to justify. However, ESTEEM predictions anticipated a 20%\u000d\u000a      savings in maintenance costs over a 30-year period, a saving of &#163;5m p.a.\u000d\u000a      The water-proofing was thus implemented at the end of 2009 for all\u000d\u000a      concrete and masonry structures and continues to this day. A further\u000d\u000a      example is the information systems used in London Underground stations.\u000d\u000a      ESTEEM has become a critical operational system used to maintain and\u000d\u000a      update these systems, and was fully implemented by summer 2011. It\u000d\u000a      includes a hand held asset survey system used across London Underground\u000d\u000a      for stations, maintains the asset register, reports condition and produces\u000d\u000a      deterministic predictions of WLC for budgetary purposes on an ongoing\u000d\u000a      basis. Currently, London Underground is maintaining condition state\u000d\u000a      reports electronically for an intended Bayes linear update of the Civil\u000d\u000a      Assets degradation predictions. In summary, the ESTEEM project, and the\u000d\u000a      Bayesian work underpinning it, has been of great benefit to TfL, with\u000d\u000a      current cost savings in the order of &#163;5m-&#163;10m for Civil assets and\u000d\u000a      development of investment policy options for stations that have enabled\u000d\u000a      prioritisation of future investment at levels that are sustainable3.\u000d\u000a    Our second example of the impact of the methodology in [1]\u000d\u000a      derives from [3]. Goldstein was a PI in the Basic Technology\u000d\u000a      funded \"Managing Uncertainty for Complex Models\" consortium of\u000d\u000a      universities. A postdoc within this consortium learned to apply Bayes\u000d\u000a      linear methodology; he then left the consortium to join FERA, the arm of\u000d\u000a      DEFRA dealing with regulation, policy and risk, as a statistician. He\u000d\u000a      applied these methods there, for example within the project Food,\u000d\u000a      Additives, Food Contact Material and Exposure Task (FACET), an 8.9 million\u000d\u000a      euro project, involving 20 research organisations, funded by the European\u000d\u000a      Commission, under the Seventh Framework Programme, which ran for four\u000d\u000a      years from September 20084. Project objectives were to\u000d\u000a      deliver to the European Community a sustainable system to monitor intake\u000d\u000a      of chemicals from food among European populations. Databases on food\u000d\u000a      intake, chemical occurrence and chemical concentration were linked in\u000d\u000a      algorithms for the estimation of probabilistic exposure to target food\u000d\u000a      chemical intake. The experts struggled to specify full probability\u000d\u000a      distributions across this complex space, but they had some experience of\u000d\u000a      average consumption rates with standard deviations and there had been some\u000d\u000a      studies into correlations between food types and across countries. As a\u000d\u000a      result, the Bayes linear approach was judged a good fit for modelling food\u000d\u000a      consumption databases for building up this model4,5.\u000d\u000a    Unilever and FERA collaborated on a hazard assessment model based on the\u000d\u000a      Bayes linear kinematic methodology, as part of Unilever's overall research\u000d\u000a      effort to find novel approaches for assuring customer safety. The model\u000d\u000a      considers the potency of chemicals that cause human sensitisation when\u000d\u000a      applied to the skin, resulting in an undesired immune response known as\u000d\u000a      allergic contact dermatitis. This presents clinically as a rash, skin\u000d\u000a      lesion, papules or blistering at the site of exposure. Risk assessors in\u000d\u000a      this area must weigh up several lines of evidence from in vivo and in\u000d\u000a      vitro experiments when characterising the potency for a new chemical\u000d\u000a      product in order to determine a safe dose for exposed individuals.\u000d\u000a      Beginning in 2010, Unilever applied the Bayes model in a series of\u000d\u000a      assessments, based around products such as cinnamic aldehyde (used to give\u000d\u000a      products a cinammon aroma, and a known skin sensitiser). This provided for\u000d\u000a      Unilever estimates such as ingredient dose on skin that would induce an\u000d\u000a      allergic response in certain percentages of consumers. The Bayes linear\u000d\u000a      kinematic provided the framework for modelling the assessors' expectations\u000d\u000a      and uncertainties and updating those beliefs in the light of the competing\u000d\u000a      data sources. This approach to synthesising multiple lines of evidence and\u000d\u000a      estimating hazard was judged to provide a transparent mechanism to\u000d\u000a      construct, defend and communicate risk management decisions. Its value to\u000d\u000a      Unilever is reflected in the fact that the company is working on extending\u000d\u000a      the model to incorporate population variance, the uncertainty in the\u000d\u000a      amount of product\/ingredient that the consumer applies and the probability\u000d\u000a      that the amount applied exceeds the adverse effect threshold for a given\u000d\u000a      consumer. A published account of the details from the study that are\u000d\u000a      publically available is provided in a 2013 paper6 and\u000d\u000a      also the document 5. The Unilever internal documents on\u000d\u000a      the outcomes are confidential and not for public dissemination, but we\u000d\u000a      have been allowed to quote the following, from two internal reports, as\u000d\u000a      illustrations of the role of Bayes linear methodology at Unilever7:\u000d\u000a    \"Due to this feature, Bayes linear theory is applied to solve the skin\u000d\u000a      sensitization risk assessment problem, as in such a problem, the available\u000d\u000a      information is usually not enough for the specification of a full\u000d\u000a      probability distribution. On the basis of the Bayes linear theory, the\u000d\u000a      Bayes Linear method is applied when the new information is deterministic\u000d\u000a      while the Bayes Linear Kinematic method is applied when there are\u000d\u000a      uncertainties existing in the new information.\" [Progress towards\u000d\u000a      modelling a population-level risk metric for skin allergy risk assessment:\u000d\u000a      page 5]\u000d\u000a    \"What we've done to date: In 2011, a model was developed with FERA to\u000d\u000a      predict median human threshold, i.e., the threshold to sensitize 50% of\u000d\u000a      population under specific clinical exposure conditions, according to the\u000d\u000a      data from both in vivo and in vitro tests. [The 50% threshold is not\u000d\u000a      chosen as the protection goal for sensitization incidence but as the\u000d\u000a      easiest percentile for experts to consider when judging correlation to\u000d\u000a      other assay results.] The objective of this model was to make transparent,\u000d\u000a      coherent and robust, an expert `weight of evidence' analysis using the\u000d\u000a      Bayes Linear method. This model was to allow comparison to be made between\u000d\u000a      tests on how informative they were on the median threshold in humans.\"\u000d\u000a      [Skin allergy risk assessment document: page 1]\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This study demonstrates how Bayes linear methodologies developed at\u000d\u000a      Durham University have impacted on industrial practice. Two examples are\u000d\u000a      given. The approach has been applied by London Underground Ltd. to the\u000d\u000a      management of bridges, stations and other civil engineering assets,\u000d\u000a      enabling a whole-life strategic approach to maintenance and renewal to\u000d\u000a      reduce costs and increase safety. The approach has won a major award for\u000d\u000a      innovation in engineering and technology. The methodology has also been\u000d\u000a      applied by Unilever and Fera to improve methods of assessing product\u000d\u000a      safety and in particular the risk of chemical ingredients in products\u000d\u000a      causing allergic skin reactions.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    UDur: University of Durham\u000d\u000a    ","Institutions":[{"AlternativeName":"Durham (University of)","InstitutionName":"University of Durham","PeerGroup":"B","Region":"North East","UKPRN":10007143}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a[1] M. Goldstein &amp; D.A. Wooff (2007) Bayes linear\u000d\u000a        statistics: theory and methods, Wiley, ISBN: 978-0-470-06567-9.\u000d\u000a    \u000a\u000a[2] M. Goldstein and A. O'Hagan (1996) Bayes linear\u000d\u000a        sufficiency and systems of expert posterior assessments, Journal of\u000d\u000a      the Royal Statistical Society, series B, 58, 301-316, Stable URL:\u000d\u000a      http:\/\/www.jstor.org\/stable\/2345978.\u000d\u000a    \u000a\u000a[3] M. Goldstein and S. Shaw (2004) Bayes linear kinematics\u000d\u000a        and Bayes linear Bayes Graphical Models, Biometrika, 91, 425-446,\u000d\u000a      doi:10.1093\/biomet\/91.2.425.\u000d\u000a    \u000a\u000a[4] F.P. Coolen, M. Goldstein &amp; D.A.Wooff (2007) Using\u000d\u000a        Bayesian statistics to support testing of software systems,\u000d\u000a      Proceedings of the Institution of Mechanical Engineers, Part O: Journal of\u000d\u000a      Risk and Reliability 221(1), 85-93, doi:10.1243\/1748006XJRR2.\u000d\u000a    \u000a\u000a[5] D. Randell, M. Goldstein, G. Hardman and P. Jonathan (2010) Bayesian\u000a        linear inspection planning for large-scale physical systems.\u000d\u000a      Proceedings of the Institution of Mechanical Engineers, Part O: Journal of\u000d\u000a      Risk and Reliability 224(4), 333-345,\u000d\u000a      DOI:10.1243\/1748006XJRR322.\u000d\u000a    \u000aQuality of Research: the work contained in [1] was supported by a\u000d\u000a      number of EPSRC grants all of which were highly graded in final review.\u000d\u000a      Journal of the Royal Statistical Society, series B (paper [2]) and\u000d\u000a      Biometrika (paper [3]) are two of the most highly rated statistics\u000d\u000a      journals in the world. Both [4] and [5] were awarded the\u000d\u000a      Donald Julius Groen Prize by the Safety &amp; Reliability Group of the\u000d\u000a      Institution of Mechanical Engineers. [5] is implemented in\u000d\u000a      software within Shell.\u000d\u000a    Grant referenced in section 2: 1999 - 2002 High reliability\u000d\u000a      testing for complex software using Bayesian graphical modelling and\u000d\u000a      program comprehension (value &#163;145,729; principal investigator M.\u000d\u000a      Goldstein, EPSRC).\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000d\u000a    \u000d\u000a       A Report for LUL Nominee BCV Limited, Trading as Metronet Rail BCV.\u000d\u000a        Asset Management Consulting Ltd, 12 May 2009. (PDF file).\u000d\u000a       Entry form submitted by London Underground Ltd. for the IET\u000d\u000a        Innovation Awards 2010. (Hard copy).\u000d\u000a       Letter signed by the ESTEEM project technical lead and project\u000d\u000a        manager, 2007-2011, confirming details of impact.\u000d\u000a       Details of the FACET project objectives, funding and partners, on the\u000d\u000a        CORDIS website http:\/\/cordis.europa.eu\/projects\/rcn\/87815_en.html,\u000d\u000a        and FACET project final report [pdf].\u000d\u000a       Document on role of Bayes linear methods at FERA written and provided\u000d\u000a        to us by the relevant statistician at FERA.\u000d\u000a       Gosling,J.P., Hart, A. et al (2013) A Bayes linear approach to\u000d\u000a          weight of evidence risk assessment for skin allergy, Bayesian\u000d\u000a        Analysis, 8, 169-186 [Detailed public document on the Unilever, Fera\u000d\u000a        collaboration]\u000d\u000a       Text from Toxicology Risk Modeller, Unilever Safety &amp;\u000d\u000a        Environmental Assurance Centre (SEAC), confirming the above description\u000d\u000a        of the impact on Unilever and the quotes from two Unilever internal\u000d\u000a        reports related to Bayes linear risk assessment.\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Industrial impact of Bayes linear analysis\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Bayesian analysis is a well established approach for combining expert\u000d\u000a      judgements with data to quantify uncertainties about real world outcomes\u000d\u000a      in a probabilistic form appropriate for inference and decision-making.\u000d\u000a      There are two practical problems with this approach, for large and complex\u000d\u000a      problems. Firstly, it requires a level of detail which goes far beyond the\u000d\u000a      ability of the expert to provide meaningful judgements, leading to many\u000d\u000a      arbitrary aspects of the prior formulation. Secondly, the analysis is very\u000d\u000a      computer intensive, typically requiring large-scale numerical simulations\u000d\u000a      which are highly sensitive to certain features of these somewhat arbitrary\u000d\u000a      prior specifications. Therefore, often the analysis is both non-robust and\u000d\u000a      too complex to allow a proper exploration of its sensitivity, particularly\u000d\u000a      in problems of optimal experimental design or sample choice.\u000d\u000a    Bayes linear analysis has been developed by Michael Goldstein, in Durham,\u000d\u000a      with many collaborators, to address these issues, by both simplifying the\u000d\u000a      specifications required to carry out the analysis and reducing the\u000d\u000a      complexity of the analysis itself. It does this through a geometric\u000d\u000a      approach to statistical inference which takes expectation, rather than\u000d\u000a      probability, as primitive, allowing us to make a limited number of\u000d\u000a      expectation statements, rather than requiring a complete probability\u000d\u000a      specification, and constructing appropriate methodology based on\u000d\u000a      orthogonal projection (which is computationally simpler than full Bayes)\u000d\u000a      for analysing uncertainties based on a partial specification. The\u000d\u000a      foundations and methodology are described in detail in [1], which\u000d\u000a      is the general underpinning research for all of the impact described in\u000d\u000a      this case study, which concerns the ways in which the Bayes linear\u000d\u000a      approach has impacted on industrial practice.\u000d\u000a    We choose two areas of research and application to demonstrate this\u000d\u000a      impact.\u000d\u000a    (i) The paper [2], by Goldstein and O'Hagan (U. of Nottingham)\u000d\u000a      considers problems where a decision maker must estimate a set of unknown\u000d\u000a      quantities and receives expert assessments at varying levels of accuracy\u000d\u000a      on samples of the quantities of interest. The paper introduces the general\u000d\u000a      notion of Bayes linear sufficiency, derives its properties and uses the\u000d\u000a      tractability associated with the Bayes linear formulation to underpin a\u000d\u000a      practical methodology for the design and analysis of studies relating to\u000d\u000a      very large systems of assets. We will describe below their impact for\u000d\u000a      London Underground.\u000d\u000a    (ii) The paper [3], by Goldstein and Shaw (a postdoc at Durham,\u000d\u000a      1999 - 2002, when this research was carried out) extends the Bayes linear\u000d\u000a      approach by introducing \"Bayes linear kinematics\" which merges aspects of\u000d\u000a      full Bayes and Bayes linear inferences. (The notion is by analogy with the\u000d\u000a      well-established \"probability kinematics\".) This allows the construction\u000d\u000a      of \"Bayes linear Bayes graphical models\", which combine the simplicity of\u000d\u000a      Gaussian graphical models with the ability to allow full conditioning on\u000d\u000a      marginal distributions of any form. The approach was first developed to\u000d\u000a      address problems in Bayesian reliability testing for complex systems (see\u000d\u000a      [4] and [5]). The flexibility of the Bayes linear kinematic\u000d\u000a      makes it an appropriate tool for risk assessors who want to quantify their\u000d\u000a      uncertainty about hazards based on disparate sources of information, and\u000d\u000a      we will describe, below, the use of such methods in FERA and Unilever.\u000d\u000a    "},{"CaseStudyId":"27166","Continent":[],"Country":[],"Funders":["Natural Environment Research Council"],"ImpactDetails":"\u000d\u000a    There were regular discussions on probabilistic climate projection, from\u000d\u000a      2002, between members of the Durham Statistics group and individuals with\u000d\u000a      responsibility for uncertainty analysis in climate projections in the Met\u000d\u000a      Office Hadley Centre. Therefore, there was general familiarity with our\u000d\u000a      approach, in which Dr. Rougier consulted closely with the Hadley Centre.\u000d\u000a      By this route, the Durham research into Bayesian modelling became a\u000d\u000a      central methodological component of the UK Climate Projections 2009\u000d\u000a      (UKCP09), the Met Office's climate analysis tool for the UK for the 21st\u000d\u000a      century, funded by DEFRA. The science and methodology used to construct\u000d\u000a      UKCP09 is described in detail in the 200 page report1, from the\u000d\u000a      Met office. The report 1 emphasises the importance of the\u000d\u000a      careful treatment of uncertainty in the climate projections. Here is an\u000d\u000a      indicative quotation from the introduction:\u000d\u000a    \"Uncertainty in climate change projections is a major problem for those\u000d\u000a      planning to adapt to a changing climate. Adapting to a smaller change than\u000d\u000a      that which actually occurs (or one of the wrong sign) could result in\u000d\u000a      costly impacts and endanger lives, yet adapting to too large a change (or,\u000d\u000a      again, one of the wrong sign), could waste money. In addition there is the\u000d\u000a      risk of maladaptation &#8212; adapting to climate change in a way that prevents\u000d\u000a      or inhibits future adaptation. The 2008 projections are the first from\u000d\u000a      UKCIP to be designed to treat uncertainties explicitly ... This means that\u000d\u000a      probabilities are attached to different climate change outcomes, giving\u000d\u000a      more information to planners and decision makers.\" [page 19]\u000d\u000a    The methods developed at Durham play an important role in the uncertainty\u000d\u000a      analysis throughout the report. Here are some indicative quotes from the\u000d\u000a      report (all references to Goldstein and Rougier (2004) and Rougier (2007)\u000d\u000a      refer to papers [1] and [2] cited in the preceding\u000d\u000a      section).\u000d\u000a    \"These results are then incorporated into our uncertainty analysis, based\u000d\u000a      on a statistical framework devised by Goldstein and Rougier (2004),\u000d\u000a      discussed in Chapter 3. This allows us to create a probability\u000d\u000a      distribution function accounting for uncertainties arising from both model\u000d\u000a      parameters and structural errors, and constrained by observations,\" [page\u000d\u000a      39]\u000d\u000a    \"The method is based on a general statistical framework for the\u000d\u000a      derivation of probabilistic projections of real systems from simulations\u000d\u000a      carried out using complex but imperfect models of those systems (Goldstein\u000d\u000a      and Rougier, 2004; Rougier, 2007).\" [page 49]\u000d\u000a    \"Our ensemble projections are converted into probabilistic projections\u000d\u000a      using a Bayesian statistical framework developed to support inference of\u000d\u000a      future information about real systems from complex but imperfect models\u000d\u000a      (Goldstein and Rougier, 2004; Rougier, 2007). This process allows our\u000d\u000a      projections to be constrained by a set of observations of past climate\u000d\u000a      (Section 3.2.9), and also involves the use of expert judgements ... The\u000d\u000a      probabilities which emerge from this approach represent the relative\u000d\u000a      credibility of a family of different possible outcomes, taking into\u000d\u000a      account our understanding of physics, chemistry, biology, observational\u000d\u000a      evidence, and expert judgement.\" [page 82]\u000d\u000a    The contribution of the Durham approach to UKCP09 is amplified in a paper2\u000d\u000a      by several of the authors involved with that uncertainty analysis, which\u000d\u000a      explains in detail the uncertainty methodology used by UKCP09. For\u000d\u000a      example, section 3 of the paper, \"Outline of the calculations\", begins\u000d\u000a      \"Here we describe the general steps in Goldstein and Rougier (2004)\u000d\u000a      necessary to determine a probability distribution of some aspects of\u000d\u000a      climate change that we want to predict.\" Similarly, the final subsection,\u000d\u000a      section 6.2 begins \"... Goldstein and Rougier (2004) gives us several key\u000d\u000a      advantages. ...First, the multivariate nature of this probabilistic\u000d\u000a      framework allows us to have more than one prediction variable. Predicting\u000d\u000a      joint probabilities provides us with important information on how\u000d\u000a      uncertainty is related across different climate variables...\"\u000d\u000a    UKCP09 plays a key role within the Government's statutory\u000d\u000a      responsibilities for assessing and responding to climate change. The\u000d\u000a      Climate Change Act 2008 constructed a legally-binding long-term framework\u000d\u000a      for the UK to cut greenhouse gas emissions and a framework for building\u000d\u000a      the UK's ability to adapt to a changing climate. The Act requires a\u000d\u000a      UK-wide climate change risk assessment (CCRA) that must take place every\u000d\u000a      five years and a national adaptation programme (NAP), setting out the\u000d\u000a      Government's objectives, proposals and policies for responding to the\u000d\u000a      risks identified in the CCRA. The CCRA, and thus the NAP, drew heavily on\u000d\u000a      the uncertainty analysis in UKCP09. The purpose of CCRA and the role of\u000d\u000a      UKCP09 are indicated by the following two quotes from the Evidence Report3:\u000d\u000a    \"The UK Climate Change Act (CCA) 2008 makes the UK the first country in\u000d\u000a      the world to have a legally binding, long-term framework to cut carbon\u000d\u000a      emissions. It also requires a series of assessments of the risks of\u000d\u000a      climate for the UK, under both current conditions and over the long term,\u000d\u000a      to 2100. The CCRA provides the first of these assessments and was laid\u000d\u000a      before parliament in January 2012.\" [page V]\u000d\u000a    \"The CCRA makes use of the UKCP09 climate projections that represent a\u000d\u000a      range of possible future changes in UK climate. The range of possibilities\u000d\u000a      is necessarily wide to take account of uncertainties in natural climate\u000d\u000a      variability, how the UK's climate may respond to global warming, the\u000d\u000a      future trajectory of emissions, and how these might magnify any regional\u000d\u000a      climate change effects.\" [page 9]\u000d\u000a    The CCRA constructed sector reports describing a wide range of potential\u000d\u000a      risks in each of the following sectors (followed by a more detailed\u000d\u000a      analysis of selected risks that were judged to be the most important):\u000d\u000a      Agriculture; Biodiversity &amp; Ecosystem Services; Built Environment;\u000d\u000a      Business, Industry &amp; Services; Energy; Floods &amp; Coastal Erosion;\u000d\u000a      Forestry; Health; Marine &amp; Fisheries; Transport; Water. The CCRA used\u000d\u000a      the UK Climate Projections (UKCP09) for three time periods &#8212; 30-year\u000d\u000a      periods centred on the 2020s, 2050s and 2080s. The CCRA attempted to\u000d\u000a      monetise the most important risks to the UK, and concluded that the\u000d\u000a      results indicated that the net economic costs to the UK are of the order\u000d\u000a      of tens of billions\/year by the 2050s (in current prices) even for the\u000d\u000a      middle of the uncertainty range for these costs at the middle of the\u000d\u000a      projected emission scenarios (see the Scoping Study4, page 6,\u000d\u000a      which suggests that this figure is an underestimate).\u000d\u000a    The CCRA is constructed to facilitate the Climate Change Act mandated\u000d\u000a      National Adaptation Programme (NAP, 2013). The NAP report5\u000d\u000a      (laid before Parliament in 2013) explains:\u000d\u000a    \"The Climate Change Risk Assessment 2012 (CCRA) for the UK brought\u000d\u000a      together the best available evidence, using a consistent framework to\u000d\u000a      identify the risks and opportunities related to climate change. The\u000d\u000a      assessment distilled approximately 700 potential risks down to more than\u000d\u000a      100 for detailed review. The government's response to the CCRA, which\u000d\u000a      meets the requirements laid down in the Climate Change Act 2008, is the\u000d\u000a      first NAP. In developing the NAP for England, we have taken the highest\u000d\u000a      order risks from the CCRA and working in partnership with businesses,\u000d\u000a      local government and other organisations, have developed objectives,\u000d\u000a      policies and proposals to address them.\" [page 8]\u000d\u000a    In addition to its statutory role, the Met Office has worked with a wide\u000d\u000a      range of public and private sector organisations to use UKCP09 to inform\u000d\u000a      decisions on investment amounting to billions of pounds to 'future proof'\u000d\u000a      projects against climate change. On the UKCP09 website (http:\/\/ukclimateprojections.defra.gov.uk)\u000d\u000a      there is a link to a Case Studies web-page6, which states\u000d\u000a      \"Working with our stakeholders, we have put together a number of case\u000d\u000a      studies to show how UKCP09 data can be used.\" As of 17\/10\/13, these\u000d\u000a      included case studies with Councils for Devon, Hampshire, Kent, Milton\u000d\u000a      Keynes and Oxford City, and organisations including Atkins\/UKWIR, CEH,\u000d\u000a      Environment Agency\/Acclimatise \/JBA consulting, Macaulay Institute, United\u000d\u000a      Sustainable Energy Agency, Proclimation, Prometheus, Royal Haskoning,\u000d\u000a      Severn Trent Water, South West Tourism. Each case study describes how the\u000d\u000a      UKCP09 products were used, and how the results would be communicated to\u000d\u000a      the target audience. The range of topics covered includes: national\u000d\u000a      assessment of river flows, climate change and pollution at water courses\u000d\u000a      in Birmingham, strategic planning for flood management, changes in flood\u000d\u000a      damages at a catchment scale, assessments of storm surge and sea level\u000d\u000a      rise, emergency planning, defining land capability for agriculture\u000d\u000a      specifications, assessing potential vulnerabilities to climate change,\u000d\u000a      future proofing design decisions in the buildings sector, investigating\u000d\u000a      coastal recession &amp; shore profile development, storm surge and sea\u000d\u000a      level rise and assessing impacts of climate change on tourism in South\u000d\u000a      West England.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The Climate Change Act, 2008, constructed a legally-binding long-term\u000d\u000a      framework for the UK to cut greenhouse gas emissions and a framework for\u000d\u000a      building the UK's ability to adapt to a changing climate. The Act requires\u000d\u000a      a UK-wide climate change risk assessment (CCRA) that must take place every\u000d\u000a      five years and a national adaptation programme (NAP), setting out the\u000d\u000a      Government's objectives, proposals and policies for responding to the\u000d\u000a      risks identified in the CCRA. The CCRA, and thus the NAP, drew heavily on\u000d\u000a      the uncertainty analysis for future climate outcomes, published in 2009 by\u000d\u000a      the Met Office as the UK Climate Projections UKCP09, which in turn drew\u000d\u000a      heavily on research into the Bayesian analysis of uncertainty for physical\u000d\u000a      systems modelled by computer simulators carried out at Durham University.\u000d\u000a      A wide range of industries and public sector organisations likely to be\u000d\u000a      affected by climate change have consulted with the Met Office on UKCP09 to\u000d\u000a      inform decisions on policy and investment, involving billions of pounds,\u000d\u000a      in sectors as diverse as flood defence, transport, energy supply and\u000d\u000a      tourism.\u000d\u000a    ","ImpactType":"Environmental","Institution":"\u000d\u000a    UDur: University of Durham\u000d\u000a    ","Institutions":[{"AlternativeName":"Durham (University of)","InstitutionName":"University of Durham","PeerGroup":"B","Region":"North East","UKPRN":10007143}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    The basic work that summarises, formalises and generalises all of the\u000d\u000a      preceding work in Durham on uncertainty in physical systems represented by\u000d\u000a      computer models, and with which this impact case is directly concerned, is\u000d\u000a      contained in the paper\u000d\u000a    \u000a[1] Goldstein, M &amp; Rougier, J (2004) Probabilistic\u000d\u000a        formulations for transferring inferences from mathematical models to\u000d\u000a        physical systems, Siam J. Sci Comput., 26, 467-487,\u000d\u000a      doi:10.1137\/S106482750342670X.\u000d\u000a    \u000aThe paper [1] has been well referenced and its practical value\u000d\u000a      can be judged by the role that it has played in areas such as climate\u000d\u000a      science, as will be discussed in the next section. This paper forms a part\u000d\u000a      of the ongoing exploration at Durham of the question, of fundamental\u000d\u000a      interest in all areas of science and technology, as to what is the actual\u000d\u000a      information about a physical system that is conveyed by one or more models\u000d\u000a      for that system, and how can that information be uncovered and exploited\u000d\u000a      for better understanding of the behaviour of the system. It provided the\u000d\u000a      core ideas for much further development of that theme at Durham; in\u000d\u000a      particular, Dr Rougier, while at Durham, wrote a paper which applied its\u000d\u000a      formulation directly to the problem of climate inference, namely\u000d\u000a    \u000a[2] Rougier, J.C. (2007), Probabilistic inference for future\u000d\u000a        climate using an ensemble of climate model evaluations, Climatic\u000d\u000a      Change, 81, 247-264, doi:10.1007\/s10584-006-9156-9\u000d\u000a    \u000aand Goldstein and Rougier wrote the first draft of the general conceptual\u000d\u000a      paper which extended the formulation and eventually appeared as\u000d\u000a    \u000a[3] Goldstein M and Rougier J.C. (2009), Reified Bayesian\u000d\u000a        modelling and inference for physical systems, Journal of Statistical\u000d\u000a      Planning and Inference, 139, 1221-1239,\u000d\u000a      doi:10.1016\/j.jspi.2008.07.019\u000d\u000a    \u000awhich was chosen as the first ever discussion paper in that journal.\u000d\u000a    During the period of this research, Dr Rougier was funded by the\u000d\u000a      following grants:\u000d\u000a    The probability of rapid climate change (01\/01\/2004 - 31\/12\/2006)\u000d\u000a      Funder: NERC; grant value: &#163;173 074.68\u000d\u000a    Uncertainties integrated assessment process (01\/09\/2002 -\u000d\u000a      31\/08\/2005)\u000d\u000a      Funder: Tyndall Centre; grant value: &#163;13 606.40\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    \u000d\u000a      Murphy JM, Sexton DMH, Jenkins GJ, Booth BBB, Brown CC, Clark RT,\u000d\u000a        Collins M, Harris GR, Kendon EJ, Betts RA, Brown SJ, Humphrey KA,\u000d\u000a        McCarthy MP, McDonald RE, Stephens A, Wallace C, Warren R, Wilby R, Wood\u000d\u000a        RA (2009) UK climate projections science report: climate projections,\u000d\u000a        Met Office Hadley Centre, Exeter, available from\u000d\u000a        http:\/\/ukclimateprojections.defra.gov.uk\/22544\u000a\u000d\u000a      Sexton DMH, Murphy JM, Collins M, Webb MJ (2012) Multivariate\u000d\u000a          probabilistic projections using imperfect climate models part 1:\u000d\u000a          outline of methodology, Climate Dynamics, 38, 2513-2542,\u000d\u000a        doi:10.1007\/s00382-011-1208-9.\u000d\u000a      \u000aThe UK Climate Change Risk Assessment 2012, Evidence Report,\u000d\u000a        DEFRA 2012.\u000d\u000a      \u000aScoping Study: Reviewing the Coverage of Economic Impacts in the\u000d\u000a          CCRA, Report to the Committee on Climate Change, Adaptation\u000d\u000a        Sub-Committee, Paul Watkiss Associates, 2009.\u000d\u000a      \u000aThe National Adaptation Programme, Making the country resilient to\u000d\u000a          a changing climate, July 2013, available at https:\/\/www.gov.uk\/government\/publications\/adapting-to-climate-change-\u000a          national-adaptation-programme.\u000d\u000a      UKCP09 Case Studies webpage http:\/\/ukclimateprojections.defra.gov.uk\/23081\u000d\u000a        (accessed 17\/10\/2013).\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Policy implications of uncertainties related to climate change\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2650628","Name":"Durham"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The Durham statistics group has developed a very general probabilistic\u000d\u000a      framework for linking one or several mathematical models to the physical\u000d\u000a      systems that the models purport to represent, taking account of all\u000d\u000a      sources of uncertainty, including model and simulator imperfections. This\u000d\u000a      framework is a necessary precondition for making probabilistic statements\u000d\u000a      about the system on the basis of historical observations and evaluations\u000d\u000a      of the computer simulators. The formulation distinguishes simulators\u000d\u000a      according to their quality and the nature of their inputs. Further\u000d\u000a      modelling constructs are introduced to account for imperfections in the\u000d\u000a      available simulators and, within the framework of Bayesian graphical\u000d\u000a      modelling, to unify the composite inference, from the collection of\u000d\u000a      available simulators, observed historical data and the judgements of\u000d\u000a      experts, for the behaviour of the actual physical system. The work [1]\u000d\u000a      in section 3 is quite general, but the group did have in mind the\u000d\u000a      application to collections of evaluations of climate models in this\u000d\u000a      development and part of the research is the paper [2] making this\u000d\u000a      connection explicit. The research led to a very general formalism [3]\u000d\u000a      for relating models to physical systems.\u000d\u000a    The particular work leading to the climate-related impacts in this case\u000d\u000a      study was carried out over the period 2001-2006 by Michael Goldstein,\u000d\u000a      permanent member of staff, and Jonathan Rougier, PDRA. Dr Rougier left\u000d\u000a      Durham at the end of 2006 and took up a lectureship at the University of\u000d\u000a      Bristol.\u000d\u000a    "},{"CaseStudyId":"27167","Continent":[],"Country":[],"Funders":["Economic and Social Research Council"],"ImpactDetails":"\u000d\u000a    Northern Doctors Urgent Care Group (NDUC) is a not-for-profit\u000d\u000a      organisation, based in\u000d\u000a      Newcastle, which is commissioned by the NHS to deliver out-of-hours urgent\u000d\u000a      GP services. It has\u000d\u000a      a turnover of &#163;15M and 600 staff. Trained call handlers liaise with\u000d\u000a      clinicians to provide patients\u000d\u000a      with appropriate care, including: telephone advice, visit to an Urgent\u000d\u000a      Care Centre, home visit, or\u000d\u000a      hospital admission.\u000d\u000a    In 2008, NDUC was receiving 100,000 calls per annum from the Northumbria\u000d\u000a      health region\u000d\u000a      (population 929,000) when it approached Knowledge House, an organisation\u000d\u000a      that facilitates links\u000d\u000a      between companies and universities in North East England, requesting a\u000d\u000a      research partnership\u000d\u000a      aimed at reducing costs and improving patient care by more accurately\u000d\u000a      forecasting call volumes\u000d\u000a      and daily patterns of call arrivals. Durham University responded and\u000d\u000a      entered into a Knowledge\u000d\u000a      Transfer Partnership, placing an early career researcher, Grace Stirling,\u000d\u000a      at NDUC from 2008-10,\u000d\u000a      supervised by Professor Wooff. The research was conducted during the\u000d\u000a      period of the KTP and\u000d\u000a      the results were announced in the talk [4] and summary note [3],\u000d\u000a      the details being published in\u000d\u000a      [2] and [1].\u000d\u000a    The statistical methodology developed in Durham in these works provided\u000d\u000a      NDUC with a forecast\u000d\u000a      of numbers of calls arriving during every 30 minutes. Forecasts are made\u000d\u000a      up to a year in\u000d\u000a      advance and are highly accurate, facilitating optimum clinical staffing,\u000d\u000a      which accounted for 64%\u000d\u000a      of expenditure. NDUC was able to save about &#163;65,000 per year by changing\u000d\u000a      GP shift patterns in\u000d\u000a      response to better forecasting. A further &#163;35,000 per year was saved by\u000d\u000a      introducing a new rota\u000d\u000a      for call handlers working at weekends.1\u000d\u000a    These improvements aided business expansion. In 2011 NDUC won a &#163;4.5M per\u000d\u000a      year contract\u000d\u000a      to handle non-emergency calls in Teesside (565,000 population) and in 2012\u000d\u000a      a &#163;4.7M per year\u000d\u000a      contract to provide a similar service in Staffordshire (700,000\u000d\u000a      population).\u000d\u000a    The Chief Executive of NDUC commented that the relationship with Durham\u000d\u000a      University and the\u000d\u000a      success of the research-based methodology were `central' to the successful\u000d\u000a      bid for the\u000d\u000a      Staffordshire contract, forming a major part of the presentation to the\u000d\u000a      NHS commissioning body\u000d\u000a      and providing a distinct competitive edge over the four other bidders. \"In\u000d\u000a      terms of better patient\u000d\u000a      care, I just think it (the Durham methodology) makes the whole\u000d\u000a      organisation operate more\u000d\u000a      effectively... At the busy times, it means we have the right number of\u000d\u000a      staff to deal with the\u000d\u000a      patients, so patients don't experience delays ... and then again we are\u000d\u000a      not having to spend a lot\u000d\u000a      of money putting too many doctors on, when they are not really needed...\u000d\u000a      The key thing is that it\u000d\u000a      enables us to provide a consistent service.\"2 The resulting\u000d\u000a      contract was very well-received:\u000d\u000a      indeed, the Clinical Director for Unplanned Care for Stoke-on-Trent\u000d\u000a      Clinical Commissioning\u000d\u000a      Group, Dr Chandra Kanneganti, commented to a local newspaper: \"This was a\u000d\u000a      comprehensive\u000d\u000a      procurement process involving five possible providers ... I am delighted\u000d\u000a      that we will be working\u000d\u000a      with Northern Doctors Urgent Care. They have a proven track record of\u000d\u000a      delivering out of hours\u000d\u000a      services.\" 3\u000d\u000a    The Teesside assessors positively highlighted the NDUC's response to the\u000d\u000a      swine flu epidemic in\u000d\u000a      2009,1 when the Durham methodology enabled diagnostic models to\u000d\u000a      forecast patient volumes\u000d\u000a      and disentangle normal load from epidemic load. This allowed NDUC to react\u000d\u000a      to increasing\u000d\u000a      volumes and provided the North East England Strategic Health Authority\u000d\u000a      with timely out-of-hours\u000d\u000a      data to help as an early warning trigger for the rest of the healthcare\u000d\u000a      system. Subsequent impact\u000d\u000a      on public policy was incorporation by the SHA of this second strand of\u000d\u000a      research into its North\u000d\u000a      East Escalation Plan for Pandemic Influenza.\u000d\u000a    The research [1-4] has had further impact in another area of\u000d\u000a      public policy. SUSTRANS is a\u000d\u000a      charity which fosters sustainable transport and had previously\u000d\u000a      collaborated on statistical analysis\u000d\u000a      of cycling data with Durham University, through the relationship with the\u000d\u000a      SUSTRANS Research\u000d\u000a      and Monitoring Unit Project Manager, a Durham graduate. In 2009, the\u000d\u000a      Project Manager\u000d\u000a      approached Professor Wooff to evaluate the first phase of the Cycling\u000d\u000a      Demonstration Town\u000d\u000a      Programme (CDTP), a multi-agency project funded by the Department for\u000d\u000a      Transport and local\u000d\u000a      authorities which had allocated &#163;1m per year to each of six participating\u000d\u000a      towns between 2005-09\u000d\u000a      for schemes aimed at stimulating levels of cycling.4 The data\u000d\u000a      concern automatic daily counts of\u000d\u000a      cyclists at 111 locations over four years. Counts of cyclists arriving at\u000d\u000a      locations is structurally the\u000d\u000a      same as volumes of calls arriving at a call centre, and so the Durham\u000d\u000a      methodology of [1-4],\u000d\u000a      which had been developed in the context of call centre data, could be\u000d\u000a      immediately applied to the\u000d\u000a      cycling data.\u000d\u000a    Professor Wooff's data analysis underpinned two evaluation reports\u000d\u000a      co-authored by the\u000d\u000a      SUSTRANS Project Manager and published by Cycling England and the\u000d\u000a      Department for\u000d\u000a      Transport in 20094,5. This analysis, especially its key figure\u000d\u000a      of a 27% increase in the cycle count\u000d\u000a      (unweighted mean percentage change relative to 2005 baseline)5,\u000d\u000a      had significant political,\u000d\u000a      economic, environmental and public engagement impacts, as will now be\u000d\u000a      described.\u000d\u000a    The 27% figure provided an easy to understand measure upon which the\u000d\u000a      Government based\u000d\u000a      further cost-benefit analyses and was quoted extensively in publicity and\u000d\u000a      media campaigns to\u000d\u000a      gain acceptance of its policies on cycling, said the Project Manager:\u000d\u000a    \"David's work gave us the possibility to report the change in one number\u000d\u000a      (27%), which was what\u000d\u000a      the Government wanted. It helped make the case for further funding for the\u000d\u000a      Cycling Cities and\u000d\u000a      Towns (Programme) and other cycling-related projects and more recently\u000d\u000a      helped make the case\u000d\u000a      for the (Local) Sustainable Transport Fund. It is still being used in\u000d\u000a      Government today as one of\u000d\u000a      the pieces of analysis underpinning a lot of benefit cost ratio\u000d\u000a      calculation in city or town wide\u000d\u000a      initiatives.\" 6\u000d\u000a    The Cycling Cities and Towns Programme attracted &#163;43M of government\u000d\u000a      funding for cycle lanes,\u000d\u000a      enhanced junction crossings, cycle parking facilities, training and\u000d\u000a      information, from 2008-2011.\u000d\u000a      An evaluation document7 says the programme was `built on\u000d\u000a      earlier experience in six Cycling\u000d\u000a      Demonstration Towns'. This is a reference to the data that was analysed in\u000d\u000a      the reports 4,5 using\u000d\u000a      the Durham methodology.\u000d\u000a    The Government subsequently allocated a further &#163;600M6,8, via\u000d\u000a      the Local Sustainable Transport\u000d\u000a      Fund, to integrated projects that encourage green transport. The first\u000d\u000a      tranche of 39 LSTF projects\u000d\u000a      funded in July 2011 included 38 with a cycling element9.\u000d\u000a      Further tranches were funded in May\u000d\u000a      2012 and June 2012. A further &#163;62M has been spent on `other projects on\u000d\u000a      cycling' during this\u000d\u000a      period6. These include projects funded through block\u000d\u000a      allocations by government to local transport\u000d\u000a      authorities.9\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The Northern Doctors Urgent Care Group, a not-for-profit organisation\u000d\u000a      that delivers out-of-hours\u000d\u000a      urgent medical services for the NHS, achieved significant efficiency\u000d\u000a      savings and improvements\u000d\u000a      in-patient care as a result of adopting statistical assessment and\u000d\u000a      forecasting processes,\u000d\u000a      developed by Durham University. These improved processes also featured in\u000d\u000a      the Group's\u000d\u000a      successful competitive bids for two new contracts worth &#163;9.2M per year. In\u000d\u000a      addition, the Durham\u000d\u000a      methodology was adapted to assess the results of a Government programme to\u000d\u000a      encourage\u000d\u000a      cycling in six UK towns, producing data on cycle use that helped to\u000d\u000a      influence subsequent\u000d\u000a      allocations of about &#163;700M for sustainable transport projects.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    UDur: University of Durham\u000d\u000a    ","Institutions":[{"AlternativeName":"Durham (University of)","InstitutionName":"University of Durham","PeerGroup":"B","Region":"North East","UKPRN":10007143}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a[1] Wooff, D. A. &amp; Stirling, S. G. (2013), Practical\u000d\u000a        statistical methods for call centres with a\u000d\u000a        case study addressing urgent medical care delivery, http:\/\/dro.dur.ac.uk\/11457.\u000d\u000a      To appear,\u000d\u000a      with minor revisions, in the Annals of Operations Research.\u000d\u000a    \u000a\u000a[2] Stirling, S.G. &amp; Wooff, D.A. (2011), Statistical\u000d\u000a        Methods for supporting urgent care\u000d\u000a        delivery through call centres, in Keynote Papers YoungOR 17,\u000d\u000a      Nottingham, UK, Monks, T. ed,\u000d\u000a      The Operational Research Society, pp. 57-78.\u000d\u000a    \u000a\u000a[3] Wooff, D. A. &amp; Stirling, S. G. (2011), Forecasting for\u000d\u000a        urgent medical care call centres, in\u000d\u000a      Forward look mathematics and industry &#8212; success stories, Lery, T.\u000d\u000a      ed., European Science\u000d\u000a      Foundation, Strasbourg, p.68.\u000d\u000a    \u000a\u000a[4] Stirling, S.G. &amp; Wooff, D.A. (2010), Forecasting for\u000d\u000a        medical emergency call centres,\u000d\u000a      Conference presentation, Institute for Mathematical Statistics 73rd\u000d\u000a      annual conference, 2010,\u000d\u000a      Gothenburg, Sweden.\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a\u0009\u000d\u000a    Knowledge Transfer Partnership final report (copy held in\u000d\u000a      Durham) and collaboration page\u000d\u000a      http:\/\/info.ktponline.org.uk\/action\/details\/partnership.aspx?id=6732\u000a\u000d\u000a    Interview with the Chief Executive, NDUC, 6 December 2012.\u000d\u000a      (Sound file held in Durham.)\u000d\u000a    Leek Post and Times (Staffordshire local newspaper), http:\/\/www.leek-news.co.uk\/Leek-News\/New-out-of-hours-provider-announced-04122012.htm\u000a\u000d\u000a    Introduction to `Analysis and Synthesis of Evidence on the\u000d\u000a      Effects of Investment in Six Cycling\u000d\u000a      Demonstration Towns' (November 2009), Department for Transport.\u000d\u000a      http:\/\/webarchive.nationalarchives.gov.uk\/20110407094607\/http:\/\/www.dft.gov.uk\/cyclingengland\/site\/wp-content\/uploads\/2010\/03\/analysis-and-synthesis-report.pdf\u000a\u000d\u000a    Pages 1-2 of `Cycling Demonstration Towns: Monitoring Project\u000d\u000a      Report 2006 to 2009'\u000d\u000a      (November 2009), Cycling England.\u000d\u000a      http:\/\/webarchive.nationalarchives.gov.uk\/20110407094607\/http:\/\/www.dft.gov.uk\/cyclingengland\/site\/wp-content\/uploads\/2009\/12\/cdts-monitoring-project-report-2006-09.pdf\u000a\u000d\u000a    Written testimony from the SUSTRANS Research and Monitoring\u000d\u000a      Unit Project Manager, April\u000d\u000a      2013. (Email kept on file in Durham.)\u000d\u000a    Introduction to Executive Summary, Evaluation of the Cycling\u000d\u000a      City and Towns Programme\u000d\u000a      Interim Report (January 2011). AECOM, Centre for Transport and Society,\u000d\u000a      The Tavistock\u000d\u000a      Institute. (Copy kept in Durham.)\u000d\u000a    Written statement to Parliament by Norman Baker, Parliamentary\u000d\u000a      Under Secretary of State for\u000d\u000a      Transport, 27 June 2012, available at https:\/\/www.gov.uk\/government\/speeches\/266-million-investment-in-local-sustainable-transport-schemes\u000d\u000a      (accessed 17\/10\/2013).\u000d\u000a    Parliamentary Questions information, Parliamentary Advisory\u000d\u000a      Council for Transport Safety (24\u000d\u000a      February 2012), available at http:\/\/www.pacts.org.uk\/2012\/02\/pqs-20th-23rd-2\/\u000d\u000a      (accessed\u000d\u000a      17\/10\/2013).\u000d\u000a\u0009  \u000d\u000a    ","Title":"\u000d\u000a    Statistical methods for urgent medical care call centres and sustainable\u000d\u000a      transport\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2636910","Name":"Stirling"},{"GeoNamesId":"2641673","Name":"Newcastle-upon-Tyne"},{"GeoNamesId":"2636841","Name":"Stoke-on-Trent"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"},{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    This work has been carried out by David Wooff (lead) with S. Grace\u000d\u000a      Stirling, who was appointed\u000d\u000a      to a two-year Knowledge Transfer Partnership associate position funded by\u000d\u000a      the ESRC and One\u000d\u000a      North East (2008-2010, &#163;105,000).\u000d\u000a    There was little extant methodology available to provide forecasts for\u000d\u000a      call-centre volumes and\u000d\u000a      daily patterns of arrival for this kind of problem. Forecasting is\u000d\u000a      difficult because (1) there are\u000d\u000a      structural daily effects &#8212; the call centre is closed during normal working\u000d\u000a      hours, but open Saturday\u000d\u000a      and Sunday; (2) there are thought to be different typical caseloads for\u000d\u000a      different days of the week,\u000d\u000a      e.g. Friday night heavy loads; (3) there are calendar effects, e.g.\u000d\u000a      national holidays, Easter,\u000d\u000a      Christmas, New Year; (4) general seasonal effects; (5) occasional epidemic\u000d\u000a      effects, such as\u000d\u000a      swine flu; (6) drift in demand because of population change or policy\u000d\u000a      changes affecting service\u000d\u000a      provision or organisation. The forecasting methodologies tend to fall into\u000d\u000a      two camps; one is quite\u000d\u000a      finely detailed, but over-tuned to special features of the application.\u000d\u000a      Another is insufficiently\u000d\u000a      detailed, for example standard methodology such as X-ARIMA-12 as used by\u000d\u000a      the US\u000d\u000a      Government for economic forecasting, which cannot be used for within-day\u000d\u000a      forecasts. In seeking\u000d\u000a      a completely general and widely applicable solution, the Durham team chose\u000d\u000a      to model the daily\u000d\u000a      volume of calls using regression models to identify and adjust for the\u000d\u000a      crucial factors. Daily arrival\u000d\u000a      rates were modelled using a mix of generalized smoothing and a Poisson\u000d\u000a      approximation to\u000d\u000a      arrival times. The resulting models offer excellent forecasts both for\u000d\u000a      day-to-day forecasts and for\u000d\u000a      patterns of call arrivals through the day.\u000d\u000a    A second research theme was identified in the summer of 2009 as a result\u000d\u000a      of the Swine Flu\u000d\u000a      epidemic. In addition to forecasting normal patient volumes, there was an\u000d\u000a      urgent need to react to\u000d\u000a      increasing volumes because of swine flu. The Durham team thus developed\u000d\u000a      diagnostic models to\u000d\u000a      disentangle the normal load from the epidemic load. In collaboration with\u000d\u000a      the North East England\u000d\u000a      Strategic Health Authority, thresholds were thereby determined for use\u000d\u000a      within a pandemic\u000d\u000a      escalation framework for North East England healthcare institutions in\u000d\u000a      order to prepare the\u000d\u000a      institutions for their responsibilities during pandemics, and to trigger\u000d\u000a      response modes as\u000d\u000a      pandemics escalate.\u000d\u000a    The methodology developed in this body of work turned out to be of\u000d\u000a      relevance beyond the health-care\u000d\u000a      sector, and was also used in the analysis of cycling data for the\u000d\u000a      Department of Transport\u000d\u000a      and SUSTRANS.\u000d\u000a    "},{"CaseStudyId":"27172","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"3144096","Name":"Norway"}],"Funders":["Engineering and Physical Sciences Research Council","Natural Environment Research Council"],"ImpactDetails":"\u000a    As a result of [1], the Durham Statistics group was contracted by\u000a      Energy SciTech Ltd (a consultancy firm to the oil industry with which we\u000a      have a long research and consultancy connection, and who provided the\u000a      reservoir information from which we developed the case study) to write the\u000a      inference engine for the system ENABLE which optimizes asset management\u000a      planning and reduces costs by accelerating the history matching process\u000a      and improving reservoir understanding. Operators now use ENABLE worldwide\u000a      for a better understanding and measurement of uncertainty in reservoir\u000a      production performance estimates. Using a Bayesian statistical framework\u000a      and emulator for the model, based on conventional reservoir simulations,\u000a      ENABLE provides companies with a rapid understanding of production\u000a      behaviour and the creation of robust uncertainty forecasts.\u000a    The contract1 for the software was very precise, in\u000a      specifying that we would implement all of the procedures described in the\u000a      research case study paper described in section 3. For example,\u000a      under Testing and Acceptance of the Software, it was stated that\u000a    \"The performance of each Module of the Prototype will be deemed\u000a      acceptable to both parties if it can be shown that a level of\u000a      functionality similar to that demonstrated in Pressure matching for\u000a        hydrocarbon reservoirs: a case study in the use of Bayes linear\u000a        strategies for large computer experiments (case Studies in Bayesian\u000a      Statistics, III, New York: Springer) where consistent with Schedule A, has\u000a      been achieved.\"\u000a    Energy Scitech, and thus ENABLE, was acquired by Roxar in 2006. Energy\u000a      Scitech existed on sales of ENABLE and services related to ENABLE. When\u000a      acquired by Roxar it had revenues of [text removed for publication]\u000a      (1 Nov 2004 - 31 Oct 2005) and [text removed for publication] (1\u000a      Nov 2005 - 31 Dec 2006)2. Since then, and throughout the\u000a      impact period 2008 - 2013, the reach and significance of ENABLE has\u000a      continued to grow: in the period 2008 - 2013 active users included [text\u000a        removed for publication]3, and the total turnover\u000a      attributed to ENABLE sales by Roxar from 1 Jan 2008 to 30 Sept 2012 was [text\u000a        removed for publication]4. In the UK there are\u000a      currently [text removed for publication] staff working full-time\u000a      on ENABLE, and an estimated [text removed for publication] who\u000a      spend a proportion of their time on the project3.\u000a    Roxar AS is an international provider of products and associated services\u000a      for reservoir management and production optimisation in the upstream oil\u000a      and gas industry. It is headquartered in Stavanger, Norway and operates in\u000a      19 countries with around 900 employees. Roxar offers software for\u000a      reservoir interpretation, modelling and simulation, as well as\u000a      instrumentation for well planning, monitoring and metering. Roxar was\u000a      acquired by Emerson Electric Company in April 2009 and is now part of the\u000a      Emerson Process Management Group.\u000a    This is how Roxar currently describe the role of ENABLE5:\u000a    \"History Matching and Uncertainty Quantification. The Roxar ENABLE\u000a      solution history matches numerous geological scenarios to create\u000a      simulation models that are fully consistent with their underlying\u000a      geological interpretation (unlike many current 3D modelling workflows).\u000a      RMS, Tempest and ENABLE provide E&amp;P companies with a statistical\u000a      framework for a rapid understanding of production behaviour and the\u000a      creation of robust estimates from a shared earth model. The result is more\u000a      informed technical and economic decision-making and a better\u000a      quantification of uncertainty.\"\u000a    This product has been very successful and as specified in the contract1\u000a      the University has received a royalty each year to date, based on the\u000a      sales of the commercial product. In particular, in the period Jan 2008 -\u000a      Sept 2012, [text removed for publication] has been received in\u000a      royalties6. Roxar have continued to develop the product,\u000a      and to pay royalties to the University, and are committed to ensuring that\u000a      it remains current: from June 2011 - May 2012 Roxar had a consultancy\u000a      contract for [text removed for publication] with the University to\u000a      consider ways to improve the application of ENABLE for complex oil\u000a      reservoirs. This was deemed successful and led to a further consultancy\u000a      contract being agreed, for [text removed for publication] from\u000a      June 2012 to May 20156.\u000a    As attested by Roxar7, the software developed in 1998\u000a      on the basis of [1] remains a key feature of the Tempest ENABLE\u000a      product. The project to integrate ENABLE into Roxar's Tempest suite was\u000a      completed in 20128; it brings the Durham developed\u000a      emulator and subsequent enhancements to a wider global community.\u000a    In 2012, Tempest ENABLE was chosen as the uncertainty platform for the\u000a      multinational oil company Statoil9. This has secured\u000a      funding for the next three years for [text removed for publication]\u000a      software developers in Roxar. The methodology developed in [1],\u000a      together with the quality of the associated work delivered by Durham, was\u000a      pivotal to the awarding of this contract7.\u000a    The Tempest ENABLE product has over [text removed for publication]\u000a      active users as of November 20127, and can be regarded\u000a      as one of the most successful uncertainty platforms in the oilfield\u000a      marketplace, to the point that it has created a new business activity.\u000a      Roxar states:7\u000a    \"The fast Durham developed emulator enables a rigorous statistical\u000a      approach to uncertainty quantification. ENABLE was the first commercial\u000a      product to allow this in the oil and gas industry. Since ENABLE's release\u000a      several competitors have emerged taking advantage of the approach ENABLE\u000a      has validated.\"\u000a    The success of ENABLE is confirmed by user feedback. To give one example\u000a      from the Tempest ENABLE product website10, Dr\u000a      Curt-Albert Schwietzer of GSC Reservoir Simulation &amp; Reserves states:\u000a    \"After using ENABLE for over one year I can say that reservoir simulation\u000a      without ENABLE is unimaginable.\"\u000a    ","ImpactSummary":"\u000a    ENABLE is a history matching and uncertainty assessment software system\u000a      for the oil industry, whose inference engine was produced by the Durham\u000a      Statistics group, based on their research on uncertainty quantification\u000a      for complex physical systems modelled by computer simulators. The system\u000a      optimizes asset management plans by careful uncertainty quantification and\u000a      reduces development costs by accelerating the history matching process for\u000a      oil reservoirs, resulting in more informed technical and economic\u000a      decision-making. ENABLE was acquired by Roxar ASA in 2006 and current\u000a      users include the multinational oil company Statoil. From January 2008 to\u000a      September 2012 (the most recent set of figures) the turnover attributed to\u000a      ENABLE was [text removed for publication].\u000a    ","ImpactType":"Technological","Institution":"\u000a    UDur: University of Durham\u000a    ","Institutions":[{"AlternativeName":"Durham (University of)","InstitutionName":"University of Durham","PeerGroup":"B","Region":"North East","UKPRN":10007143}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"5128638","Name":"New York"}],"References":"\u000a    The underpinning work for this research was funded by EPSRC, under the\u000a      Complex Stochastic Systems initiative on the grant `Bayes linear\u000a      strategies for history matching for hydrocarbon reservoirs' (1993-95,\u000a      value &#163;105,000).\u000a    One of the outcomes of this research was the invitation to present the\u000a      work at the third Case Studies in Bayesian Statistics meetings at Carnegie\u000a      Mellon University in October 1995. The Case Study format allowed the\u000a      Durham group to make a complete presentation of all of the aspects of its\u000a      research. It appeared subsequently as\u000a    \u000a[1] P.S. Craig, M. Goldstein, A.H. Seheult, J.A. Smith (1997), Pressure\u000a        matching for hydrocarbon reservoirs: a case study in the use of Bayes\u000a        linear strategies for large computer experiments (with discussion),\u000a      in Case Studies in Bayesian Statistics, vol. III, eds. C. Gastonis et al.\u000a      37-93. Springer-Verlag. New York, ISBN 0-387-94990-9.\u000a      doi:10.1007\/978-1-4612-2290-3_2.\u000a    \u000aOnly a few invitations to present a case study at this meeting are made,\u000a      with the intention of producing very careful and detailed case studies for\u000a      a small number of substantial applications. The printed version contains a\u000a      discussion by Galway and Lucas, from the RAND Corporation, who refer to\u000a      this paper as \"superb\" and \"outstanding\". As of 25\/10\/2013, the paper had\u000a      73 citations on google scholar.\u000a    Grant applications following from [1] have been very successful,\u000a      resulting in support for postdocs on grants from NERC (under the RAPID\u000a      programme and the PURE programme), Leverhulme (the Durham Tipping Points\u000a      project), EPSRC (The Managing Uncertainty for Complex Models consortium,\u000a      funded by the Basic technology initiative) and industry (for example the\u000a      Joint Inversion using Bayesian Analysis project, funded by an oil\u000a      consortium). (Total value of these grants to the Department: &#163;903,000.)\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"},{"Level1":"8","Level2":"6","Subject":"Information Systems"}],"Sources":"\u000a    \u000a      Initial contract between Energy SciTech Ltd and Durham University,\u000a        1999.\u000a      Audited financial statements for Energy SciTech for the period\u000a        2004&#8212;2006, immediately prior to its acquisition by Roxar AS.\u000a      Supporting document from Roxar sent to Durham 27\/09\/2013.\u000a      Audited financial statements for Energy Scitech Ltd covering the\u000a        period 2008 - 2012.\u000a      The Roxar Software brochure, accessible from http:\/\/www2.emersonprocess.com\/en-US\/brands\/roxar\/reservoirmanagement\/\u000a          Pages\/ReservoirManagementSoftware.aspx\u000a\u000a      Royalty invoices and consultancy contracts between Roxar and Durham\u000a        University.\u000a      Supporting document from Roxar sent to Durham 27\/11\/2012.\u000a      The press release for the project to integrate ENABLE into Roxar's\u000a        Tempest suite: http:\/\/www2.emersonprocess.com\/en-US\/news\/pr\/Pages\/1210-Tempest.aspx\u000a\u000a      The press release describing the contract with Statoil to develop\u000a        Tempest ENABLE: http:\/\/www2.emersonprocess.com\/en-US\/news\/pr\/Pages\/1210-Statoil-Enable.aspx\u000a\u000a      Customer quote listed under `Proven results' on the Tempest ENABLE\u000a        product website http:\/\/www2.emersonprocess.com\/en-us\/brands\/roxar\/reservoirmanagement\/reservoirsimulation\/pages\/tempestenable.aspx\u000a      \u000a    \u000a    ","Title":"\u000a    History matching and uncertainty assessment in the oil and gas industry\u000a    ***Redacted version with confidential information removed***\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The Durham Statistics Group has a long track record of work on the\u000a      quantification of uncertainty for large and complex physical systems\u000a      modelled by computer simulators. Much of this work was developed in the\u000a      context of history matching for oil reservoirs. This problem may be\u000a      described as follows. Reservoir simulators are key tools to help oil\u000a      companies manage production for oil reservoirs. The simulator takes as\u000a      inputs a description of the reservoir (rock properties, fault distribution\u000a      and so on) and returns as outputs the well performance (pressure profiles,\u000a      production, water cut and so forth). As the appropriate input choices are\u000a      not known, a priori, the input space must be searched to identify choices\u000a      of reservoir specification for which the output of the simulator at the\u000a      wells corresponds, to an acceptable degree, to recorded historical\u000a      behaviour. This process is termed history matching. It is difficult and\u000a      challenging because the input space is high dimensional and the evaluation\u000a      of the simulator, for a single choice of inputs, takes many hours.\u000a    The Durham group devised a detailed Bayesian solution [1] to this\u000a      problem, based on building an emulator for the simulator. This is a\u000a      probabilistic surrogate for the simulator, giving both a fast\u000a      approximation to the simulator and a measure of uncertainty related to the\u000a      quality of the approximation. In order to construct the emulator, the\u000a      group solved novel problems in prior elicitation, joint Bayesian modelling\u000a      for multi-level versions of the simulator, experimental design for\u000a      multi-level computer experiments, and diagnostic evaluation for the\u000a      resulting construction. This emulator, in combination with an uncertainty\u000a      representation for the difference between the simulator and the reservoir,\u000a      formed the basis of the history matching methodology that we developed.\u000a      This proceeds by eliminating those parts of the input space for which\u000a      emulated outputs were too far from observed history, according to a\u000a      collection of appropriate implausibility measures, then re-sampling and\u000a      re-emulating the simulator within the reduced space, eliminating further\u000a      parts of the input space and continuing in this fashion. This is a form of\u000a      iterative global search aimed at finding all of the input regions\u000a      containing good matches to history.\u000a    This work was developed under EPSRC funding, from 1993 to 1995, and was\u000a      published in 1997 [1]. The key researchers for the work were\u000a      Michael Goldstein, Peter Craig and Allan Seheult, all permanent members of\u000a      the Durham Statistics group at that time, and James Smith, PDRA on the\u000a      grant from 1993 to 1995.\u000a    "},{"CaseStudyId":"27175","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    Suppose you want to buy a laptop. What you might do is go to a search\u000d\u000a      engine and type laptop (known as the keyword) in the\u000d\u000a      search field. If you are using Google, you will see sponsored links\u000d\u000a      (adverts), as well as the results of natural search. The adverts appear\u000d\u000a      because the keyword you typed is one that a company has paid Google to\u000d\u000a      display whenever someone searches for it. Broadly what happens is that if\u000d\u000a      you then click on the advert, the sponsoring company pays Google a small\u000d\u000a      amount. This is called Pay Per Click (PPC). Total paid-for search spend\u000d\u000a      was &#163;3.1 billion in the UK in 2012, growing at about 14% per year1,\u000d\u000a      with Google's market share about 88% in December 20122.\u000d\u000a    The amount a company is prepared to pay per click determines the position\u000d\u000a      of the sponsor's advert in the list. A count is made of the number of\u000d\u000a      times the advert appears &#8212; these are called impressions. Once you\u000d\u000a      click on the link, you are taken to the sponsor's website. This is a visit.\u000d\u000a      Once within that website, you might visit several pages and might buy\u000d\u000a      something from the sponsoring company. Your browsing history is recorded\u000d\u000a      using cookies stored on your computer, and is usually aggregated\u000d\u000a      with the browsing histories of thousands of customers visiting the\u000d\u000a      sponsor's website from various sources. This results in an enormous volume\u000d\u000a      of data. For example, for every keyword for a given sponsor, there is a\u000d\u000a      daily summary which reports the total number of adverts shown, the\u000d\u000a      resulting number of visits, the total amount paid by the sponsoring\u000d\u000a      company for the clicks, the total sales revenue generated by the clicks,\u000d\u000a      and so forth.\u000d\u000a    One of Summit Media's tasks is to maximise the flow of traffic to a\u000d\u000a      client. Traditionally they manage the keywords and PPC bids using their\u000d\u000a      expertise. However, there were many relationships which were poorly\u000d\u000a      understood. For example, what is the relationship between number of\u000d\u000a      impressions and PPC price, and does this lead to increased sales revenue\u000d\u000a      for the sponsor. Further, how does this vary by keyword and how does it\u000d\u000a      vary by sponsor?\u000d\u000a    The Durham research has addressed these issues by modelling the complex\u000d\u000a      interrelationships, simulating from the constructed models, and then\u000d\u000a      providing individual predictions of expected revenue (together with\u000d\u000a      uncertainties) for every advert position for every keyword in the client's\u000d\u000a      portfolio. The portfolio is then optimized through an algorithm developed\u000d\u000a      by the Durham team. Several different feasible solutions are available,\u000d\u000a      depending on the risk attitude of the client &#8212; for example higher returns\u000d\u000a      might be associated with greater uncertainty. Many of the issues posed\u000d\u000a      challenges which the group has needed to overcome: for example, the\u000d\u000a      massive scale of the problem requires special statistical modelling.\u000d\u000a    This has a very major impact for the collaborating company in several\u000d\u000a      important ways. Firstly, far greater precision and certainty in their\u000d\u000a      operations, as previous practice focused only on default pricing and rough\u000d\u000a      heuristics, together with sharper human judgements for a tiny fraction of\u000d\u000a      keywords. Secondly, a much reduced need for human intervention and so a\u000d\u000a      significant saving in personnel costs. Thirdly, the ability to obtain much\u000d\u000a      better revenue streams for clients because of understanding the\u000d\u000a      uncertainties involved and through optimization of budget. Fourthly, the\u000d\u000a      ability to attract more clients because it will be able to offer both a\u000d\u000a      cheaper service and a more precisely quantified expected revenue.\u000d\u000a    Summit Media, based in Hull, approached Durham University while it was\u000d\u000a      seeking a higher education research partner to enhance the services it\u000d\u000a      offered to clients. During the resulting KTP period, from 2010-13, the\u000d\u000a      company... [text removed for publication].\u000d\u000a    Summit Media listed the main achievements of the KTPs as (i) the\u000d\u000a      \"development of a market leading solution (Forecaster) which has\u000d\u000a      revolutionised the way pay PPC advertising is optimised\"; (ii) the\u000d\u000a      creation of a new department (Insight, now headed by one of the KTP\u000d\u000a      associates as a full time employee); (iii) tighter and automated control\u000d\u000a      over key business drivers; (iv) client retention and (v) The acquisition\u000d\u000a      of new clients... [text removed for publication].\u000d\u000a    In the KTP final report3, Summit Media says that it\u000d\u000a      \"definitely wouldn't be making similar progress\" without the Durham KTPs\u000d\u000a      and that \"the main outcome from the project, Forecaster, is seen as the\u000d\u000a      single most important piece of innovation that Summit has developed over\u000d\u000a      the past few years... \" [text removed for publication].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This work revolutionizes the way in which pay-per-click (PPC, internet\u000d\u000a      search) advertising is optimised. Recent estimates suggest that 80% of\u000d\u000a      companies market online, yielding $30 billion of revenue to companies\u000d\u000a      managing the process. The research of the Durham team gives a\u000d\u000a      collaborating company, Summit Media Ltd, a substantial lead in statistical\u000d\u000a      tools to provide PPC optimisation. This lead is giving the company a more\u000d\u000a      dominant position in the UK and is leading it to pursue international\u000d\u000a      business more aggressively, as an industry leader. The company confirms\u000d\u000a      that implementation of the research has generated new business worth\u000d\u000a      millions of pounds and enabled the employment of additional staff.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    UDur: University of Durham\u000d\u000a    ","Institutions":[{"AlternativeName":"Durham (University of)","InstitutionName":"University of Durham","PeerGroup":"B","Region":"North East","UKPRN":10007143}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a[1] Wooff, D. A. &amp; Jamalzadeh, A. (2013), Robust and\u000d\u000a        scale-free effect sizes for non-Normal two-sample comparisons, with\u000d\u000a        applications in e-commerce, Journal of Applied Statistics 40,\u000d\u000a      2495-2515, doi: 10.1080\/02664763.2013.818625,\u000d\u000a      http:dx.doi.org\/10.1080\/02664763.2013.818625.\u000d\u000a    \u000a\u000a[2] Wooff, D. A. &amp; Anderson, J. (2013), Time-weighted\u000a        multi-touch attribution and channel relevance in the customer journey to\u000d\u000a        online purchase, Journal of Statistical Theory and Practice, doi:\u000d\u000a      10.1080\/15598608.2013.862753, http:\/\/dx.doi.org\/10.1080\/15598608.2013.862753.\u000d\u000a    \u000a\u000a[3] Wooff, D. A. &amp; Anderson, J. (2013), Time-weighted\u000a        attribution of revenue to multiple e-commerce marketing channels in the\u000d\u000a        customer journey, http:\/\/dro.dur.ac.uk\/11458.\u000d\u000a    \u000a\u000a[4] Wooff, D. A. &amp; Anderson, J. (2013), Inferring\u000d\u000a        marketing channel relevance in the customer journey to online purchase,\u000d\u000a      http:\/\/dro.dur.ac.uk\/11459.\u000d\u000a    \u000a\u000a[5] Wooff, D.A. (2013), Optimization of pay-per-click bidding\u000d\u000a        for search engines. Invited conference presentation, GDRR 2013:\u000d\u000a      Third Symposium on Games and Decisions in Reliability and Risk, Kinsale,\u000d\u000a      County Cork, Ireland, July 8th - 10th, 2013.\u000d\u000a    \u000a\u000a[6] Anderson, J. (2013), Weighted Attribution of Revenue to\u000d\u000a        Marketing Channels within a Customer Purchase Path, and Jamalzadeh,\u000a        A. (2013), Measuring PPC Brand Keywords Incrementality Using Geo\u000d\u000a        Experiments, talks at the Young Statisticians' Meeting YSM2013,\u000d\u000a      Imperial College London, July 4th-5th 2013.\u000d\u000a    \u000aReferences [2-6] explain work undertaken through the parallel KTP\u000d\u000a      awards described in section 2 above. The first of these awards, which has\u000d\u000a      now finished, was given the highest grade of `Outstanding' by the KTP\u000d\u000a      grading panel for its achievement in meeting KTP's Objectives.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000d\u000a    \u000d\u000a      Internet Advertising Bureau UK 2012 Full Year Digital Adspend\u000d\u000a        Factsheet, available from\u000d\u000a        http:\/\/www.iabuk.net\/research-subjects\/online-adspend\u000d\u000a        (copy accessed 17\/10\/2013).\u000d\u000a      Google market share data from Experian Hitwise, available at\u000d\u000a        http:\/\/www.experian.co.uk\/marketing-services\/news-december-2012-search.html\u000d\u000a        (copy accessed 17\/10\/2013).\u000d\u000a      Technology Strategy Board: Knowledge Transfer Partnership KTP007499\u000d\u000a        Partners Final Report.\u000d\u000a      [text removed for publication].\u000d\u000a      [text removed for publication].\u000d\u000a      [text removed for publication].\u000d\u000a      [text removed for publication].\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Statistical Modelling For Digital Marketing\u000d\u000a    *** Redacted version with confidential information removed***\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2645425","Name":"Hull"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    This work has been carried out by David Wooff (lead) with PDRAs Dr\u000d\u000a      Jillian Anderson and Dr Amin Jamalzadeh. Funding for the research is two\u000d\u000a      parallel EPSRC KTP awards 2009-2013 (&#163;211,000) and 2012-2015 (&#163;138,000)\u000d\u000a      with industrial funding from Summit Media Ltd (SM), the initial end-user.\u000d\u000a      A summary of staff engaged in the project, with dates, is:\u000d\u000a    Prof David Wooff, Feb 2010 &#8212;\u000d\u000a    Dr Jillian Anderson, Feb 2010 - Sep 2011, Jul 2012-Jun 2013 (now a\u000d\u000a      full-time company employee)\u000d\u000a    Dr Amin Jamalzadeh, Sep 2011 - Jul 2012 (now a full-time company\u000d\u000a      employee)\u000d\u000a    The research concerns exploitation of vast amounts of information\u000d\u000a      routinely collected as part of SM's digital marketing activities. SM\u000d\u000a      manages these activities for clients such as major UK online retailers\u000d\u000a      -Argos, John Lewis, Homebase, and so forth. The purpose of the research is\u000d\u000a      to optimize Pay-Per-Click (PPC) positioning and pricing in order to\u000d\u000a      maximise customer traffic and revenue subject to constraints, for example\u000d\u000a      to maximize expected revenue subject to a stated budget and cost-of-sale\u000d\u000a      target. Traditionally this has been thought almost impossibly large scale.\u000d\u000a      A typical client account may require optimization over hundreds of\u000d\u000a      thousands of keywords (search terms), each of which has a multivariate\u000d\u000a      data history going back months or years.\u000d\u000a    The major advances in this research are:\u000d\u000a    \u000d\u000a      a logical structuring of the problem and its decomposition into\u000d\u000a        conditionally independent components;\u000d\u000a      statistical modelling of each component using classical and Bayesian\u000d\u000a        statistical methods;\u000d\u000a      development of methods to carry out in reasonable time millions of\u000d\u000a        regression models as required;\u000d\u000a      predictive simulation of revenues, generated from the underlying\u000d\u000a        models;\u000d\u000a      optimization of the allocation of budget to keywords, together with\u000d\u000a        their positions;\u000d\u000a      multi-layered diagnostic checking to compare observed to predicted\u000d\u000a        outcomes.\u000d\u000a    \u000d\u000a    The most recent (Jan 2012) relevant academic publication in this area\u000d\u000a      suggests a methodology which is appropriate for up to three keywords. By\u000d\u000a      contrast, the Durham group's recent test-bed dataset contains 115000\u000d\u000a      keywords and optimization procedures which take around one hour on a\u000d\u000a      desktop. The Durham methodology is implemented in bespoke software,\u000d\u000a      written by the company, which calls code written by us in the statistical\u000d\u000a      language R. This software went live in November 2012.\u000d\u000a    The methodology is explained in references [2-6] cited in section\u000d\u000a      3 below, which build on the preliminary work published in [1]. It\u000d\u000a      is internationally leading and in an area, digital commerce, flagged as an\u000d\u000a      EPSRC theme.\u000d\u000a    "},{"CaseStudyId":"28137","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Context\u000d\u000a      Matrix functions underpin computer simulations used to inform decision\u000d\u000a      making across almost all industrial sectors. The need for high-quality\u000d\u000a      simulation results makes it increasingly important to enable the accurate\u000d\u000a      and efficient computation of matrix functions. Our research has provided\u000d\u000a      the first reliable and efficient algorithm for computing general matrix\u000d\u000a      functions, as well as the state of the art algorithms for the matrix\u000d\u000a      exponential and matrix logarithm, which play a key role in control theory.\u000d\u000a      MATLAB, from The MathWorks (head office in Natick, MA, USA, with other\u000d\u000a      offices including in Cambridge, UK) is a commercial mathematical software\u000d\u000a      system with almost 2 million users in industry and academia. Its\u000d\u000a      associated toolboxes and Simulink, an environment for multidomain\u000d\u000a      simulation, run within MATLAB and provide specialist capabilities that are\u000d\u000a      widely used in the engineering and finance sectors. The Numerical\u000d\u000a      Algorithms Group (NAG) Ltd Library is a product of NAG Ltd. (head office\u000d\u000a      in Oxford UK), which has as its core business the production and sale of\u000d\u000a      mathematical and statistical software components, including libraries.\u000d\u000a    Pathways to Impact\u000d\u000a      Higham and Tisseur have long-standing professional relationships with\u000d\u000a      colleagues from NAG and The MathWorks, going back to the 1980s. As a\u000d\u000a      result of these links, the research reported here has been strongly\u000d\u000a      influenced by the needs of NAG and The MathWorks; and both companies have\u000d\u000a      been in a position to rapidly incorporate the resulting algorithms into\u000d\u000a      their products. A Knowledge Transfer Partnership (2010-2013) funded a\u000d\u000a      full-time KTP Associate to translate algorithms for matrix functions\u000d\u000a      developed by Higham and Tisseur into NAG products.\u000d\u000a    Reach and Significance of the Impact\u000d\u000a      The research has reached a large proportion of the relevant community\u000d\u000a      through its incorporation in widely used commercial and open source\u000d\u000a      software, including R [S4], Eigen [S5] and Scipy (Python) [S6]. Indeed,\u000d\u000a      the majority of end users of matrix functions will be using the algorithms\u000d\u000a      that are the focus of this case study, albeit unknowingly. We have found\u000d\u000a      the non-academic impact of open source implementations of our algorithms\u000d\u000a      impossible to quantify, and so instead quantify the impacts of\u000d\u000a      implementations made by the commercial companies The Mathworks and NAG.\u000d\u000a    The MathWorks\u000d\u000a      The Mathworks state that [S2] \"it is difficult to measure direct\u000d\u000a        economic impact of the ... algorithms, it is inarguable that your\u000d\u000a        contribution to Matlab and MathWorks' product line has been significant\u000d\u000a        in accelerating the pace of engineering and science in a wide variety of\u000d\u000a        industries from aerospace and automotive to financial and\u000d\u000a        pharmaceutical.\" Nonetheless all copies of MATLAB licensed within\u000d\u000a      the assessment period contained our algorithms and the annual revenue of\u000d\u000a      The Mathworks in 2012 was US$750 million [S7], mostly due to licensing\u000d\u000a      income.\u000d\u000a    Specific direct impacts for the company are detailed below:\u000d\u000a    \u000d\u000a      The MathWorks incorporated the new algorithm [2] in MATLAB 7.0. From\u000d\u000a        2004 to 2011 this was the only numerically reliable code for general\u000d\u000a        matrix functions available in any commercial software, giving The\u000d\u000a        MathWorks a commercial advantage and its customers a unique capability.\u000d\u000a      A crucial aspect of MATLAB functionality is its ability to detect ill\u000d\u000a        conditioned problems and warn the user of ill conditioning. From MATLAB\u000d\u000a        6.0 (2000) the condition number of a matrix is estimated using the block\u000d\u000a        algorithm of [3], which provides more reliable estimates than the\u000d\u000a        previous point algorithm. This block algorithm has continuing impact as\u000d\u000a        a key component of MATLAB, used in the ordinary differential equation\u000d\u000a        solvers and fitting functions used in the Statistics and Curve Fitting\u000d\u000a        Toolboxes [S2].\u000d\u000a      Our algorithms for the principal matrix logarithm [4, Alg. 11.11] and\u000d\u000a        matrix exponential [1] are \"an intrinsic part of MathWorks' Control\u000d\u000a          Systems Toolbox, underpinning the reliability of MATLAB in control\u000d\u000a          engineering applications\" [S2].\u000d\u000a      The implementation of our matrix exponential algorithm in MATLAB\u000d\u000a        function expm led to a faster, more accurate code and better\u000d\u000a        maintainability of the code base due to the use of an M-file rather\u000d\u000a        than C [S1].\u000d\u000a    \u000d\u000a    NAG\u000d\u000a    Specific direct impacts for NAG within the assessment period are detailed\u000d\u000a      below:\u000d\u000a    \u000d\u000a      A new code f01ecf (f01ecc) introduced in Mark 22 (2011) of the NAG\u000d\u000a        Library for computing the matrix exponential using the algorithm of [1].\u000d\u000a      A new family of NAG Library codes for evaluating general matrix\u000d\u000a        functions by the algorithm of [2] and the matrix logarithm using the\u000d\u000a        algorithm of [4, Alg. 11.11] was released in Mark 23 (spring 2012).\u000d\u000a      A NAG Library code f0y4d for matrix norm estimation using the block\u000d\u000a        algorithm of [4] was released at Mark 24 (spring 2013).\u000d\u000a    \u000d\u000a    The new matrix function codes are helping NAG gain more revenue across\u000d\u000a      all sectors, both from new customers and from existing users who are more\u000d\u000a      likely to renew with the new features. The total additional income\u000d\u000a      resulting from these codes, from new licenses plus renewals, is estimated\u000d\u000a      by NAG as &#163;100k during the project (January 2010 - July 2013), &#163;240k in\u000d\u000a      the year after project completion, with a further doubling for each of the\u000d\u000a      following two years, which is very significant for a company with about 70\u000d\u000a      FTE staff [S3].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Functions of matrices underpin many of the fundamental algorithms used in\u000d\u000a      the finance, engineering and pharmaceutical industries. Our research has\u000d\u000a      provided the state of the art algorithms for computing the most common\u000d\u000a      matrix functions. The algorithms have been implemented commercially in\u000d\u000a      MATLAB (with almost 2 million users) and the Numerical Algorithms Group\u000d\u000a      (NAG) Ltd Library; as well as in open source packages such as R and Eigen.\u000d\u000a      Software companies have enjoyed direct impact through improved products\u000d\u000a      and increased revenues, whilst the greater indirect impact is for the\u000d\u000a      users of the software, leading to acceleration in the pace of science and\u000d\u000a      engineering worldwide.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Manchester (UoM)\u000d\u000a    ","Institutions":[{"AlternativeName":"Manchester (University of)","InstitutionName":"University of Manchester","PeerGroup":"A","Region":"North West","UKPRN":10007798}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    The research has been published in leading, high impact numerical\u000d\u000a      analysis journals and has been widely cited. Citations are shown for the\u000d\u000a      Web of Science (WOS) and Google Scholar (GS) as of 9-8-13.\u000d\u000a    \u000a[1] N. J. Higham, The scaling and squaring method for the matrix\u000d\u000a      exponential revisited, SIAM J. Matrix Anal. Appl., 26(4):1179-1193, 2005.\u000d\u000a      DOI . [WoS: 98, GS: 201].\u000d\u000a      doi.org\/10.1137\/04061101X\u000d\u000a    \u000a\u000a[2] P. I. Davies and N. J. Higham, A Schur-Parlett algorithm for\u000d\u000a      computing matrix functions, SIAM J. Matrix Anal. Appl., 25(2):464-485,\u000d\u000a      2003. DOI . [WoS: 44, GS: 96].\u000d\u000a      doi.org\/10.1137\/S0895479802410815\u000d\u000a    \u000a\u000a[3] N. J. Higham and F. Tisseur. A block algorithm for matrix 1-norm\u000d\u000a      estimation, with an application to 1-norm pseudospectra. SIAM J. Matrix\u000d\u000a      Anal. Appl., 21(4):1185-1201, 2000. DOI . [WoS: 29, GS: 83]. doi.org\/10.1137\/S0895479899356080\u000d\u000a    \u000aThe following research monograph includes treatments of [1-3] and\u000d\u000a      contains a new algorithm for the matrix logarithm.\u000d\u000a    \u000a[4] N. J. Higham, Functions of Matrices: Theory and Computation, SIAM,\u000d\u000a      2008. xx+425 pages, . [WoS: 195, GS: 494]\u000d\u000a      http:\/\/books.google.co.uk\/books?hl=en&amp;lr=&amp;id=S6gpNn1JmbgC&amp;oi=fnd&amp;pg=PR3&amp;dq=N.+J.+Higha\u000a        m,+Functions+of+Matrices:+Theory+and+Computation,+SIAM,+2008\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000d\u000a    [S1] Email from Principal Numerical Analyst at The MathWorks, August 19,\u000d\u000a      2005.\u000d\u000a      (Supports claim of increased speed and accuracy and better maintainabiilty\u000d\u000a      of the MATLAB function expm)\u000d\u000a    [S2] Letter from Manager, The MathWorks, May 21, 2013.\u000d\u000a      (Supports the claims of increasing functionality and accuracy in MATLAB)\u000d\u000a    [S3] Letter from VP for Sales and Marketing, NAG, July 22, 2013.\u000d\u000a      (Supports financial claims and the importance of the algorithms for NAG)\u000d\u000a    [S4] Vincent Goulet, Christophe Dutang, Martin Maechler, David Firth,\u000d\u000a      Marina Shapira, Michael Stadelmann, R package expm: Matrix exponential,\u000d\u000a      Computation of the matrix exponential and related quantities. expm-developers@lists.R-forge.R-project.org,\u000d\u000a      July 2010 onwards. http:\/\/cran.r-project.org\/web\/packages\/expm\/index.html\u000d\u000a      (Demonstrates that algorithm is implemented in R)\u000d\u000a    [S5] J. Niesen, Matrix functions module for Eigen C++ template library\u000d\u000a      for linear algebra. http:\/\/eigen.tuxfamily.org\/dox\u000d\u000a      (Demonstrates that algorithm is implemented in Eigen)\u000d\u000a    [S6] Comments at http:\/\/nickhigham.wordpress.com\/2013\/03\/10\/siam-conference-on-computational-science-and-engineering-2013.\u000d\u000a      (Demonstrates that algorithm is implemented in Python)\u000d\u000a    [S7] Matlab factsheet (Confirms the revenue of The Mathworks in 2012)\u000d\u000a    ","Title":"\u000d\u000a    Mathematical Software for Computing Matrix Functions\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2640729","Name":"Oxford"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The impact is based on research that took place in the unit of assessment\u000d\u000a      from 1993-date, with the first major publication in 2000. The key\u000d\u000a      researchers were\u000d\u000a    Professor Nick Higham (1993-date).\u000d\u000a      Professor Fran&#231;oise Tisseur (1997-date: PDRA '98-`00, Lecturer `01-'05, SL\u000d\u000a      '06-'07, Reader '09-'11, Professor '12-date).\u000d\u000a      Dr Philip Davies (PhD student '98-'00, PDRA '01-'04).\u000d\u000a    The algorithms on which this case study is based are:\u000d\u000a    \u000d\u000a      Scaling and squaring algorithm for the matrix exponential [1]. This\u000d\u000a        algorithm improves on earlier scaling and squaring algorithms by\u000d\u000a        choosing the amount of scaling and the Pad&#233; degree dynamically based on\u000d\u000a        the matrix norm and precomputed constants determined from backward error\u000d\u000a        analysis.\u000d\u000a      Inverse scaling and squaring algorithm for the matrix logarithm [4,\u000d\u000a        Sec. 11.5]. This algorithm uses a dynamic choice of number of square\u000d\u000a        roots and Pad&#233; degree.\u000d\u000a      The Schur-Parlett algorithm for computing arbitrary matrix functions\u000d\u000a        [2].\u000d\u000a      An algorithm for matrix 1-norm estimation [3] that generalizes to\u000d\u000a        block form and provides more accurate estimates than an earlier\u000d\u000a        algorithm of Higham that is the basis of the LAPACK condition number\u000d\u000a        estimator.\u000d\u000a    \u000d\u000a    "},{"CaseStudyId":"28138","Continent":[{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"}],"Funders":[],"ImpactDetails":"\u000a    Context\u000a    Correlation plays a fundamental part in any financial model dealing with\u000a      more than one asset e.g. CAPM, Markowitz portfolio theory, the LIBOR\u000a      market model, or any multi-asset extension of the various market models\u000a      used. However estimating correlation is notoriously difficult: in practice\u000a      market data is often missing or stale; different assets are sampled at\u000a      different time points (e.g. some daily and others weekly); and the data\u000a      may even contain arbitrages due to averaging of bid and offer quotes. As a\u000a      result, estimated correlation matrices are frequently not positive\u000a      semidefinite, with the consequence that variances are negative, which is\u000a      forbidden by definition. There is a real need to correct the non-positive\u000a      semi-definiteness of estimated correlation matrices, while at the same\u000a      time staying true to the correlations implied by the market data &#8212; in\u000a      other words, not changing the estimated matrix too much, but just enough\u000a      to make it mathematically sound. This is what the NCM routines do, and\u000a      this is why they are crucial in the financial industry and in many other\u000a      contexts.\u000a    Prior to our research, ad-hoc methods had been developed in an attempt to\u000a      compute the nearest correlation matrix, but none were guaranteed to\u000a      compute it and some were not even guaranteed to converge. Our work has\u000a      provided fast and reliable algorithms for computing the unique nearest\u000a      correlation matrix in the Frobenius norm.\u000a    Pathways to Impact\u000a    The Numerical Algorithms Group (NAG) Ltd. has been a world leader in the\u000a      development and distribution of numerical software for more than 40 years,\u000a      and has offices in Oxford and Manchester and subsidiaries in Chicago,\u000a      Tokyo and Taipei. Higham has long-standing professional relationships with\u000a      colleagues from NAG going back to the 1980s. As a result of these links,\u000a      the research reported here has been strongly influenced by the needs of\u000a      NAG and indeed NAG provided partial funding for Borsdorf`s PhD studies in\u000a      Manchester. The Manchester researchers have assisted in translation of the\u000a      algorithm in [2] into NAG software, and NAG has been in a position to\u000a      rapidly incorporate the software into their products.\u000a    In order to maximise impact in the financial sector, NAG is devoting\u000a      significant effort to promote the developed software using marketing\u000a      material, brochures [S1], web videos, targeted site visits and publication\u000a      via seminars and Trade shows. Higham has joined NAG representatives on\u000a      visits to BNP-Paribas (London) and Barclays Capital (London), both in June\u000a      2011, at Standard and Poors (New York, December 2011), and at\u000a      Credit-Suisse and Morgan Stanley in March 2013. He gave seminars on the\u000a      NCM problem at all these venues and at the Institute of Actuaries in March\u000a      2013.\u000a    In addition, the algorithm of [1] is freely available in implementations\u000a      in MATLAB, R, and SAS.\u000a    Reach and Significance of the Impact\u000a    The key user base is the financial industry and at least six of the top\u000a      ten Tier 1 Investment Banks are using NAG nearest correlation matrix (NCM)\u000a      codes [S3] that implement the preconditioned Newton algorithm [2]. The\u000a      codes can be directly incorporated into customers' existing financial\u000a      models because they can be called from Fortran or C, from MATLAB via the\u000a      NAG Toolbox for MATLAB, and from Excel. In particular, the link to Excel\u000a      allows the research to reach practitioners who are not necessarily\u000a      programmers, such as those from the actuarial community [S2].\u000a    Due to the importance of the codes, and based on feedback from NAG\u000a      customers, NAG has worked with Higham and Borsdorf to improve the codes,\u000a      gaining a factor two increase in the speed since their first introduction\u000a      [S4].\u000a    The nearest correlation matrix codes in the NAG Library are helping NAG\u000a      to gain more revenue across all sectors, both from new customers and from\u000a      existing users who are more likely to renew with the new features. The\u000a      estimated total additional income to NAG as a consequence of the inclusion\u000a      of our codes into the library (new licenses plus renewals) in the period\u000a      January 2010-July 2013 is &#163;250,000 [S3]; this is very significant for a\u000a      company with about 70 FTE staff.\u000a    Commercial sensitivity means that the Tier 1 Banks are unwilling to\u000a      disclose any revenue increases as a consequence of using these algorithms,\u000a      but, for context, Morgan Stanley report assets of US$347 billion under\u000a      management or supervision (June 30, 2013) [S5], and Credit Suisse report\u000a      CHF 408 billion under management at the end of 2011 [S6]. Thus, if used in\u000a      management of 1% of the assets (a conservative estimate) our algorithms\u000a      would still inform financial decisions on the scale of billions of\u000a      dollars.\u000a    ","ImpactSummary":"\u000a    Correlation matrices play a key role in financial modelling, but their\u000a      empirical construction (based on the actual statistical data) may lead to\u000a      negative variances, which can lead to complete failure of a model. Our\u000a      research has resulted in algorithms for efficiently computing the unique nearest\u000a        correlation matrix (NCM) that does not yield negative variances. The\u000a      most direct impact is to Numerical Algorithms Group (NAG) Ltd, whose\u000a      library sales and renewals have been increased by an estimated &#163;250k\u000a      following the inclusion of our NCM codes. Further impact is to NAG\u000a      clients, including the Tier 1 Investment Banks, with at least six of the\u000a      top ten [e.g., Credit Suisse and Morgan Stanley] known to be using the new\u000a      NAG nearest correlation matrix codes, leading to improved reliability of\u000a      their financial models.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Manchester (UoM)\u000a    ","Institutions":[{"AlternativeName":"Manchester (University of)","InstitutionName":"University of Manchester","PeerGroup":"A","Region":"North West","UKPRN":10007798}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"7280290","Name":"Taipei"},{"GeoNamesId":"5128638","Name":"New York"},{"GeoNamesId":"4887398","Name":"Chicago"}],"References":"\u000a    The research has been published in leading high impact numerical analysis\u000a      journals. Citations are shown for the Web of Science (WOS) and Google\u000a      Scholar (GS) as of 9-8-13. Reference [1] has been the most downloaded\u000a      full-text PDF in IMA J. Numer. Anal. every year since 2008 (source: IMA\u000a      Journal of Numerical Analysis Publisher's Reports, OUP, 2009-2012).\u000a    \u000a[1] N. J. Higham, Computing the nearest correlation matrix &#8212; A problem\u000a      from finance. IMA J. Numer. Anal., 22(3):329-343, 2002. DOI 10.1093\/imanum\/22.3.329.\u000a      [WoS: 121, GS: 304].\u000a    \u000a\u000a[2] R. Borsdorf and N. J. Higham, A preconditioned Newton algorithm for\u000a      the nearest correlation matrix. IMA J. Numer. Anal., 30(1):0 94-107, 2010.\u000a      DOI 10.1093\/imanum\/drn085.\u000a      [WoS: 9, GS: 25].\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000a    [S1] Nearest Correlation Matrix brochure\u000a      www.nag.co.uk\/IndustryArticles\/Nearest_Correlation_Matrix.pdf,\u000a      23 July, 2010\u000a      (Evidence of promotion by NAG of the software based on our algorithms)\u000a    [S2] John Holden and Jacques duToit, Numerical software &amp; tools for\u000a      the actuarial community,\u000a      www.nag.co.uk\/market\/seminars\/nag_actuarial_community_sep2012.pdf;\u000a      accessed 12-1-13.\u000a      (Demonstrates that research is being made available to actuarial\u000a      community)\u000a    [S3] Letter from Vice President for Sales, NAG, July 22, 2013.\u000a      (Supports finanical claims about NAG)\u000a    [S4] Matrix functions, correlation matrices, and news from NAG,\u000a      www.nag.co.uk\/Market\/events\/craig_nag_wilmott_2011.pdf,\u000a      talk at NAG Quant Event, London, by Dr Craig Lucas, 2011; accessed 9-8-13.\u000a      (Demonstrates increased speed of the code)\u000a    [S5] http:\/\/www.morganstanley.com\/about\/ir\/earnings_releases.html\u000a      (Reports assets managed by Morgan Stanley)\u000a    [S6] https:\/\/www.credit-suisse.com\/who_we_are\/en\/asset_management.jsp\u000a      (Reports assets managed by Credit Suisse) \u000a    ","Title":"\u000a    Mathematical Software for the Nearest Correlation Matrix\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"},{"GeoNamesId":"2640729","Name":"Oxford"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The impact is based on research that took place in the unit of assessment\u000a      from 2000-date, with the first major publication in 2002. The key\u000a      researchers were\u000a      Professor Nick Higham (2000-date).\u000a      Dr Rudiger Borsdorf (MSc student and PhD student, 2006-2012).\u000a    The correlation matrix is a classical concept from statistics and\u000a      specifies the degree of linear dependence between various random\u000a      quantities (e.g. various financial assets). The aim of the research was to\u000a      develop efficient algorithms for computing the nearest correlation matrix\u000a      to an arbitrary matrix. The algorithms on which this case study is based\u000a      are:\u000a    \u000a      The alternating projections algorithm [1]. This is the first algorithm\u000a        proven to compute the global minimizer of the distance to the set of\u000a        correlation matrices.\u000a      The preconditioned Newton algorithm [2], which takes the Newton method\u000a        derived in [Hou-Duo Qi and Defeng Sun, A quadratically convergent Newton\u000a        method for computing the nearest correlation matrix, SIAM J. Matrix\u000a        Anal. Appl., 28(2):360-385, 2006] and constructs an efficient and\u000a        reliable algorithm, by using preconditioned iterative solution of the\u000a        Newton equations, carefully avoiding roundoff problems in the line\u000a        search, and making other improvements explained in [2].\u000a    \u000a    "},{"CaseStudyId":"28139","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Context\u000a    With constant growth of air traffic and greater security\u000a      concerns, RTT scanning for airport baggage has become increasingly\u000a      important in recent years. Current aviation hold baggage screening systems\u000a      can deliver high speed and resolution, but with a high false alarm rate.\u000a      Rapiscan Systems is a global leader in high quality security inspection\u000a      solutions and advanced threat identification techniques, with more than\u000a      70,000 systems installed worldwide. Rapiscan reported 33% growth in\u000a      revenue in 2012 and the parent company, OSI, had annual revenue of $793m\u000a      in 2012 [S1].\u000a    The Rapiscan RTT scanner combines high speed, high resolution and a low\u000a      false alarm rate at the first level of screening. The key to Rapiscan\u000a      RTT's innovative design is that, unlike other CT baggage screening\u000a      systems, it does not use a moving gantry &#8212; one revolving around the bag,\u000a      typically taking 12 to 15 views. Instead, Rapiscan RTT adopts a new\u000a      technology: a stationary array consisting of a very large number of micro\u000a      X-ray emitters, which captures tens of thousands of views of a typical\u000a      bag, therefore generating images with significantly better resolution in\u000a      all planes at much greater speed, and keeping maintenance costs low [S2].\u000a      However, reconstructing the three-dimensional images from the planar data\u000a      collected by the new hardware design required a different approach from\u000a      the state of the art at the time.\u000a    Pathways to the Impact\u000a    Realizing the need for major technological improvements in the prototype\u000a      for RTT scanning, Rapiscan approached our research group with the aim to\u000a      develop novel theory and more accurate algorithms for such scanning. The\u000a      underpinning research has been funded jointly by Rapiscan and EPSRC\u000a      including 3 KTA projects and two sponsored PhD studentships. This\u000a      industry-driven project ensured rapid incorporation of the novel\u000a      algorithms into the new generation of RTT scanners [S3].\u000a    Dr Wadeson had a KTA secondment with Rapiscan for a year in order to\u000a      enable technology transfer; specifically this was to use Geant4 modelling\u000a      within Rapiscan. Dr Thompson has completed a KTA concept and feasibility\u000a      study to implement his reconstruction algorithms on a GPU, again with\u000a      Rapiscan, and a further year's secondment to Rapiscan to complete GPU\u000a      implementation and systems integration.\u000a    Reach and Significance of the Impact\u000a    By improving the accuracy of the reconstruction, using our algorithms and\u000a      scatter correction, the RTT80 and RTT110 scanners are better able to\u000a      detect threat objects and reduce the false alarm rate [S2]. By determining\u000a      the presence and position of a threat, our industry-leading resolution and\u000a      reconstruction process delivers optimal performance for the detection of\u000a      materials in configurations typically difficult to detect [S2], which\u000a      improves baggage handling efficiency, while lowering operational costs.\u000a      RTT can also measure density levels in liquids, identifying threat liquids\u000a      and alerting the operator to potentially concealed explosives [S2].\u000a    The prototype RTT80 scanner won The Engineer Innovation and Technology\u000a      Award 2010 in the Defence and Security category [S4] and was the first\u000a      ultra-high speed system to pass ECAC's Standard 3 threat detection test\u000a      [S5], the highest such standard. Rapiscan already has substantial orders\u000a      for RTT80 systems and has begun manufacturing in a UK factory [S3]. The\u000a      larger RTT110 (110 cm diameter aperture in the scanner) was field-tested\u000a      in Manchester Airport in 2010 [S6] and has recently been ECAC (Standard 3)\u000a      certified (2013) [S7]. From 2014, all European airports must install\u000a      Standard 3 certified equipment for any new hold-baggage screening\u000a      equipment and the RTT110 will form part of the new terminal 4 ECAC\u000a      Standard 3 test facility at Heathrow airport [S8].\u000a    The new scanners capture detailed 3-D images at speeds of between 1200 to\u000a      1800 bags per hour, compared to legacy CT rates of 600 bags per hour, and\u000a      at higher resolution. When comparing the RTT maintenance costs with\u000a      current CT technology, due to need for fewer machines, higher throughput\u000a      and the revolutionary stationary gantry design, annual maintenance cost\u000a      savings of 35-50% can be realized [S2]. Moreover, the significantly\u000a      improved resolution, combined with built-in diagnostic tools, allows the\u000a      operator to quickly resolve alarms [S2].\u000a    In 2011 Rapiscan received a US$20m order to provide multiple units of the\u000a      new RTT scanners to Manchester Airport Group [S9]. The RTT110 machines\u000a      have been deployed in Seattle airport. RTT systems have European\u000a      regulatory approval and RTT110 systems are currently being manufactured in\u000a      the UK [S3].\u000a    The new scanners are \"positioned to change the aviation security\u000a        screening industry\" [S5] and \"are making air travel safer\"\u000a      [S6], which will reduce risks to the security of all nation states.\u000a    ","ImpactSummary":"\u000a    RTT (Real Time Tomography) scanning systems for airport baggage are\u000a      becoming increasingly important due to growing air traffic and greater\u000a      security concerns. Prior to our research, Rapiscan, a leading producer of\u000a      baggage scanners, had been unable to make full use of the hardware in\u000a      their latest generation of scanner prototypes. Our novel theory and image\u000a      reconstruction algorithms are now a core part of a commercially successful\u000a      3D scanner that is significantly faster and more accurate than previous\u000a      generations. The two models, RTT80 and large RTT110, have been approved by\u000a      regulatory authorities and have already been field trialled at Manchester\u000a      Airport and deployed at Seattle airport, with further US$20m orders\u000a      placed.\u000a    The research and impact described herein was flagged in the citation for\u000a      the UoM's 2013 Queen's Anniversary Prize for Higher and Further Education\u000a      for its work in imaging techniques to support advanced materials and\u000a      manufacturing.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Manchester (UoM)\u000a    ","Institutions":[{"AlternativeName":"Manchester (University of)","InstitutionName":"University of Manchester","PeerGroup":"A","Region":"North West","UKPRN":10007798}],"Panel":"B         ","PlaceName":[],"References":"\u000a    The research has been published in the leading journal in the field:\u000a      Inverse Problems, as well as in refereed conference proceedings.\u000a    Three key references:\u000a    [1] Granted Patent: M. M. Betcke, W.R.B. Lionheart, E.J. Morton. X-ray\u000a      tomography system i.e. real-time tomography system, for generating\u000a      three-dimensional image of object, has controller generating\u000a      three-dimensional image from reconstructed images on surface. Patent\u000a      Number: WO2011008787-A1, 2011.\u000a      https:\/\/www.google.com\/patents\/WO2011008787A1?cl=en&amp;dq=WO2011008787\u000a    \u000a[2] M. M. Betcke, W. R.B. Lionheart. Multi-Sheet Surface Rebinning\u000a      Methods for Reconstruction from Asymmetrically Truncated Cone Beam\u000a      Projections. I: Approximation and Optimality, Inverse Problems, vol 29,\u000a      115003, 2013. doi:10.1088\/0266-5611\/29\/11\/115003\u000a    \u000a\u000a[3] M. M. Betcke, W. R.B. Lionheart. Multi-Sheet Surface Rebinning\u000a      Methods for Reconstruction from Asymmetrically Truncated Cone Beam\u000a      Projections. II: Reconstruction on Multi-Sheet Surfaces and Axial\u000a      Deconvolution.Inverse Problems, vol 29, 115004, 2013. doi:10.1088\/0266-5611\/29\/11\/115004\u000a    \u000aAdditional references\u000a    [4] W. Thompson, W. R. B. Lionheart, E. J. Morton. Optimization of the\u000a      Source Firing Pattern for X-Ray Scanning Systems. US Patent application\u000a      US20120219116 A1, 2011.\u000a      https:\/\/www.google.com\/patents\/US20120219116\u000a    \u000a[5] N. Wadeson, E. J. Morton and W. R. B. Lionheart. Scatter in an\u000a      Uncollimated X-Ray CT Machine Based on a Geant4 Monte Carlo Simulation.\u000a      SPIE Medical Imaging 2010: Physics of Medical Imaging, 15-18 February\u000a      2010, San Diego, USA. doi 10.1117\/12.843981\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"},{"Level1":"9","Level2":"6","Subject":"Electrical and Electronic Engineering"}],"Sources":"\u000a    [S1] OSI Systems Annual Report 2012 (supports financial claims)\u000a    [S2] Rapiscan RTT Brochure (supports technical claims about the RTT\u000a      device)\u000a    [S3] Letter from Technical Director, Rapiscan Systems (supports our\u000a      involvement with the project)\u000a    [S4] RTT80 Baggage Scanner, The Engineer, 3rd Dec 2010.\u000a      http:\/\/www.theengineer.co.uk\/awards\/rtt80-baggage-scanner\/1006324.article\u000a      (supports claim that the work won Engineer Innovation and Technology\u000a      Award)\u000a    [S5] Press releases for RTT80 certification and `Rapiscan RTT Ultra-High\u000a      Speed Checked Baggage Scanner Sets New Bar with EU Security\u000a      Certification', April 2012. Rapiscan:\u000a      http:\/\/www.rapiscansystems.com\/en\/press_releases\/article\/rapiscan_rtt_ultra_high_speed_checked_baggage_scanner_sets_new_bar_with_eu\u000a      (supports claim of RTT80 passing ECAC Standard 3 test)\u000a    [S6] http:\/\/www.manchestereveningnews.co.uk\/news\/local-news\/a-clear-winner-the-high-speed-new-scanner-850776\u000a      (supports claim of field-testing in Manchester airport)\u000a    [S7] RTT110 `OSI Systems Receives Highest Level European Approval for\u000a      Large Tunnel RTT&#8482; Ultra-High Speed Checked Baggage Inspection System',\u000a      June 2013. OSI:\u000a      http:\/\/investors.osi-systems.com\/releasedetail.cfm?ReleaseID=773801\u000a      (supports claim of RTT110 passing ECAC Standard 3 test)\u000a    [S8] http:\/\/www.baglogix.com\/projects-clients\/terminal-4-apv-ecac-standard-3-test-facility-london-heathrow-airport\/\u000a      (supports claim that RTT110 will be used at Heathrow Terminal 4)\u000a    [S9] Press release: Rapiscan Systems Receives $20M Order for Advanced\u000a      Checked Baggage Inspection Systems Based on RTT, June 6, 2011.\u000a      http:\/\/www.rapiscansystems.com\/en\/press_releases\/articlerapiscan_systems_receives_20m_order_for_advanced_checked_baggage_inspection\u000a      (supports claim that Manchester airport have placed order for RTT\u000a      machines)\u000a    ","Title":"\u000a    X-ray tomography for airport security\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The impact is based on on-going research conducted in the unit of\u000a      assessment since 2007.\u000a    The key researchers were:\u000a    Professor William Lionheart (1999 - date)\u000a      Dr Marta Betcke (PDRA 2007-2010)\u000a      Dr Nicola Wadeson (PhD student 2007 - 2010, KTA intern 2012, Home Office\u000a      funded PDRA 2013-)\u000a      Dr Will Thompson (PhD student 2007 - 2010, KTA intern 2010 - 2013, PDRA\u000a      2013-)\u000a    The prototype Rapiscan RTT system used a novel geometric configuration of\u000a      X-ray emitters and detectors, which required the development of new\u000a      algorithms to reconstruct three-dimensional images of the scanned objects\u000a      from the data collected. This geometry is the result of the design of the\u000a      RTT, with no moving parts, which dramatically increases scanning speed and\u000a      reduces maintenance. The key insight, which formed the basis of a patent\u000a      [1], was that multi-surface rebinning, a new algorithm we developed for\u000a      the RTT, would lead to a faster, more accurate and more reliable\u000a      reconstruction algorithms [2,3]. In addition, our research characterised\u000a      the instability inherent in reconstruction from RTT data, proved a\u000a      uniqueness result that demonstrates the RTT collects sufficient data for a\u000a      reliable reconstruction, and lead to an optimal source firing order that\u000a      improves resolution of the image [4]. A final important step was the use\u000a      of Monte Carlo simulation to calculate scattered X-rays and thus develop a\u000a      scatter correction procedure and simulation software that has been used as\u000a      an aid to the design and development of RTT systems including the new\u000a      RTT110 [5].\u000a    "},{"CaseStudyId":"28140","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    Context\u000d\u000a    Federal-Mogul Corporation (FM) is a leading global supplier of products\u000d\u000a      and services to the world's\u000d\u000a      manufacturers and servicers of vehicles and equipment in the automotive,\u000d\u000a      light, medium and\u000d\u000a      heavy-duty commercial, marine, rail, aerospace, power generation and\u000d\u000a      industrial markets. The\u000d\u000a      company's products and services enable improved fuel economy, reduced\u000d\u000a      emissions and\u000d\u000a      enhanced vehicle safety.\u000d\u000a    In 2008 FM approached Dr. Alexander Donev for help in reviewing\u000d\u000a      statistical issues in their\u000d\u000a      experimental practices both in R&amp;D and in the manufacture of friction\u000d\u000a      materials for use in disc\u000d\u000a      brakes. Poor repeatability and prediction from experiments using different\u000d\u000a      material compositions\u000d\u000a      had led them to suspect that there could be something missing in the\u000d\u000a      existing statistical\u000d\u000a      methodology.\u000d\u000a    Prior to the work described in this case study, FM had used open source\u000d\u000a      and commercial software\u000d\u000a      (e.g. R, JMP) to design and analyse their experimental studies. However,\u000d\u000a      there is no such software\u000d\u000a      that can be used to construct designs for studying variability, and\u000d\u000a      therefore the company was using\u000d\u000a      only the so-called six-sigma methods. The benefits that such methods can\u000d\u000a      bring were already\u000d\u000a      exhausted, and more advanced statistical methods were required to achieve\u000d\u000a      the necessary\u000d\u000a      reduction in manufacturing variability.\u000d\u000a    Pathways to the Impact\u000d\u000a    Since approaching Dr Donev in 2008, Federal-Mogul has funded the PhD\u000d\u000a      studentship of Liam\u000d\u000a      Brown, as well as an sKTP project with Sergio Loeza-Serrano. The very\u000d\u000a      nature of the projects\u000d\u000a      provided a natural pathway to implementing the results in the company's\u000d\u000a      technological processes.\u000d\u000a      The novel models have been reported directly to Federal-Mogul [2] along\u000d\u000a      with computer codes that\u000d\u000a      implement the new statistical methods. Consequently, the new experimental\u000d\u000a      designs have become\u000d\u000a      standard procedures within the company. The collaboration remains active\u000d\u000a      and an EPSRC grant\u000d\u000a      application, with financial commitment of &#163;250k from Federal-Mogul, has\u000d\u000a      been submitted.\u000d\u000a      An additional pathway to broader impact is provided via open source\u000d\u000a      versions of the computer\u000d\u000a      codes that implement the new statistical methods, which have been made\u000d\u000a      available for other\u000d\u000a      academic and non-academic users.\u000d\u000a    Reach and Significance of the Impact\u000d\u000a    The research has had a significant influence on the practices within FM.\u000d\u000a      Comparative analysis of\u000d\u000a      variation data from a recent production study and the methodology proposed\u000d\u000a      [1] showed that\u000d\u000a      statistically the same conclusions could have been reached with only 12%\u000d\u000a      of the experimental trials\u000d\u000a      originally used by the company, leading to considerable savings in R&amp;D\u000d\u000a      and in manufacturing [S1].\u000d\u000a      As a result, the new experimental designs have been adopted as standard\u000d\u000a      procedures within\u000d\u000a      Federal-Mogul [S1].\u000d\u000a    The first commercial application of the research was for modelling and\u000d\u000a      optimising the production of\u000d\u000a      a new design of the disc pad for Ford P415 SUV vehicles (typical US sales\u000d\u000a      of 40,000 per year\u000d\u000a      since 2010 [S2]). Response surface models based on the design were\u000d\u000a      verified experimentally and\u000d\u000a      found to be highly accurate. Disc brake pad production costs are due to\u000d\u000a      combination of raw\u000d\u000a      material costs and manufacture. The manufacturing costs are particularly\u000d\u000a      sensitive to process cycle\u000d\u000a      times, which determine the production rates on the allocated presses.\u000d\u000a      Application of the new\u000d\u000a      statistical methodology identified optimal process conditions that lead to\u000d\u000a      a 30% reduction in overall\u000d\u000a      cycle time compared to that previously obtained with a conventional\u000d\u000a      production engineering\u000d\u000a      approach. This reduction in cycle time was critical to the economic\u000d\u000a      viability of the product and lead\u000d\u000a      to substantial cost savings for the company [S1].\u000d\u000a    Further analysis of the new model identified multiple solutions that made\u000d\u000a      it possible to select\u000d\u000a      production process conditions to yield the desired specification with\u000d\u000a      minimum inherent variation. To\u000d\u000a      combine these aspects of maximising production efficiency with maximizing\u000d\u000a      product quality\u000d\u000a      requires accurate and detailed process models, which the company had been\u000d\u000a      unable to achieve\u000d\u000a      without such an effective experimental design [S1].\u000d\u000a    Following the outstanding success of this Ford project, the use of same\u000d\u000a      basic design has become a\u000d\u000a      standard procedure within Federal Mogul for introducing new friction\u000d\u000a      products into production.\u000d\u000a      During 2012, based on this experimental design, new disc brake pad\u000d\u000a      products for Audi, BMW,\u000d\u000a      Ford, GM, Mercedes Benz and VW have been optimised for production [S1].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Novel statistical methods were developed in order to address the needs of\u000d\u000a      Federal-Mogul\u000d\u000a      Corporation (FM), an innovative and diversified $6.9bn global component\u000d\u000a      supplier to vehicle\u000d\u000a      manufacturers, with a broad range of customers in the industrial sector.\u000d\u000a      During 2012, the research\u000d\u000a      underpinned the production of new disc brake pad products for Audi, BMW,\u000d\u000a      Ford, GM, Mercedes\u000d\u000a      Benz and VW. The research has already resulted in significant benefits for\u000d\u000a      the company by\u000d\u000a      improving the manufacturing process, allowing it to be optimised to a mean\u000d\u000a      specification, and by\u000d\u000a      reducing the production cycle time by 30%.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Manchester (UoM)\u000d\u000a    ","Institutions":[{"AlternativeName":"Manchester (University of)","InstitutionName":"University of Manchester","PeerGroup":"A","Region":"North West","UKPRN":10007798}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    The research has been published in a leading journal in computation and\u000d\u000a      statistics, the official\u000d\u000a      journal of the International Association of Statistical Computing [1].\u000d\u000a    \u000a[1] Loeza-Serrano, S. and Donev, A. N. (2012). Construction of\u000d\u000a      Experimental Designs for\u000d\u000a      Estimating Variance Components. Computational Statistics and Data\u000d\u000a        Analysis,\u000d\u000a      http:\/\/dx.doi.org\/10.1016\/j.csda.2012.10.008.\u000d\u000a    \u000a\u000a[2] Technical Report prepared for Federal-Mogul.\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a    [S1] Letter from Federal Mogul supporting claim that FM have adopted the\u000d\u000a      new experimental\u000d\u000a      designs; that cost savings have been made; production cycle times reduced;\u000d\u000a      and that the designs\u000d\u000a      have been used in brake pad production for the vehicle manufacturers\u000d\u000a      listed in the case.\u000d\u000a    [S2] http:\/\/www.goodcarbadcar.net\/2011\/01\/ford-expedition-sales-figures.html\u000d\u000a      (Gives yearly US sales figures for the Ford Expedition (P415 SUV))\u000d\u000a    ","Title":"\u000d\u000a    Novel Statistical Methods for Optimising Production of Disc Brake Pads\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The impact is based on research that has been conducted at The University\u000d\u000a      of Manchester since\u000d\u000a      2008 and was carried out by\u000d\u000a    Dr. Alexander Donev (Senior Lecturer, September 2007 &#8212; present)\u000d\u000a    Mr. Liam Brown (PhD student 2010 &#8212; present) [fully funded by FM],\u000d\u000a    Mr. Sergio Loeza-Serrano (PhD student 2009 &#8212; present), [undertook sKTP\u000d\u000a      with FM in 2011]\u000d\u000a    The principal outcomes of the research were:\u000d\u000a    \u000d\u000a      the development of novel statistical methodology to inform the design\u000d\u000a        of physical\u000d\u000a        experiments used in product testing. The key insights were to take prior\u000d\u000a        information into\u000d\u000a        account when evaluating sources of variability in the manufacturing\u000d\u000a        process; and to develop\u000d\u000a        a novel graphical representation of the results to aid decision making\u000d\u000a        [1]. The new methods\u000d\u000a        allow the design of experiments in the most efficient and economical way\u000d\u000a        [2].\u000d\u000a      the development of new statistical models for studies of products\u000d\u000a        constructed from different\u000d\u000a        mixtures of materials, as well as methods for designing experiments to\u000d\u000a        collect the data\u000d\u000a        required by such models. The key insight was to use models that are\u000d\u000a        nonlinear in\u000d\u000a        parameters, formulated in terms of estimable flexible regressors. These\u000d\u000a        can describe fast\u000d\u000a        changing and localized effects which had not been possible with standard\u000d\u000a        statistical models\u000d\u000a        for mixture experiments. Nonlinear models of this kind had never\u000d\u000a        been proposed before\u000d\u000a        and are potentially applicable to a huge range of other industrial\u000d\u000a        processes.\u000d\u000a    \u000d\u000a    "},{"CaseStudyId":"28141","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Royal Society"],"ImpactDetails":"\u000a    Context\u000a    An oilfield reservoir has soured when an increased concentration of\u000a      hydrogen sulphide H2S is observed in production fluids. This foul smelling\u000a      and corrosive `sour gas' is toxic to life and liable to cause cracking and\u000a      pitting of susceptible steels [S1], leading to the failure of hydrocarbon\u000a      pipelines both on land and subsea which can have a catastrophic impact\u000a      upon both the environment and the Operators' (oil companies) public\u000a      reputation. Reservoirs are categorised as either `sweet' or `sour', and\u000a      the origin of H2S from an erstwhile sweet reservoir is linked to secondary\u000a      oil recovery, in which (sea)water is injected into the reservoir to\u000a      maintain pressure.\u000a    Prior to our mathematical modelling the phenomenon of souring was not\u000a      understood and, accordingly, remediation measures were poorly targeted,\u000a      expensive and usually ineffective. Only after the advent of our robust\u000a      physical and numerical model was it possible for the Operator to identify\u000a      the appropriate, targeted, prevention methodology and\/or remediation\u000a      treatment.\u000a    By 1993, the results of a previous statistical analysis of Oilfield\u000a      Reservoir Souring had demonstrated strong correlations between seawater\u000a      injection parameters and subsequent souring; more specifically those\u000a      conditions which created a `downhole' environment in which anaerobic\u000a      sulphate-reducing bacteria could live. This was reinforced by field\u000a      evidence. Based on these data, our research led to the development of a\u000a      biogenic souring model to explain unexpected increasing concentrations of\u000a      H2S in produced fluids.\u000a    Pathways to Impact\u000a    The initial modelling work was carried out as part of the UK Oilfield\u000a      Reservoir Souring Programme at CAPCIS (Dr Robert Eden) and UMIST (Prof\u000a      Patrick Laycock) and funded by the oil and gas industry. The very nature\u000a      of the project ensured a swift introduction of the souring prediction\u000a      algorithms based on our models into the industry. The early model\u000a      forecasts were subsequently validated through field evidence and lead to\u000a      the model's wider dissemination and use.\u000a    In 2000 Rawwater Engineering Company Limited was established and the\u000a      originators of the DynamicTVS&#169; model, Laycock and Eden were granted title\u000a      to commercialise it. The research has followed a continuous development\u000a      line funded by Operator money initially through the multiclient programmes\u000a      and the UK Department of Energy, and then by single client studies.\u000a    The sponsors are now well represented in the Forbes 500 companies, and\u000a      beyond, including BP, BG Energy Holdings Limited, Braspetro Petrobras\u000a      Internacional S.A., ConocoPhillips, Chevron Corporation, Hoang Long JOC,\u000a      Ithaca Energy UK Ltd, Lundin ASA, M&#230;rsk Olie og Gas AS, Nexen Inc,\u000a      Petro-Canada Inc., Rhodia UK Ltd, Saudi Aramco, Statoil ASA, Tullow Oil UK\u000a      (Ltd) and Yara International ASA.\u000a    Much of the work is necessarily confidential and so has unfortunately not\u000a      been available for publication. However, the track record and positive\u000a      reputation of the model and the technology has enabled Rawwater\u000a      Engineering Company Limited to market the model's on-going development and\u000a      exploitation through industry contacts and its internet presence [S2].\u000a    Reach and Significance of the Impact\u000a    The founding of Rawwater was a direct consequence of the understanding\u000a      gained from our research. The company has built and operates the World's\u000a      largest facility to study biogenic souring under simulated reservoir\u000a      conditions using a suite of pressurised, flowing, sand-packed bioreactors,\u000a      whose design was directly influenced by the research.\u000a    The only two souring forecasting models in common usage in the Oil\u000a      Industry today are Rawwater's DynamicTVS&#169;, a direct consequence of our\u000a      research, and a more recent model SourSim&#169;, released in 2006, which uses\u000a      many of the ideas that we developed, and data from Rawwater's bioreactor\u000a      suite. SourSim&#169; is only available to a restricted set of Operators for\u000a      incorporation in their existing reservoir simulation packages, but\u000a      DynamicTVS&#169; is commercially available to the entire global community (and\u000a      therefore all potential beneficiaries of the research).\u000a    The research has had impact in the period 2008 - 2013 by providing\u000a      revenue for Rawwater, with 10 associated jobs, and also by providing the\u000a      Operators with assistance in souring management. One of the `big five'\u000a      Operators reports that today the cost of souring and its control consumes\u000a      1\/3 of the production budget, and this same company has set aside $50M for\u000a      souring research over the next ten years [S3].\u000a    With respect to impact of the DynamicTVS&#169; model upon the Operators, the\u000a      figure is difficult to calculate. For example, the specification of sour\u000a      service materials, which resist cracking, will add 10% to the pipeline\u000a      inventory costs (typically an additional US$1m per well). Since 2008,\u000a      DynamicTVS&#169; has been used by 12 Operators to either save costs against\u000a      unnecessary treatments or to identify appropriate mechanisms to control\u000a      souring (costs can be in the range of US$1M &#8212; US$10m per annum per\u000a      `typical' reservoir). This, in turn, has a direct impact upon `lifting\u000a      costs', oil quality and profit margin, but the precise details are\u000a      commercially sensitive and not available for public scrutiny. Nonetheless\u000a      an approximate calculation gives a net saving over the REF period of 12\u000a      operators x 6 years x US$5m \/year = US$360m.\u000a    We can also describe an illustrative case study: in 2012 the output of\u000a      the model was used to demonstrate that an Operator should not take\u000a      delivery of a US$100m sulphate-removal plant allocated to the field for\u000a      biologenic souring control. The model forecast that the field would not go\u000a      sour and hence this sulphate-removal technology was inappropriate, despite\u000a      the Contractor's insistence to the contrary. At a high level internal\u000a      Operator meeting, the results of the model were endorsed by Contractor\u000a      representatives and the sulphate-removal plant remained unsold [S2, S3].\u000a    ","ImpactSummary":"\u000a    Accurate forecasting of oilfield souring is vital for the oil industry.\u000a      Souring (an increase in concentration of toxic hydrogen sulphide)\u000a      increases the cost of maintenance and repairs four-fold, and reduces the\u000a      value of crude oil by up to 20%. Our research led to development of the\u000a      World's first predictive models for the souring of oil-wells. The\u000a      implementation of the models in software, commercialised by Rawwater\u000a      Engineering Limited, provides accurate forecasts and has been validated by\u000a      major Operators, including BP, Shell and Chevron. Since 2008, twelve\u000a      different Operators have used the models in souring management, which has\u000a      led to an estimated cost saving of US$360m since 2008.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Manchester (UoM)\u000a    ","Institutions":[{"AlternativeName":"Manchester (University of)","InstitutionName":"University of Manchester","PeerGroup":"A","Region":"North West","UKPRN":10007798}],"Panel":"B         ","PlaceName":[],"References":"\u000a    The research was published as a report [1] for the Health and Safety\u000a      Executive, London, which was peer reviewed by international senior\u000a      scientists working in the Oil Industry; and in a peer-reviewed conference\u000a      proceedings [2] of the fifth international symposium on chemistry in the\u000a      oil industry, organised by the Industrial Division of the Royal Society of\u000a      Chemistry. Citations are shown for Google Scholar (GS) as of 30-9-13.\u000a    \u000a1. Oilfield Reservoir Souring, R.D. Eden, P.J. Laycock &amp; M.\u000a      Fielder, HMSO, OTH series: London, September 1993. [GS: 23].\u000a      http:\/\/www.hse.gov.uk\/research\/othpdf\/200-399\/oth385.pdf\u000a    \u000a\u000a2. Oilfield Reservoir Souring &#8212; Model Building and Pitfalls, R.D. Eden,\u000a      P.J. Laycock &amp; G. Wilson, pp 179-188, in Recent Advances in Oilfield\u000a      Chemistry, pub Royal Society of Chemistry, London, 1994. [GS: 3] ISBN: 0851869416\u000a        \/ 0-85186-941-6\u000a    \u000a\u000a3. Dynamic TVS&#169; Software Package, available through Rawwater Engineering\u000a      Company Ltd, http:\/\/www.rawwater.com\/souring\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"9","Level2":"14","Subject":"Resources Engineering and Extractive Metallurgy"}],"Sources":"\u000a    S1] https:\/\/www.osha.gov\/SLTC\/etools\/oilandgas\/general_safety\/h2s_monitoring.html\u000a      (Supports claim that sour gas is toxic and can damage metals)\u000a    [S2] www.rawwater.com\/souring\u000a      (Demonstrates web presence of Rawwater and marketing of DynamicTVS)\u000a    [S3] Letter from Managing Director of Rawwater (Supports all financial\u000a      claims, details of Rawwater Engineering Company Ltd and further specific\u000a      details of the impact)\u000a    ","Title":"\u000a    Oilfield Reservoir Souring Research and Development\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The impact is based on research conducted in the unit of assessment in\u000a      partnership with the industrial company CAPCIS (a spin-out from UMIST).\u000a      The key researcher was\u000a    Professor Patrick Laycock (Professor 1993 - 2001)\u000a    The research was initiated via the UK Oilfield Reservoir Souring\u000a      Programme and a primary statistical analysis of a set of large databases\u000a      supplied by the contributing oil companies was conducted before the\u000a      assessment period. The research involved the development, in conjunction\u000a      with biologists and chemists, of a biogenic souring model that describes\u000a      the down-hole microbiological generation of hydrogen sulphide from\u000a      sulphate, and dispersal of that hydrogen sulphide. The crucial insight was\u000a      that the origin of the hydrogen sulphide is biological and dependent on\u000a      the `water flood' (`seawater injection for secondary recovery', which\u000a      forces additional oil from the reservoir). The model development took\u000a      place within the REF qualifying period (1993-1994) and initial models were\u000a      published in 1993 [1] and 1994 [2].\u000a    Applied research into the practical control of souring continued at UMIST\u000a      (now The University of Manchester) in collaboration with Rawwater\u000a      Engineering Company Limited resulting in the sour gas forecasting model\u000a      DynamicTVS&#169; [3]. This complex mathematical model combines physical balance\u000a      laws and data-derived descriptions of the biology. In general, it\u000a      describes the cooling of an oil reservoir due to water flooding, the\u000a      opportunity for growth of sulphate-reducing bacteria (SRB) in the cooled\u000a      zone, the transport of the hydrogen sulphide produced by the SRB to the\u000a      production well and finally the partitioning of the sulphide at specified\u000a      pressure and temperature in the production facilities.\u000a    "},{"CaseStudyId":"29356","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000a    The close interaction and collaboration with BAE Systems resulted in the\u000a      creation of a computational electromagnetics technology that has empowered\u000a      design engineers to use computational modelling to shorten design cycle\u000a      times. In addition, engineering insight, hitherto unavailable, has been\u000a      gained through the comparison of high fidelity modelling results with\u000a      trials data. The adoption and use of robust, low-order, Galerkin-based\u000a      schemes, using unstructured grids, for the time-domain solution of\u000a      Maxwell's equations for the analysis of scattering from complex\u000a      multi-material bodies, resulted in a step-change in capability for\u000a      modelling electromagnetic systems within BAE Systems. The work guided the\u000a      adoption of mesh refinement strategies at BAE Systems, resulting in\u000a      increased accuracy in Radar Cross Section (RCS) prediction for problems\u000a      involving scattering from electrically large objects with localised\u000a      small-scale features. The accurate modelling of multi-material,\u000a      multi-layer structures enabled the optimisation of radar absorbing\u000a      material, the characterisation of in situ frequency selective\u000a      surfaces by means of full-field optimisation and the investigation of\u000a      sophisticated Jaumann absorbers for low observable platforms.\u000a    \"BAE Systems has worked very closely with the Swansea group both\u000a        defining the requirements and guiding the research. This has involved\u000a        co-working and the exchange of research personnel between Swansea and\u000a        the Advanced Technology Centre (ATC), including the recruitment of\u000a        experienced research personnel from Swansea as an adjunct to technology\u000a        transfer. The success of the high-risk research undertaken by Swansea\u000a        has resulted in developed technology being taken in-house and further\u000a        developed by BAE Systems' personnel for business-specific requirements,\u000a        not least of these being the inclusion of sophisticated frequency\u000a        dependent material's models. Given the scale of BAE Systems and its\u000a        diverse interests, the enormity of the breadth and scope of problem to\u000a        which the resulting time-domain capability has been applied is extensive\u000a        and its geographic application international. The technology has enabled\u000a        the analysis of problems across the broad spectrum of the\u000a        Electromagnetic Engineering (EME) domain.\" [Former Executive\u000a      Scientist and Technology Fellow, BAE Systems, ATC, Filton]\u000a    The results of electrically large simulations were also used to devise\u000a      trials for complex engineering platforms. The modelling process was used,\u000a      and further developed to a high degree of sophistication, by BAE Systems\u000a      Military Aircraft and Information. This led to savings, in both time and\u000a      money, on the radar range and in the quantification of the manufacturing\u000a      and engineering tolerances required to avoid spurious scattering from\u000a      features on low observable platforms.\u000a    The developed capability was used on such diverse programmes as TYPHOON\u000a      and the Taranis UCAV. In 2009, the four nations of the Eurofighter\u000a      consortium signed the contract for the first 112 of the Tranche 3\u000a      production aircraft.\u000a    The underpinning modelling capability developed by Swansea assisted BAE\u000a      Systems participation in European Framework projects FULMEN, CATE and\u000a      EMHAZ which addressed aircraft protection from the direct and indirect\u000a      effects of lightning, issues of electromagnetic compatibility and\u000a      protection against electromagnetic interference. The ability of the\u000a      approach to model non-coordinate aligned wires and cables, contributed to\u000a      analyses performed in the pan-European 26M&#8364; HIRF-SE (High Intensity\u000a      Radiated Field Synthetic Environment) 7th Framework project.\u000a    \"In the area of electromagnetics, financial support from, and close\u000a        technical liaison with, BAE Systems ATC led to the adoption of the\u000a        Swansea unstructured mesh approach as a design tool within BAE Systems\u000a        at the ATC, at Naval Ships and at Military Air and Information. The\u000a        technology has had critical impact on:\u000a    (a) unmanned Combat Air Vehicle (UCAV) design, demonstrating the\u000a        principle of rapid full field solutions, showing the art of the possible\u000a        and formulating the future technology direction for key programmes for\u000a        the MoD;\u000a    (b) testing and evaluation for the analysis of design options prior to\u000a        trials and subsequent comparison with experimental range results;\u000a    (c) the modelling of frequency selective surfaces (FSS) for radomes\u000a        and aperture windows;\u000a    (d) the analysis of lightning strike induced arcing and sparking\u000a        phenomena for product reliability;\u000a    (e) EMC analysis and meshing for aircraft and naval vehicles,\u000a        radiation hazard (RADHAZ) analysis for naval platforms including Type 23\u000a        and Future Aircraft Carrier (CVF, Elizabeth Class);\u000a    (f) greater integration of EM and CFD modelling as the commonality of\u000a        unstructured numerics ensured that the subsequent development and\u000a        industrialisation cost of the delivered codes was greatly reduced,\u000a        enabling BAE Systems to capitalise on its expertise and deliver tighter\u000a        integration between electromagnetic and aerodynamics, which is the key\u000a        to BAE Systems core competencies in integrated design and utilised on\u000a        the Taranis UCAV programme for example.\"\u000a    [University &amp; Collaborative Programmes Relationship Manager, BAE\u000a      Systems, ATC, Filton]\u000a    ","ImpactSummary":"\u000a    Research at Swansea University in the area of computational\u000a      electromagnetics has led to better design of aircraft with respect to\u000a      radar detection and the screening of internal systems from the effect of\u000a      unwanted electromagnetic field ingress. A key issue was the development of\u000a      an ability to accommodate electromagnetically large complex bodies having\u000a      spatially small, but electromagnetically important, features. In addition,\u000a      procedures for modelling RF threats, including lightning strikes and\u000a      electromagnetic hazards, were also developed. Such progress has enabled\u000a      significant improvement in electromagnetic performance of technology\u000a      produced by BAE Systems reaching across its Advanced Technology Centre and\u000a      its business units (Military Aircraft and Information, and Naval Ships).\u000a      This research enabled two-orders-of-magnitude improvement in efficiency of\u000a      BAE software compared to previously used techniques, significantly\u000a      reducing design time. These developments were used on major international\u000a      programmes such as TYPHOON, the Taranis UCAV (unmanned Combat Air Vehicle).\u000a    ","ImpactType":"Technological","Institution":"\u000a    Swansea University\u000a    ","Institutions":[{"AlternativeName":"Swansea University","InstitutionName":"Swansea University","PeerGroup":"B","Region":"Wales","UKPRN":10007855}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Publications Swansea authors in bold. Publications\u000a      1, 2 and 4 best indicate the quality of the research.\u000a    \u000aR1. K. Morgan, O. Hassan, N. E. Pegg and N. P.\u000a        Weatherill, The simulation of electromagnetic scattering in\u000a      piecewise homogeneous media using unstructured grids, Computational\u000a        Mechanics, 25:438-447, 2000.\u000a    \u000a\u000aR2. K. Morgan, P. J. Brookes, O. Hassan and N. P.\u000a        Weatherill, Parallel processing for the simulation of problems\u000a      involving scattering of electromagnetic waves, Computer Methods in\u000a        Applied Mechanics and Engineering, 152:157&#8212;174, 1998.\u000a    \u000a\u000aR3. K. Morgan, N. P. Weatherill, O. Hassan, P. J.\u000a        Brookes, R. Said and J. Jones, A parallel framework for\u000a      multidisciplinary aerospace engineering simulations using unstructured\u000a      meshes, International Journal for Numerical Methods in Fluids,\u000a      31:159&#8212;173, 1999.\u000a    \u000a\u000aR4 M. El hachemi, O. Hassan, K. Morgan, D. Rowse\u000a        and N. P. Weatherill, A low-order unstructured mesh approach for\u000a      computational electromagnetics in the time domain, Philosophical\u000a        Transactions of the Royal Society of London, Series A, 362:445&#8212;469,\u000a      2004.\u000a    \u000a\u000aR5. C. Christopoulos, J. F. Dawson, L. Dawson, I. D.\u000a      Flintoft, O. Hassan, A. C. Marvin, K. Morgan, P. Sewell,\u000a      C. J. Smartt and Z. Q. Xie, Characterisation and modelling of\u000a      electromagnetic interactions in aircraft, Proceedings of the\u000a        Institution of Mechanical Engineers Part G: Journal of Aerospace\u000a        Engineering, 224:449 -458, 2010\u000a    \u000a\u000aR6. P. D. Ledger, J. Peraire, K. Morgan, O.\u000a        Hassan and N. P. Weatherill, Adaptive hp finite element computations\u000a      of the scattering width output of Maxwell's equations, International\u000a        Journal for Numerical Methods in Fluids, 43:953&#8212;978, 2003.\u000a    \u000aMajor Relevant Research Grants\u000a    G1. K. Morgan, High frequency CEM simulation, BAE\u000a      Systems, 1997-1998, &#163;60K.\u000a    G2. N. P. Weatherill, Joint industrial interface\u000a      for end-user simulations (JULIUS), EU, 1998-2001, &#163;270K,\u000a    G3. K. Morgan, Adaptive finite element procedures\u000a      for electromagnetic scattering, EPSRC, 1999-2002, &#163;170K,\u000a    G4. K. Morgan, Error estimation applied to FE\u000a      methods for CEM, EPSRC Visiting Fellowship for Professor J. Peraire from\u000a      MIT, 2000, &#163;20K.\u000a    G5. O. Hassan, Time domain CEM solver developments\u000a      suitable for hybrid and overset structured grids, BAE Systems, 2000-2003,\u000a      &#163;130K.\u000a    G6. O. Hassan, Grid enabled computational\u000a      electromagnetics, BAE Systems and EPSRC, 2003-2005, &#163;103K.\u000a    G7. K. Morgan, Integrated programme of research in\u000a      aeronautical engineering, EPSRC and BAE Systems, 2004-2009, &#163;267K.\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"}],"Sources":"\u000a    (i) Letter from University &amp; Collaborative Programmes Relationship\u000a      Manager, BAE Systems, Advanced Technology Centre, Filton, Bristol BS34 7QW\u000a    (ii) Letter from Former Executive Scientist and Technology Fellow, BAE\u000a      Systems , Advanced Technology Centre, Filton, Bristol BS34 7QW\u000a    ","Title":"\u000a    Techniques for Improved Electromagnetic Design in the Aerospace Industry\u000a    ","UKLocation":[{"GeoNamesId":"2636432","Name":"Swansea"}],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The UoA and the College of Engineering have a long tradition of\u000a      collaboration across the field of applied mathematics resulting in many\u000a      joint publications. This interdisciplinary activity was officially\u000a      formulised by the creation of WIMCS and the appointment of Prof Ken Morgan\u000a      to lead the computational modelling activities at the University.\u000a      Recently, a new appointment in the UoA (Arranz-Carreno) has further\u000a      strengthened the collaborative programme, which is planned to continue\u000a      into the future. Research carried out by Morgan, in partnership with BAE\u000a      Systems, was initiated using applied mathematics and numerical methods to\u000a      address specific industrial requirements in computational\u000a      electromagnetics. The main problem of interest was the simulation of the\u000a      scattering of electromagnetic waves by complex, electrically-large,\u000a      multi-material objects (aircraft and sea-going vessels in particular).\u000a      Also addressed was the major problem of electromagnetic field penetration\u000a      into the interiors of bodies, and the threat of their coupling to internal\u000a      systems. This problem is of particular relevance to electromagnetic\u000a      hazards and compatibility.\u000a    The mathematical models and solution methods developed involved a novel\u000a      combination of an unstructured grid continuous Galerkin formulation within\u000a      material layers, and a discontinuous Galerkin approach at material\u000a      interfaces which used a characteristics-based formulation ensuring phase\u000a      accuracy [R1]. This approach resulted in a stable, phase-accurate\u000a      time-domain algorithm for dealing with the electromagnetic jump conditions\u000a      across curvilinear material boundaries. The basic Swansea grid generator\u000a      was developed to enable the handling of both geometric detail and\u000a      electromagnetic features, such as wires and thin resistive sheets via\u000a      embedding techniques. The parallelisation and integration of grid\u000a      generators and solvers was achieved in major European projects and enabled\u000a      the modelling of electrically large, engineering problems [R2, R3],\u000a      White-space reduction techniques were developed by hybridising solution\u000a      methods with overlapping grids [R4]. This achieved an overall reduction of\u000a      computational complexity with two-orders-of-magnitude improvements in\u000a      efficiency over unstructured grid finite element methods previously used\u000a      by BAE Systems. Stability analyses of hybrid schemes and their dependence\u000a      on inter-grid interpolation was established, and limits obtained for phase\u000a      accuracy for wave propagation over electrically-large bodies. Phase\u000a      accuracy studies were performed to assess effectiveness of high-order and\u000a      hybrid methods on near-resonant coupling to wires and dipole antennas. In\u000a      later work, undertaken as part of the five year EPSRC\/BAE Systems FLAVIIR\u000a      project, a method for prediction of RF threats, including high intensity\u000a      radiated fields (HIRF) and lightning strike, was developed [R5]. The\u000a      FLAVIIR project was the winner of the Aerospace Category of the 2011\u000a      Technology and Innovation Awards in The Engineer.\u000a    Related frequency domain research [R6], undertaken with EPSRC support and\u000a      in collaboration with the group of Professor Mark Ainsworth at Strathclyde\u000a      University, led to significant insights into the application of\u000a      hp-adaptivity in the solution of Maxwell's equations.\u000a    Main personnel involved:\u000a    \u000a      Academic Staff at Swansea University: Prof O. Hassan (1994-present),\u000a        Prof K. Morgan (1991-present), N.P. Weatherill (1987- 2008)\u000a      Research Staff at Swansea University: M. El hachemi (2000-2008), Z.Q.\u000a        Xie (2004-12)\u000a      Research Students at Swansea University: P. Brookes, J. W. Jones, P.\u000a        D. Ledger (1999-2001, Research Staff 2002-2003, Academic Staff since\u000a        2006), R. Said\u000a    \u000a    "},{"CaseStudyId":"29357","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The case described here centres on the impact of our new technology on\u000a      the business sector.\u000a    a) Improving piracy detection for the film industry [C1]\u000a    The geometric techniques developed at Swansea are being used as a tool\u000a      for inserting robust\u000a      watermarks in individual frames of a film in order to improve piracy\u000a      detection in the film industry.\u000a      Fortium Technologies Ltd were impressed by a test example of using the\u000a      technique to embed a\u000a      text-based watermark in an X-ray. Piracy is a major commercial problem for\u000a      film companies,\u000a      costing the US economy alone over $20 billion per year#, and\u000a      there are currently only a couple of\u000a      companies worldwide involved with film watermarking. There is thus a\u000a      strong interest within the film\u000a      industry to find alternative approaches; Fortium is seeking to develop a\u000a      method of embedding a\u000a      watermark in each frame of a film so that, if the film is pirated,\u000a      information about when or where the\u000a      piracy took place can be obtained.\u000a    Zhang, Crooks and Orlando have subsequently entered into a formal\u000a      contract with Fortium to\u000a      develop their geometric method to meet the specific needs of film\u000a      watermarking. Trials of\u000a      embedding a watermark into a test image provided by Fortium have shown\u000a      that our approach\u000a      allows embedding of a watermark in a localised way that gives a high PSNR\u000a      (peak signal to noise\u000a      ratio) that is well above the threshold that the industry accepts as being\u000a      enough to ensure the\u000a      watermark will be imperceptible to the viewer: the PSNR for our locally\u000a      embedded watermark is\u000a      66dB and the industry threshold is around 45dB. Our technique allows\u000a      watermarks that are either\u000a      image or text based, and can be incorporated anywhere within the image\u000a      frame, both of which are\u000a      seen as key advantages in comparison with existing watermarking\u000a      techniques. The inclusion of\u000a      text-based watermarks allows the possibility of human-readable watermarks.\u000a      Current development\u000a      is focussing on trying to improve the robustness of our watermarking\u000a      approach.\u000a    \"...the project has already enough promise to get some of the biggest\u000a        content owners and\u000a        producers in the movie and television industry eager to monitor its\u000a        progress\"\u000a        (Fortium CEO)\u000a    b) Feature and gap detection for computer aided design [C2, C5]\u000a    Working with BAE Systems, the Department has provided a confidential\u000a      report concerning the\u000a      extraction of intersection and high curvature parts and gaps for\u000a      geometrical objects based only on\u000a      given loosely sampled point clouds defining the surfaces of the object.\u000a      Following a visit of Zhang\u000a      and Crooks to BAE Systems Advanced Technology Centre in Bristol in June\u000a      2011, including a\u000a      well-received presentation on some of our new methods and subsequent\u000a      discussions, we were\u000a      provided with some data files of surface meshes for the surfaces of\u000a      aircraft, to which our\u000a      intersection\/high curvature and gap detection methods were successfully\u000a      applied.\u000a    One application of our gap-detection method is as an automatic tool to\u000a      find gaps between parts of\u000a      the underlying geometric design of an aircraft, for instance in data files\u000a      provided by manufacturers\u000a      to engineers for the purpose of performing fluid-dynamics simulations.\u000a      These gaps are deliberately\u000a      left for soldering purposes, whereas for fluid dynamics simulations,\u000a      different parts must be\u000a      connected. Currently such gaps are detected by a time-consuming manual\u000a      process, while our\u000a      methods provide a fast, automatic gap-detection tool. Work is ongoing in\u000a      the field of application.\u000a    c) Quantitative comparison for medical images [C3, C6]\u000a    A further confidential report was provided in June 2012 to clinicians\u000a      from the John Radcliffe\u000a      Hospital, Oxford, who are investigating the feasibility of replacing\u000a      traditional, benchmark\u000a      computerised tomography (CT) scans of children's skulls, known to be\u000a      accurate but also to impart\u000a      high levels of radiation, by safer magnetic resonance (MR) scans. The\u000a      report was concerned with\u000a      quantitative Hausdorff-distance measurement between image sets based on\u000a      magnetic resonance\u000a      (MR) and computerised tomography (CT) scans of a phantom box, with the aim\u000a      of providing a\u000a      quantitative method of comparing CR and MR scans.\u000a    d) Commercial potential identified by consulting company [C4]\u000a    After a demonstration in January 2013 of a selection of our image\u000a      processing and feature\u000a      extraction methods using the interface initially developed with the EPSRC\u000a      `Pathways to impact'\u000a      grant described above, the consulting company Cadarn Technik, which has\u000a      extensive experience\u000a      of dealing with video algorithms and of interacting with electronics,\u000a      chemical, materials, and life\u000a      science companies, expressed an interest in commercialising our\u000a      technology. A draft of a licence\u000a      for the purpose of commercialisation of this technology is in process.\u000a    # http:\/\/www.theguardian.com\/film\/2012\/may\/11\/release-date-piracy-time-warner\u000a    ","ImpactSummary":"\u000a    Researchers in the Department of Mathematics at Swansea University have\u000a      developed novel\u000a      geometric methods for image processing, feature extraction and shape\u000a      interrogation. The research\u000a      has delivered commercial and clinical impact in a variety of settings,\u000a      ranging from new water\u000a      marking techniques to improve piracy detection in the film industry, to\u000a      medical research\u000a      investigating the replacement of traditional CT scans with safer MR scans.\u000a      The research has also\u000a      delivered an automatic feature and gap detection tool that has been\u000a      successfully applied to aircraft\u000a      data files provided by BAE Systems. A consultancy company is exploiting\u000a      the methods and a\u000a      licence for the commercialisation of the technology is in process.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Swansea University\u000a    ","Institutions":[{"AlternativeName":"Swansea University","InstitutionName":"Swansea University","PeerGroup":"B","Region":"Wales","UKPRN":10007855}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Publications and submitted articles (R1, R2 and R5 best show the quality\u000a      of the research):\u000a    \u000aR1) Kewei Zhang, Compensated convexity and its applications, Ann.\u000a      Instit. H. Poincare\u000a      Analyse Non-Lineaire 25 (2008) 743-771\u000a    \u000a\u000aR2) Kewei Zhang, Convex analysis based smooth approximations of\u000a      maximum functions and\u000a      squared-distance functions. J. Nonlinear and Convex Analysis, 9 (2008)\u000a      379-406\u000a    \u000a\u000aR3) Kewei Zhang, Antonio Orlando and Elaine Crooks, Compensated\u000a      Convexity and\u000a      Geometric Singularity Extraction Part I &#8212; Basic Ridge, Valley and Edge\u000a      Transforms. (56\u000a        pages; submitted to Math. Models Methods Appl. Sci.)\u000a    \u000a\u000aR4) Kewei Zhang, Antonio Orlando and Elaine Crooks, Compensated\u000a      Convexity and\u000a      Geometric Singularity Extractions Part II &#8212; Hausdorff Stable Ridge and\u000a      Exterior Corner\u000a      Transforms. (35 pages; submitted to Math. Models Methods Appl. Sci.)\u000a    \u000aR5) Patent application: Image Processing and Feature Extraction &#8212; UK\u000a      patent application ((GB\u000a      0921863.7) December 2009, PCT (Patent Corporation Treaty) application (WO\u000a      55010)\u000a      December 2010. Application for the UK National Phase, June 2012.\u000a    Grant awarded: EPSRC `Pathways to Impact' grant (&#163;25,070 &#8212; Development of\u000a      tools in image\u000a      processing, feature extraction, approximation and interpolation, and shape\u000a      interrogations in\u000a      computer aided geometric design), awarded to Kewei Zhang via the College\u000a      of Science Research\u000a      Committee, Swansea University, which funded the employment of two research\u000a      assistants in the\u000a      period March-June 2012 to develop a user interface and webpage to aid\u000a      demonstration of the\u000a      image processing software to industry, etc. This work was important in\u000a      underpinning impact d).\u000a    Following the initial identification of the potential impact of this\u000a      research in late 2008\/early 2009, the\u000a      Swansea University Department of Research and Innovation funded the patent\u000a      and PCT\u000a      applications (2009 and 2010, respectively), the Wales Institute of\u000a      Mathematical and Computational\u000a      Sciences (WIMCS) and the Department of Mathematics, Swansea University,\u000a      allowed Kewei\u000a      Zhang and Elaine Crooks to spend time developing the theory and numerical\u000a      methods for\u000a      applications, and the Department of Mathematics also provided computing\u000a      equipment and\u000a      software.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000a    External contacts:\u000a    [C1] CEO, Fortium Technologies Ltd\u000a    [C2] Communications, Networks &amp; Image Analysis, BAE Systems\u000a    [C3] Nuffield Department of Surgical Sciences, University of Oxford, and\u000a      Department of\u000a      Maxillofacial Surgery, John Radcliffe Hospital, Oxford\u000a    [C4] Director, Cadarn Technik Ltd, Dylan Thomas Centre, Swansea, SA1 1RR\u000a      Confidential reports to external bodies:\u000a    [C5] `Shape Interrogation of Some BAE Systems Geometric Models', provided\u000a      to BAE Systems\u000a      Advanced Technology Centre (October 2011)\u000a    [C6] `A Comparison of 3D Reconstruction from MR and CT Scans of a phantom\u000a      Box', provided to\u000a      Nuffield Department of Surgical Sciences, University of Oxford, and\u000a      Department of Maxillofacial\u000a      Surgery, John Radcliffe Hospital, Oxford (June 2012)\u000a    ","Title":"\u000a    Benefits to the business and medical sectors through the application of\u000a      geometric convexity-based methods to image and data processing\u000a    ","UKLocation":[{"GeoNamesId":"2654675","Name":"Bristol"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The research underpinning this impact case is based on compensated\u000a      convexity theory, a powerful\u000a      mathematical tool discovered by Kewei Zhang (Professor of Mathematics at\u000a      Swansea University\u000a      until December 2012), which introduces a new class of geometric\u000a      convexity-based transforms.\u000a      The initial paper on this work, that provides the foundation for all later\u000a      developments, was\u000a      published in a highly-regarded pure mathematics journal in 2008 [R1].\u000a    Compensated convexity theory provides a geometric convexity-based\u000a      tight-approximation method\u000a      for general functions [R1, R2]. This can be used, in particular, as a new\u000a      way to identify singularities\u000a      in functions, and thus can be exploited, via a numerical implementation of\u000a      the transforms, to detect\u000a      features in images or data or remove noise from images.\u000a    Based on this insight, innovative geometric methods for image processing,\u000a      feature extraction and\u000a      geometric interrogation have subsequently been developed [R3-R6]. Key\u000a      advantages of this new\u000a      geometric approach over previous image and data processing techniques\u000a      include its use of blind\u000a      global methods which are stable under perturbation and different sampling\u000a      techniques, and also\u000a      provide scales for features that allow users to select which size of\u000a      feature they wish to detect.\u000a    Applications for the numerical implementation of the new convexity-based\u000a      tight-approximation\u000a      methods to perform specific image and data processing and feature\u000a      extraction tasks have been\u000a      developed in collaboration with organisations such as BAE Systems, the\u000a      John Radcliffe Hospital,\u000a      Oxford, and Fortium Technologies since late 2008 onwards. The research has\u000a      been conducted in\u000a      conjunction with parallel work in the theory of geometric singularity\u000a      extraction.\u000a    Applications include image processing, reconstruction based on scattered\u000a      data such as impulse\u000a      noise removal, image repair, affine ridge, valley and edge detections,\u000a      corner detection, sharp\u000a      turning point and intersection detection for lower dimensional objects,\u000a      end point and boundary\u000a      detections for manifolds and curves, multiscale medial axis, boundary\u000a      detection of objects based\u000a      on scattered samples, high oscillation area detection, outlier enhancement\u000a      and suppression, and\u000a      geometric watermarking.\u000a    Following the initial work of Zhang, the further development of the\u000a      theory, its applications to image\u000a      and data processing, and the associated numerical methods and software,\u000a      has been carried out by\u000a      a research group consisting of Professor Kewei Zhang, Dr Elaine Crooks\u000a      (Associate Professor in\u000a      Mathematics, Swansea University) and Dr Antonio Orlando (Lecturer\u000a      in Engineering at Swansea\u000a      University until September 2010), between late 2008 and the present. Two\u000a      short-term research\u000a      assistants (Dr Yasmin Friedmann, March-June 2012, July-August 2013,\u000a      and Ms Natalia Ubilla,\u000a      May-June 2012) were also employed by Swansea University to assist with\u000a      this project.\u000a    Two substantial research papers on the theoretical aspects of geometric\u000a      singularity extraction have\u000a      been submitted for publication [R3, R4] and further research papers are in\u000a      preparation; these\u000a      papers collectively develop the theoretical framework that underpins all\u000a      of our applications to\u000a      image and data processing and feature extraction tasks. A UK patent\u000a      application on these new\u000a      methods of image and data processing (GB 0921863.7) was filed by Swansea\u000a      University, with\u000a      inventors Zhang, Crooks and Orlando, in December 2009, and a PCT (Patent\u000a      Corporation Treaty)\u000a      application (WO 55010) was filed in December 2010 [R5]. The inventors\u000a      entered the UK national\u000a      phase of the patent application process in June 2012.\u000a    "},{"CaseStudyId":"30193","Continent":[{"GeoNamesId":"6255150","Name":"South America"},{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"3144096","Name":"Norway"},{"GeoNamesId":"798544","Name":"Poland"},{"GeoNamesId":"390903","Name":"Greece"},{"GeoNamesId":"2629691","Name":"Iceland"},{"GeoNamesId":"453733","Name":"Estonia"},{"GeoNamesId":"2017370","Name":"Russia"},{"GeoNamesId":"2802361","Name":"Belgium"},{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"2963597","Name":"Ireland"},{"GeoNamesId":"2264397","Name":"Portugal"},{"GeoNamesId":"2661886","Name":"Sweden"},{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"2782113","Name":"Austria"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"285570","Name":"Kuwait"},{"GeoNamesId":"3996063","Name":"Mexico"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"3469034","Name":"Brazil"},{"GeoNamesId":"289688","Name":"Qatar"},{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"1168579","Name":"Pakistan"},{"GeoNamesId":"3723988","Name":"Haiti"},{"GeoNamesId":"798549","Name":"Romania"},{"GeoNamesId":"3190538","Name":"Slovenia"},{"GeoNamesId":"248816","Name":"Jordan"},{"GeoNamesId":"2623032","Name":"Denmark"},{"GeoNamesId":"1522867","Name":"Kazakhstan"},{"GeoNamesId":"6251999","Name":"Canada"},{"GeoNamesId":"3175395","Name":"Italy"},{"GeoNamesId":"660013","Name":"Finland"},{"GeoNamesId":"1733045","Name":"Malaysia"}],"Funders":[],"ImpactDetails":"\u000d\u000a    What is the link between the research and the benefit? HURDLE is\u000d\u000a      the cryptographic primitive that underpins authentication and key\u000d\u000a      derivation in TETRA [1]. Authentication allows two mobile devices, or a\u000d\u000a      base station and a mobile device, to confirm that each is a valid party in\u000d\u000a      the network. Key derivation allows the generation of secret keys (such as\u000d\u000a      session keys) used in communication protocols from longer-term secret key\u000d\u000a      material. The TETRA standard [4] specifies authentication and key\u000d\u000a      derivation operations in terms of TETRA Algorithms (denoted TAn in\u000d\u000a      [4], where n is an integer). HURDLE is the cryptographic component\u000d\u000a      in all the TETRA Algorithms in [4].\u000d\u000a    Who benefits? The TETRA mobile radio and `walkie-talkie'\u000d\u000a      communication standard is tailored for use by the public safety sector\u000d\u000a      (such as police, fire and ambulance services), government agencies and the\u000d\u000a      military. It was first developed as a European standard in the late 1990s,\u000d\u000a      but is now marketed worldwide for a wide variety of safety-critical\u000d\u000a      applications. There are now more than 1400 TETRA contracts, and TETRA is\u000d\u000a      in use in over 130 countries, with over 200,000 users in the UK alone\u000d\u000a      [3,5,6]. For example, the police forces from the following European\u000d\u000a      countries use TETRA: Austria, Belgium, Denmark, Estonia, Finland, Germany,\u000d\u000a      Greece, Iceland, Ireland, Italy, the Netherlands, Norway, Portugal,\u000d\u000a      Poland, Romania, Slovenia, Sweden and the U.K. The standard is used by a\u000d\u000a      range of other organisations with safety-critical needs. UK examples\u000d\u000a      include London Underground, airport services at Aberdeen, Birmingham,\u000d\u000a      Glasgow, Heathrow and Manchester and the UK Highways Agency. The TETRA\u000d\u000a      Industry Group [5] lists a selection of recent TETRA implementations,\u000d\u000a      showing that new users continue to switch to the standard. Since 2008,\u000d\u000a      there have been applications to airport services, bus and tram services,\u000d\u000a      disaster relief, fire services, gas extraction, the military, mining, oil\u000d\u000a      extraction, roadside assistance, train communications, and communications\u000d\u000a      in underground\/metro networks. A wide range of European countries have\u000d\u000a      been involved, plus Australia, Brazil, Canada, China, Haiti, India,\u000d\u000a      Jordan, Kazakhstan, Kuwait, Malaysia, Mexico, Pakistan, Qatar, Russia and\u000d\u000a      Singapore. Beyond the systems themselves, society as a whole benefits from\u000d\u000a      the provision of secure and efficient infrastructure that keeps many\u000d\u000a      millions of citizens protected from crime and terrorism, and safe in cases\u000d\u000a      of emergency.\u000d\u000a    How do they benefit? Authentication in TETRA is used to prevent\u000d\u000a      cloned devices from becoming part of the network, and to prevent\u000d\u000a      illegitimate parties from masquerading as base stations. Key derivation\u000d\u000a      algorithms are an essential part of other security functions provided by\u000d\u000a      the network; for example, an insecure key derivation algorithm could\u000d\u000a      result in decryption of TETRA communications, thereby compromising the\u000d\u000a      confidentiality of sensitive data.\u000d\u000a    Security is a major feature of TETRA; indeed, the Pocket Guide [7]\u000d\u000a      produced by the TETRA Association lists Communications Security as its\u000d\u000a      first benefit, with authentication and encryption (both dependent on\u000d\u000a      HURDLE) specifically highlighted:\u000d\u000a    Communications security is a prerequisite for public safety agencies, and\u000d\u000a      a critical requirement for the increasing number of commercial\u000d\u000a      organisations that rely on TETRA.\u000d\u000a    TETRA builds on the inherent security strengths of digital technology. A\u000d\u000a      key feature of TETRA is the protection of the radio connection between\u000d\u000a      devices and radio sites through the application of advanced Air Interface\u000d\u000a      Encryption techniques.\u000d\u000a    TETRA's security measures deliver the strongest levels of protection;\u000d\u000a      ensuring the privacy of conversations and the secure transmission of\u000d\u000a      sensitive data.\u000d\u000a    A potential security loophole in networks &#8212; devices &#8212; is also addressed.\u000d\u000a      Authentication at the connection between device and network controls\u000d\u000a      traffic to ensure that transmissions are from approved users. If a\u000d\u000a      terminal is misplaced or stolen it can be immediately disabled, preventing\u000d\u000a      unauthorised personnel listening to private conversations or viewing\u000d\u000a      sensitive information.\u000d\u000a    It is of national importance for a country's security-critical services\u000d\u000a      to have a radio network that is not vulnerable to eavesdropping and\u000d\u000a      outside manipulation, made up of devices that cannot be cloned. The secure\u000d\u000a      design of the HURDLE cipher has ensured the integrity and confidentiality\u000d\u000a      of a growing number of safety-critical networks over the past 10 years,\u000d\u000a      with consequent benefits to national security, to safety, and to the\u000d\u000a      reliable operations of the systems they support.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Terrestrial Trunked Radio (TETRA) is a very well known, international\u000d\u000a      specification for secure mobile radio and `walkie-talkie' communication,\u000d\u000a      that is extensively used and relied upon by emergency and public safety\u000d\u000a      services such as police, ambulance and fire services, as well as\u000d\u000a      governmental and private bodies. The European Telecommunications Standards\u000d\u000a      Institute (ETSI) began standardising TETRA in the 1990s and it is now\u000d\u000a      widely used throughout the world. Foundations of its success include\u000d\u000a      resilience and reliability, but security is a major feature, being\u000d\u000a      underpinned by expert cryptographic design. In particular the\u000d\u000a      authentication and key generation mechanisms in TETRA rely on a block\u000d\u000a      cipher (HURDLE) which was designed by a team of cryptographers at Royal\u000d\u000a      Holloway.\u000d\u000a    The work carried out at Royal Holloway underpins the integrity and\u000d\u000a      security of TETRA safety- critical networks throughout the world to the\u000d\u000a      present day. A secure design for emergency service communications\u000d\u000a      minimises both the amount of disruption criminals can cause to service\u000d\u000a      operations, and the amount of operational information such criminals can\u000d\u000a      glean from eavesdropping, contributing to the safety and security of\u000d\u000a      society as a whole as well as the economic benefits to manufacturers of\u000d\u000a      TETRA-based equipment.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    Royal Holloway, University of London\u000d\u000a    ","Institutions":[{"AlternativeName":"Royal Holloway, University of London","InstitutionName":"Royal Holloway, University of London","PeerGroup":"B","Region":"London","UKPRN":10005553}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"1880252","Name":"Singapore"}],"References":"\u000d\u000a    \u000aETSI\/SAGE Specification, `Specification of the HURDLE-II Algorithm',\u000d\u000a      European Telecommunications Standards Institute, 20 January 1997.\u000d\u000a      Available under an appropriate NDA.\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"8","Level2":"4","Subject":"Data Format"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000d\u000a    [1] Supporting statement from the former Chair of ETSI SAGE (Security\u000d\u000a      Algorithms Group of Experts) and Chair of ETSI Project TETRA WG 6 (the\u000d\u000a      TETRA security group), 11 October 2013. Copy available on request.\u000d\u000a      [Quality and authorship of underpinning research; link to impact.]\u000d\u000a    [2] Supporting statement from the President of the International\u000d\u000a      Association of Cryptologic Research, 11 May 2013. Copy available on\u000d\u000a      request. [Quality of underpinning research.]\u000d\u000a    [3] Supporting statement from the Chief Executive of the TETRA+ Critical\u000d\u000a      Communications Association, October 2013. Copy available on request.\u000d\u000a      [Authorship of research; link between research and impact; reach and\u000d\u000a      significance of impact]\u000d\u000a    [4] ETSI EN 300 392-7 V2.1.1 (2001-02), Terrestrial Trunked Radio\u000d\u000a      (TETRA); Voice plus Data (V+D); Part 7: Security, European\u000d\u000a      Telecommunications Standards Institute, 2001. http:\/\/www.etsi.org.\u000d\u000a      Copy available on request. [Link between research and impact]\u000d\u000a    [5] TETRA Industry Group fact sheet \"TETRA Around the World\" (copy\u000d\u000a      available on request) and http:\/\/www.tetrahealth.info\/worldIintro.htm\u000d\u000a      Retrieved 9 October 2013. [Reach of impact.]\u000d\u000a    [6] TETRA Industry Group, FAQs &#8212; Who uses TETRA and Why? http:\/\/www.tetrahealth.info\/pages\/FAQs_WhoUses.html\u000d\u000a      Retrieved 9 October 2013. [Reach and significance of impact.]\u000d\u000a    [7] The TETRA Pocket Guide. http:\/\/pocketguide.tetra-association.com\/english\/\u000d\u000a      Retrieved May 2012. [Reach and significance of impact.]\u000d\u000a    ","Title":"\u000d\u000a    Design of a block cipher used in TETRA secure radio\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2643123","Name":"Manchester"},{"GeoNamesId":"2648579","Name":"Glasgow"},{"GeoNamesId":"2655603","Name":"Birmingham"},{"GeoNamesId":"2657832","Name":"Aberdeen"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"},{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    A block cipher is an algorithm to efficiently realise a family of\u000d\u000a      permutations of binary strings of a fixed length, these permutations being\u000d\u000a      indexed by a (secret) key. The block cipher should be designed so that the\u000d\u000a      permutations it realises behave as if they were randomly chosen to an\u000d\u000a      observer not in possession of the key.\u000d\u000a    The underpinning research. HURDLE is a block cipher that was\u000d\u000a      designed by a team of cryptographers at Royal Holloway: Matthew Dodd (PhD\u000d\u000a      student; now an independent security consultant), Sean Murphy (lecturer;\u000d\u000a      now Professor), Kenny Paterson (post-doctoral researcher; now Professor)\u000d\u000a      and Fred Piper (Professor; now retired) [1]. This work was undertaken as\u000d\u000a      part of a wider project to design and evaluate the security mechanisms of\u000d\u000a      the TETRA standard. The project to standardise TETRA security was\u000d\u000a      commissioned by ETSI-SAGE, the Security Algorithms Group of Experts at the\u000d\u000a      European Telecommunications Standards Institute. The team at Royal\u000d\u000a      Holloway designed the block cipher in 1996, and the specification was\u000d\u000a      issued by ETSI-SAGE in January 1997 [1].\u000d\u000a    Quality. The design of a secure and efficient block cipher is a\u000d\u000a      delicate process, which requires a combination of experience (in cipher\u000d\u000a      design and cryptanalysis), technical precision and creativity. Any cipher\u000d\u000a      to be used in a critical and large-scale project such as TETRA is expected\u000d\u000a      to be world- leading in terms of its design and performance, and indeed\u000d\u000a      cannot be allowed to fail in operational use.\u000d\u000a    The specification for HURDLE has been subject to a rigorous process of\u000d\u000a      peer review by top experts in the area: the design was reviewed in detail\u000d\u000a      by SAGE participants and contractors, including security experts from the\u000d\u000a      mobile and wireless industries. These experts were drawn from the major\u000d\u000a      companies of the time that were active in international standardisation,\u000d\u000a      and included Alcatel, British Telecommunications, Deutsche Telecom, France\u000d\u000a      Telecomm, KPN Research, Philips Electronics Eindhoven and Vodafone. This\u000d\u000a      review process replaces, and is more rigorous than is usual for, the\u000d\u000a      standard academic review process. The full specification for HURDLE and\u000d\u000a      derivative TETRA Algorithms is available under an NDA to approved parties,\u000d\u000a      but is otherwise confidential and so cannot be reviewed in the standard\u000d\u000a      way. Attesting to the academic quality of the specification, The President\u000d\u000a      of the IACR (the main international organisation concerned with\u000d\u000a      cryptographic research), a consultant for industry, and a member of ISO\u000d\u000a      standards committees for security technologies) writes [2]:\u000d\u000a    I would like to make two points: first, that good cipher specification is\u000d\u000a      regarded as a significant research contribution in my field; second, that\u000d\u000a      the review process for a key industrial cipher can be more demanding than\u000d\u000a      the refereeing process for a top cryptography conference [...] A typical\u000d\u000a      submission to a cryptography conference will be reviewed by 2 or 3\u000d\u000a      academics (members of the programme committee, or their nominees). It is\u000d\u000a      very unlikely that a typical reviewer will spend more than half a day\u000d\u000a      examining each paper. For the [...] TETRA ciphers above, the design will\u000d\u000a      be reviewed by several teams (certainly more than 3), each team looking at\u000d\u000a      the cipher for (as an absolute minimum) 2 days. The review procedure is\u000d\u000a      therefore typically much longer than for a submission for an academic\u000d\u000a      conference. Moreover, high-profile academics and highly-regarded\u000d\u000a      industrial consultants are often the same people. This leads me to believe\u000d\u000a      that the industrial review process is often more rigorous than for a top\u000d\u000a      academic conference. I should mention a second, unofficial, 'reviewing'\u000d\u000a      process of an industrial cipher takes place when the deployed\u000d\u000a      cryptographic system is attacked by third parties. If the system remains\u000d\u000a      resistant to real- world attacks, this gives further evidence of the\u000d\u000a      quality of the cipher.\u000d\u000a    All of this context points to the two ciphers that Royal Holloway are\u000d\u000a      putting forward as being clearly of 2 star or higher research quality, as\u000d\u000a      defined above.\u000d\u000a    In his letter of support, President of the IACR gives more detailed\u000d\u000a      evidence of the high esteem the community gives to research of this type.\u000d\u000a    The President of the IACR makes the point above that resistance to\u000d\u000a      real-world attacks is a measure of quality.\u000d\u000a    There is no evidence that HURDLE has been broken, despite being widely\u000d\u000a      deployed in security- critical applications for many years. The former\u000d\u000a      Chair of ETSI SAGE (Security Algorithms Group of Experts) and Chair of\u000d\u000a      ETSI Project TETRA WG 6 (the TETRA security group) when the TETRA standard\u000d\u000a      was created. He writes [1]:\u000d\u000a    The security of TETRA was state-of-the-art, and I believe it is has stood\u000d\u000a      up very well to developments over the past 15 years. I am not aware of any\u000d\u000a      successful attacks on the security of TETRA.\u000d\u000a    The Chief Executive of the TETRA+ Association, a trade association to\u000d\u000a      support TETRA which counts over 150 operators, manufacturers and other\u000d\u000a      interested parties as members. He writes [3] of TETRA:\u000d\u000a    As far as I am aware there have not been any reports of this security\u000d\u000a      being breached ever and it continues to be deployed in existing and new\u000d\u000a      implementations around the world. I am pleased to give credit for this\u000d\u000a      remarkable achievement to Royal Holloway, University of London who\u000d\u000a      designed the algorithms that provide this security.\u000d\u000a    Context. The design of HURDLE forms part of a strong tradition of\u000d\u000a      the study of cryptology in the School that continues to the present day.\u000d\u000a      Royal Holloway is designated as an Academic Centre of Excellence in Cyber\u000d\u000a      Security Research (2012-) and hosts a Centre for Doctoral Training in\u000d\u000a      Cyber Security (2013-); and our expertise in cryptography (as part of an\u000d\u000a      interdisciplinary group spanning mathematics and computer science)\u000d\u000a      contributes significantly to this. Highlights of work completed over the\u000d\u000a      history of the group include the invention of key distribution schemes\u000d\u000a      (Mitchell-Piper), the cryptanalysis of FEAL (the first use of differential\u000d\u000a      cryptanalysis; Murphy), the algebraic framework for the cryptanalysis of\u000d\u000a      AES (Cid-Murphy-Robshaw), pairing-based cryptography (Galbraith-McKee),\u000d\u000a      ID-based cryptography (Paterson), key predistribution for Wireless Sensor\u000d\u000a      Networks (Blackburn-Martin-Ng), codes for copyright protection\u000d\u000a      (Blackburn-Ng) and group-based cryptography (Blackburn-Cid). Consultancy\u000d\u000a      in the field of information security is regularly carried out, including\u000d\u000a      the design and cryptanalysis of ciphers and work with new digital mobile\u000d\u000a      telephony standards. Blackburn, Cid, Martin, McKee, Murphy, Ng and\u000d\u000a      Paterson are current academic staff who have published cryptography papers\u000d\u000a      and\/or undertaken cryptographic consultancy within the current REF period.\u000d\u000a    "},{"CaseStudyId":"30194","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2802361","Name":"Belgium"}],"Funders":[],"ImpactDetails":"\u000d    What is the link between the research and the benefit? There is a\u000d      clear and direct link between the specification produced as underpinning\u000d      research and the impact, due to the specification being recommended by the\u000d      GSMA as an authentication algorithm for GSM networks and subsequently\u000d      adopted by over half of all GSM operators.[2]\u000d    Who benefits? Phones based on the GSM standards were first\u000d      commercially released in 1992; the GSM Association (GSMA) of network\u000d      operators announced in 2010 that more than 5 billion phones have been\u000d      manufactured under this standard to date. There are billions of GSM\u000d      subscribers and hundreds of GSM networks and a significant proportion will\u000d      be using the COMP128-2 and COMP128-3 algorithms for authentication. Then\u000d      there are all the businesses and services that rely on these networks as a\u000d      trusted infrastructure. The Chair of ETSI Security Algorithms Group of\u000d      Experts, states [2]:\u000d    COMP128-2 and COMP128-3 have been a huge success. Although we do not have\u000d      precise figures, we estimate that more than half of the world's network\u000d      operators, representing a number of subscribers in the billions, use one\u000d      of these algorithm variants.\u000d    How do they benefit? As an individual, the COMP128-2\/3 algorithm\u000d      safeguards you in a number of ways. It protects against cloning, which\u000d      stops criminals making costly calls charged to your account; when a clone\u000d      is detected by the network you will be blocked from the network, whereas\u000d      the criminal will pick a new identity. Furthermore, the clone is linked to\u000d      your telephone number, so when the criminal makes a call it appears to be\u000d      you in a friends\/family phone book. Phone numbers are also used in\u000d      business systems and form parts of security processes such as text message\u000d      warnings when changing bank instructions, as well as alerts that may leak\u000d      other personal information and location; cloning compromises these\u000d      processes. The algorithm prevents your secret key from becoming known to a\u000d      criminal to safeguard the authentication process, but also stops a\u000d      criminal from regenerating the current cipher key in order to decipher\u000d      radio transmission to obtain private or sensitive call data.\u000d    Cloning is a real threat to mobile networks. The introduction of GSM,\u000d      replacing analogue systems, led to a sharp reduction in phone cloning, a\u000d      practice that was close to making analogue mobile telephony uneconomic\u000d      towards the end of its operating life (with 1% of all phones in the UK\u000d      found to be cloned in 1994\/5 [6]).\u000d    The GSMA originally distributed an algorithm known as COMP128 (or\u000d      COMP128-1) to its members, as an example of an authentication algorithm\u000d      that complied with the GSM standard. Though a specific authentication\u000d      algorithm was not mandated by the GSM standard, in practice many operators\u000d      used COMP128 rather than developing their own algorithm. The specification\u000d      of COMP128 was not made public, but COMP128 was reverse engineered in 1998\u000d      by Briceno, Goldberg and Wagner, and in 2002 Rao, Rohatgi, Scherzer and\u000d      Tinguely were the first to publicly demonstrate that a GSM phone using\u000d      COMP128 could be cloned after access to the SIM card for only one minute.\u000d      Thus cloning was possible on networks still using COMP128. Indeed, by\u000d      mid-2002 shrink-wrapped cloning kits for COMP128-based SIM cards were\u000d      being sold in some countries; blank cards and cards with multiple\u000d      identities became available [4].\u000d    In order to prevent cloning attacks on their networks, several mobile\u000d      operators commissioned the group at Royal Holloway to design an algorithm\u000d      to replace COMP128. Recognising a common need, the GSM Association\u000d      commissioned the group at Royal Holloway to design the algorithm that\u000d      became known as COMP128-2, and this algorithm and its variant COMP128-3\u000d      became the example authentication algorithms provided by the GSMA to its\u000d      members. No successful cryptanalysis has been demonstrated to date, and\u000d      there is no evidence that SIM Card cloning has returned on networks using\u000d      these algorithms [2]. Ten years on, the algorithm is still recommended by\u000d      ETSI, and authorisation to use this algorithm is regarded as a significant\u000d      benefit of a network operator's membership of the GSMA.\u000d    One single fraudulent SIM card on a network can lose an operator in\u000d      excess $3000 (&#163;1,885) a month and these operations usually use hundreds or\u000d      even thousands of cards. This illegal activity often goes on to fund other\u000d      criminal activity.\u000d    says Andy Gent, CEO of Revector (a company detecting fraud on mobile\u000d      networks) in a 2012 BBC Technology interview [5]. The research effort to\u000d      design COMP128-2\/3 has led to the elimination of SIM card cloning for over\u000d      ten years on networks using this technology [3], resulting in significant\u000d      financial benefits for operators and users alike throughout the REF\u000d      period.\u000d    ","ImpactSummary":"\u000d    Mobile telecommunication networks serve nearly 7 billion users; over 90%\u000d      of the world's population. The flexibility and pervasive nature of mobile\u000d      networks underpin an enormous range of business and personal activities.\u000d      Many systems are based on GSM (Global System for Mobile Communications)\u000d      standards for digital cellular networks that were created by the European\u000d      Telecommunications Standards Institute (ETSI) in the 1990s to replace\u000d      analogue network standards. A key factor in the success of GSM has been\u000d      the ability to authenticate legitimate users and to provide privacy for\u000d      wireless transmissions. A strong authentication mechanism is critical for\u000d      the economic operation of mobile telephony.\u000d    The security of GSM is based on a secret key, known only to the network\u000d      operator and the Subscriber Identity Module (SIM), and an authentication\u000d      algorithm implemented by the SIM and the network operator. A network\u000d      operator may implement its own authentication algorithm, but many adopted\u000d      the example implementation (known as COMP128, or COMP128-1) suggested by\u000d      the GSM Association (GSMA). COMP128-1 was later found to be flawed.\u000d      Cryptographers at Royal Holloway, at the request of GSMA, designed a\u000d      replacement algorithm (COMP128-2), the example implementation offered by\u000d      the GSM Association (GSMA) to over 800 Mobile Network Operators (MNO) in\u000d      over 200 countries. The algorithm is still regarded as robust and it and\u000d      derivative algorithms are relied upon by enormous numbers of users every\u000d      day.\u000d    ","ImpactType":"Technological","Institution":"\u000d    Royal Holloway, University of London\u000d    ","Institutions":[{"AlternativeName":"Royal Holloway, University of London","InstitutionName":"Royal Holloway, University of London","PeerGroup":"B","Region":"London","UKPRN":10005553}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2797656","Name":"Ghent"}],"References":"\u000d    \u000aS.P. Murphy, F. Piper, P.R. Wild, Functional description of COMP128-2,\u000d      GSM Association, 2002. Available under an appropriate NDA.\u000d    \u000a","ResearchSubjectAreas":[{"Level1":"8","Level2":"4","Subject":"Data Format"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000d    [1] Supporting statement from a Professor at KU Leuven and President of\u000d      the International Association of Cryptologic Research, 11 May\u000d      2013. Copy available on request. [To corroborate quality.]\u000d    [2] Supporting statement from the Custodian of GSM Algorithms, 25 March\u000d      2013. Copy available on request. [To corroborate quality and authorship,\u000d      and the reach of impact.]\u000d    [3] Supporting statement from the Chair of ETSI SAGE (European\u000d      Telecommunications Standards Institute Security Algorithms Group of\u000d      Experts), 28 March 2012. Copy available on request. [To corroborate\u000d      quality and authorship of the underpinning research; the reach and\u000d      significance of impact.]\u000d    [4] Charles Brookson, `Can you clone a GSM Smartcard (SIM)?' July 2002.\u000d      Available from: www.brookson.com\/gsm\/clone.pdf.\u000d      [To corroborate the reach and significance of impact.]\u000d    [5] BBC Technology News Report, `Mobile firms bleed billions to fraud and\u000d      bill errors', 29 March 2012, http:\/\/www.bbc.co.uk\/news\/technology-17551858.\u000d      [To corroborate the significance of impact.]\u000d    [6] `Mobile Telephone Crime', Parliamentary Office of Science and\u000d      Technology Note 64, June 1995. [To corroborate the significance of\u000d      impact.] \u000d    ","Title":"\u000d    Design of Authentication Algorithms for GSM Phones\u000d    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d    An authentication protocol enables one entity (the verifier) to confirm\u000d      the identity of a second entity (the claimant). In the case of GSM\u000d      networks, the verifier is the network operator and the claimant is the\u000d      SIM. The claimant and the verifier share a secret key and the claimant\u000d      proves its identity to the verifier by applying a cryptographic algorithm\u000d      to a message chosen by the verifier (a \"challenge\"). The generation of an\u000d      appropriate response to the challenge demonstrates knowledge of the shared\u000d      secret (assuming that only the network and the SIM know the secret key).\u000d    The underpinning research is the specification for the\u000d      authentication algorithm COMP128-2 that is used in GSM phones. Three Royal\u000d      Holloway academics, Sean Murphy (then Reader, now Professor), Fred Piper\u000d      (then Professor, retired 2004) and Peter Wild (then Professor, retired\u000d      2010), designed the original algorithm in the late 90's [2]. The\u000d      specification for the GSM authentication algorithm is confidential, and is\u000d      distributed under a suitable Non-Disclosure Agreement to GSM Operators as\u000d      required. The algorithm was reviewed by ICO Services Ltd (a small UK-based\u000d      telecommunications operator) and then by ETSI, before being adopted by\u000d      ETSI as their recommended authentication algorithm COMP128-2. Later the\u000d      variant of this algorithm known as COMP128-3 was introduced; this\u000d      algorithm is identical to COMP128-2 except that an artificial limitation\u000d      on the effective key length (originally imposed due to export restrictions\u000d      for cryptographic algorithms) is removed.\u000d    Quality: Chairman of the GSM Association Security Group [2], after\u000d      confirming that the design originated at Royal Holloway in the late 90's,\u000d      writes:\u000d    The design of COMP128-2 by the group at Royal Holloway is a good example\u000d      of the impact academic study can have outside academia: the design of a\u000d      robust and novel cryptographic algorithm such as this is a delicate\u000d      business, requiring a great deal of technical proficiency. All the\u000d      evidence that I have seen indicates that COMP128-2 remains secure after\u000d      years of use in high-profile applications, and this is a significant\u000d      achievement.\u000d    The specification for COMP128-2 has been subject to a more rigorous peer\u000d      review process than is usual for academic publication. The President of\u000d      the IACR (the main international organisation concerned with cryptographic\u000d      research), a consultant for industry, and a member of ISO standards\u000d      committees for security technologies. He writes [1]:\u000d    I would like to make two points: first, that good cipher specification is\u000d      regarded as a significant research contribution in my field; second, that\u000d      the review process for a key industrial cipher can be more demanding than\u000d      the refereeing process for a top cryptography conference. [...] For the\u000d      GSM [...] ciphers above, the design will be reviewed by several teams\u000d      (certainly more than 3), each team looking at the cipher for (as an\u000d      absolute minimum) 2 days. The review procedure is therefore typically much\u000d      longer than for a submission for an academic conference. Moreover,\u000d      high-profile academics and highly-regarded industrial consultants are\u000d      often the same people. This leads me to believe that the industrial review\u000d      process is often more rigorous than for a top academic conference. I\u000d      should mention a second, unofficial, 'reviewing' process of an industrial\u000d      cipher takes place when the deployed cryptographic system is attacked by\u000d      third parties. If the system remains resistant to real-world attacks, this\u000d      gives further evidence of the quality of the cipher.\u000d      All of this context points to the two ciphers that Royal Holloway are\u000d      putting forward as being clearly of 2 star or higher research quality, as\u000d      defined above.\u000d    In his letter of support, President of the IACR gives more detailed\u000d      evidence of the high esteem the community gives to research of this type.\u000d    Context: This design of COMP128-2 forms part of a strong tradition\u000d      of the study of cryptology in the School that continues to the present\u000d      day. Royal Holloway is designated as an Academic Centre of Excellence in\u000d      Cyber Security Research (2012-) and hosts a Centre for Doctoral Training\u000d      in Cyber Security (2013-); and our expertise in cryptography (as part of\u000d      an interdisciplinary group spanning mathematics and computer science)\u000d      contributes significantly to this. Highlights of work completed over the\u000d      history of the group include the invention of key distribution schemes\u000d      (Mitchell- Piper), the cryptanalysis of FEAL (the first use of\u000d      differential cryptanalysis; Murphy), the algebraic framework for the\u000d      cryptanalysis of AES (Cid-Murphy-Robshaw), pairing-based cryptography\u000d      (Galbraith-McKee), ID-based cryptography (Paterson), key predistribution\u000d      for Wireless Sensor Networks (Blackburn-Martin-Ng), codes for copyright\u000d      protection (Blackburn-Ng) and group-based cryptography (Blackburn-Cid).\u000d      Consultancy in the field of information security is regularly carried out,\u000d      including the design and cryptanalysis of ciphers and work with new\u000d      digital mobile telephony standards. Blackburn, Cid, Martin, McKee, Murphy,\u000d      Ng and Paterson are current academic staff who have published cryptography\u000d      papers and\/or undertaken cryptographic consultancy within the current REF\u000d      period.\u000d    "},{"CaseStudyId":"30507","Continent":[{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2077456","Name":"Australia"}],"Funders":[],"ImpactDetails":"\u000a    Society and infrastructure are vulnerable to weather-related hazards\u000a      including floods, droughts and wind storms. To protect against such\u000a      hazards, engineers and policymakers must assess the effectiveness of\u000a      potential hazard mitigation and management strategies. To do this, it is\u000a      becoming increasingly common to build computer simulators of the systems\u000a      of interest, and to generate synthetic weather sequences to drive these\u000a      simulators and determine the system response. The research described above\u000a      has underpinned UCL's development of a software package, GLIMCLIM, for\u000a      generating such synthetic weather sequences.\u000a    The main mechanism by which impact has been achieved is via the\u000a      increasing use of GLIMCLIM to generate synthetic weather sequences for use\u000a      in engineering and water resource management applications. Some examples\u000a      are as follows:\u000a    Improved calculations of rainfall and flood mapping: The\u000a      engineering consultancy Halcrow (now part of CH2M HILL) has used the\u000a      software as part of three projects (one since 2008) relating to flood\u000a      mapping and flood defence construction [A]. In each case the software was\u000a      used alongside other methods, most of which are more standard in the\u000a      application area, to provide rainfall scenarios that can be used to\u000a      estimate flood extent and river flows. The rationale for using GLIMCLIM\u000a      alongside these other methods is that it is more conceptually defensible\u000a      but less familiar within the industry; it was therefore seen to provide an\u000a      \"independent verification\" of the more standard calculations. Taking into\u000a      account the use of other methods alongside GLIMCLIM in these projects,\u000a      Halcrow's Senior Hydrologist estimates that since 2008 the software has\u000a      provided a value of around &#163;5,000 to the company [A].\u000a    Informing Environment Agency recommendations: The software is\u000a      becoming recognised as a state-of-the-art tool for advanced use in\u000a      applications that require it. For example, the Environment Agency proposed\u000a      that Anglian Water make use of GLIMCLIM to model rainfall in a realistic\u000a      way, for use in the design of their flood defences. This demonstrates that\u000a      the research has not only informed the Agency's awareness and\u000a      understanding of available modelling techniques, but has also improved\u000a      their ability to make informed suggestions about the design of storm\u000a      overflows at Anglian Water. The Environment Agency, in a letter to Anglian\u000a      Water dated 10 October 2012 [B], explain that they expect improvements to\u000a      storm overflows to be designed following the principles set out in the\u000a      Urban Pollution Management (UPM) manual. These principles are that \"simple\u000a      models and assumed data may be used where they lead to protective\u000a      (conservative) solutions\" but that \"more refined techniques [of which\u000a      GLIMCLIM is an example] are appropriate where they lead to a reduction in\u000a      the combined cost of the modelling and the solution\" [B]. The Environment\u000a      Agency letter goes on to say that Anglian Water's \"designs have generally\u000a      assumed rain falls uniformly across the catchment varying only in time.\u000a      This simplified approach conforms to the above UPM principle but clearly\u000a      leaves scope for improved realism. We therefore support the principle of\u000a      modelling rainfall in a more true to life way as proposed by James Lau and\u000a      Christian Onof\" [B]. The proposal of these researchers (at Imperial\u000a      College London) was to use GLIMCLIM to generate realistic rainfall\u000a      sequences [C].\u000a    Development of climate change projections: In Australia, GLIMCLIM\u000a      is one of the primary tools being used in the development of an agreed set\u000a      of climate change projections for the state of South Australia, in a\u000a      project involving collaboration between universities, CSIRO and state\u000a      government agencies [D]. This work is on-going and will result in the\u000a      production of hydrological models that will be critical in the planning\u000a      required to adapt the state's water resource management strategies to\u000a      future climate conditions.\u000a    These examples are included to indicate that the software is being used,\u000a      and that it is recognised as a state-of-the-art tool. Unfortunately,\u000a      however, it is not easy to track the use of such tools outside the\u000a      academic community, so the full extent of its uptake cannot be\u000a      established.\u000a    Informing water resource management strategies: Although GLIMCLIM\u000a      has been the main vehicle for dissemination of the research, it is not the\u000a      only one; the results from the precipitation trend analysis for southwest\u000a      Western Australia (see section 2) are being used to inform water resource\u000a      management strategies in that state.\u000a    The results of the trend analysis were summarised in an animation of\u000a      statewide rainfall changes from 1940 to 2010, demonstrating clearly a\u000a      substantial decline in rainfall over the last 30 years except in the most\u000a      southwesterly corner of the state. The Water Supply Planning Branch of\u000a      Western Australia's Department of Water (DoW) used this animation at a\u000a      meeting of the Water Supply Planning senior officers group on 21 July\u000a      2011, to \"create a sense of urgency that the drying climate and increasing\u000a      number of dry seasons are impacting on Perth's existing water supplies\"\u000a      and to support the argument that Perth urgently needed a new water source\u000a      [E]. A Supervising Engineer at Surface Water Assessment, DoW, reported:\u000a      \"The decision at the end of the meeting was that DoW prepares an urgent\u000a      cabinet submission to government on the need for a new water source for\u000a      the IWSS [integrated water supply scheme]. The outcome is that cabinet\u000a      approved the expansion of the Binningup Desalination Plant in July 2011.\"\u000a      [E]\u000a    The Southern (Binningup) Seawater Desalination Plant was first opened in\u000a      September 2011, providing 50 billion litres of water to Perth and the\u000a      surrounding area per year. Seawater desalination is a more expensive means\u000a      of providing potable water than traditional groundwater or surface water\u000a      sources, but because it does not depend on rainfall it has become an\u000a      important water supply source in the increasingly dry climate of Western\u000a      Australia. The animation of rainfall changes in the state, produced from\u000a      UCL research, was instrumental in securing state approval for the\u000a      expansion of the plant [E] to provide 100 billion litres of water per\u000a      year, twice the original capacity [F, G]. This expansion, which cost\u000a      AUS$450 million [F] and was completed in January 2013, enables the plant\u000a      to now produce almost one third of Perth's water supply [G], benefiting\u000a      around 600,000 people in the city.\u000a    The animation was also used within the REF impact period for informing\u000a      community groups and the general public about the need to expand the plant\u000a      and to justify the high costs involved. For example, it was used in a\u000a      presentation given by the Water Allocation Planning Branch of the DoW to\u000a      the Jandakot Community Consultative Committee [E], and it was shown on\u000a      state television as a means of communicating to the public the need to\u000a      spend such a large amount of money.\u000a    ","ImpactSummary":"\u000a    Research conducted in UCL's Department of Statistical Science has led to\u000a      the development of a state-of-the-art software package for generating\u000a      synthetic weather sequences, which has been widely adopted, both in the UK\u000a      and abroad. The synthetic sequences are used by engineers and policymakers\u000a      when assessing the effectiveness of potential mitigation and management\u000a      strategies for weather-related hazards such as floods. In the UK, the\u000a      software package is used for engineering design; for example, to inform\u000a      the design of flood defences. In Australia it is being used to inform\u000a      climate change adaptation strategies. Another significant impact is that\u000a      UCL's analysis of rainfall trends in southwest Western Australia directly\u000a      supported the decision of the state's Department of Water to approve the\u000a      expansion of a seawater desalination plant at a cost of around AUS$450\u000a      million. The capacity of the plant was doubled to 100 billion litres per\u000a      year in January 2013 and it now produces nearly one third of Perth's water\u000a      supply.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University College London (UCL)\u000a    ","Institutions":[{"AlternativeName":"University College London","InstitutionName":"University College London","PeerGroup":"A","Region":"London","UKPRN":10007784}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2063523","Name":"Perth"},{"GeoNamesId":"2058645","Name":"State of Western Australia"}],"References":"\u000a    \u000a[1] An analysis of daily maximum wind speed in northwestern Europe using\u000a      generalized linear models, Z. Yan, S. Bate, R. E. Chandler, V. Isham and\u000a      H. Wheater, J. Climate, 15(15), 2073-2088 (2002) doi:10\/bnp5sf\u000a    \u000a\u000a[2] Analysis of rainfall variability using generalized linear models: A\u000a      case study from the west of Ireland, R. E. Chandler and H. S. Wheater, Water\u000a        Resources Research, 38(10), 1192 (2002) doi:10\/cvfq4q\u000a    \u000a\u000a[3] Spatial-temporal rainfall simulation using generalized linear models,\u000a      C. Yang, R. E. Chandler, V. Isham and H. S. Wheater, Water Resources\u000a        Research, 41, W11415 (2005) doi:10\/dwp476\u000a    \u000a\u000a[4] On the use of generalized linear models for interpreting climate\u000a      variability, R. E. Chandler, Environmetrics, 16(7), 699-715 (2005)\u000a      doi:10\/dpvttx\u000a    \u000a\u000a[5] Inference for clustered data using the independence loglikelihood, R.\u000a      E. Chandler and S. Bate, Biometrika, 94(1), 167-183 (2007) doi:10\/fs4b4b\u000a    \u000a\u000a[6] Rainfall trends in southwest Western Australia, R. E. Chandler, B. C.\u000a      Bates and S. P. Charles, In Statistical Methods for Trend Detection\u000a        and Analysis in the Environmental Sciences (R.E. Chandler and E.M.\u000a      Scott, eds.), Chapter 5, pp. 283-306. Wiley, Chichester (2011) &#8212; submitted\u000a        to REF2\u000a    \u000aReferences [4], [3] and [5] best indicate the quality of the\u000a        underpinning research.\u000a    Research grants\/contracts: The work was funded by several\u000a      different bodies including the Irish Office of Public Works (via a\u000a      consultancy contract), the TSUNAMI consortium (a consortium of UK\u000a      insurance companies) and Defra. The total value of the grants from which\u000a      the work was funded was around &#163;1.04 million.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"4","Level2":"1","Subject":"Atmospheric Sciences"}],"Sources":"\u000a    [A] Supporting statement from Senior Hydrologist at Halcrow &#8212;\u000a      corroborates that Halcrow benefited from their use of GLIMCLIM on a\u000a      project in 2009 and that this use had a commercial value to the\u000a      consultancy of around &#163;5,000. Available on request.\u000a    [B] Letter from the Environment Agency to Anglian Water (dated 10 October\u000a      2012) - corroborates that the Environment Agency has informed Anglian\u000a      Water that they support the principle of modelling rainfall in a more\u000a      realistic way, and corroborates that the research has informed awareness\u000a      about modelling techniques. Available on request.\u000a    [C] Proposal from researchers at Imperial College London for the\u000a      development of a rainfall generator for East Anglia &#8212; corroborates that\u000a      the proposal involves the use of GLIMCLIM to generate rainfall sequences.\u000a      Available on request.\u000a    [D] A summary of the project \"Development of an agreed set of climate\u000a      change projections for South Australia\" can be seen online at: http:\/\/goyderinstitute.org\/index.php?id=31\u000a      &#8212; corroborates that GLIMCLIM is being used in the project.\u000a    [E] Supporting statement from Supervising Engineer, Surface Water\u000a      Assessment, Department of Water, Western Australia &#8212; corroborates that the\u000a      animation was used at the Water Supply Planning meeting and that it\u000a      impacted upon the decision to expand the Binningup plant. Also\u000a      corroborates the use of the animation in a Water Allocation Planning\u000a      Branch presentation. Available on request.\u000a    [F] ABC News article about the expansion of the Binningup plant (1 August\u000a      2011): http:\/\/www.abc.net.au\/news\/2011-08-01\/desalination-plant-capacity-doubles\/2819766\u000a      - corroborates the expansion to 100 billion litres per year and the cost\u000a      of the expansion.\u000a    [G] Western Australia Water Corporation desalination website: http:\/\/bit.ly\/1htGcxE\u000a      and\u000a      http:\/\/bit.ly\/1iu9SZn &#8212; corroborates\u000a      that the Binningup plant produces almost one third of Perth's water supply\u000a      and corroborates the doubling of the capacity to 100 billion litres per\u000a      year.\u000a    ","Title":"\u000a    Synthetic weather sequences informing engineering design and supporting\u000a      decisions about infrastructure\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research centred on the use of generalised linear models\u000a      (GLMs) for simulating daily climate time series at multiple spatial\u000a      locations. Although GLMs had been widely established in statistics for\u000a      around two decades by the mid-1990s, their potential for the applications\u000a      considered here had not been recognised. Barriers to their use included\u000a      limited interaction between the statistics and climate\/engineering\u000a      communities, difficulties in representing the complex structures in\u000a      observed weather sequences, and challenges relating to the analysis of\u000a      large spatiotemporal data sets. A further challenge was to provide\u000a      efficient algorithms for simulating dependent sequences at multiple\u000a      spatial locations. Multi-site Gaussian simulation was widely studied;\u000a      however, at fine timescales (e.g. daily), many weather variables &#8212; notably\u000a      precipitation and wind speed &#8212; have distributions that are far from\u000a      Gaussian and the range of tools for handling such situations was (and\u000a      remains) much more limited.\u000a    Against this background, a programme of research within UCL's Department\u000a      of Statistical Science sought to build on the limited previous\u000a      applications of GLMs to climate data, firstly by demonstrating how the\u000a      complex structures in such data can be represented tractably within the\u000a      GLM framework using flexible basis functions and, in particular,\u000a      interactions between covariates representing seasonal and regional\u000a      variation along with temporal autocorrelation [1, 2]; next by developing\u000a      simulation algorithms for non-Gaussian situations, motivated in particular\u000a      by the need to generate synthetic precipitation sequences for hydrological\u000a      applications [3]; and finally by addressing some of the issues of\u000a      statistical inference, uncertainty assessment and model intercomparison\u000a      that arise in the analysis of large space-time data sets [4, 5]. More\u000a      recent developments have focused on the characterisation of trends in\u000a      precipitation in particular, with specific and high-impact applications in\u000a      southwest Western Australia. An initial small-scale study is reported in\u000a      reference [6]; this identified spatially varying time trends in rainfall\u000a      over a small part of the region and was subsequently developed further,\u000a      using nonparametric statistical techniques, to form the basis for the\u000a      fourth example cited in section 4 below.\u000a    The programme has been led by Richard Chandler (Research Associate\u000a      1994-1997; Lecturer in Statistics 1997-2004; Senior Lecturer in Statistics\u000a      2004-2013; Professor of Statistics 2013) since the mid-1990s, working with\u000a      postdoctoral researchers Zhongwei Yan (2000-2001), Chi Yang (2002-2005)\u000a      and Chiara Ambrosino (2011-2013). On-going collaborations with Bryson\u000a      Bates and Stephen Charles at CSIRO (the Commonwealth Scientific and\u000a      Industrial Research Organisation) have led to widespread\u000a      interest in the work in Australia. As the work has evolved,\u000a      the developments have been incorporated into the GLIMCLIM software\u000a      package, a tool created by UCL's Chandler for generating synthetic weather\u000a      sequences.\u000a    This research programme has been application-driven throughout, so the\u000a      most appropriate dissemination outlets have often been outside the\u000a      traditional statistical literature. Moreover, to encourage uptake of the\u000a      methods by non-statisticians (particularly those outside academia), the\u000a      work has focused on the provision of transparent, easily implemented,\u000a      computationally efficient but nonetheless statistically rigorous ways of\u000a      doing things.\u000a    "},{"CaseStudyId":"30511","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"660013","Name":"Finland"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Radon is the single biggest source of public radiation exposure in the UK\u000d\u000a      and is responsible for an estimated 1,100 lung cancer deaths a year [A].\u000d\u000a      Prior to the case-control studies of domestic radon described in section\u000d\u000a      2, it was generally thought that radon-related lung cancer occurred mainly\u000d\u000a      in individuals exposed to the gas at very high concentrations. This belief\u000d\u000a      arose because most of the previous evidence of a link between radon\u000d\u000a      exposure and lung cancer came from studies of miners who had been exposed\u000d\u000a      to high doses occupationally. Consequently, guidelines issued by official\u000d\u000a      public health bodies &#8212; both nationally and internationally &#8212; focused\u000d\u000a      almost entirely on the avoidance of high exposures above certain levels.\u000d\u000a    The finding of the domestic studies that there is a clearly detectable\u000d\u000a      risk of lung cancer at radon concentrations below the minimum levels for\u000d\u000a      intervention suggests, however, that the majority of radon-related lung\u000d\u000a      cancers occur in people exposed to only moderate concentrations; the risk\u000d\u000a      is low, but in the absence of preventive measures many more people are\u000d\u000a      exposed to the gas at these levels. This research finding therefore has\u000d\u000a      substantial implications for the most appropriate way to reduce the\u000d\u000a      average public exposure to radon, with a shift in emphasis from\u000d\u000a      measurement and remediation in existing homes to the installation of\u000d\u000a      preventive measures &#8212; such as thicker and better sealed damp-proof\u000d\u000a      membranes in floors &#8212; in large numbers of new homes. This change in\u000d\u000a      emphasis has been highlighted in national and international reports\u000d\u000a      evaluating the public health aspects of radon.\u000d\u000a    In the UK, the Health Protection Agency (HPA) has, since 2008, provided\u000d\u000a      government with updated advice about the risk of radon, which replaced the\u000d\u000a      previous advice published in 1990 by the National Radiological Protection\u000d\u000a      Board. This updated advice was influenced by the research findings of the\u000d\u000a      European pooling study (references [2] and [3] in section 3) and consisted\u000d\u000a      of recommendations about building regulations made in 2008 [B], followed\u000d\u000a      by further advice in 2010 on limiting public exposure to radon [C].\u000d\u000a      Recognition that there is a risk of lung cancer from radon concentrations\u000d\u000a      below the UK intervention level (together with other factors) led the HPA\u000d\u000a      to recommend to the Department of Health in the 2008 document that\u000d\u000a      \"building regulations and supporting documents should be amended to ensure\u000d\u000a      that all new buildings, extensions, conversions and refurbished buildings\u000d\u000a      in the UK include basic radon protective measures\" [B].\u000d\u000a    This advice was followed in 2009 by a substantial review by the HPA's\u000d\u000a      independent Advisory Group on Ionising Radiation (AGIR) of the effects of\u000d\u000a      radon on the health of the population [A], which further concluded that\u000d\u000a      changes by government to building regulations have the potential to reduce\u000d\u000a      the total number of deaths due to radon in a cost-effective way. The\u000d\u000a      European pooling study is extensively cited in this document and provided\u000d\u000a      important evidence that helped shape the AGIR's advice to the HPA; the\u000d\u000a      report states: \"The association between the long-term average residential\u000d\u000a      radon concentration and the risk of lung cancer found in a pooled analysis\u000d\u000a      of individual data from 13 European studies is the best current basis for\u000d\u000a      risk estimation\" [A]. It goes on to repeat the finding from the European\u000d\u000a      pooling study that there is an estimated 16% increase in lung cancer risk\u000d\u000a      per 100 Bq m-3 increase in concentration of radon gas. This\u000d\u000a      estimate was used by the AGIR in a cost-benefit analysis, to evaluate the\u000d\u000a      cost-effectiveness of current and possible future radon control policies\u000d\u000a      for the UK. Amongst other things, this analysis concluded that (1) not\u000d\u000a      only was the current government policy to install radon preventive methods\u000d\u000a      in all new homes in areas with average concentrations above 52 Bq m-3\u000d\u000a      cost-effective; but that (2) extending this requirement to all new homes\u000d\u000a      nationwide would also be cost-effective and would avert considerably more\u000d\u000a      lung cancers (242 in the first 10 years of the policy compared with 28)\u000d\u000a      [A]. In response to the AGIR review, the HPA published its current advice\u000d\u000a      to government in 2010 [C], which reiterates the recommendations of the\u000d\u000a      AGIR and additionally recommends that a new Target Level of 100 Bq m-3\u000d\u000a      should be introduced alongside the current Action Level of 200 Bq m-3.\u000d\u000a      Preventive measures are already required in new homes if the probability\u000d\u000a      of exceeding the Action Level is 1-3%; the Target Level is the\u000d\u000a      concentration above which the HPA recommends householders seriously\u000d\u000a      consider taking remedial action. Having two levels also \"avoids the false\u000d\u000a      impression that there is a clear boundary between safe and unsafe radon\u000d\u000a      concentrations\" [C]. This 2010 advisory document also states that the\u000d\u000a      European pooling study provides the \"best information on the risks from\u000d\u000a      radon exposure in homes\" currently available and repeats the findings of\u000d\u000a      that work [C].\u000d\u000a    Internationally, in 2009 the World Health Organization (WHO) published a\u000d\u000a      handbook on indoor radon [D], designed to aid the development of national\u000d\u000a      radon programmes and to inform stakeholders involved in radon control,\u000d\u000a      such as the construction industry. This handbook also cites the European\u000d\u000a      pooling study extensively, noting that it is the only one of the three\u000d\u000a      existing pooling exercises (this one and two smaller ones involving US and\u000d\u000a      Chinese data respectively) to correct for measurement error; indeed, the\u000d\u000a      authors of the WHO report applied approximate corrections to the risk\u000d\u000a      estimates from the other two studies to bring them into line with the\u000d\u000a      European one [D]. Two conclusions of the WHO that are heavily dependent on\u000d\u000a      the European pooling study are: \"The majority of radon-induced lung\u000d\u000a      cancers are caused by low and moderate radon concentrations rather than by\u000d\u000a      high radon concentrations, because in general less people are exposed to\u000d\u000a      high indoor radon concentrations\", and \"Radon is much more likely to cause\u000d\u000a      lung cancer in people who smoke...\" [D]. The handbook goes on to outline\u000d\u000a      the implications for policy, such as the need to provide protection\u000d\u000a      against low exposures. It also summarises the cost-benefit analysis in the\u000d\u000a      AGIR report and highlights it as a good example of how to use the risk\u000d\u000a      estimates as a scientific basis for policy making [D]. The increase in\u000d\u000a      absolute risk from radon exposure for smokers was also highlighted in the\u000d\u000a      2010 WHO guidelines on indoor air quality [E], where the different levels\u000d\u000a      of risk according to smoking status were provided and the European pooling\u000d\u000a      study was cited as one of the primary sources of evidence. These\u000d\u000a      guidelines are targeted at public health professionals involved in\u000d\u000a      preventing health risks of environmental exposures, and aim to provide a\u000d\u000a      scientific basis for legally enforceable standards.\u000d\u000a    The process of moving from reports by committees to changes in\u000d\u000a      legislation is a slow one, but in several countries there is a move\u000d\u000a      towards revising national policy on the control of risks from domestic\u000d\u000a      radon exposure, with a shift from searching for homes with high radon\u000d\u000a      concentrations and remediating them, to the introduction of radon\u000d\u000a      preventive measures in large numbers of homes. For example, Finland,\u000d\u000a      influenced by the south-west England study (reference [1] in section 3) as\u000d\u000a      well as by its own national study, changed its building code in 2004 to\u000d\u000a      require more effective radon preventive measures in new buildings; a\u000d\u000a      survey conducted in 2009 revealed that these improved measures had\u000d\u000a      resulted in a 33% lower average indoor radon concentration in new homes\u000d\u000a      [F]. In Germany there is current discussion, motivated in part by the risk\u000d\u000a      estimates from the European pooling study, on a move in this direction\u000d\u000a      [G], and the UK is also considering changes in light of the cost-benefit\u000d\u000a      study in the AGIR report.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    UCL research on statistical methodology has underpinned important\u000d\u000a      investigations by scientists at Oxford University into the relationship\u000d\u000a      between exposure to the naturally occurring radioactive gas radon and lung\u000d\u000a      cancer. The resulting dose-response relationships and risk estimates have\u000d\u000a      informed advice given in 2008-10 to the UK government by the Health\u000d\u000a      Protection Agency about the risk of radon exposure and the\u000d\u000a      cost-effectiveness of radon control policies. They have also influenced\u000d\u000a      the conclusions of the World Health Organization about indoor radon and\u000d\u000a      lung cancer, as reported in their 2009 handbook. Furthermore, the research\u000d\u000a      findings have led to proposals for changes to building regulations in the\u000d\u000a      UK and elsewhere, and changes to the building code in Finland have\u000d\u000a      resulted in a reduction in the average indoor radon concentration in new\u000d\u000a      homes.\u000d\u000a    ","ImpactType":"Political","Institution":"\u000d\u000a    University College London (UCL)\u000d\u000a    ","Institutions":[{"AlternativeName":"University College London","InstitutionName":"University College London","PeerGroup":"A","Region":"London","UKPRN":10007784}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a[1] Risk of lung cancer associated with residential radon exposure in\u000d\u000a      south-west England: a case-control study, S. Darby, E. Whitley, P.\u000d\u000a      Silcocks, B. Thakrar, M. Green, P. Lomas, J. Miles, G. Reeves, T. Fearn\u000d\u000a      and R. Doll, British Journal of Cancer, 78, 394-408 (1998) doi:\u000d\u000a\u0009  10.1038\/bjc.1998.506\u000d\u000a    \u000a\u000a[2] Residential radon and lung cancer&#8212;detailed results of a collaborative\u000d\u000a      analysis of individual data on 7,148 subjects with lung cancer and 14,208\u000d\u000a      subjects without lung cancer from 13 epidemiological studies in Europe, S.\u000d\u000a      Darby, et al., Scandinavian Journal of Work, Environment and Health,\u000d\u000a      32, suppl. 1, 1-84 (2006) http:\/\/www.sjweh.fi\/show_abstract.php?abstract_id=982\u000d\u000a    \u000a\u000a[3] Radon in homes and risk of lung cancer: collaborative analysis of\u000d\u000a      individual data from 13 European case-control studies, S. Darby et al., BMJ,\u000d\u000a      330, 223 (2005) doi:10\/fkjn77\u000d\u000a    \u000a\u000a[4] Seasonal correction factors for indoor radon measurements in the\u000d\u000a      United Kingdom, J. Pinel, T. Fearn, S. C. Darby and J. C. H. Miles, Radiation\u000a        Protection Dosimetry, 58, 127-132 (1995) http:\/\/rpd.oxfordjournals.org\/content\/58\/2\/127.abstract\u000d\u000a    \u000a\u000a[5] Measurement error in the explanatory variable of a binary regression:\u000d\u000a      Regression calibration and integrated conditional likelihood in studies of\u000d\u000a      residential radon and lung cancer, T. Fearn, D. C. Hill and S. C. Darby, Statistics\u000a        in Medicine, 27, 2159-2176 (2008) doi:10.1002\/sim.3163\u000d\u000a    \u000aReferences [5], [4] and [1] best indicate the quality of the\u000d\u000a        underpinning UCL research.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    [A] Radon and Public Health: Report of the independent Advisory Group on\u000d\u000a      Ionising Radiation (2009), available online: http:\/\/bit.ly\/1cRj9JF\u000d\u000a      &#8212; corroborates the influence of the European pooling study on the advice\u000d\u000a      in the report. Also corroborates the number of estimated deaths and the\u000d\u000a      conclusions and recommendations made. In particular, see pages 21-25,\u000d\u000a      43-55 and 58.\u000d\u000a    [B] HPA Advice on Radon Protective Measures in New Buildings (2008),\u000d\u000a      available online: http:\/\/bit.ly\/187nfK6\u000d\u000a      &#8212; corroborates the influence of the research on the advice, and\u000d\u000a      corroborates the recommendation to government to amend building\u000d\u000a      regulations.\u000d\u000a    [C] Limitation of Human Exposure to Radon: Advice from the Health\u000d\u000a      Protection Agency (2010), available online: http:\/\/bit.ly\/1bYjPM9\u000d\u000a      &#8212; corroborates the recommendations made to government, and the influence\u000d\u000a      of the research on the advice. In particular, see pages 7, 8 and 15-18.\u000d\u000a    [D] WHO Handbook on Indoor Radon: a Public Health Perspective (2009),\u000d\u000a      available online: http:\/\/bit.ly\/1bbmf8A\u000d\u000a      &#8212; corroborates the influence of the research findings on the content and\u000d\u000a      conclusions of the handbook. In particular, see pages 3, 7-16 and 63-69.\u000d\u000a    [E] WHO Guidelines for Indoor Air Quality (2010), available online: http:\/\/bit.ly\/1grkUSx\u000d\u000a      &#8212; corroborates the influence of the research on the guidelines. In\u000d\u000a      particular, see pages 361-362.\u000d\u000a    [F] Radon prevention in new construction in Finland: a nationwide sample\u000d\u000a      survey in 2009, H. Arvela, O. Holmgren and H. Reisbacka, Radiat. Prot.\u000d\u000a        Dosimet., 148, 465-474 (2012) doi:10\/dz65j2\u000d\u000a      &#8212; corroborates that Finland changed its building code in 2004 and the\u000d\u000a      benefits of this change.\u000d\u000a    [G] The Working Group Manager at the Institute of Radiation Protection in\u000d\u000a      Germany can be contacted to corroborate that changes to building codes are\u000d\u000a      being discussed in Germany, motivated by the research findings. Contact\u000d\u000a      details provided separately. \u000d\u000a    ","Title":"\u000d\u000a    Radon exposure: Informing advisory guidelines\u000d\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Evidence of an association between exposure to radon gas and increased\u000d\u000a      rates of lung cancer has been available for over 100 years, but this early\u000d\u000a      evidence was based on the exposure of uranium miners to very high doses of\u000d\u000a      radon. However, people are also exposed to low doses of radon simply by\u000d\u000a      living in their homes; granite &#8212; the underlying rock in many areas &#8212;\u000d\u000a      contains low concentrations of the gas, which seeps up through the ground\u000d\u000a      into buildings (the average concentration inside homes is 20 Bq m-3).\u000a      It is therefore important to estimate the risk of lung cancer associated\u000d\u000a      with these low doses, in order to judge whether and when preventive\u000d\u000a      measures should be taken. In the past, this estimation was done by\u000d\u000a      extrapolating from high to low doses of radon, but about 20 years ago\u000d\u000a      advances in instrumentation for measuring radon concentrations in the home\u000d\u000a      opened up the possibility of direct estimation of the risk of developing\u000d\u000a      lung cancer due to domestic exposure. In the last 20 years, several\u000d\u000a      countries have carried out case-control studies, in which lung cancer\u000d\u000a      patients are matched with a control group and the radon exposure histories\u000d\u000a      of both groups &#8212; obtained by retrospective measurements in the homes they\u000d\u000a      have lived in &#8212; are compared and, in particular, are used to estimate a\u000d\u000a      dose-response relationship. This requires adjustments for various other\u000d\u000a      factors, of which smoking history is the most important.\u000d\u000a    Since the early 1990s, Tom Fearn (Senior Lecturer 1989-96; Reader\u000d\u000a      1996-99; Professor 1999-present) in UCL's Department of Statistical\u000d\u000a      Science has worked closely with a group of scientists in the University of\u000d\u000a      Oxford's Clinical Trials Support Unit (CTSU) on a number of studies\u000d\u000a      relating to radon exposure. The CTSU group has been instrumental in\u000d\u000a      carrying out a large case-control study in the south-west of England [1],\u000d\u000a      and in pooling the results of 13 European case-control studies [2, 3].\u000d\u000a      This latter exercise did not just combine the published risk estimates,\u000d\u000a      but rather involved an analysis of the pooled data from all of the\u000d\u000a      studies. Fearn's contribution to the work was in providing statistical\u000d\u000a      methodology that underpinned the data analyses in the CTSU group's\u000d\u000a      studies, as well as contributing to the data analysis.\u000d\u000a    One relatively straightforward contribution from Fearn, with background\u000d\u000a      and data provided by Sarah Darby at CTSU and Jon Miles at the National\u000d\u000a      Radiological Protection Board, was a methodology [4] for applying a\u000d\u000a      seasonal variation correction to domestic radon measurements, which are\u000d\u000a      typically taken by placing a detector in the home for six months. This\u000d\u000a      seasonal variation correction is important because summer measurements of\u000d\u000a      internal radon concentrations are lower than winter ones (due to the\u000d\u000a      release of radon to the outside through open windows), and so a six-month\u000d\u000a      average is not an accurate estimate of the annual dose. The methodology\u000d\u000a      developed at UCL involved smoothing and extrapolation with periodic\u000d\u000a      functions, and was used to correct the dose measurements from the\u000d\u000a      south-west England study [1] that was carried out between 1988 and 1998.\u000d\u000a    A major difficulty in correctly estimating the dose-response relationship\u000d\u000a      in case-control studies of radon and lung cancer is that there is very\u000d\u000a      substantial measurement error in the radon measurements, with a typical\u000d\u000a      coefficient of variation being 50%. It is well known that error in the\u000d\u000a      x-variable flattens regressions, leading to slope estimates that are too\u000d\u000a      low. Correcting for this effect is a simple matter for linear regression\u000d\u000a      models; however, the model typically used to analyse these case-control\u000d\u000a      studies is a logistic regression of case-control status on dose and\u000d\u000a      covariates (such as smoking), fitted by maximum likelihood, for which the\u000d\u000a      simple correction methods do not apply. In the analysis of the south-west\u000d\u000a      England study [1], an approximate method by Cox and Reeves of Oxford\u000d\u000a      University was used to make the correction. Fearn was closely involved in\u000d\u000a      the implementation of this methodology; for example, advising on the\u000d\u000a      estimation of measurement error variances in the situation where the\u000d\u000a      exposure measurement is a sum over several homes with a proportion of\u000d\u000a      missing data.\u000d\u000a    For the European pooling study, on which work began in about 2003, this\u000d\u000a      approximate method for correcting for the radon measurement error was\u000d\u000a      considered to be inadequate. The main reason for this was the need to\u000d\u000a      include study-specific adjustment for a substantial number of covariates.\u000d\u000a      With large numbers of covariates maximum likelihood is biased, and it is\u000d\u000a      preferable to implement the analysis using stratification and a\u000d\u000a      conditional logistic likelihood, for which the Cox-Reeves approach does\u000d\u000a      not work. Fearn therefore developed a new methodology based on numerically\u000d\u000a      integrating the conditional logistic likelihood [5], with input on the\u000d\u000a      context from Darby at CTSU and some computing assistance from David Hill\u000d\u000a      at CTSU. This method is exact (if the distributional assumptions are\u000d\u000a      correct), but computationally expensive, and was used in the data analysis\u000d\u000a      reported in the European pooling study [2, 3].\u000d\u000a    In the case-control [1] and European pooling [2, 3] studies it was found\u000d\u000a      that appropriate correction for radon measurement error using the methods\u000d\u000a      described above roughly doubled the estimate of the slope in the linear\u000d\u000a      (for log odds) dose-response, i.e. the risk of developing lung cancer from\u000d\u000a      exposure to radon was estimated to be twice as great as previously\u000d\u000a      thought. This has important implications for policy decisions, where, for\u000d\u000a      example, cost-benefit analyses of the value of remedial action in\u000d\u000a      buildings need to be based on the correct estimates of risk. The two\u000d\u000a      studies resulted in similar estimates of the risk of lung cancer from\u000d\u000a      exposure to radon, but that from the pooled analysis has the advantage of\u000d\u000a      being both much more precise and more appropriate for international use,\u000d\u000a      because it was based on almost all the international data available at the\u000d\u000a      time. The pooled analysis also found that there was no evidence of a dose\u000d\u000a      threshold (i.e. a radon dose below which there is no effect), which also\u000d\u000a      has important implications for public health policy since there is no dose\u000d\u000a      that can be regarded as safe [2, 3]. A further important finding was that\u000d\u000a      exposure to radon multiplies the risk of lung cancer from smoking, so that\u000d\u000a      those who smoke &#8212; or who have smoked in the past &#8212; are at much higher\u000d\u000a      absolute risk than lifelong non-smokers.\u000d\u000a    Note that Fearn is not an author on references [2] and [3], despite\u000d\u000a      contributing substantially to the data analysis, because of a\u000d\u000a      two-per-study limit on authors (his contribution is instead acknowledged\u000d\u000a      on page 53 of reference [2]). Please also note that although the\u000d\u000a      methodology in reference [5] is described in detail in reference [2], the\u000d\u000a      paper is not cited there because it was published later, the large time\u000d\u000a      gap being due to differences in editorial practices between medical and\u000d\u000a      statistical journals.\u000d\u000a    "},{"CaseStudyId":"30520","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    UCL research into the London riots of 2011 has stimulated a significant\u000a      amount of public interest, debate and discussion about the issues\u000a      surrounding the riots and about mathematical modelling. This has been\u000a      achieved through a number of public engagement activities, including\u000a      talks, a short film and a game.\u000a    While conducting the underpinning research, Fry felt it was important to\u000a      initiate a discussion with members of the communities affected by the\u000a      riots, particularly given the striking links between rioter involvement\u000a      and deprivation in London that were uncovered in the statistical analysis\u000a      of the data. To help prevent riots from happening again, and to understand\u000a      why so many young people across London chose to riot, she wanted to learn\u000a      more about the social issues within rioters' communities that led to their\u000a      involvement; this was the initial motivation behind a series of meetings\u000a      between Fry and various youth charities, youth workers and ex-gang members\u000a      across the capital.\u000a    After several informal discussions, Fry took part in the inaugural EPSRC\u000a      and UCL Focus on the Positive public event in May 2012. In these events,\u000a      researchers present their ideas for tackling issues and the audience vote\u000a      for a project that they think will make a real difference. Fry suggested a\u000a      need to raise awareness of the difficulties affecting young people, of\u000a      which the riots were just a single manifestation, and pledged to spend the\u000a      &#163;1,000 prize creating a short film to give a voice to young people in\u000a      deprived areas of London: the very people blamed in the wake of the 2011\u000a      riots. Her aim was to improve public understanding of the issues uncovered\u000a      in the paper as well as the difficulties around a lack of funding for\u000a      youth services, an additional problem that had been highlighted in her\u000a      conversations with affected communities. After debating the various issues\u000a      that had been presented, the 58 members of the audience voted to fund\u000a      Fry's project, demonstrating that the research stimulated public discourse\u000a      and interest in the topic [A].\u000a    During the research for the film, Fry visited a number of affected\u000a      communities, including Blackfriars Settlement, a young people's charity in\u000a      Southwark; youth workers in Brixton; ex-gang members in Croydon; and\u000a      teachers within Pupil Referral Units in Enfield (PRUs are centres for\u000a      pupils unable to attend mainstream school, often due to emotional and\/or\u000a      behavioural difficulties). These meetings allowed for a two-way\u000a      discussion, providing a context to the data for Fry, while changing the\u000a      attitudes of the individuals and organisations involved towards\u000a      mathematics, police intervention and academia. One of these discussions,\u000a      between Fry and Hackney resident Pauline Pearce (who gained exposure\u000a      during the riots for confronting looters in her local area), was filmed as\u000a      a case study for a secondary project, funded by the EPSRC, celebrating the\u000a      impact of the Focus on the Positive events [B]. Pearce said of Fry's work\u000a      \"You have really opened my eyes &#8212; I don't know numbers [...] but you have\u000a      spoken a language that people can understand\" [B].\u000a    Can Maths Predict a Riot?, the short film produced by Fry with her\u000a    Focus on the Positive funding, was released in January 2013. It has been\u000a    well received by a variety of audiences, with over 5,600 views on YouTube as\u000a    of 31 July 2013, and has sparked discussion and debate amongst the public,\u000a    as evidenced by the comment stream on YouTube [C]. Pearce, who has since\u000a    joined the Liberal Democrats and stood for election within Hackney, offered\u000a    her feedback on the film and pledged to promote it within her community,\u000a    helping further to raise awareness of the work and its links to social\u000a    issues within deprived areas of the city. She said: \"You'll be amazed at how\u000a    much your word will get around\" [B]. The film also stimulated members of the\u000a    public and the media to write about the work online; for example,\u000a    influential visualisation blog flowingdata (300,000 visitors a month) [D]\u000a    and science communication website popsci.com (1.2 million visitors a month)\u000a    [E] both featured articles about Fry's work on their homepages (on 18 and 19\u000a    July 2013 respectively). These high-profile articles then led to many other\u000a    websites (e.g. Professional Security Magazine, Phys.org and Laboratory\u000a    Equipment), blogs (e.g. Urban Demographics, Paul de Gregorio and Snap VRS),\u000a    Twitter and Google+ users writing about the research, including a tweet\u000a    shared by Labour MP Tom Watson to his 130,000 followers [F].\u000a    Alongside the short film, the mathematical model potentially offered\u000a      police an opportunity to examine various strategic scenarios, but was\u000a      inaccessible in its original form. To address this issue and present the\u000a      model in a way that is useful and intuitive for potential stakeholders, a\u000a      proof of concept `computer-game' style interface was created by Fry and a\u000a      team of UCL computer scientists in April 2012. The game is presented on a\u000a      touch table, with a map of London projected on to the screen. The player\u000a      reacts to riots spreading across the map by arranging Lego police cars in\u000a      a configuration to minimise the damage. As the simulation detects the\u000a      placement of the cars using an Xbox Kinect, the model runs in the\u000a      background, reacting accordingly.\u000a    By interacting with the riot table, members of the public are able to\u000a      explore its mechanisms in a tactile and engaging manner. This helps them\u000a      to gain an improved understanding of how this kind of mathematical\u000a      modelling could be used in the future to benefit the police and society.\u000a      Since its creation, the riot table has been taken to various conferences\u000a      and festivals, including the Analogies conference at UCL on 20 April 2012\u000a      (with 400 visitors) and the Leeds Smart Cities Exhibition between 8 and 10\u000a      November 2012 (1,500 members of the public). At each event, participants'\u000a      scores were tweeted to a live leader-board, which added a competitive\u000a      element and stimulated public interest throughout the day. In total, the\u000a      game was played 226 times in 2012, with each score recorded on the riot\u000a      table Twitter account [G].\u000a    In terms of the research, the Nature Scientific Reports paper was also\u000a      well received by the public. The open-access nature of the publication\u000a      allowed the UCL researchers to distribute and publicise the work to a\u000a      wider audience, informing people of the potential of mathematical\u000a      modelling and improving their understanding of the methods employed. As of\u000a      31 July 2013, the paper had over 12,200 views on the Nature website [H].\u000a      It also had an altmetric score of 149, ranking in the top 1% of all\u000a      articles of a similar age rated by the system. An altmetric score is based\u000a      on the online attention that an article receives, and includes both social\u000a      media and mainstream news media; the high score received by the UCL paper\u000a      indicates that it stimulated a large volume of public interest in the\u000a      research.\u000a    The paper also attracted a good deal of media coverage on publication,\u000a      with articles in Wired magazine (audience of 3 million) and science blog\u000a      ars-technica (website has more than 10 million unique readers per month).\u000a      BBC London News also ran a feature on the model on 21 February 2013,\u000a      including an interview with a UCL research student, and the work received\u000a      national attention with a piece about the model by BBC's Newsnight on 12\u000a      March 2013. The associated publicity raised awareness of UCL's work,\u000a      evidenced by increased online views of the paper on both occasions [H],\u000a      and in turn led to a greater public understanding of this relatively new\u000a      field of research. The online attention surrounding the Can Maths\u000a        Predict a Riot? film in the latter part of July 2013 also prompted a\u000a      significant increase in views of the paper and reignited a general\u000a      interest in the underlying research [H].\u000a    Alongside the paper, Fry was invited to speak about the research into the\u000a      riots at the first TEDxUCL event in June 2012, to an audience of 100\u000a      members of the public. The footage of the talk was featured on the front\u000a      page of TED.com for over three weeks in August 2012, and as of 31 July\u000a      2013 had over 500,000 views across all TED channels, ranking in the top 60\u000a      TEDx videos of all time (out of more than 30,940 videos). The stimulation\u000a      of public interest and discourse on the research topic, and on complexity\u000a      science in general, is further evidenced by the intense debate sparked\u000a      amongst the public, with over 1,000 detailed comments and `likes' on the\u000a      TED website [I] and YouTube channel [J] by the end of the REF impact\u000a      period. The talk also prompted several members of the public to contact\u000a      Hannah Fry about the work, opening the opportunity for a two-way\u000a      discussion. Examples of comments received online and via email include\u000a      [K]:\u000a    \"Very, very good, opened my eyes.\"\u000a    \"I was very inspired by the fascinating TEDx talk.\"\u000a    \"Your TEDx talk explained complexity in an accessible succinct manner,\u000a        I'm planning my next semester and have your presentation on a list of\u000a        supplemental materials.\"\u000a    ","ImpactSummary":"\u000a    While conducting research into the London riots of 2011, researchers in\u000a      UCL's Department of Mathematics initiated two additional outreach\u000a      projects: (1) a short film, made to explore the social impacts of the work\u000a      and to engage with the communities affected by the events, and (2) a\u000a      visualisation tool, developed to demonstrate to the public the potential\u000a      of UCL's mathematical riot model.\u000a    The film involved a two-way discussion with youth charities, youth\u000a      workers and ex-gang members across the capital, improving their attitudes\u000a      towards the research and their understanding of the model, while the final\u000a      version of the film raised awareness of the underlying social issues and\u000a      stimulated a great deal of public debate online. The visualisation tool &#8212;\u000a      in game form &#8212; has proved successful at events and festivals across the\u000a      country in stimulating public interest in science.\u000a    These projects, along with the original paper, have received a good deal\u000a      of media attention online, in press and on national television, serving to\u000a      raise awareness, improve understanding and prompt public debate about the\u000a      riots, the role of youth charities in London and the potential use of\u000a      mathematical modelling to address questions about the UK's social systems.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University College London (UCL)\u000a    ","Institutions":[{"AlternativeName":"University College London","InstitutionName":"University College London","PeerGroup":"A","Region":"London","UKPRN":10007784}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] Systems approaches for critical decisions, J.C.R. Hunt, S.R. Bishop\u000a      and Y. Timoshkina, pp. 197-202, In: Progress in Industrial Mathematics\u000a        at ECMI 2008, Mathematics in Industry 15 (2010), doi:10\/d9csq6\u000a    \u000a\u000a[2] System dynamics applied to operations and policy decisions, J.C.R.\u000a      Hunt, Y. Timoshkina, P.J. Baudains and S.R. Bishop, European Review,\u000a      20, 324-342 (2012) doi:10\/ph8\u000a    \u000a\u000a[3] A mathematical model of the London riots and their policing, T.P.\u000a      Davies, H. Fry, A.G. Wilson and S. Bishop, Nature Scientific Reports,\u000a      3, 1303 (2013) doi:10.1038\/srep01303\u000a    \u000aResearch grant: EPSRC ENFOLD-ing &#8212; Explaining, Modelling, and\u000a      Forecasting Global Dynamics; Reference: EP\/H02185X\/1; Dates: 2010-2015;\u000a      Value: circa &#163;3 million across several UCL departments; PI: Professor Sir\u000a      Alan Wilson, Centre for Advanced Spatial Analysis; Professor Steven\u000a      Bishop, Department of Mathematics\u000a    ","ResearchSubjectAreas":[{"Level1":"16","Level2":"2","Subject":"Criminology"},{"Level1":"16","Level2":"5","Subject":"Policy and Administration"}],"Sources":"\u000a    [A] Corroboration that Fry won the Focus on the Positive event can be\u000a      found online; for example, see the New York Times webpage: http:\/\/nyti.ms\/GWDL93\u000a      (halfway down the page).\u000a    [B] Video exploring the impact of Focus on the Positive, available online\u000a      at YouTube: http:\/\/youtu.be\/G4XrhcQ5khs\u000a      &#8212; corroborates the quotes from Pauline Pearce.\u000a    [C] Comment stream on the Can maths predict a riot? video,\u000a      available online at YouTube: http:\/\/youtu.be\/cY5iARq0nCc\u000a      &#8212; corroborates that the research and the video sparked public discussion\u000a      and debate.\u000a    [D] Blog post on flowingdata.com: http:\/\/flowingdata.com\/2013\/07\/18\/predicting-riots\/\u000a      &#8212; corroborates that the film stimulated public interest in the research.\u000a    [E] Article on popsci.com: http:\/\/www.popsci.com\/technology\/article\/2013-07\/math-rioting-looks-lot-shopping\u000a      &#8212; corroborates that the film stimulated media interest in the research.\u000a    [F] Tweet shared by Labour MP Tom Watson:\u000a      https:\/\/twitter.com\/TimPendry\/status\/361164461022781440\u000a      &#8212; corroborates that the film stimulated public interest in the research.\u000a    [G] Riot table Twitter account: https:\/\/twitter.com\/RiotSim\u000a      &#8212; corroborates the number of scores recorded on the riot table and hence\u000a      the number of players.\u000a    [H] Article metrics for the Nature Scientific Reports paper:\u000a      http:\/\/www.nature.com\/srep\/2013\/130221\/srep01303\/metrics\u000a      &#8212; corroborates the number of page views and corroborates that the paper\u000a      has a high altmetric score, which indicates that it has stimulated a high\u000a      amount of public interest.\u000a    [I] TEDx talk on TED.com: http:\/\/www.ted.com\/talks\/hannah_fry_is_life_really_that_complex.html\u000a      &#8212; the detailed discussions in the comments corroborate that the research\u000a      has stimulated public interest and discourse on the topic and on\u000a      complexity science in general.\u000a    [J] TEDx talk on the TEDx YouTube channel: http:\/\/youtu.be\/LnQYJa9-aR0\u000a      &#8212; the detailed discussions in the comments corroborate that the research\u000a      has stimulated public interest and discourse on the topic and on\u000a      complexity science in general.\u000a    [K] A compilation of feedback from three members of the public is\u000a      available on request, and corroborates that the research stimulated public\u000a      interest and informed understanding of mathematical modelling and\u000a      applications of complexity theory to the real world. \u000a    ","Title":"\u000a    2011 London riots: Improving public understanding of mathematical\u000a      modelling\u000a    ","UKLocation":[{"GeoNamesId":"6690877","Name":"Brixton"},{"GeoNamesId":"2644688","Name":"Leeds"},{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The Department of Mathematics at UCL has a strong background in exploring\u000a      the relevance of mathematical concepts to problems of the real world,\u000a      particularly those of interest to policy makers. For example, Steven\u000a      Bishop (Professor 2004-present) examined in 2010 the potential of\u000a      employing systems dynamics approaches to policy decisions [1], and\u000a      conceptualised combining the methods of bottom-up system dynamics with\u000a      statistical modeling in 2012 [2]. The techniques and ideas from this work\u000a      formed the basis of the department's mathematical model of a recent real\u000a      world problem: the 2011 London riots.\u000a    In summer 2011, London experienced the worst period of sustained rioting,\u000a      violence, looting and arson seen in the UK for over 20 years. Following\u000a      these events, Bishop and Hannah Fry (Research Associate 2010-2012) from\u000a      the Department of Mathematics collaborated with researchers from UCL's\u000a      Department of Security and Crime Science, UCL's Centre for Advanced\u000a      Spatial Analysis (CASA) and the Metropolitan Police Service to undertake\u000a      an interdisciplinary project, researching the causes of the London riots\u000a      with the aim of informing policing strategies for similar events in the\u000a      future.\u000a    The initial part of the collaborative London riots research project\u000a      revolved around a dataset of all arrests made in connection with the\u000a      riots, including the home and offence location of each suspect. The\u000a      richness of the dataset allowed for a spatial and temporal statistical\u000a      analysis of the riots, highlighting the important factors such as\u000a      deprivation, transport and the role that retail centres played in the\u000a      events. This statistical analysis then served as the basis for a second,\u000a      more speculative research challenge: constructing a mathematical model of\u000a      the riots capable of replicating the general patterns and dynamics\u000a      identified, with the potential to investigate the effects of a range of\u000a      alternative policing strategies. The model, built by Fry and Bishop in\u000a      2012 and published in Nature's Scientific Reports in February 2013 [3],\u000a      aimed to exploit the analogies between the mechanisms highlighted in the\u000a      earlier analysis, and existing well-studied models of the physical and\u000a      natural world. In the three-stage process employed, models of epidemiology\u000a      reflected the contagious nature of the `idea to riot'; models of retail\u000a      spending flows were exploited to describe how the rioters converged at\u000a      certain areas throughout the city; and Lotka-Volterra dynamics allowed for\u000a      simulated interaction between rioters and police.\u000a    Despite limited information on initial disturbances, the model captures\u000a      many of the significant patterns seen in the data and accurately predicts\u000a      which areas of the city were more susceptible than others, yielding\u000a      insight into the mechanisms that drew people into the riots. The\u000a      statistical analysis served to demonstrate that rioters were predominantly\u000a      in their late teens or early twenties and came from some of the poorest\u000a      areas of London &#8212; areas with the worst schools, lowest incomes and highest\u000a      unemployment levels. Alongside these findings, the model also provides a\u000a      means with which to test various policing strategies and serves as a proof\u000a      of concept for future collaborations with the police service.\u000a    Mathematical models based on real-world data to assist with predictive\u000a      policing are a relatively new area of interest within the research\u000a      community. This contribution by UCL offers some novel techniques and\u000a      serves well as a proof-of-concept for future work.\u000a    "},{"CaseStudyId":"30651","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2921044","Name":"Germany"}],"Funders":["Royal Society"],"ImpactDetails":"\u000d\u000a    UCL research findings on long-range noise propagation have led to\u000d\u000a      improved understanding and changes in thinking about highway noise\u000d\u000a      mitigation at ADOT. The findings have also had an impact on a variety of\u000d\u000a      other organisations and individuals worldwide, informing their\u000d\u000a      understanding and stimulating discourse about the assessment of noise\u000d\u000a      propagation, not only from roads, but also from wind turbines, and\u000d\u000a      additionally from a gas compressor station and various sources in its\u000d\u000a      vicinity, including a railway line.\u000d\u000a    Impact on thinking about traffic noise mitigation at ADOT: The\u000d\u000a      principal goal of ADOT's ten-year QPPP is to assess the effectiveness of\u000d\u000a      rubberised asphalt at reducing environmental noise from urban freeways, in\u000d\u000a      comparison with other noise mitigation strategies, such as noise barriers.\u000d\u000a      In addition to the public health benefits arising from a reduction of\u000d\u000a      freeway noise in residential areas, using alternative noise mitigation\u000d\u000a      strategies may also be cost effective, as the typical concrete noise\u000d\u000a      barriers used in Arizona are very expensive and cost approximately $1\u000d\u000a      million per mile to construct. Since the UCL research was funded by ADOT\u000d\u000a      as part of the QPPP, major policy changes will not occur until the\u000d\u000a      project's field trials have been completed in 2015; however, UCL research\u000d\u000a      findings have already impacted upon thinking about noise mitigation at\u000d\u000a      ADOT, causing the department to consider more carefully the effectiveness\u000d\u000a      of costly noise barriers and how they invest in noise mitigation\u000d\u000a      strategies [A].\u000d\u000a    ADOT currently makes use of the widely used FHWA Traffic Noise Model\u000d\u000a      (TNM) v2.5 for assessing the environmental noise impact of new and\u000d\u000a      existing freeways. The UCL research findings suggest, however, that the\u000d\u000a      noise barriers used throughout Arizona and elsewhere may not be as\u000d\u000a      effective at reducing noise as the FHWA-TNM might indicate; this\u000d\u000a      difference occurs because TNM does not incorporate local meteorological\u000d\u000a      effects whereas the model in reference [1] above does. Indeed, references\u000d\u000a      [1] and [2] above strongly suggest that typical meteorological conditions\u000d\u000a      can readily defeat noise barriers that are erected based simply on\u000d\u000a      blocking the line-of-sight to residential areas of concern. These research\u000d\u000a      findings helped convince ADOT that investigation into better noise barrier\u000d\u000a      technology is required. In October 2012, this realisation led ADOT to\u000d\u000a      announce a call for a follow-on three-year project entitled \"Materials\u000d\u000a      Properties and Optimum Geometries for the Design of Noise Walls\" [text\u000d\u000a      removed for publication] [A]. The project entails inviting noise barrier\u000d\u000a      manufacturers to present novel noise barrier designs to a panel of project\u000d\u000a      researchers, with selected designs subsequently being tested using the\u000d\u000a      combined experimental\/theoretical approach developed in references [1],\u000d\u000a      [2] and [3] above, incorporating both measured and simulated\u000d\u000a      meteorological data.\u000d\u000a    Raised awareness at the Institute of Acoustics (IoA): In May 2013,\u000d\u000a    Ovenden was contacted by a community campaigner in the UK who was interested\u000d\u000a    in the relevance of the findings of reference [1] above to the assessment of\u000d\u000a    noise propagation from wind turbines. In particular, the campaigner was\u000d\u000a    concerned that the \"Good Practice Guide for Wind Turbine Noise\", being\u000d\u000a    launched at that time by the IoA at the request of the Department of Energy\u000d\u000a    and Climate Change, recommended a propagation model (ISO9613-2) that did not\u000d\u000a    sufficiently include wind shear and therefore did not account for some of\u000d\u000a    the enhanced effects that had been predicted using the model in [1] above.\u000d\u000a    This interest stimulated a detailed discussion between Ovenden and the\u000d\u000a    campaigner about the science and the issues [B], and led to Ovenden writing\u000d\u000a    to the UK's Chief Scientific Adviser and to the President of the IoA raising\u000d\u000a    his concerns about the use of ISO9613-2 - a method originally designed for\u000d\u000a    near-ground noise sources &#8212; in assessing the environmental impact of\u000d\u000a    high-altitude noise sources such as wind turbine facilities, with the\u000d\u000a    results of [1] used as evidence of this concern. The President of the IoA\u000d\u000a    reported back that Ovenden's \"concern in relation to the use of ISO9613-2 as\u000d\u000a    the method of propagation was discussed at length by the GPG [Good Practice\u000d\u000a    Guide] working group\" [C] at the IoA's council meeting in June, and that,\u000d\u000a    while changes will not be made at this point, they recognise \"that many\u000d\u000a    arguments are made for a different approach to the assessment of wind farm\u000d\u000a    noise.\" [C]\u000d\u000a    Improved understanding of sound-related issues in AZ tribal areas:\u000d\u000a      In October 2010, Ovenden was contacted by a co-founder of a consultancy\u000d\u000a      firm called EN3 Professionals in Flagstaff AZ, which specialises in\u000d\u000a      engineering, environmental and energy-related matters, with some questions\u000d\u000a      about the research findings in reference [1] above in relation to one of\u000d\u000a      the firm's current projects. After continued discussion and a meeting in\u000d\u000a      April 2013, the co-founder invited Ovenden to offer his opinion (based on\u000d\u000a      his work with ADOT) on an environmental baseline sound level report for a\u000d\u000a      groundwater remediation project in the vicinity of a gas compressor\u000d\u000a      station near Topock, AZ. The project is of interest to local tribes,\u000d\u000a      including the Hualapai, Cocopah, Fort Mojave, Chemehuevi and Colorado\u000d\u000a      River Indian tribes, as the area is of great cultural significance to\u000d\u000a      these communities. Ovenden's contribution led to improved understanding of\u000d\u000a      the project's sound-related issues; EN3's co-founder said: \"Dr. Ovenden's\u000d\u000a      review of and comment on the baseline sound studies completed to date at\u000d\u000a      Topock was vital for identifying how the studies can be improved to better\u000d\u000a      address tribal issues.\" [D]\u000d\u000a    Raised awareness and understanding at Coconino County authorities:\u000d\u000a      Also in April 2013, a meeting was held between Ovenden and the Head of\u000d\u000a      Coconino County's Community Development Department and a colleague to\u000d\u000a      discuss noise concerns from wind farms. Their awareness and understanding\u000d\u000a      of the issues were informed by the findings of reference [1] above, which\u000d\u000a      were discussed at the meeting, and they expressed much interest in using\u000d\u000a      such a methodology to see how local meteorology could be taken into\u000d\u000a      account when assessing the noise impact of the county's current and\u000d\u000a      proposed wind energy sites.\u000d\u000a    Greater understanding at the Acoustic Ecology Institute (AEI): The\u000d\u000a      AEI is a US non-profit organisation that aims to inform public debate\u000d\u000a      about the environmental impact of noise. The institute's Executive\u000d\u000a      Director contacted Ovenden in early 2010 as he was very intrigued by the\u000d\u000a      fact that the model in reference [1] above predicts in some cases\u000d\u000a      increasing near-ground sound levels and sound focusing beyond 400-plus\u000d\u000a      metres from the source. Subsequent correspondence with Ovenden resulted in\u000d\u000a      greater understanding within the AEI about such work and how it might be\u000d\u000a      used to assess the long-range propagation of noise from wind farms. This\u000d\u000a      led to reference [1] being highlighted in the organisation's newsletter in\u000d\u000a      February 2010 [E], where it is noted that Ovenden's model combines field\u000d\u000a      recordings and new acoustic modelling, and demonstrates significant\u000d\u000a      differences of 10-20dB in noise levels from those predicted by traditional\u000d\u000a      sound models at 300 metres and beyond, thereby offering the possibly of\u000d\u000a      regulatory limits being breached at much greater distances than perhaps\u000d\u000a      previously considered.\u000d\u000a    Stimulation of public interest and discourse: The research in\u000d\u000a      reference [1] above was highlighted by the Acoustical Society of America\u000d\u000a      (ASA) in April 2011 as being of interest to the general public. The\u000d\u000a      society asked Ovenden to prepare a lay language version of the paper to\u000d\u000a      help disseminate information about acoustics to the general public and\u000d\u000a      elevate awareness about topics in acoustics to a worldwide audience.\u000d\u000a      Following this lay language version [F] and an invited talk at the 161st\u000d\u000a      ASA meeting in May 2011, the research received significant media attention\u000d\u000a      (including a news story on the NBC News website [G] and an interview on\u000d\u000a      Deutschlandradio [H], both in May 2011), stimulating public discourse and\u000d\u000a      interest internationally.\u000d\u000a    Examples include:\u000d\u000a    \u000d\u000a      In June 2011, Ovenden was contacted by a community group in Germany\u000d\u000a        concerned about traffic noise from a neighbouring autobahn, where the\u000d\u000a        local mountainous topography creates significant highway crosswinds. The\u000d\u000a        group requested a copy of reference [1] above, as they believed the\u000d\u000a        paper's results would be useful in the group's battle with their local\u000d\u000a        authorities about what measures should be employed to reduce noise.\u000d\u000a      At the ASA meeting in May 2011, Ovenden was approached by employees\u000d\u000a        at Washington State Department of Transportation concerning noise\u000d\u000a        complaints they received from residential areas located a mile or more\u000d\u000a        from the apparent noise sources of concern (e.g. construction noise in\u000d\u000a        particular). They indicated their strong interest in any further field\u000d\u000a        tests validating the research findings.\u000d\u000a      The US Federal Aviation Administration contacted the authors of\u000d\u000a        reference [1] by email in June 2011 requesting a copy of the paper, as\u000d\u000a        they also have an interest in using detailed weather information for the\u000d\u000a        prediction of sound propagation.\u000d\u000a      In March 2013, a Berlin-based artist requested to know more about the\u000d\u000a        propagation mechanisms in the research [I]. The artist stated in her\u000d\u000a        email: \"Being myself a sonic artist living and working directly by the\u000d\u000a        river Spree in Berlin, I recognize similarities in my experiences and\u000d\u000a        observations with those of your article\" [I]. She wishes to compare the\u000d\u000a        research work with her own observations and ideas in producing sonic\u000d\u000a        artwork.\u000d\u000a    \u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research carried out in UCL's Department of Mathematics addresses the\u000d\u000a      accurate coupling of acoustic source fields to noise propagation models,\u000d\u000a      for the determination of far-field environmental noise exposure. The work\u000d\u000a      has increased understanding of issues related to noise propagation from\u000d\u000a      infrastructure including roads and wind turbines, in the UK and\u000d\u000a      internationally. For example, it has led to changes in thinking about\u000d\u000a      freeway noise mitigation strategies at Arizona Department of\u000d\u000a      Transportation (ADOT), discussion of concerns about the UK's assessment of\u000d\u000a      noise propagation from wind turbines by the Institute of Acoustics, and\u000d\u000a      improved understanding of sound-related issues associated with a gas\u000d\u000a      compressor station in the southwestern US that are of interest to local\u000d\u000a      Indian tribes. The research also stimulated interest and discourse by\u000d\u000a      groups and individuals including the Acoustic Ecology Institute in the US,\u000d\u000a      a community group in Germany, Washington State Department of\u000d\u000a      Transportation, the US Federal Aviation Administration, and an artist\u000d\u000a      based in Berlin.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University College London (UCL)\u000d\u000a    ","Institutions":[{"AlternativeName":"University College London","InstitutionName":"University College London","PeerGroup":"A","Region":"London","UKPRN":10007784}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"5551752","Name":"Arizona"},{"GeoNamesId":"1179245","Name":"Federally Administered Tribal Areas"},{"GeoNamesId":"2950159","Name":"Berlin"}],"References":"\u000d\u000a    \u000a[1] Impact of meteorological conditions on noise propagation from freeway\u000d\u000a      corridors, N.C. Ovenden, S.R. Shaffer and H.J.S. Fernando, J. Acoust.\u000d\u000a        Soc. Am., 126(1), 25-35 (2009) doi:10\/b7rr7d\u000d\u000a    \u000a\u000a[2] Investigations of environmental effects on freeway acoustics, H.J.S.\u000d\u000a      Fernando, N.C. Ovenden and S.R. Shaffer, Arizona Dept. of\u000d\u000a        Transportation Research Reports SPR-605(1) (2010) http:\/\/bounced.azdot.gov\/TPD\/ATRC\/publications\/project_reports\/PDF\/AZ605-1.pdf\u000d\u000a    \u000a\u000a[3] Investigations of environmental effects on freeway acoustics, S.R.\u000d\u000a      Shaffer, H.J.S. Fernando and N.C. Ovenden, Arizona Dept. of\u000d\u000a        Transportation Research Reports SPR-605(2) (2013) http:\/\/wwwa.azdot.gov\/adotlibrary\/publications\/project_reports\/PDF\/AZ605(2).pdf\u000d\u000a    \u000aResearch grant: Royal Society International Exchanges Scheme (Feb\u000d\u000a      2012-Jan 2014) &#8212; &#163;11,200\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"9","Level2":"13","Subject":"Mechanical Engineering"},{"Level1":"2","Level2":"3","Subject":"Classical Physics"}],"Sources":"\u000d\u000a    [A] Supporting letter from QPPP Project Manager, ADOT &#8212; corroborates that\u000d\u000a      the research has led to improved understanding and changes in thinking\u000d\u000a      about noise mitigation at ADOT. Also corroborates the details of the\u000d\u000a      follow-on project. Available on request.\u000d\u000a    [B] Email correspondence with community campaigner &#8212; corroborates that\u000d\u000a      the research has informed understanding about the science and stimulated\u000d\u000a      discourse. Available on request.\u000d\u000a    [C] Email correspondence with the President of the IoA &#8212; corroborates\u000d\u000a      that Ovenden's concerns were discussed by the IoA's GPG working group.\u000d\u000a      Available on request.\u000d\u000a    [D] Supporting letter from a co-founder of EN3 Professionals &#8212;\u000d\u000a      corroborates that the research and further discussions with Ovenden have\u000d\u000a      informed understanding of sound-related issues at EN3. Available on\u000d\u000a      request.\u000d\u000a    [E] The AEI's February 2010 newsletter is available online: http:\/\/bit.ly\/18ppiMV\u000d\u000a      (the review of [1] is on page 17) &#8212; corroborates that understanding of the\u000d\u000a      research and its potential use was informed.\u000d\u000a    [F] The lay language version of reference [1] is available online: http:\/\/bit.ly\/1hZmKGt\u000d\u000a    [G] The news story about the research on NBC News can be seen at: http:\/\/nbcnews.to\/1cZ3j2I\u000d\u000a      &#8212; corroborates that the research stimulated media interest.\u000d\u000a    [H] A transcript of the broadcast about the research on Deutschlandradio\u000d\u000a      can be seen at:\u000d\u000a      http:\/\/bit.ly\/17oUxaI &#8212; corroborates\u000d\u000a      that the research stimulated media interest.\u000d\u000a    [I] Email received from a Berlin-based artist &#8212; corroborates that the\u000d\u000a      artist's creative interest was stimulated by the research. Available on\u000d\u000a      request. \u000d\u000a    ","Title":"\u000d\u000a    Enhancing civil and public understanding of environmental noise\u000d\u000a      propagation\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Assessing the impact of noise mitigation strategies on communities\u000d\u000a      residing in the vicinity of freeways, airports and wind turbine farms\u000d\u000a      requires accurate determination of the often highly localised sound\u000d\u000a      sources in combination with efficient modelling of the transmission of\u000d\u000a      that noise over great distances. Nick Ovenden, a researcher in UCL's\u000d\u000a      Department of Mathematics (Lecturer 2005-2012; Senior Lecturer\u000d\u000a      2012-present), has worked with international collaborators to tackle such\u000d\u000a      issues in the case of urban freeway noise, in order to enable urban\u000d\u000a      planners to assess efficiently how modifications or mitigation strategies\u000d\u000a      implemented at or near the source affect the far-field noise\u000d\u000a      characteristics.\u000d\u000a    From 2006 to 2009, Ovenden worked with Professor Harindra Fernando at\u000d\u000a      Arizona State University (ASU) in a combined experimental and theoretical\u000d\u000a      investigation for ADOT [1, 2], to study the effect of local meteorological\u000d\u000a      conditions on the propagation of freeway noise. The research was funded by\u000d\u000a      ADOT as part of their ten-year Quiet Pavement Pilot Programme (QPPP),\u000d\u000a      approved by the Federal Highways Administration (FHWA) in June 2003. Sound\u000d\u000a      data and experimental measurements of vertical wind and temperature\u000d\u000a      gradients were collected simultaneously at two freeway sites and input\u000d\u000a      into a theoretical noise propagation model to predict the noise levels far\u000d\u000a      from the freeway. The theoretical modelling was carried out by Ovenden,\u000d\u000a      who also participated in the field experiments led by Fernando's team at\u000d\u000a      ASU. The acoustic field close to the source was constructed using an\u000d\u000a      extended Green's function formulation for several virtual line sources,\u000d\u000a      where the height and strength of each line source were determined via a\u000d\u000a      least-squares minimisation to replicate accurately the sound measurements\u000d\u000a      made in the field. This Green's function near-source model was then\u000d\u000a      coupled to a parabolic equation model for sound propagation out to 600\u000d\u000a      metres from the freeway. The research demonstrated that measured\u000d\u000a      meteorological conditions, such as temperature inversions and wind shear,\u000d\u000a      could create sound-focusing phenomena over a third of a mile from the\u000d\u000a      freeway, potentially into residential areas, and that this sound could\u000d\u000a      exceed state guidelines on acceptable noise levels. In addition, the\u000d\u000a      research questioned the effectiveness of noise barriers, particularly\u000d\u000a      those of reduced height as they do not block sound emitted at higher\u000d\u000a      angles of incidence that can still be refracted back downwards towards\u000d\u000a      residential areas.\u000d\u000a    Between 2009 and 2012, Ovenden and Fernando extended their technique to\u000d\u000a      incorporate terrain effects and also to enable vertically refined local\u000d\u000a      meteorological data derived from the National Center for Atmospheric\u000d\u000a      Research's Weather Research and Forecasting model to be used in predicting\u000d\u000a      long-range noise propagation [3]. The findings from the work on freeway\u000d\u000a      noise are also applicable to wind turbine noise; indeed, in 2012 Ovenden\u000d\u000a      and Fernando (now at the University of Notre Dame, USA) began a two-year\u000d\u000a      project funded by the Royal Society to examine noise propagation from wind\u000d\u000a      turbines in complex environmental and topographical conditions. In this\u000d\u000a      latest project, the theoretical model from the earlier work [1] is being\u000d\u000a      developed further with (i) more focus on accurately determining the\u000d\u000a      behaviour of the ducted part of the sound field close to the ground, and\u000d\u000a      (ii) incorporation of the shear effects from the wake of a single wind\u000d\u000a      turbine or a line of wind turbines. Furthermore, field measurements from\u000d\u000a      wind turbines based at the University of Notre Dame and meteorological\u000d\u000a      measurements taken in mountainous terrain are currently being analysed for\u000d\u000a      input into the modified sound propagation model.\u000d\u000a    "},{"CaseStudyId":"30652","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"3144096","Name":"Norway"},{"GeoNamesId":"3996063","Name":"Mexico"}],"Funders":["Natural Environment Research Council"],"ImpactDetails":"\u000d\u000a    The research on wind resource estimation (project (a), above) has\u000d\u000a      improved the understanding of\u000d\u000a      wind power production estimates at the Norwegian power company Agder\u000d\u000a      Energi, whose wind\u000d\u000a      mast data were used in the research. The finding that missing wind speed\u000d\u000a      data caused by frozen\u000d\u000a      wind masts can introduce bias into wind power production estimates and\u000d\u000a      profitability assessments,\u000d\u000a      and the new method for imputing the missing data that was developed,\u000d\u000a      raised awareness in the\u000d\u000a      Wind &amp; Site division at Agder Energi in 2008 about the extent of the\u000d\u000a      problem with missing data\u000d\u000a      from wind measurements and the need to employ more suitable statistical\u000d\u000a      methods for dealing with\u000d\u000a      this problem than those that are currently used within the company [A].\u000d\u000a    Specifically, the research made a two-fold direct contribution: (i)\u000d\u000a      highlighted how Agder Energi's\u000d\u000a      matrix approach is not as stable as their sectoral regression one in\u000d\u000a      dealing with missing data; and\u000d\u000a      (ii) developed a methodology to assess the economic impact of missing data\u000d\u000a      both in terms of\u000d\u000a      expected revenues and the distribution of revenues. Although these\u000d\u000a      knowledge transfers cannot\u000d\u000a      yet be quantified, they have provided valuable insights about the\u000d\u000a      suitability of alternative\u000d\u000a      approaches for correcting for missing data.\u000d\u000a    DER-CAM (project (b), above) is enabling energy consumers to\u000d\u000a      manage their energy resources\u000d\u000a      better and minimise energy costs. It is also having an environmental\u000d\u000a      impact since the use of\u000d\u000a      energy resources is being managed more efficiently. DER-CAM has been\u000d\u000a      deployed at sites\u000d\u000a      including the Segundo Dining Commons building at the University of\u000d\u000a      California at Davis (in 2011),\u000d\u000a      the Santa Rita Jail in Alameda County, California (in 2012), and the\u000d\u000a      Mechanical Engineering\u000d\u000a      building at UNM (in 2012) [B].\u000d\u000a    DER-CAM was installed at UNM to manage the operation of chilled water\u000d\u000a      storage tanks and an\u000d\u000a      absorption chiller powered by hot water from a solar array. The building\u000d\u000a      required an automated\u000d\u000a      procedure for forecasting cooling demand, scheduling optimal dispatch of\u000d\u000a      the absorption chiller,\u000d\u000a      and charging-discharging storage units optimally, operations which could\u000d\u000a      not be handled by relying\u000d\u000a      on simple rules of thumb or heuristics. The university reports that\u000d\u000a      \"DER-CAM proved to be the\u000d\u000a      most effective solution\" for managing their systems optimally and that the\u000d\u000a      use of DER-CAM has\u000d\u000a      resulted in tangible cost savings that are \"especially significant over\u000d\u000a      the shoulder season\" [C].\u000d\u000a      Thus, they are realising significant reductions in their energy bill by\u000d\u000a      using solar power for cooling\u000d\u000a      via storage in an automated way. UNM also benefits from the ease of use of\u000d\u000a      DER-CAM; they\u000d\u000a      informed us that \"one of the principal advantages of using DER-CAM is\u000d\u000a      that, once set up, it does\u000d\u000a      not require user intervention to set system parameters\" [C].\u000d\u000a    UCL research has also improved knowledge and understanding of the power\u000d\u000a      sector and options\u000d\u000a      pricing among industry professionals and policymakers through the\u000d\u000a      following knowledge transfer\u000d\u000a      events:\u000d\u000a    (1) Two ELDEV workshops, underpinned by the UCL research in project (a),\u000d\u000a      above, in addition to\u000d\u000a      other UCL research conducted as part of the ELDEV project, were held in\u000d\u000a      Trondheim, Norway,\u000d\u000a      during 2009 and 2010, and were attended by numerous power companies. For\u000d\u000a      example, Agder\u000d\u000a      Energi, Statkraft, and Tr&#248;nder Energi represented the Norwegian power\u000d\u000a      sector, which faces issues\u000d\u000a      about valuing renewables and transmission investment. Participants gained\u000d\u000a      an improved\u000d\u000a      understanding of the factors involved in making decisions under\u000d\u000a      uncertainty, e.g., modelling energy\u000d\u000a      prices, hedging risk exposure via financial and physical positions, and\u000d\u000a      investment appraisal. In\u000d\u000a      addition, their increased awareness of the issues raised by the ELDEV\u000d\u000a      project stimulated industrial\u000d\u000a      companies in Norway to co-fund subsequent research projects. For example,\u000d\u000a      ELCARBONRISK\u000d\u000a      (2010-2014, NOK 13.5 million with participation of power companies Eidsiva\u000d\u000a      Energi and Tafjord\u000d\u000a      Karft) and PURELEC (2011-2014, NOK 8.5 million with involvement of power\u000d\u000a      companies NTE and\u000d\u000a      SAE Vind) are both focusing more on modelling energy prices and analysing\u000d\u000a      investment in\u000d\u000a      renewables, two issues that were highlighted in ELDEV.\u000d\u000a    (2) Two workshops sponsored by the UK Energy Research Centre (UKERC) in\u000d\u000a      2008 and 2009 on\u000d\u000a      financial methods, underpinned by the UCL research in references [4] and\u000d\u000a      [5] in project (c), have\u000d\u000a      transferred knowledge from UCL academics to industry and policymakers. The\u000d\u000a      2008 workshop had\u000d\u000a      around 35 attendees, including representatives from power companies\u000d\u000a      (including EDF Energy),\u000d\u000a      consulting firms and ministries (including the Ministry of Energy, Mexico)\u000d\u000a      [D]. The 2009 UKERC\u000d\u000a      workshop was hosted by the Department of Energy and Climate Change (DECC)\u000d\u000a      with about fifty\u000d\u000a      participants. This exchange allowed UK policymakers to gain a better\u000d\u000a      understanding of the benefits\u000d\u000a      and limitations of using financial methods. Furthermore, the 2009 workshop\u000d\u000a      counted as part of\u000d\u000a      DECC's economics training for the year.\u000d\u000a    (3) The findings of the optionality research conducted in project (c),\u000d\u000a      above, during the \"An Options\u000d\u000a      Approach to UK Energy Futures\" project were disseminated to the public and\u000d\u000a      private sector via two\u000d\u000a      events held at the LBS: \"Decision Analysis and Real Options for Energy\"\u000d\u000a      (24 April 2012) and\u000d\u000a      \"Climate Policy, Risk and Energy Investment\" (2-3 May 2012). The knowledge\u000d\u000a      transfer facilitated\u000d\u000a      by these events, which had around 20 and 80 attendees respectively, has\u000d\u000a      enabled companies\u000d\u000a      active in the power sector to advise managers and clients better about\u000d\u000a      financial fundamentals and\u000d\u000a      the propagation of risk. An attendee (of both events), from the PR firm\u000d\u000a      Ketchum, reported that \"The\u000d\u000a      workshop really helped us to understand the energy market dynamics and\u000d\u000a      investment options\u000d\u000a      much better, including the risks and inherent optionality involved in\u000d\u000a      capex-related corporate\u000d\u000a      decision-making process\" and that \"For us as communication consultants,\u000d\u000a      this knowledge and\u000d\u000a      increased understanding of the underlining financial and business\u000d\u000a      fundamentals of the energy\u000d\u000a      infrastructure investment has enabled us to advise our clients much better\u000d\u000a      in terms of both the\u000d\u000a      corporate positioning and strategic communications &#8212; especially given the\u000d\u000a      challenging and rather\u000d\u000a      uncertain operating environment\" [E].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Financial engineering and optimisation provide both power companies and\u000d\u000a      consumers with better\u000d\u000a      decision support in deregulated energy sectors. UCL research has delivered\u000d\u000a      the following benefits\u000d\u000a      to decision makers: (i) a clearer understanding of the role of statistical\u000d\u000a      analysis in imputing missing\u000d\u000a      data on wind speeds and (ii) reduction in energy costs by optimised\u000d\u000a      scheduling of energy\u000d\u000a      technologies. Other benefits have been (i) investment in follow-up\u000d\u000a      research projects by industrial\u000d\u000a      companies and (ii) knowledge transfer via workshops.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University College London (UCL)\u000d\u000a    ","Institutions":[{"AlternativeName":"University College London","InstitutionName":"University College London","PeerGroup":"A","Region":"London","UKPRN":10007784}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"3133880","Name":"Trondheim"}],"References":"\u000d\u000a    \u000a[1] The effect of missing data on wind resource estimation, A. Coville,\u000d\u000a      A. S. Siddiqui and K.\u000d\u000a      Vogstad, Energy, 36(7), 4505-4517 (2011) doi:10.1016\/j.energy.2011.03.067\u000d\u000a    \u000a\u000a[2] Applications of optimal building energy system selection and\u000d\u000a      operation, C. Marnay, M. Stadler,\u000d\u000a      A. S. Siddiqui, N. DeForest, J. Donadee, P. Bhattacharya and J. Lai, Journal\u000d\u000a        of Power and\u000d\u000a        Energy, 227(1), 82-93 (2013) doi:10.1177\/0957650912468408\u000d\u000a    \u000a\u000a[3] The value of capacity sizing under risk aversion and operational\u000d\u000a      flexibility, M. Chronopoulos, B.\u000d\u000a      De Reyck and A. Siddiqui, IEEE Transactions on Engineering Management,\u000d\u000a      60, 272-288 (2013)\u000d\u000a      doi:10.1109\/TEM.2012.2211363\u000d\u000a    \u000a\u000a[4] How to proceed with competing alternative energy technologies: A real\u000d\u000a      options analysis, A. S.\u000d\u000a      Siddiqui and S.-E. Fleten, Energy Economics, 32(4), 817-830 (2010)\u000d\u000a      doi:10\/bvffnf\u000d\u000a    \u000a\u000a[5] Capacity switching options under rivalry and uncertainty, A. S.\u000d\u000a      Siddiqui and R. Takashima,\u000d\u000a      European Journal of Operational Research, 222(3), 583-595 (2012) doi:10\/nt3\u000d\u000a    \u000aReferences [1], [4] and [5] best indicate the quality of the\u000d\u000a        underpinning research.\u000d\u000a    Selected research grants:\u000d\u000a    (i) Financial Engineering Analysis of Electricity Spot and Derivatives\u000d\u000a      Markets (ELDEV); PI: Sjur\u000d\u000a      Westgaard (Trondheim Business School), co-I: Stein-Erik Fleten (NTNU) and\u000d\u000a      Afzal Siddiqui (UCL);\u000d\u000a      sponsor: Mid-Norway Business Research Fund, Tr&#248;nder Energi, and Trondheim\u000d\u000a      Energi; 2008-2011; value: NOK 10 million (of which &#163;93,000 was UCL's share)\u000d\u000a    (ii) An Options Approach to UK Energy Futures; PI: Derek Bunn (London\u000d\u000a      Business School), co-I:\u000d\u000a      Afzal Siddiqui (UCL); sponsor: NERC NE\/GOO7748\/1 (via the UK Energy\u000d\u000a      Research Centre's Third\u000d\u000a      Round Research Fund); 2011-2012; value: &#163;130,000 (of which &#163;30,000 was\u000d\u000a      UCL's share)\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"9","Level2":"6","Subject":"Electrical and Electronic Engineering"}],"Sources":"\u000d\u000a    [A] Supporting statement from former Head of Wind &amp; Site at Agder\u000d\u000a      Energi (employed by Agder\u000d\u000a      Energi at the time of the impact) &#8212; corroborates the impact of research (a)\u000d\u000a      on Agder Energi.\u000d\u000a      Available on request.\u000d\u000a    [B] DER-CAM website: http:\/\/der.lbl.gov\/der-cam\u000d\u000a      &#8212; corroborates deployment of DER-CAM at sites\u000d\u000a      including Santa Rita Jail (http:\/\/der.lbl.gov\/microgrids-lbnl\/santa-rita-jail)\u000d\u000a      and University of California\u000d\u000a      at Davis (http:\/\/der.lbl.gov\/sites\/der.lbl.gov\/files\/davis_report_w_cover_LBNL-4285E.pdf).\u000d\u000a    [C] Supporting statement from Professor of Mechanical Engineering at the\u000d\u000a      University of New\u000d\u000a      Mexico (UNM) &#8212; corroborates the impact of DER-CAM (b) on the\u000d\u000a      Mechanical Engineering building\u000d\u000a      at the UNM, including energy cost savings. Available on request.\u000d\u000a    [D] A webpage about the 2008 UKERC workshop is available at:\u000d\u000a      http:\/\/www.ukerc.ac.uk\/support\/tiki-index.php?page=0807FinancialMethods.\u000d\u000a      The number and\u000d\u000a      types of attendees are corroborated by the `Attendee List' (http:\/\/bit.ly\/1aG6zNf).\u000d\u000a    [E] Supporting statement from Account Director at Ketchum &#8212; corroborates\u000d\u000a      the claim that the two\u000d\u000a      events in (3) led to increased knowledge and understanding at Ketchum and\u000d\u000a      enabled them to\u000d\u000a      advise clients better. Available on request.\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Improved decision making by the power sector and energy saving by\u000d\u000a      consumers\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Since 2005, UCL's Department of Statistical Science has been involved\u000d\u000a      with three research\u000d\u000a      projects that have addressed real-world problems: (a) ELDEV\u000d\u000a      (funded by the Mid-Norway\u000d\u000a      Business Research Fund) focused on using financial engineering to improve\u000d\u000a      business practices at\u000d\u000a      power companies; (b) The Distributed Energy Resources Customer\u000d\u000a      Adoption Model, or DER-CAM,\u000d\u000a      (funded by the US Department of Energy and the California Energy\u000d\u000a      Commission) used\u000d\u000a      optimisation to devise investment and operational strategies for users of\u000d\u000a      small-scale on-site power\u000d\u000a      generation; and (c) \"An Options Approach to UK Energy Futures\"\u000d\u000a      (funded by the Natural\u000d\u000a      Environment Research Council) took a more qualitative perspective to\u000d\u000a      illustrate the consequences\u000d\u000a      for UK energy policy of incorporating real options, i.e., managerial\u000d\u000a      discretion over investment\u000d\u000a      timing\/sizing or technology choice. Thus, all problems have directly\u000d\u000a      addressed real-world decision\u000d\u000a      making in deregulated energy sectors in Norway, the US, and the UK.\u000d\u000a    (a) To assess the likely generating capacities of wind farms at\u000d\u000a      promising sites, power companies\u000d\u000a      use anemometers to measure the wind speed at those locations. However,\u000d\u000a      these instruments tend\u000d\u000a      to freeze in the winter, resulting in the systematic loss of data. Since\u000d\u000a      wind speeds are highest in\u000d\u000a      the winter, ignoring these missing data or using a na&#239;ve correction\u000d\u000a      adversely affects the value of\u000d\u000a      new wind farms. A new methodology to correct for missing wind speed data\u000d\u000a      from anemometers\u000d\u000a      was developed during 2008 and 2009 by UCL researcher Afzal Siddiqui\u000d\u000a      (Lecturer in Statistics\u000d\u000a      2005-2010; Senior Lecturer 2010-present) together with Agder Energi\u000d\u000a      employee Klaus-Ole\u000d\u000a      Vogstad [1]. This work was conducted as part of the ELDEV project.\u000d\u000a    The seasonality-based approach that was developed is novel in that it\u000d\u000a      preserves the\u000d\u000a      autocorrelation structure of wind speeds and directions while modelling\u000d\u000a      electricity prices accurately\u000d\u000a      via a mean-reverting jump-diffusion (MRJD) process. Using artificially\u000d\u000a      removed data from the\u000d\u000a      Geitvassfjellet site, the researchers demonstrated that the proposed\u000d\u000a      methodology is able to restore\u000d\u000a      accurately the probability density function of the annual revenues from a\u000d\u000a      proposed wind turbine [1].\u000d\u000a      UCL's specific contribution was in the form of stochastic models for\u000d\u000a      electricity prices and a\u000d\u000a      statistical procedure for imputing wind speeds.\u000d\u000a    (b) Greater deregulation of the energy sector is likely to lead to\u000d\u000a      more decentralised decision\u000d\u000a      making by energy producers and consumers alike. While large power\u000d\u000a      companies have the\u000d\u000a      resources to manage such a transition, consumers often lack the expertise\u000d\u000a      in optimisation and\u000d\u000a      computation to do so. DER-CAM has been developed to help consumers with\u000d\u000a      this problem; it\u000d\u000a      enables them to minimise emissions or the cost of operating their energy\u000d\u000a      system by assessing\u000d\u000a      their energy profile, prevailing market information, and information on\u000d\u000a      their distributed energy\u000d\u000a      resources technology. A large number of resources (both for energy\u000d\u000a      production and storage) can\u000d\u000a      be handled, as well as flexible demand and details of building\u000d\u000a      thermodynamics. DER-CAM can\u000d\u000a      determine which on-site power generation and combined heat and power\u000d\u000a      systems a user should\u000d\u000a      install, and how and when they should be operated in order to minimise\u000d\u000a      energy bills.\u000d\u000a    DER-CAM has been developed jointly by Siddiqui and researchers from\u000d\u000a      Berkeley Lab since 2000.\u000d\u000a      Since joining UCL in 2005, Siddiqui has made significant mathematical\u000d\u000a      contributions to the current\u000d\u000a      version of DER-CAM, including the incorporation of complex thermodynamic\u000d\u000a      constraints [2]. This\u000d\u000a      improvement to DER-CAM was implemented so that the University of New\u000d\u000a      Mexico (UNM) could\u000d\u000a      use the model to optimise their cooling equipment scheduling.\u000d\u000a    (c) Most models used for energy policy assume perfect foresight\u000d\u000a      and price-taking behaviour by\u000d\u000a      decision makers. The work in the \"An Options Approach to UK Energy\u000d\u000a      Futures\" project &#8212; conducted\u000d\u000a      by Siddiqui together with Derek Bunn and Michail Chronopoulos from the\u000d\u000a      London\u000d\u000a      Business School (LBS) &#8212; showed that, in contrast to this assumption,\u000d\u000a      managerial discretion over\u000d\u000a      investment timing\/sizing or technology choice is actually affected by\u000d\u000a      uncertainty and imperfect\u000d\u000a      competition.\u000d\u000a    The project, conducted during 2011 and 2012, involved the development of\u000d\u000a      a high-level framework\u000d\u000a      for illustrating the principal features of optionality in the energy\u000d\u000a      sector. In particular, the theory of\u000d\u000a      real options was used to illustrate how uncertainty, competition, and\u000d\u000a      power companies' flexibility to\u000d\u000a      delay investment decisions lead to vastly different outcomes than those\u000d\u000a      predicted by traditional\u000d\u000a      models [3]. This finding indicates that policymakers need to adjust how\u000d\u000a      their support schemes or\u000d\u000a      market designs are implemented. Taking a UK example, it was shown how a CO2\u000d\u000a      price floor would\u000d\u000a      subvert the incentive of a fossil-fuel power plant to be the industry\u000d\u000a      leader by removing the\u000d\u000a      operational flexibility that makes it preferable to a renewable-energy\u000d\u000a      plant. UCL's specific\u000d\u000a      contribution was in Siddiqui's review of real options models to showcase\u000d\u000a      their policy relevance,\u000d\u000a      assistance with positioning the paper, delivery of tutorials on options\u000d\u000a      pricing, and numerous\u000d\u000a      presentations.\u000d\u000a    Much of the theoretical background to this project, concerning the value\u000d\u000a      of flexibility and strategic\u000d\u000a      interactions, was conducted by Siddiqui working with Stein-Erik Fleten\u000d\u000a      from the Norwegian\u000d\u000a      University of Science and Technology [4] and Ryuta Takashima from the\u000d\u000a      Chiba Institute of\u000d\u000a      Technology [5]. UCL's contribution was central: in both papers, Siddiqui\u000d\u000a      formulated the problem,\u000d\u000a      solved the models numerically where necessary, and formalised the\u000d\u000a      analytical propositions.\u000d\u000a      Siddiqui and Fleten [4] examined mutually exclusive investment\u000d\u000a      opportunities in either a readily\u000d\u000a      available renewable energy technology or a more ambitious one that needs\u000d\u000a      further R&amp;D to bring\u000d\u000a      down its cost for subsequent commercialisation. The main result was a\u000d\u000a      valuation of the option to\u000d\u000a      conduct R&amp;D in the ambitious technology. Surprisingly, it was found\u000d\u000a      that a high level of uncertainty\u000d\u000a      in the electricity price reduced this option value as long as the learning\u000d\u000a      rate was low because\u000d\u000a      higher expected prices made even a rudimentary ambitious technology\u000d\u000a      attractive. Siddiqui and\u000d\u000a      Takashima [5] took a strategic real options approach to investigate how\u000d\u000a      staged investment under\u000d\u000a      uncertainty is affected by the presence of a rival. Such discretion over\u000d\u000a      not only investment timing\u000d\u000a      but also modularity is a hallmark of most infrastructure industries\u000d\u000a      including energy and\u000d\u000a      telecommunications. It was found that a modular investment strategy is\u000d\u000a      worth relatively more to a\u000d\u000a      duopolist than to a monopolist because the former is partially able to\u000d\u000a      offset the loss in market\u000d\u000a      share. Moreover, it was also shown analytically that a duopolist's\u000d\u000a      disadvantage relative to a\u000d\u000a      monopolist worsens as volatility increases only if its loss in market\u000d\u000a      share from the entrance of a\u000d\u000a      rival is relatively high.\u000d\u000a    "},{"CaseStudyId":"30944","Continent":[{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"1814991","Name":"China"}],"Funders":["Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000a    Traffic noise reduction is an important area for Computer Aided\u000a      Engineering (CAE) both for minimising noise pollution and to meet high\u000a      customer expectations. Providing quieter means of transportation leads to\u000a      significant competitive advantages and enhances the reputation of a\u000a      vehicle brand. The automotive industry alone spends over US$1bn p.a. on\u000a      NV-related costs [B1] and virtual NV simulation and testing account for a\u000a      steadily increasing slice of this outlay. However, due to a lack of\u000a      reliable simulation tools, NV costs are still dominated by expensive\u000a      measurements and prototype testing. NV simulation is thus an area of\u000a      growing importance for vehicle manufacturers and CAE providers.\u000a    Since 2009, DEA has advanced from a mathematical concept to a practical\u000a      numerical tool for engineers in the commercial NV sector. The UoN group\u000a      developed the product alongside the main business partner, inuTech GmbH, a\u000a      provider of numerical modelling solutions and expertise as well as\u000a      software products. The strategy is based on transferring specialised\u000a      research knowledge via inuTech into the wider modelling and simulation\u000a      market, integrating it into the standard CAE environments used by key\u000a      players in that market. From there it is introduced into the end user\u000a      market of vehicle manufacturers and suppliers.\u000a    In order to achieve full impact in the commercial sector, it is vital to\u000a      build up a solid reputation in engineering communities as well as within\u000a      the academic sectors. The group therefore presents results regularly at\u000a      mechanical engineering conferences (13 since 2009) and collaborates with\u000a      key mechanical engineering groups in Southampton (ISVR), Cambridge,\u000a      Leuven, Lyon, Graz and Munich. These efforts won the confidence of\u000a      industrial partners and led to representatives of Bombardier, EADS, ViF\u000a      and JLR speaking at the group's conference InnoWave2012 at UoN:\u000a\u0009  http:\/\/inutech.de\/midea\/events\/InnoWave2012\/.\u000a    While the main impact so far (in terms of direct investment and change of\u000a      business practice) is on the numerical simulation and software industry,\u000a      the team at UoN has (alongside those industrial partners) made concerted\u000a      efforts to advertise and collaborate with end users in the vehicle\u000a      manufacturing industry: the impacts of DEA on both these industrial\u000a      sectors are described below.\u000a    Numerical Simulation &amp; Software Industry\u000a      (Impact: economic benefits, change in company practice and adoption of new\u000a      technology)\u000a    A close collaboration between inuTech and UoN has developed over recent\u000a      years boosted by various grants from EPSRC and the EU. inuTech has\u000a      recently committed further to DEA: \"Due to positive feedback from\u000a        customers, we decided in September 2012 to invest directly in the method\u000a        with an emphasis on constructing a software package with user-friendly\u000a        pre- and post-processing tools and adjusting the method to the needs of\u000a        specific customers such as in the ship and aviation industry. inuTech's\u000a        development efforts amount to in total approx. 160K &#8364; including the\u000a        salaries of two members of staff as well as part funding your\u000a      [Tanner] sabbatical.\" [B2] This represents a substantial investment\u000a      from the company into DEA of 10% of their annual revenue (2012 total\u000a      revenues were &#8364;1.65 M; 21 employees).\u000a    Via contacts from inuTech, a business partnership with another numerical\u000a      software company, CADFEM GmbH, Munich, started in Oct 2012. Investment by\u000a      the company in DEA is already paying dividends: \"CADFEM has decided to\u000a        support the commercialisation efforts of inuTech by actively promoting\u000a        DEA to its own customers and contacts. Currently, CADFEM is leading the\u000a        negotiations with a potential customer in the ship building industry in\u000a        China via its partner company Peraglobal Inc. based in China.\" [B3]\u000a    UoN is also a coordinating partner in a new grant (MHiVec, [B7]) together\u000a      with the company CDH AG, Ingolstadt, who have leading expertise in\u000a      vibro-acoustics in the car industry and whose research focus the team has\u000a      influenced: \"CDH AG has also adapted its internal R&amp;D policy in\u000a        line with the collaboration with the MHIVec project and has extended its\u000a        own research efforts in the mid-to-high frequency regime to reflect the\u000a        wider objectives of the project.\" [B4]\u000a    Vehicle Manufacturers\u000a      (Impact: economic benefits and competitive advantage, working towards\u000a      reduction in vehicle noise and improved passenger comfort)\u000a    Following refinement of DEA tools in the past few years, inuTech is\u000a      promoting the method widely to its userbase: \"New DEA tools developed\u000a        over the past years make it possible to contemplate going into this\u000a        market and consequently we sought and received additional funding\u000a        through a new IAPP (Mid-to-High Frequency Modelling tool for Vehicle\u000a        Noise and Vibration (MHiVec) starting in Sept 2013. inuTech is confident\u000a        that DEA constitutes a true innovation in vibro-acoustic modelling and\u000a        provides an additional lucrative market segment for the company.\"\u000a      [B2]\u000a    Since 2010, benchmarking projects with various partners from the sector\u000a      have taken place to validate the method for real engineering structures.\u000a      This is vital to convince the community of the advantages of the new\u000a      methodology. The team has for example obtained benchmark problems and\u000a      meshes from: Germanischer Lloyd (shipbuilding) [B5]; JLR (car\u000a      manufacturer) [B6]; EADS\/Airbus (aerospace) and Bombardier (train\u000a      manufacturer).\u000a    The group has demonstrated that the method delivers cost and time savings\u000a      and gives correct results. The method is now being assessed by end user\u000a      industries for adoption in future design cycles.\u000a    JLR supported a knowledge transfer secondment, hosting Chappell for 8\u000a      months within the company and provided in-kind contributions worth\u000a      &#163;15,000. Based on the success of this project (culminating in the\u000a      development of DFM introduced in [A5]), JLR joined the MHiVec consortium\u000a      and has pledged to invest in the order of &#163;65,000 over 4 years in terms of\u000a      staff time, measurements and training of consortium members [B7]. The\u000a      company comments that: \"We believe that DEA together with the DFM\u000a        technology is a serious candidate for closing the mid-frequency gap.\"\u000a      [B6]\u000a    The combination of DEA with DFM is of great interest to industrial\u000a      partners, and ensures the impact of the work will be further enhanced,\u000a      e.g. Germanischer Lloyd: \"We see great potential in the new method, in\u000a        particular in the extension of DEA to a mesh based approach using the\u000a        DFM method as published recently in the Proceedings of the Royal Society\u000a      [A5].\" [B5]\u000a    ","ImpactSummary":"\u000a    Having itself developed Dynamical Energy Analysis (DEA), a\u000a      numerical simulation tool that significantly enhances the modelling of\u000a      noise and vibration (NV) in large-scale engineering structures in the mid-\u000a      to high-frequency range, The University of Nottingham (UoN) has advanced\u000a      the method to a practical numerical tool used in a commercial environment.\u000a      By working with industrial partners, the team has influenced numerical\u000a      simulation products developed by consultants to the transport sector and\u000a      obtained investment in the new technology from vehicle manufacturers.\u000a    The method provides time and cost savings by making it possible to\u000a      undertake NV modelling over the entire frequency range, and further\u000a      contributes to the industry's objectives to reduce traffic noise and\u000a      enhance passenger comfort.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Nottingham\u000a    ","Institutions":[{"AlternativeName":"Nottingham (University of)","InstitutionName":"University of Nottingham","PeerGroup":"A","Region":"East Midlands","UKPRN":10007154}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2867714","Name":"Munich"},{"GeoNamesId":"2867714","Name":"München"},{"GeoNamesId":"2778067","Name":"Graz"},{"GeoNamesId":"2996944","Name":"Lyon"}],"References":"\u000a    The three publications that best indicate the quality of the research are\u000a      indicated *\u000a    \u000a[A1]* Dynamical Energy Analysis &#8212; determining wave energy\u000a        distributions in complex vibro-acoustical structures, G. Tanner,\u000a      Journal of Sound and Vibration 320, 1023 (2009).\u000a      DOI:10.1016\/j.jsv.2008.08.032 (also listed in REF2)\u000a    \u000a\u000a[A2] Solving the stationary Liouville equation via a boundary\u000a        element method, D. J. Chappell, and G. Tanner, Journal of\u000a      Computational Physics, 234 487-498 (2013).\u000a      DOI: 10.1016\/j.jcp.2012.10.002\u000a    \u000a\u000a[A3] Boundary element dynamical energy analysis: A versatile\u000a        method for solving two or three dimensional problems in the high\u000a        frequency limit, D. J. Chappell, G. Tanner, and S. Giani, Journal of\u000a      Computational Physics 231, 6181-6191 (2012).\u000a      DOI: 10.1016\/j.jcp.2012.05.028\u000a    \u000a\u000a[A4] Dynamical energy analysis for built-up acoustic systems\u000a        at high frequencies, D. J. Chappell, S. Giani, and G. Tanner,\u000a      Journal of the Acoustical Society of America 130, 1420 (2011).\u000a      DOI: 10.1121\/1.3621041\u000a    \u000a\u000a[A5]* Discrete flow mapping: transport of phase space\u000a        densities on triangulated surfaces, D. J. Chappell, G. Tanner, D.\u000a      L&#246;chel and N. S&#248;ndergaard, Proceedings of the Royal Society A 469,\u000a      20130153 (2013).\u000a      DOI: 10.1098\/rspa.2013.0153 (also listed in REF2)\u000a    \u000a\u000a[A6]* A hybrid approach for predicting the distribution of\u000a        vibro-acoustic energy in complex built-up structures, D. N.\u000a      Maksimov, and G. Tanner, Journal of the Acoustical Society of America 130,\u000a      1337 (2011).\u000a      DOI: 10.1121\/1.3621321 (also listed in REF2)\u000a    \u000aGrants:\u000a    &#8226; EPSRC Springboard Fellowship (EP\/D064422\/1) Towards the\u000a        mid-frequency regime &#8212; combining wave chaos methods and 'Statistical\u000a        Energy Analysis' (&#163;81,708, Sep 2006 &#8212; Sep 2007).\u000a    &#8226; EPSRC grant (EP\/F069189\/1) Vibrational energy distributions in\u000a        large built-up structures &#8212; a wave chaos approach together with Prof\u000a      Brian Mace, ISVR, Southampton and inuTech GmbH; (Nottingham's share &#8212;\u000a      &#163;217,690, Jan 2009 &#8212; Dec 2011).\u000a    &#8226; EC Industrial and Academic Partnership &amp; Pathways (IAPP) grant\u000a      230597 Mid-frequency Energy Analysis MIDEA; Coordinating partner:\u000a      UoN; other main partners: inuTech GmbH, associate partners: ISVR\u000a      (Southampton), Virtual Vehicle GmbH (Graz) and Bosch GmbH (&#8364;794,401 Jan\u000a      2009 &#8212; Dec 2012).\u000a    &#8226; EPSRC Knowledge Transfer Secondment Grant in collaboration with JLR:\u000a      Competitive award from part of University KTS allocation EP\/H500286\/1\u000a      (&#163;32,000, Jan &#8212; Aug 2012).\u000a    &#8226; Cash contribution by inuTech GmbH of &#163;50,850 to support a 16-month\u000a      sabbatical of Tanner, also supported by UoN, Sep 2012 &#8212; Dec 2013.\u000a      Corroboration available from [B2].\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"}],"Sources":"\u000a    [B1] http:\/\/tinyurl.com\/ntja6vz,\u000a      retrieved 1 August 2013 [copy on file]\u000a    [B2] CEO, inuTech GmbH [letter on file]\u000a    [B3] CEO, CADFEM International GmbH [letter on file]\u000a    [B4] Managing Director, CDH AG [letter on file]\u000a    [B5] Vice President Information Management and Tools, Germanischer\u000a      Lloyd SE [letter on file]\u000a    [B6] Technical Specialist Body Structure NVH CAE, Jaguar Land\u000a      Rover Ltd [letter on file]\u000a    [B7] Grant &#8212; MHiVec (Mid-High Frequency Modelling for Vehicle\u000a        Noise and Vibrations, with partners: Nottingham Trent University,\u000a      ISVR, inuTech, CDH, JLR (Associate Partner); total value: &#8364;1,944,975; Sep\u000a      2013 &#8212; Aug 2017, http:\/\/cordis.europa.eu\/projects\/rcn\/109993_en.html)\u000a    \u000a    ","Title":"\u000a    Enhancing vibro-acoustic modelling of built-up structures for\u000a        industrial partners in the transport sector\u000a    ","UKLocation":[{"GeoNamesId":"2637487","Name":"Southampton"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Research into traffic noise reduction is becoming increasingly important\u000a      as highlighted in a recent joint report by the World Health Organisation\u000a      and the European Commission (EC) &#8212; tinyurl.com\/64mm7fm\u000a      &#8212; as well as by the Directorate-General for Internal Policies of the\u000a      European Parliament &#8212; tinyurl.com\/pfatzcr\u000a      &#8212; stating that \"With regard to road traffic noise, the most effective\u000a      control strategies involve noise control at source. This can only be\u000a      achieved by stricter and more ambitious targets for reduction in\u000a      permissible noise levels from motor vehicles\" (page 9). In addition,\u000a      manufacturers constantly seek to improve prototypes and to enhance\u000a      passenger comfort by providing a noise-free driving experience. This leads\u000a      to a growing demand for accurate tools covering the full audible frequency\u000a      range.\u000a    Simulating the distribution of vibro-acoustic energy in complex built-up\u000a      structures (e.g. vehicles) in the mid-to-high frequency regime is an\u000a      extremely challenging task for standard numerical tools such as Finite\u000a      Element Methods (FEM). A range of high-frequency techniques have been\u000a      developed in the engineering community, the most successful being based on\u000a      Statistical Energy Analysis (SEA) and SEA-FEM hybrid techniques. SEA has a\u000a      limited range of applicability and does not describe accurately the\u000a      mid-frequency range. The potential value of DEA is reflected in e.g.\u000a      Jaguar Land Rover (JLR) commenting in this context that: \"Both of these\u000a        codes [FEM and SEA] are suitable for their respective frequency\u000a        ranges. FE is used when the modal count is low and the response is\u000a        dominated by single resonance peaks. SEA is used whenever the modal\u000a        count is high. Unfortunately between the two frequency ranges, there is\u000a        an area, mid-frequency, where the conditions of each code cannot be\u000a        satisfied, leaving a gap within the CAE [computer aided engineering]\u000a        techniques which then leaves a reliance on physical testing.\" [B1]\u000a    For cars, this range lies between 1-5 kHz, well within the audible range\u000a      important for passenger comfort. Nevertheless, software packages based on\u000a      SEA are offered commercially, with market leader ESI generating\u000a      multi-million dollar revenues (tinyurl.com\/pgb9ono).\u000a    A new method, Dynamical Energy Analysis, proposed by Dr Gregor\u000a      Tanner (UoN, 1998 to date, Associate Professor and Reader) in 2009 [A1]\u000a      and developed further with his research assistant Dr David Chappell at\u000a      UoN, overcomes the limitations of SEA. Based on ideas of wave or quantum\u000a      chaos, it approximates wave transport in terms of ray dynamics; the flow\u000a      of ray trajectories is described in terms of linear phase space operators\u000a      [A2]. These types of operators have been studied intensively in\u000a      mathematical-research areas such as dynamical system theory and\u000a      Frobenius-Perron or transfer-operator methods. By using operator\u000a      techniques, DEA established for the first time an effective implementation\u000a      of a ray-tracing algorithm in mechanical structures [A3, A4]. In\u000a      particular, the implementation of DEA on FEM meshes (termed Discrete Flow\u000a      Mapping &#8212; DFM, [A5]) opened up the method to curved shells and complex\u000a      built-up structures and embeds the algorithm in a typical engineering\u000a      simulation environment [A5]. The group also developed hybrid FEM-DEA\u000a      methods using wave chaos results [A6].\u000a    An efficient numerical implementation of these ideas has been achieved\u000a      with the help of funding from the EPSRC and an EC Industrial Partnership\u000a      grant joint with inuTech GmbH, Germany, the Institute of Sound and\u000a      Vibration Research (ISVR), UK, and industrial partners such as JLR, UK,\u000a      and Germanischer Lloyd, Germany. The following main practical results have\u000a      been established:\u000a    i. Improved bounds for the applicability of SEA [A1, A2, A4].\u000a    ii. Demonstrable improvements to SEA by including non-diffusive wave\u000a      transport [A2, A3, A4].\u000a    iii. Hybrid DEA-FEM methods [A6].\u000a    iv. An implementation of DEA for multi-component structures (with\u000a      Germanischer Lloyd) [A5].\u000a    v. An efficient DEA algorithm on triangulated meshes thus extending the\u000a      method to complex, curved structures given in terms of FEM grids for JLR\u000a      [A5].\u000a    "},{"CaseStudyId":"30945","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    Asset Management\u000d\u000a      For companies with large infrastructure such as UK water companies, the\u000d\u000a      development of an asset management plan is an expensive and time-consuming\u000d\u000a      task. O'Hagan described how to combine a few high-quality studies with\u000d\u000a      many lower-level expert judgements, thus realising great reductions in the\u000d\u000a      number, and hence costs, of detailed studies required [A4]. Convincing the\u000d\u000a      water companies to change their approach based in classical statistics for\u000d\u000a      an approach using subjective Bayesian methods (O'Hagan's ABLE methodology)\u000d\u000a      represented a major success for Bayesian statistics at this time (mid to\u000d\u000a      late 1990s).\u000d\u000a    From 2007 to 2011, Metronet Rail and then London Underground ran the\u000d\u000a      Engineering Strategy for Economic and Efficient Management (ESTEEM)\u000d\u000a      project to optimise and manage the investment in their engineering assets\u000d\u000a      over a 100-year period. O'Hagan was employed as the statistical consultant\u000d\u000a      for the project, and worked with ESTEEM to develop a new approach, based\u000d\u000a      on ABLE [A4], for asset management on the railways by estimating asset\u000d\u000a      degradation, costs, risks and their probabilities for each maintainable\u000d\u000a      item. Uncertainty was carefully characterized throughout the rail system,\u000d\u000a      so that credible ranges could be placed on estimates for individual assets\u000d\u000a      through to the company-wide asset management plan. The project won the\u000d\u000a      prestigious international Institute of Engineering and Technology award\u000d\u000a      for best asset management innovation in 2010 [B1] and an\u000d\u000a      independent audit [B2] has concluded that ESTEEM represented a \"world best\u000d\u000a      practice\" approach to asset management for long-lived assets.\u000d\u000a    Following a re-organisation in September 2009, aspects of the project\u000d\u000a      continued such as the Least Whole Life Cost (WLC) predictions, and these\u000d\u000a      have led to significant benefits, as noted by John Darbyshire, project\u000d\u000a      manager for ESTEEM at both Metronet and Transport for London [B3]:\u000d\u000a    \"The ESTEEM Civil Assets least WLC predictions substantiated a basis\u000d\u000a        for long term investment in the asset base and justified inclusion of\u000d\u000a        preventative maintenance in a new performance contract for maintaining\u000d\u000a        those assets. A particular example of the benefits reaped from this\u000d\u000a        project is in the waterproofing of structures. Prior to ESTEEM, this was\u000d\u000a        thought to be too expensive to justify. However, ESTEEM predictions\u000d\u000a        anticipated a 20% savings in maintenance costs over a 30-year period, a\u000d\u000a        saving of &#163;5m p.a. The water-proofing was thus implemented at the end of\u000d\u000a        2009 for all concrete and masonry structures and continues to this day.\"\u000d\u000a    Darbyshire goes on to note that: \"A further example is the\u000d\u000a        information systems used in London Underground stations. ESTEEM has\u000d\u000a        become a critical operational system used to maintain and update these\u000d\u000a        systems, and was fully implemented by summer 2011.\"\u000d\u000a    Evidence-Based Policy\u000d\u000a      In 2009, the Food and Environment Research Agency (FERA) conducted a\u000d\u000a      consultation into the formation of a new body to manage livestock diseases\u000d\u000a      in the UK [B4]. The impact assessment for the consultation required\u000d\u000a      estimates of the cost and frequency of various exotic infectious disease\u000d\u000a      outbreaks, such as bluetongue and avian influenza. The government's\u000d\u000a      timetable for the publication of a draft bill meant that there was no time\u000d\u000a      for a large-scale project to assess these costs, so instead expert\u000d\u000a      elicitation techniques were used to capture government veterinarians' and\u000d\u000a      economists' knowledge and uncertainty about these quantities. Because the\u000d\u000a      estimates from the experts were highly uncertain, it was decided that this\u000d\u000a      uncertainty should be accounted for in order to make an informed decision\u000d\u000a      about the value of the new body.\u000d\u000a    The FERA consultation [B4, B5] used the methodology developed by O'Hagan\u000d\u000a      [A2, A3] to elicit 90% credible intervals for each of the unknown values\u000d\u000a      (i.e. an estimate of uncertainty). They used O'Hagan's approach for\u000d\u000a      fitting parametric distributions to the experts' opinions [A4], and\u000d\u000a      followed his advice for avoiding under-estimation and for suggested\u000d\u000a      feedback to the experts to ensure their satisfaction with the final\u000d\u000a      results [A3]. The consultation formed a key part of the Draft Animal\u000d\u000a      Health Bill [B5] presented to parliament by the Secretary of State for\u000d\u000a      Environment, Food and Rural Affairs in January 2010. Although the bill\u000d\u000a      never passed into law due to the change of government in June 2010, it\u000d\u000a      demonstrated the increasing acceptance of the need for the quantification\u000d\u000a      of uncertainty when using expert opinion, and the use of elicitation\u000d\u000a      methodology to quantify this uncertainty, in government policy [B6].\u000d\u000a    Emulators\u000d\u000a      One example of the use of O'Hagan's fundamental research on emulators and\u000d\u000a      the analysis of simulators is UKCP09, an &#163;11M DEFRA and DECC funded\u000d\u000a      project, developed at the Met Office for predicting climate change and its\u000d\u000a      effects in the UK. The climate projections in UKCP09 are based upon\u000d\u000a      simulators such as the HadCM3 simulator, which contains over one million\u000d\u000a      lines of code and takes several days to run at a single combination of\u000d\u000a      parameter values. A key aspect of UKCP09 is that users are provided with\u000d\u000a      the uncertainties associated with future climate information, something\u000d\u000a      that would not be possible without the use of emulators.\u000d\u000a    Emulators are used by the Met Office as statistical surrogates of HadCM3\u000d\u000a      to assess the uncertainty associated with the simulator prediction [B7],\u000d\u000a      because it would be too costly to run the simulator enough times to\u000d\u000a      quantify the uncertainty in any predictions. The statistical methodology\u000d\u000a      used in UKCP09 is based on methodology developed directly from O'Hagan's\u000d\u000a      research at UoN. For example, UKCP09 uses the emulator methodology\u000d\u000a      described in Rougier [B8], which draws extensively upon O'Hagan's work\u000d\u000a      [A1, A8].\u000d\u000a    UKCP09 is used by a wide range of organisations, government departments,\u000d\u000a      and programmes (http:\/\/ukclimateprojections.defra.gov.uk\/23081).\u000a      For example, Hampshire County Council (HCC)'s Emergency Planning Unit used\u000d\u000a      UKCP09 to undertake comprehensive mapping of Hampshire [B9]. This enabled\u000d\u000a      them, in conjunction with other tools, to anticipate future emergency\u000d\u000a      planning service demands and determine if existing provisions were\u000d\u000a      adequate (e.g. in heatwave events the frequency of events was used to\u000d\u000a      estimate the number of deaths). Meanwhile, South West Tourism (SWT) used\u000d\u000a      UKCP09 to explore the potential impact of climate change to their region\u000d\u000a      [B10], results of which were included in a report that was used to inform\u000d\u000a      regional tourism strategy, which notes e.g. that \"The UKCP09 regional\u000d\u000a        projections and the results from this study also show that extreme\u000d\u000a        weather events are likely to increase for the SW, so the likelihood of\u000d\u000a        erratic and extreme weather also needs to be investigated and\u000d\u000a        considered.\" (Page 37 of report to be found at [B10].)\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Techniques developed at The University of Nottingham (UoN) have enabled\u000d\u000a      organisations to deal with uncertainty in complex industrial and policy\u000d\u000a      problems that rely on the elicitation of expert opinion and knowledge. The\u000d\u000a      statistical toolkit produced for use in complex decision-making processes\u000d\u000a      has been deployed in a wide range of applications. It has been\u000d\u000a      particularly useful in asset management planning in organisations such as\u000d\u000a      the London Underground, government approaches to evidence-based policy,\u000d\u000a      and the Met Office UK Climate Projection tool (UKCP09), which is used by\u000d\u000a      hundreds of organisations across the UK such as environment agencies, city\u000d\u000a      and county councils, water companies and tourist boards.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Nottingham\u000d\u000a    ","Institutions":[{"AlternativeName":"Nottingham (University of)","InstitutionName":"University of Nottingham","PeerGroup":"A","Region":"East Midlands","UKPRN":10007154}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    The three publications that best indicate the quality of the research are\u000d\u000a      indicated *\u000d\u000a    \u000a[A1]* O'Hagan, A. (1994). Kendall's Advanced Theory of Statistics\u000d\u000a      Volume 2B, Bayesian Inference. Edward Arnold, London. ISBN: 0-340-80752-0\u000d\u000a      (available on request)\u000d\u000a    \u000a\u000a[A2] O'Hagan, A. (1998). Eliciting expert beliefs in substantial\u000d\u000a      practical applications. Journal of the Royal Statistical Society: Series D\u000d\u000a      (The Statistician), 47, 21-35. (with discussion, pp 55-68). DOI:\u000d\u000a      10.1111\/1467-9884.00114\u000d\u000a    \u000a\u000a[A3] Garthwaite, P. H. and O'Hagan, A. (2000). Quantifying expert\u000d\u000a      opinion in the UK water industry: an experimental study. Journal of the\u000d\u000a      Royal Statistical Society: Series D (The Statistician), 49, 455-477. DOI:\u000d\u000a      10.1111\/1467-9884.00246\u000d\u000a    \u000a\u000a[A4]* O'Hagan, A. (1997). The ABLE story: Bayesian asset\u000d\u000a      management in the water industry. In The Practice of Bayesian Analysis, S.\u000d\u000a      French and J. Q. Smith (eds.). Arnold, 173-198. ISBN-10: 0340662409;\u000d\u000a      ISBN-13: 978-0340662403 (available on request)\u000d\u000a    \u000a\u000a[A5]* O'Hagan, A., Kennedy, M. C. and Oakley, J. E. (1999).\u000d\u000a      Uncertainty analysis and other inference tools for complex computer codes\u000d\u000a      (with discussion). In Bayesian Statistics 6, J. M. Bernardo et al.\u000d\u000a      (eds.). Oxford University Press, 503-524.\u000d\u000a      ISBN-10: 0198504853; ISBN-13: 978-0-19-850485-6 (available on request)\u000d\u000a    \u000a\u000a[A6] Kennedy, M. and O'Hagan, A. (2000). Predicting the output\u000d\u000a      from a complex computer code when fast approximations are available.\u000d\u000a      Biometrika 87, 1-13. DOI: 10.1093\/biomet\/87.1.1 Funded by [A9] and\u000d\u000a        first released as Nottingham Statistics Group Technical Report 1998-09.\u000d\u000a    \u000a\u000a[A7] Kennedy, M. C. and O'Hagan, A. (2001). Bayesian calibration\u000d\u000a      of computer models (with discussion). Journal of the Royal Statistical\u000d\u000a      Society B 63, 425-464.\u000d\u000a      DOI: 10.1111\/1467-9868.00294\u000d\u000a      Funded by [A9] and first released as Nottingham Statistics Group\u000d\u000a        Technical Report 1998-10.\u000d\u000a    \u000a\u000a[A8] Oakley, J. E. and O'Hagan, A. (2002). Bayesian inference for\u000d\u000a      the uncertainty distribution of computer model outputs. Biometrika 89,\u000d\u000a      769-784. DOI: 10.1093\/biomet\/89.4.769 First released as Nottingham\u000d\u000a        Statistics Group Technical Report 1998-11.\u000d\u000a    \u000aGrants:\u000d\u000a    [A9] Bayesian uncertainty analysis and calibration of complex\u000d\u000a      computer models, PI O'Hagan, EPSRC Grant GR\/K54557\/01, 1 October 1995 - 30\u000d\u000a      September 1998, &#163;114,666.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a    [B1] The Institution of Engineering and Technology Awards past\u000d\u000a      winners:\u000d\u000a      http:\/\/tv.theiet.org\/channels\/news\/10135.cfm\u000d\u000a      (copy also filed 6 September 2013, together with original application\u000d\u000a      form)\u000d\u000a    [B2] AMEM Assessment of ESTEEM. A report for LUL Nominee BCV\u000d\u000a      Limited, Trading as Metronet Rail BCV from Asset Management Consulting\u000d\u000a      Limited (AMCL), May 2009. (copy on file)\u000d\u000a    [B3] Stations Whole Life Cost Manager, Transport for London\u000d\u000a      (2006-2011). (copy of letter on file)\u000d\u000a    [B4] Gosling, J. P., Hart, A., Mouat, D. C., Sabirovic, M.,\u000d\u000a      Scanlan, S. and Simmons, A. (2012) Quantifying Experts' Uncertainty About\u000d\u000a      the Future Cost of Exotic Diseases. Risk Analysis, 32(5) 881-893.\u000d\u000a      DOI: 10.1111\/j.1539-6924.2011.01704.x (copy also on file)\u000d\u000a    [B5] Annex 8, J.P. Gosling (FERA), Draft Animal Health Bill, HM\u000d\u000a      Government, January 2010\u000d\u000a      www.official-documents.gov.uk\/document\/cm77\/7784\/7784.pdf\u000d\u000a      (copy also on file)\u000d\u000a    [B6] Expert Elicitation Task Force, White Paper. Prepared for the\u000d\u000a      U.S. Environmental Protection Agency, Washington, DC, 2011. www.epa.gov\/stpc\/pdfs\/ee-white-paper-final.pdf\u000d\u000a      (copy also on file)\u000d\u000a    [B7] Statistical methodology used by UKCP09: http:\/\/ukclimateprojections.defra.gov.uk\/23253,\u000d\u000a      http:\/\/ukclimateprojections.defra.gov.uk\/22783\u000d\u000a      (copies also on file)\u000d\u000a    [B8] Rougier, J. (2007). Probabilistic inference for future\u000d\u000a      climate using an ensemble of climate model evaluations. Climatic\u000d\u000a        Change, 81(3-4), 247-264. DOI: 10.1007\/s10584-006-9156-9\u000d\u000a    [B9] UKCP09 use by HCC: http:\/\/ukclimateprojections.defra.gov.uk\/23089\u000d\u000a      (copy also on file)\u000d\u000a    [B10] UKCP09 use by SWT: http:\/\/ukclimateprojections.defra.gov.uk\/23098\u000d\u000a      (copy also on file) \u000d\u000a    ","Title":"\u000d\u000a    Incorporating expert knowledge in complex industrial and policy\u000d\u000a        applications\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Professor Tony O'Hagan's (UoN 1990-1999, Department of Mathematics and\u000d\u000a      School of Mathematical Sciences) research at Nottingham between 1993 and\u000d\u000a      1998 showed how to incorporate expert knowledge into statistical analyses.\u000d\u000a      Classical statistics primarily deals with situations in which there are\u000d\u000a      large amounts of data, so that information in the data swamps any\u000d\u000a      modelling choices made. O'Hagan's work focussed on the other extreme,\u000d\u000a      where data are limited. He developed a toolkit of methods for\u000d\u000a      incorporating expert beliefs in statistical analyses, enabling accurate\u000d\u000a      uncertainty quantification in complex problems. The research comprises two\u000d\u000a      related threads: directly using and eliciting expert judgements; and using\u000d\u000a      expert knowledge that has been coded into a complex computer simulator.\u000d\u000a    Eliciting and using expert judgements\u000d\u000a      Elicitation is the process of translating someone's beliefs about an\u000d\u000a      uncertain quantity into a probability distribution, and is a key aspect of\u000d\u000a      many uncertainty quantification exercises, particularly risk assessments.\u000d\u000a      While the incorporation of expert beliefs in a prior distribution is a\u000d\u000a      fundamental part of Bayesian statistics, the problem of how to elicit\u000d\u000a      these beliefs received relatively little attention before the 1990s.\u000d\u000a      Expert elicitation is now widely used; for example, it has been conducted\u000d\u000a      and used by at least six American federal agencies and international\u000d\u000a      organisations [B6].\u000d\u000a    O'Hagan was a leading proponent of the use of subjective priors and\u000d\u000a      argued strongly that the subjective interpretation of probability should\u000d\u000a      be embraced. This was described in his influential textbook [A1] and in\u000d\u000a      two papers [A2, A3]. He examined in detail the elicitation process, and\u000d\u000a      determined which questions lead to the most accurate representation of an\u000d\u000a      expert's beliefs, particularly when eliciting complex quantities such as\u000d\u000a      variances and distributions.\u000d\u000a    This research was motivated by O'Hagan's work with UK water companies on\u000d\u000a      developing asset management plans. The ABLE project [A2, A3, A4], with\u000d\u000a      &#163;800K of funding from 8 water companies and the DoE (now DEFRA), arose\u000d\u000a      from this work, and led to a methodology and software which is used to\u000d\u000a      estimate the need for (and the cost of) maintenance, renewal and\u000d\u000a      replacement of assets (sewers and water pipes in the case of the water\u000d\u000a      companies).\u000d\u000a    Statistical Analysis of Complex Computer Simulators\u000d\u000a      Simulators are now widely adopted in science, engineering, industry and\u000d\u000a      public policy to encode and combine expert knowledge of mechanisms and\u000d\u000a      processes in order to make predictions. The work of O'Hagan facilitated\u000d\u000a      this by showing how to combine complex mechanistic models with statistical\u000d\u000a      models in order to quantify uncertainties.\u000d\u000a    O'Hagan (with PhD students Haylock, Kennedy and Oakley) published a\u000d\u000a      series of research papers between 1996 and 2002 showing how to quantify\u000d\u000a      the different types of uncertainty present when using complex simulators\u000d\u000a      [A5, A6, A7, A8]. The basis of these papers, and the key methodological\u000d\u000a      development, is the use of Gaussian process emulators (a term\u000d\u000a      coined by O'Hagan) as meta-models of simulators. For simulators that are\u000d\u000a      computationally expensive to evaluate, the ability to draw inferences is\u000d\u000a      constrained as the simulator can only be evaluated a limited number of\u000d\u000a      times, making standard uncertainty quantification approaches impossible to\u000d\u000a      apply. O'Hagan showed [A6, A7] how to build a statistical model of the\u000d\u000a      simulator, called an emulator, which is cheaper to run and is trained\u000d\u000a      using an ensemble of simulator evaluations. Once the emulator is\u000d\u000a      available, tasks such as uncertainty analysis [A8], sensitivity analysis\u000d\u000a      [A5], and calibration [A7] could be undertaken, which would otherwise have\u000d\u000a      been impossible.\u000d\u000a    Research on Gaussian process emulators continues within the School,\u000d\u000a      applications including to sea surge and wave levels (Dr Richard Wilkinson,\u000d\u000a      UoN 2009 to date) and carbon capture and storage (Professor Andrew Cliffe,\u000d\u000a      UoN 2005 to date, and Wilkinson).\u000d\u000a    "},{"CaseStudyId":"30946","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"2921044","Name":"Germany"}],"Funders":["Wellcome Trust","Biotechnology and Biological Sciences Research Council","Engineering and Physical Sciences Research Council","Medical Research Council"],"ImpactDetails":"\u000a    CMMB established and has utilised a number of distinct mechanisms to\u000a      increase the visibility and uptake of mathematical and statistical methods\u000a      in medical and biological problems. These include:\u000a    Industrial skills development through training courses and Study\u000a        Groups: The BBSRC Modular Training for Industry course `Introduction\u000a      to Mathematical Modelling for Life Scientists' from 2008, the `Noise in\u000a      Life' conference in 2009 and the `Introduction to Biological Modelling'\u000a      summer schools led by Owen have all been well received by industry. Ian\u000a      Barrett (Astra Zeneca): \"...I am actually currently leading a project\u000a        using modelling to support one of our drug projects... and also line\u000a        manage a mathematical modeller. I would say your course was an important\u000a        factor in enabling that progression, and I came away from it feeling\u000a        inspired to learn more.\" [B1]. Owen was further consulted by Roche\u000a      regarding the implementation of multiscale modelling for oncology (from\u000a      molecular targets right up to patient health and survival).\u000a    CMMB members established Study Groups for Mathematics in Medicine (MMSG)\u000a      in 2000, for Plant Sciences in 2007 and for the Virtual Physiological\u000a      Human in 2009. These engendered new industrial collaborations (e.g.\u000a      AstraZeneca, Health and Safety Laboratory, Pfizer, Syngenta, Unilever) and\u000a      changed company procedures. Unilever ran an internal Study Group\u000a      (Facilitator: King), with Brendan O'Malley (ex-Systems Biology Project\u000a      Leader, Unilever) stating: \"Critically the Study Group also\u000a        offered an opportunity to familiarise both mathematicians and biologists\u000a        with the different approaches and ways of working of their respective\u000a        disciplines\". [B2]\u000a    This and other activity (CASE, MSc project, consulting) contributed to\u000a      Unilever's Framework Partnership with the University (tinyurl.com\/d8dypj8),\u000a      one of only three in the UK, to carry out research in certain strategic\u000a      areas exclusively at UoN.\u000a    Postgraduate and postdoctoral training: Through extensive\u000a      mathematical biology MSc and PhD programmes (the latter with support from\u000a      European funding [A6, A8]), internships of students with industry (via\u000a      e.g. CASE or [A8]) and tailored training in both scientific and\u000a      professional development skills, individuals trained within CMMB have made\u000a      significant contributions to the adoption of UoN's mathematical and\u000a      statistical methods by industry and other end users:\u000a    Robert Jenkins (CRUK), the first theoretician recruited by CRUK with a\u000a      background purely in mathematics, stated that: \"...the expertise I\u000a        developed there [CMMB] was crucial to my being awarded a permanent\u000a        contract as a Senior Scientific Officer at the London Research\u000a        Institute, which reflects the growing profile and impact of mechanistic\u000a        modelling approaches within CRUK's research and the increasing\u000a        confidence in their value for improving our understanding of a wide\u000a        range of cancers.\" [B3];\u000a    Greg Lemon (Fellow at Harvard Bioscience, PDRA in CMMB until 2011 and\u000a      recent collaborator) applies mathematical modelling frameworks and skills\u000a      he developed while in CMMB in work with Harvard Apparatus (USA), Novalung\u000a      (Germany) and others to (for example) optimise performance of a membrane\u000a      oxygenator for paracorporeal lung support, and to quantify cell coverage\u000a      of tissue engineering scaffolds in a patent for a novel colorimetric\u000a      assay;\u000a    Susan Franks (Health and Safety Laboratory) also benefitted: \"My own\u000a        training as a PhD student and postdoctoral student within CMMB\u000a        (1997-2004) was instrumental in the formulation, and in establishing the\u000a        direction, of a significant (c. &#163;900K) programme of work (of which I was\u000a        lead Investigator) funded by HSL to enhance their capabilities in using\u000a        mathematical modelling...\" [B4].\u000a    Public engagement and widening dissemination: CMMB engages\u000a      extensively with a wider public audience, including through: YouTube 2009\u000a      `Meet the Mathematicians' (tinyurl.com\/db2q72),\u000a      77k visitors with 97% positive feedback, `Biology Meets Mathematics' (tinyurl.com\/ox28q26),\u000a      31k visitors with 95% positive feedback; outreach activities with CPIB in\u000a      local primary schools to stimulate interest from an early age; and the\u000a      annual UoN `MayFest' community open day where CMMB and CPIB ran a\u000a      multicellular image competition judged by the public.\u000a    A drive by CMMB and the School more broadly to publish in prestigious and\u000a      highly visible outlets, such as Nature and PNAS, has raised the profile\u000a      and coverage of our research. CMMB papers have been cited by numerous\u000a      clinical groups, such as Bart's in London, Mt. Sinai Hospital in Toronto\u000a      and Shanghai Peoples Hospital in areas such as allergy, cardiology, gene\u000a      therapy, microbiology, oncology and wound repair, as well as industry,\u000a      e.g. AstraZeneca, GSK, Pfizer, Roche and Unilever. In particular, [A2] has\u000a      been cited by cardiac surgeons, epidemiologists and infectious disease\u000a      units in hospitals, microbiologists, the Health Protection Agency (now\u000a      Public Health England) and the Health Policy Research Institute in\u000a      California: this publication in a BMC journal read by clinicians and\u000a      policy-makers has led to impact on public health policy (see below).\u000a    Two specific examples detailed below further illustrate the scope of the\u000a      impact of CMMB's work:\u000a    Software development for clinical and industrial use: UoN work in\u000a      the late 1990s on tumour growth (e.g. [A1]) inspired a topic at the first\u000a      MMSG in 2000, on the effects of a new anti-tumour drug. This was taken\u000a      further with Sheffield Medical School and led to a patent filed in 2007\u000a      and published in 2009 (tinyurl.com\/kdnhb3d).\u000a      Following this and similar successes, CMMB members made founding\u000a      contributions to the open source C++ library for computational biology\u000a      `Chaste' (Cancer Heart and Soft Tissue Environment), UoN leading the\u000a      cancer strand on the EPSRC grant eScience Pilot Project in Integrative\u000a        Biology, GR\/S72023\/01, 2004-2008 that funded UoN researchers Inge\u000a      van Leeuwen, Gary Mirams and Alex Walter. Mirams, now at Oxford, continued\u000a      to work on Chaste with academic and industrial partners, participating in\u000a      producing the first public release in 2009.\u000a    A 2013 review prepared in collaboration with partners such as Microsoft\u000a      Research UK and Fujitsu Laboratories of Europe Ltd stated: \"A number\u000a        of other groups are using Chaste for a large variety of simulations...\u000a        the effects of radiation on tissue (Shaowen Hu, NASA); ...cardiac\u000a        electrophysiological modelling (US Food and Drug Administration);\u000a        drug-induced changes to cardiac rhythm (Safety Pharmacology,\u000a        GlaxoSmithKline)...\" [B5]. Mirams adds that: \"A Chaste\u000a        simulation tool is now in use by GlaxoSmithKline Safety Pharmacology\u000a        teams in the UK and USA. ... A recent paper [B7] evaluates how well this\u000a        tool predicts the results of later animal-based safety tests that are\u000a        performed at GlaxoSmithKline with a view to 3Rs benefits.\" and\u000a      further that: \"This work has led directly to the US drug regulators\u000a        (Food &amp; Drug Administration, FDA) including a simulation aspect in\u000a        their proposed replacement for an expensive ... human trial for\u000a        pro-arrhythmic side effects ...\" [B6]\u000a    Influence on public health policy: Work undertaken by O'Neill and\u000a      Kypraios on MRSA stimulated UK Government policy debate and led to a\u000a      review of the current screening policy. Dr Julie Robotham (Public Health\u000a      England) stated: \"...your [Kypraios'] work directly informed the\u000a        model-based evaluation of MRSA control policies, undertaken for the\u000a        Department of Health (DoH), England\" and \"... [A2] was critical in that\u000a        it represented the only study able to provide direct estimates of the\u000a        effectiveness of contact precautions in reducing MRSA transmission...it\u000a        enables the national policy of MRSA screening to be evaluated with more\u000a        realistic estimates of how much benefit MRSA screening can actually\u000a        provide.\" [B8].\u000a    The research suggested that the current national MRSA screening policy in\u000a      England was unlikely to be cost-effective [B9]. Whilst a change in policy\u000a      itself is still under discussion with the DoH, Robotham stated that this\u000a      finding: \"generated discussions both nationally ... and\u000a        internationally ...\" and moreover caused the DoH to commission \"...\u000a        a direct follow on to this work such that a cost-effectiveness\u000a        evaluation at a national level, including a national MRSA audit, could\u000a        be conducted. This follow on project, whilst using previous estimates,\u000a        also used more up-to-date estimates, including estimates taken directly\u000a        from your study Estimating the effectiveness of isolation and\u000a        decolonization measures in reducing transmission of\u000a        methicillin-resistant Staphylococcus aureus in hospital general wards.\u000a        Worby CJ,[et al.] Am J Epidemiol. 2013 Jun 1;177(11):1306-13. doi:\u000a        10.1093\/aje\/kws380. Again, these estimates represented the best\u000a        available evidence, and directly informed the cost-effectiveness model.\"\u000a    In addition, Kypraios presented his work on avian influenza (to model and\u000a      predict in real-time spread of a potential epidemic in the UK poultry\u000a      industry) to Professor Sir John Beddington as UK Government Chief\u000a      Scientific Adviser 2008-2013.\u000a    ","ImpactSummary":"\u000a    Research at The University of Nottingham's (UoN) Centre for Mathematical\u000a      Medicine and Biology (CMMB) has informed a wide array of beneficiaries\u000a      including public policy-makers, clinicians and industry in biomedical\u000a      fields such as cancer and hospital infections. Through a wide range of\u000a      mechanisms such as Study Groups, training, outreach and user-engagement,\u000a      the CMMB has established an outstanding track record of furthering the use\u000a      of mathematics and statistics to address medical and biological problems.\u000a    ","ImpactType":"Health","Institution":"\u000a    University of Nottingham\u000a    ","Institutions":[{"AlternativeName":"Nottingham (University of)","InstitutionName":"University of Nottingham","PeerGroup":"A","Region":"East Midlands","UKPRN":10007154}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"6167865","Name":"Toronto"},{"GeoNamesId":"1796231","Name":"Shanghai Shi"}],"References":"\u000a    The three publications that best indicate the quality of the research are\u000a      indicated *\u000a    Cancer:\u000a    \u000a[A1]* Ward, J.P. and King, J.R., 1997. Mathematical modelling of\u000a      avascular-tumour growth, IMA Journal of Mathematics Applied in Medicine\u000a      and Biology [now Mathematical Medicine and Biology: A Journal of the IMA].\u000a      14(1), 39-69. DOI: 10.1093\/imammb\/14.1.39\u000a      Its impact includes via inclusion amongst the `100 seminal papers' as\u000a        part of the centenary of Oxford University Press in 2006. http:\/\/www.oxfordjournals.org\/news\/centenary.html\u000a    \u000aHospital Infections:\u000a    \u000a[A2]* Kypraios, T., O'Neill, P.D., Huang, S.S., Rifas-Shiman, S.L.\u000a      and Cooper, B.S., 2010. Assessing the role of undetected colonization and\u000a      isolation precautions in reducing Methicillin-Resistant Staphylococcus\u000a      aureus transmission in intensive care units, BMC Infectious Diseases. 10,\u000a      29. DOI: 10.1186\/1471-2334-10-29\u000a    \u000a\u000a[A3] Kypraios, T., O'Neill, P.D., Jones, D.E., Ware, J., Batra,\u000a      R., Edgeworth, J.D. and Cooper, B.S., 2011. Effect of systemic antibiotics\u000a      and topical chlorhexidine on methicillin-resistant Staphylococcus aureus\u000a      (MRSA) carriage in intensive care unit patients. Journal of Hospital\u000a      Infection 7, 222-226. DOI: 10.1016\/j.jhin.2011.05.008\u000a    \u000aNeuroscience:\u000a    \u000a[A4] Laudanski, J., Coombes, S., Palmer, A. R. and Sumner, C. J.,\u000a      2010. Mode-locked spike trains in responses of ventral cochlear nucleus\u000a      chopper and onset neurons to periodic stimuli. Journal of Neurophysiology.\u000a      103, 1226-1237. DOI: 10.1152\/jn.00070.2009.\u000a    \u000aPlants:\u000a    \u000a[A5]* Middleton, A.M., King, J.R., Bennett, M.J. and Owen, M.R.,\u000a      2010. Mathematical modelling of the Aux\/IAA negative feedback loop.\u000a      Bulletin of Mathematical Biology. 72(6), 1383-1407. DOI:\u000a      10.1007\/s11538-009-9497-4.\u000a    \u000aGrants:\u000a    [A6] (2006-2010) EC FP6 project 20723, Marie Curie Training\u000a      Programme: MMBNOTT &#8212; Mathematical Medicine and Biology at Nottingham, PI\u000a      Jensen, &#8364;1.7m.\u000a    [A7] (2007-2012) BBSRC project BB\/D019613\/1, Centre for Plant\u000a      Integrative Biology, PI Hodgman, Biosciences, Nottingham; 20 Co-Is\u000a      including Byrne, Dryden, Jensen, King, Owen, Wood from Mathematical\u000a      Sciences, &#163;9.2m.\u000a    [A8] (2012-2016) EC FP7 project 289146, Marie Curie Initial\u000a      Training Network: NETT &#8212; Neural Engineering Transformative Technologies,\u000a      PI Coombes, &#8364;5.3m.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000a    [B1] Associate Principal Scientist, Astra Zeneca, UK. (email on\u000a      file)\u000a    [B2] European Study Group with Industry 59 Case Study with\u000a      Unilever &#8212; webpage www.smithinst.ac.uk\/Events\/ESGI59\/CaseStudy\u000a      (copy also on file)\u000a    [B3] Senior Scientific Officer, Cancer Research UK London Research\u000a      Institute, UK. (email on file)\u000a    [B4] Group Leader in Biomathematical Modelling, Health and Safety\u000a      Laboratory, UK. (email on file)\u000a    [B5] Mirams, G.R. et al., 2013, Chaste: an open source C++ library\u000a      for computational physiology and biology, PLoS Computational Biology,\u000a      9(3), e1002970. DOI:10.1371\/journal.pcbi.1002970\u000a    [B6] Senior Research Fellow, Dept. of Computer Science, University\u000a      of Oxford, UK. (email on file)\u000a    [B7] Beattie, K.A. et al., Evaluation of an In Silico Cardiac\u000a      Safety Assay: Using Ion Channel Screening Data to Predict QT Interval\u000a      Changes in the Rabbit Ventricular Wedge, Journal of Pharmacological and\u000a      Toxicological Methods, 68(1), 88-96. DOI:10.1016\/j.vascn.2013.04.004\u000a    [B8] Modelling and Economics Unit, Public Health England, UK.\u000a      (email on file)\u000a    [B9] The Department of Health-commissioned report `National One\u000a      Week Prevalence Audit of MRSA Screening' (copy on file) \u000a    ","Title":"\u000a    Advancing the use of mathematics and statistics to address medical and\u000a        biological problems\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The CMMB was formally established in 1998 by Professors John King (UoN\u000a      1988 to date) and Helen Byrne (UoN 1998 to 2011). It is currently led by\u000a      Professor Markus Owen (UoN 2004 to date), and includes 20 academics from\u000a      the School of Mathematical Sciences, together with over 50 affiliates\u000a      (from the Science, Engineering and Medical Faculties), PDRAs and PGRs. Its\u000a      aim is to use mathematical modelling and statistics to understand\u000a      biological systems and predict their behaviour, thus underpinning systems\u000a      biology in industrial biological and biomedical research. The CMMB is one\u000a      of the largest and most successful such groups in the world and has built\u000a      up a wide network of active links to end users in clinical and industrial\u000a      settings. Applications such as those below, and numerous other medical and\u000a      biological areas of research, have served to form a solid foundation for\u000a      CMMB outreach and advancement activity.\u000a    Cancer: Cancer modelling is a key area that contributed to the\u000a      establishment of the CMMB, members including Byrne, King, Owen, John Ward\u000a      (PGR and Wellcome Trust Fellow, now at Loughborough) and Steve Webb (PDRA,\u000a      now at Liverpool) have undertaken seminal work on multi-phase models for\u000a      tumour spheroids and multi-scale models for avascular tumour growth, e.g.\u000a      [A1], deepening our understanding of the treatment of cancers including\u000a      using a patient's own immune cells to deliver drugs deep inside tumours.\u000a    Hospital Infections: The CMMB hosts an internationally-recognised\u000a      team working on stochastic epidemic models, including modelling and data\u000a      analysis for antibiotic resistant pathogens such as MRSA. Professor Philip\u000a      O'Neill (UoN 1999 to date) and Dr Theo Kypraios (UoN 2006 to date) have\u000a      worked with colleagues at the former Health Protection Agency (now Public\u000a      Health England) and clinical academic colleagues in the USA (e.g. Dr Susan\u000a      Huang, School of Medicine, University of California, Irvine) [A2] and UK\u000a      (e.g. Dr Jonathan Edgeworth, Guys' and St Thomas' Hospital Trust, London)\u000a      [A3] to address questions pertinent to the understanding and control of\u000a      hospital infections. Recent work by King, Dr Sara Jabbari (PGR, PDRA and\u000a      MRC Fellow, now at Birmingham) and co-workers pioneered a systems biology\u000a      approach (combining cutting-edge experimental work and computational\u000a      modelling and analysis) to accelerate our understanding of e.g.\u000a      Staphylococcus aureus and Clostridium difficile (see e.g. 2012 Bulletin of\u000a      Mathematical Biology, 74, 1292-1325).\u000a    Neuroscience: Professor Stephen Coombes (UoN 2003 to date) has\u000a      enabled the CMMB to gain recognition as an internationally leading centre\u000a      for mathematical and computational neuroscience. Coombes is active in\u000a      translating ideas toward application, in fostering collaborations with a\u000a      range of disciplines including Psychology, Engineering and clinical groups\u000a      e.g. Radiology, and in leading an EC-funded Marie Curie Initial Training\u000a      Network in Neural Engineering [A8] which is training 20 early career\u000a      researchers across Europe. Underpinning works include using tools from\u000a      nonlinear dynamical systems to understand auditory encoding of natural\u000a      sounds [A4], on the back of which work Jonathan Laudanski (PGR, 2006-2010)\u000a      gained employment as a research scientist for Neurelec developing cochlear\u000a      implants.\u000a    Plants: CMMB members began collaboration with Professor Malcolm\u000a      Bennett (Biosciences, UoN 1999 to date) in 2006 on modelling for plant\u000a      biology (Arabidopsis roots). This led to a grant [A7] to establish the\u000a      Centre for Plant Integrative Biology (CPIB) in 2007. A combination of\u000a      multiscale modelling and statistical inference has proved ideally suited\u000a      to tackling a wide variety of problems in this field, from how hormone\u000a      dynamics create root branches through seed germination and plant\u000a      fertility, to understanding how tomatoes ripen. Underpinning works include\u000a      [A5].\u000a    "},{"CaseStudyId":"30947","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a      Pratt &amp; Whitney are one of the `big three' global aerospace\u000a        manufacturers whose engines power more than 25% of the world's mainline\u000a        passenger fleet. They reported revenue of US$14 billion in the year 2012\u000a        [B1]. In 2003, the United Technologies Company (UTC &#8212; Pratt &amp;\u000a        Whitney's parent company) technical advisory committee issued a\u000a        challenge to require all engineering analyses to have an associated\u000a        uncertainty (covering accuracy and precision) and range of\u000a        applicability. In response, Pratt &amp; Whitney undertook the\u000a        Uncertainty Quantification Initiative which, in 2008 [B2], became the\u000a        Design for Variation (DFV) Strategic Initiative [B3]. This initiative\u000a        moved the design process within the company from a deterministic to a\u000a        probabilistic design framework, and is described in a paper written by\u000a        18 Pratt &amp; Whitney employees [B4] as a `paradigm shift' in their\u000a        engineering approach. Their introduction states that\u000a      \"Much of the analytical structure of DFV is derived from the Kennedy\u000a          and O'Hagan (2001) [A1] paper on Bayesian model calibration. Kennedy\u000a          and O'Hagan developed models and methods for Bayesian model\u000a          calibration which, in addition to calibrating model parameters,\u000a          quantify systematic and random discrepancies between model and data.\"\u000a      The main impacts of DFV at Pratt &amp; Whitney are:\u000a      \u000a         Increasing the time that an engine stays \"on wing\". This is\u000a          directly related to reducing important sources of variation, thus\u000a          aircraft availability or readiness can be managed and improved using\u000a          DFV technologies [B4].\u000a         Improved identification of cost-reduction opportunities. DFV\u000a          technologies are used to highlight design and process features that\u000a          have little impact on the part or system performance but are expensive\u000a          to maintain [B4].\u000a      \u000a      Since DFV's inception, the company has realised a range of benefits,\u000a        including increased speed of design studies and optimisation, root cause\u000a        investigations using engineering model emulators, and improvement of\u000a        quality systems through identification of inspectable characteristics\u000a        that are more highly correlated with service performance [B3, B4].\u000a      DFV has catalysed a shift in design paradigm across Pratt &amp;\u000a        Whitney, and has involved quantifying all of the uncertainties in their\u000a        design and simulation process. Grant Reinman (senior statistician and\u000a        DFV leader) based the company's approach upon [A1], and in 2011\/12 he\u000a        gave a series of conference talks (including at NASA and the Isaac\u000a        Newton Institute [B5]) describing how the methods of O'Hagan have been\u000a        successfully implemented at Pratt &amp; Whitney. Al Brockett, former\u000a        vice president at Pratt &amp; Whitney, recently described how DFV has \"changed\u000a          from a special initiative focused on statistical training to a\u000a          high-visibility strategic priority\" for the company [B3]. This\u000a        successful and large investment of time and money is the most complete\u000a        demonstration of the impact of the work developed by O'Hagan at UoN.\u000a      Pratt &amp; Whitney estimates \"that its component-level DFV\u000a          initiatives have yielded a 64 percent to 88 percent return on\u000a          investment by reducing design iterations, improving manufacturability,\u000a          increasing reliability, improving on-time deliveries, and providing\u000a          other performance benefits. As Pratt &amp; Whitney focuses\u000a          increasingly on the systems level, it estimates that it will realize a\u000a          40-times return on investment by achieving systems-level reliability\u000a          goals much earlier in the development cycle.\" [B3, see also B5]. A\u000a        Business Case Study undertaken at Pratt &amp; Whitney quantifies savings\u000a        in costs for a large fleet of military aircraft at approximately US$1\u000a        billion [B6]. Applying DFV is estimated to save 18-37% of scrap and\u000a        rework costs and 80% of the engineering support costs associated with\u000a        turbine aerofoils that do not meet final air flow requirements [B4].\u000a      The scale of a typical DFV project is highly proprietary to Pratt &amp;\u000a        Whitney, but it has involved the creation of a new group within the\u000a        company (Parametric Modeling, Design Automation, Optimization and DFV),\u000a        and extensive training of a large number of staff across the entire\u000a        company in UTC's degree programme [B3, B6, B7]. DFV has grown into a\u000a        core competency, and is applied as a 10-step process that guides all\u000a        engineering activities [B3]. Pratt &amp; Whitney has created hundreds of\u000a        internal courses and over 200 engineers have taken the advanced\u000a        emulation and calibration classes, and 5 engineers have taken complete\u000a        graduate degrees in statistics [B6]. Since the start of the DFV\u000a        initiative in 2008 [B2], 32 different key modelling processes have been\u000a        DFV enabled [B4], ranging through the entire engine design process, from\u000a        fan, compressor and turbine design, to performance analysis and engine\u000a        validation.\u000a      The Uncertainty Quantification methodology is considered by Pratt &amp;\u000a        Whitney to be part of their competitive advantage, as controlling\u000a        variation has become one of the keys to improving performance while also\u000a        improving part yield and quality [B3]. Other key competitors and\u000a        collaborators (e.g. General Electric, Airbus) are beginning to use\u000a        similar methods actively in their design processes [B8].\u000a      ","ImpactSummary":"\u000a    Methods of emulation, model calibration and uncertainty analysis\u000a      developed by Professor Tony O'Hagan and his team at The University of\u000a      Nottingham (UoN) have formed the basis of Pratt &amp; Whitney's Design for\u000a      Variation (DFV) initiative which was established in 2008. The global\u000a      aerospace manufacturers describe the initiative as a `paradigm shift' that\u000a      aims to account for all sources of uncertainty and variation across their\u000a      entire design process.\u000a    Pratt &amp; Whitney considers their implementation of the methods to\u000a      provide competitive advantage, and published savings from Pratt &amp;\u000a      Whitney adopting the DFV approach for a fleet of military aircraft are\u000a      estimated to be approximately US$1billion.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Nottingham\u000a    ","Institutions":[{"AlternativeName":"Nottingham (University of)","InstitutionName":"University of Nottingham","PeerGroup":"A","Region":"East Midlands","UKPRN":10007154}],"Panel":"B         ","PlaceName":[],"References":"\u000a    The three publications that best indicate the quality of the research are\u000a      indicated *\u000a    \u000a[A1]* Kennedy, M. C. and O'Hagan, A. (2001). Bayesian calibration\u000a      of computer models (with discussion). Journal of the Royal Statistical\u000a      Society B 63, 425-464.\u000a      DOI: 10.1111\/1467-9868.00294\u000a    \u000aFunded by [A5]. Initially released as Nottingham Statistics Group\u000a        Technical Report 1998-10.\u000a    \u000a[A2]* O'Hagan, A., Kennedy, M. C. and Oakley, J. E. (1999).\u000a      Uncertainty analysis and other inference tools for complex computer codes\u000a      (with discussion). In Bayesian Statistics 6, J. M. Bernardo et al.\u000a      (eds.). Oxford University Press, 503-524.\u000a      ISBN-10: 0198504853; ISBN-13: 978-0-19-850485-6 (available on request)\u000a    \u000aThis conference paper provides most of the theory that is eventually\u000a        published in [A1, A3, A4].\u000a    \u000a[A3]* Oakley, J. and O'Hagan, A. (2002). Bayesian inference for\u000a      the uncertainty distribution of computer model outputs. Biometrika, 89,\u000a      769-784.\u000a      DOI: 10.1093\/biomet\/89.4.769\u000a    \u000aInitially released as Nottingham Statistics Group Technical Report\u000a        1998-11.\u000a    \u000a[A4] Kennedy, M.C. and O'Hagan, A. (2000). Predicting the output\u000a      from a complex computer code when fast approximations are available.\u000a      Biometrika 87, 1-13. DOI: 10.1093\/biomet\/87.1.1\u000a    \u000aFunded by [A5] and first released as Nottingham Statistics Group\u000a        Technical Report 1998-09.\u000a        Grants\u000a      [A5] Bayesian uncertainty analysis and calibration of complex\u000a        computer models, PI O'Hagan, EPSRC grant GR\/K54557\/01, 1 October 1995 &#8212;\u000a        30 September 1998, &#163;114,666.\u000a      Employed Mark Kennedy as a research associate and led to the papers\u000a          [A1, A2, A3, A4].The foundations for the grant were the PhD project of\u000a          Haylock and associated work by O'Hagan.\u000a      ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"}],"Sources":"\u000a      [B1] Pratt &amp; Whitney website www.pw.utc.com\/Who_We_Are\u000a        (copy also filed 5 August 2013)\u000a      [B2] Pratt &amp; Whitney's history of corporate quality\u000a        initiatives (2012). A time-line of the dates of the DFV initiative.\u000a        Pratt &amp; Whitney document. (copy on file)\u000a      [B3] Al Brockett interview, ANSYS Advantage Magazine, vol VII,\u000a        issue 2, pp 16-21 (2013). http:\/\/www.ansys.com\/About+ANSYS\/ANSYS+Advantage+Magazine\u000a        (copy also on file)\u000a      [B4] Reinman, G., Ayer, T., Davan, T., Devore, M., Finley, S.,\u000a        Glanovsky, J., Gray, L., Hall, B., Jones, C., Learned, A., Mesaros, E.,\u000a        Morris, R., Pinero, S., Russo, R., Stearns, E., Teicholz, M.,\u000a        Teslik-Welz, W. and Yudichak, D., (2012). Design for Variation, Quality\u000a        Engineering, 24:317-345. DOI:10.1080\/08982112.2012.651973 (copy also on\u000a        file)\u000a      Many of the other statistics papers they cite (besides the O'Hagan\u000a          papers) are direct extensions of the approaches developed by O'Hagan\u000a          during his time in Nottingham (such as Higdon et al. 2008, and Santner\u000a          et al. 2003, Williams et al. 2006).\u000a      [B5] Reinman G., Design for Variation, invited\u000a        conference presentation, Uncertainty in Computer Models, Sheffield 2012.\u000a        www.mucm.ac.uk\/UCM2012\/Forms\/Downloads\/Reinman.pptx\u000a        (copy also on file)\u000a      Versions of this talk were also given at NASA and the Isaac Newton\u000a          Institute (amongst other places) during 2011.\u000a      [B6] Senior Statistician, Pratt &amp; Whitney, Connecticut, USA\u000a        (email on file).\u000a      [B7] http:\/\/bits.blogs.nytimes.com\/2012\/01\/31\/a-1-billion-model-employee-education-program\/\u000a        (copy also filed 5 August 2013)\u000a      [B8] http:\/\/www.stirling-dynamics.com\/dipart-loads-and-aeroelastics-news-and-events\/uncertainty-quantification-and-management-workshop\"\u000a        (copy also filed 5 August 2013) \u000a      ","Title":"\u000a    Enhancing competitive advantage at Pratt &amp; Whitney using Design\u000a        for Variation\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research is O'Hagan's work on uncertainty\u000a      quantification, carried out in Nottingham between 1993 and 1998. In\u000a      particular, O'Hagan (UoN, 1990-1999, Department of Mathematics and School\u000a      of Mathematical Sciences) explained how to perform analysis of complex\u000a      computer simulators when computational resources are limited or there is\u000a      uncertainty about any aspect of the system. The research was motivated by\u000a      O'Hagan's work with various organisations, including the National\u000a      Radiological Protection Board [A2], which wished to draw inferences from\u000a      complex computer simulators.\u000a    Computer simulators used to make predictions usually have the following\u000a      characteristics: they i) rely upon unknown parameter values; ii) are\u000a      imperfect representations of the physical system they are modelling; and\u000a      iii) are inherently deterministic, i.e., there is no natural variability,\u000a      unlike in physical systems. O'Hagan and his team (including PhD students\u000a      Haylock, Kennedy and Oakley, and then PDRA Kennedy) developed a range of\u000a      techniques for analysing computer simulators which are widely used by\u000a      industry and scientific researchers.\u000a    The first of these methods is an approach to calibration, namely how to\u000a      estimate fixed but unknown input parameters. This work appeared in papers\u000a      [A1, A2] published after O'Hagan had left UoN in January 1999, but which\u000a      were initially published as UoN technical reports in 1998 as a result of\u000a      an EPSRC-funded project [A5]. In particular, [A1] is an important Royal\u000a      Statistical Society discussion paper that has had wide-ranging impact\u000a      (over 460 citations according to Scopus in journals covering mathematics,\u000a      engineering, computer science, as well as environmental, decision, earth,\u000a      agricultural and biological sciences and other fields). The key idea is\u000a      that if a simulator is imperfect then this imperfection must be modelled\u000a      to learn anything meaningful about the model parameters. O'Hagan's\u000a      research showing how to do this for complex simulators using Gaussian\u000a      processes is the starting point for looking at quantifying simulator\u000a      discrepancy.\u000a    The second method is for undertaking uncertainty and sensitivity analysis\u000a      in complex simulators [A1, A3], which is the problem of how to propagate\u000a      uncertainty in initial conditions and parameters through the simulator to\u000a      find the corresponding uncertainty in the predictions. Imperfect knowledge\u000a      of parameter values and lack of natural variability in the simulator mean\u000a      that this is often (along with model error) the main source of uncertainty\u000a      in predictions; computationally expensive simulators make Monte Carlo\u000a      approaches infeasible.\u000a    O'Hagan showed how to build and use emulators of simulators to solve the\u000a      calibration and uncertainty\/ sensitivity analysis problems [A1, A2, A3,\u000a      A4]; emulators are statistical models of the computer simulator that can\u000a      be used as surrogate models to perform the inference that would be too\u000a      computationally expensive to do with the full simulator. He showed how to\u000a      build Gaussian process emulators within a Bayesian framework [A2, A4], and\u000a      demonstrated how they can be used in highly complex problems.\u000a    The statistical design paradigm used by Pratt &amp; Whitney is very\u000a      clearly dependent on, both in spirit and in the specifics, the uncertainty\u000a      quantification approach developed and advocated by O'Hagan during his time\u000a      in Nottingham. Pratt &amp; Whitney make extensive use of emulators; they\u000a      use both the approach in [A1] for simulator calibration and the\u000a      sensitivity analysis approach that was developed from [A2] and [A3].\u000a      Reinman et al. [B4] also cites other papers that rely heavily upon\u000a      (and cite) the work of O'Hagan during his time in Nottingham.\u000a    "},{"CaseStudyId":"30949","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2921044","Name":"Germany"}],"Funders":["Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000a    The impacts from the research developments at Nottingham in shape\u000a      analysis have been made\u000a      possible via a number of routes, including making the `shapes' software\u000a      [A5] freely available and\u000a      the description of the work in textbooks, such as Dryden and Mardia's book\u000a      on Statistical Shape\u000a      Analysis (1998, Wiley) and Kendall et al. [A1] in 1999, which have sold\u000a      approximately 1800 and\u000a      760 copies respectively (as of 14\/08\/2013). These have helped to create\u000a      links with users in\u000a      industry and the police, who having become aware of the research and\u000a      software via these links\u000a      have subsequently made contact with Nottingham directly or via other\u000a      academic collaborators.\u000a    The `shapes' package [A5] in particular has been one of the more popular\u000a      downloaded packages\u000a      on the CRAN (Comprehensive R Archive Network) website. Data from the\u000a      University of California\u000a      Los Angeles mirror on 30\/09\/2010 (http:\/\/neolab.stat.ucla.edu\/cranstats\/),\u000a      one of the few from the\u000a      92 mirror sites in 43 countries\/regions (http:\/\/cran.r-project.org\/mirmon_report.html)\u000a      that provides\u000a      download data, reported that out of 2531 contributed packages the `shapes'\u000a      package was the\u000a      112th most downloaded (520 downloads) from the 116th most separate IP\u000a      addresses (379 IPs),\u000a      i.e. in the top 5% of downloaded contributed R packages at that time (the\u000a      most downloaded\u000a      package from the mirror had 935 downloads, less than twice that of the\u000a      `shapes' package). Sites at\u000a      which the `shapes' package was publicised throughout the period 2003-2013\u000a      include locally at\u000a      www.maths.nottingham.ac.uk\/~ild\/shapes\u000a      and the SUNY Stony Brook Morphometrics website\u000a      http:\/\/life.bio.sunysb.edu\/morph\/.\u000a    Commercial software\u000a      One specific illustration of the impact of the research on size-and-shape\u000a      analysis [A2] is through\u000a      the work of Dr Joel Mitchelson who works for Charnwood Dynamics Ltd and\u000a      the start-up company\u000a      Ogglebox Sensory Computing Ltd. Charnwood Dynamics has an established 3D\u000a      movement\u000a      analysis brand, Codamotion (www.codamotion.com),\u000a      which is used in many settings, including\u000a      clinical analysis, mobile gait labs, biomechanics, sports, orthotics and\u000a      prosthetics, ergonomics,\u000a      virtual reality and visualisation (tinyurl.com\/kr64xfw,\u000a      page 12). The company has an average\u000a      annual turnover of &#163;713k (2006-12) and is one of 8 main global players in\u000a      the market for marker-based\u000a      3D movement analysis for life sciences (clinical and research use).\u000a    Mitchelson first confirms in his letter [B1] that: \"The published\u000a        results on shape spaces from\u000a        University of Nottingham were instrumental in proving the convergence of\u000a        an algorithm for\u000a        measurement of the mean size-and-shape of a moving cluster of 3D\u000a        markers. The algorithm and\u000a        proof have now been accepted for publication in the Journal of\u000a        Biomechanics, and are\u000a        implemented in the open source library, Open3DMotion [tinyurl.com\/ldyhkkc].\"\u000a    The publication mentioned here is [B2], which makes clear its reliance on\u000a      Le's work. Mitchelson's\u000a      work arose after he contacted Le for a copy of [A3]. This led to\u000a      independent work by Mitchelson\u000a      and his team which, in particular, uses the sectional curvature\u000a      calculations for size-and-shape\u000a      space from Le &amp; Kendall 1993 in the condition for uniqueness of the\u000a      mean [A2, Condition C] to\u000a      develop further results for occluded data. Mitchelson states by way of\u000a      clarification that:\u000a      \"This library forms the basis of the calculation engine within\u000a        Codamotion's commercial ODIN\u000a        software (http:\/\/www.codamotion.com\/the-odin-software-suite.html)\u000a      [Introduced in October 2011].\"\u000a      He adds: \"The benefit to Codamotion customers is that the method\u000a        allows rigid motions of clusters\u000a        of markers on a human body to be tracked without a calibration trial,\u000a        which can save them time. It\u000a        also allows small portable 3D movement analysis systems to be moved\u000a        around to measure very\u000a        large volumes, using reference markers used to transform measurements\u000a        from a moving system to\u000a        a static reference frame. This opens up new market opportunities for the\u000a        company, particularly for\u000a        analysis of sports movements in the field, and ergonomics in industrial\u000a        environments. The\u000a        published shape space results from University of Nottingham are\u000a        important for giving confidence in\u000a        the results obtained from these new products.\"\u000a    That impact from this has already been realised is made explicit by\u000a      Mitchelson in an email dated\u000a      23 July 2013 (copy on file): \"A beta version of the algorithm has been\u000a        in the software for some\u000a        months. We're already able to engage with customers and potential\u000a        customers about this due to\u000a        the solid mathematical foundation, which is a result of the\u000a        size-and-shape spaces work.\"\u000a    Fingerprinting\u000a      A further illustration is through the work of Dr Thomas Hotz (Ilmenau\u000a      University of Technology) with\u000a      the Bundeskriminalamt (Federal Criminal Police Office of Germany), where\u000a      Nottingham research\u000a      has led to efficiency savings after the methods were introduced into\u000a      standard operating procedure\u000a      within the Force. Hotz describes the profound impact of Nottingham\u000a      research in [B3]:\u000a    \"The aim of the study was to understand and predict the impact growth\u000a        has on fingerprints of\u000a        adolescents [B4, B5]. The difficulty automated fingerprint\u000a        identification systems face when\u000a        confronted with fingerprints of adolescents was that the points of\u000a        interest used in matching\u000a        algorithms move during growth, so that either tolerances in matching\u000a        have to be increased,\u000a        resulting in a worsened overall performance, or the fingerprint can no\u000a        longer reliably be found in\u000a        records after some years of growth. Understanding growth firstly is a\u000a        matter of understanding\u000a        whether it occurs isotropically, i.e. uniformly in all directions. If\u000a        that were the case the shapes would\u000a        not change during growth. We thus used the software package \"shapes\"\u000a        developed by that\u000a        [Nottingham] group [A5] as well as the methods described by the book\u000a        co-authored by Ian Dryden\u000a        in order to measure the anisotropy of growth. We found it to occur\u000a        essentially isotropically,\u000a        reducing the task of predicting its effects to the prediction of a\u000a        single number, the growth factor,\u000a        which simplified matters dramatically.\"\u000a    In specific experiments, [B4], error rates on a test set of 462\u000a      fingerprints were halved by scaling;\u000a      this result was confirmed on the Bundeskriminalamt's database of 3.25\u000a      million right index\u000a      fingerprints, where nine failures to retrieve a juvenile fingerprint out\u000a      of 48 such identification\u000a      attempts could be avoided by rescaling. Hotz notes the rescaling \"...effect\u000a        had not been\u000a        understood, the European Union asked for a study to be conducted in this\u000a        direction [Official J\u000a        European Union, L131, 52, Regulation 390\/2009, Annex 2, Article 2], and\u000a        decided against the use\u000a        of fingerprints of children under the age of 12 in visa applications [in\u000a        May 2009].\" This clearly\u000a      demonstrates the importance of this problem and moreover the importance of\u000a      the `shapes'\u000a      package in this context. Further, Hotz notes: \"It is worth mentioning\u000a        that, roughly at the same time,\u000a        the U.S. Department of Justice also had a study on the topic conducted\u000a        which however appears to\u000a        have failed to determine the effect of growth on fingerprints, and to\u000a        produce a useful means for\u000a        predicting it. This study did not use any shape analysis to look at\u000a        anisotropy first, as they probably\u000a        did not know of these techniques.\"\u000a    Overall, Hotz summarises by saying: \"... I believe that without the\u000a        Nottingham Group's research,\u000a        making it available through software and textbooks, spreading their\u000a        knowledge through further\u000a        publications and conferences, this study could not have been conducted.\"\u000a    Thus Hotz's acquaintance with shape analysis, heavily influenced by\u000a      personal contact with and\u000a      publications of the members of the Nottingham group, has been key in\u000a      solving the problem at\u000a      hand. As Hotz notes in his letter, the results of his study were\u000a      disseminated in 2011 at conferences\u000a      involving persons from academia, industry and public office, e.g. from the\u000a      biometrics industry such\u000a      as Morpho (www.morpho.com),\u000a      representatives of security forces such as the Metropolitan Police\u000a      Service, and from the European Commission Joint Research Centre.\u000a    Other impacts of the work\u000a      There are other beneficiaries of the `shapes' package and, more broadly,\u000a      of the underlying\u000a      research undertaken at Nottingham. These include an impact as a teaching\u000a      aid by introducing\u000a      geometric morphometrics to biologists via an on-line workbook [B6], a\u000a      face-shape study in patients\u000a      with epilepsy [B7], use in radar signal processing, and applications in\u000a      car headlight shape design\u000a      [B8]. It is thus likely that the strong interest from the groups mentioned\u000a      here and others will ensure\u000a      impact from the Nottingham research will grow in reach and significance\u000a      yet further.\u000a    ","ImpactSummary":"\u000a    Methodologies for shape analysis developed by the Shape and Object Data\u000a      Analysis group at The\u000a      University of Nottingham (UoN) have underpinned important applications\u000a      resulting in a range of\u000a      benefits for companies and organisations, including in human movement\u000a      capture and fingerprint\u000a      modelling.\u000a    Firstly, the economic benefits of the methodologies developed at\u000a      Nottingham to capture human\u000a      movement data without a calibration trial have been used by a commercial\u000a      software company,\u000a      Charnwood Dynamics Ltd, and have saved time for its users and increased\u000a      portability. Secondly,\u000a      by incorporating research methods into practice, practitioners have\u000a      improved standard processes,\u000a      which have resulted in efficiency savings. Organisations which have\u000a      benefitted from the research\u000a      methods include the German Federal Police, where the methodology has been\u000a      used in modelling\u000a      growth in adolescent fingerprints, resulting in lower error rates and a\u000a      reduction in false matches.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Nottingham\u000a    ","Institutions":[{"AlternativeName":"Nottingham (University of)","InstitutionName":"University of Nottingham","PeerGroup":"A","Region":"East Midlands","UKPRN":10007154}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2896514","Name":"Ilmenau"}],"References":"\u000a    The three publications that best indicate the quality of the research are\u000a      indicated *\u000a    \u000a[A1]* Kendall, D. G., Barden, D., Carne, T. K. and Le, H. (1999).\u000a      Shape and Shape Theory. Wiley,\u000a      Chichester. DOI: 10.1002\/9780470317006.fmatter (available on request)\u000a    \u000a\u000a[A2]* Le, H. (1995). Mean size-and-shapes and mean shapes: a\u000a      geometric point of view.\u000a      Advances in Applied Probability, 27, 44-55. http:\/\/www.jstor.org\/stable\/1428094\u000a      (also available on\u000a      request)\u000a    \u000a\u000a[A3]* Le, H. (2001). Locating Fr&#233;chet means with application to\u000a      shape spaces. Advances in\u000a      Applied Probability, 33, 324-338. DOI:10.1239\/aap\/999188316\u000a    \u000a\u000a[A4] Dryden, I.L., Koloydenko, A. and Zhou, D. (2009).\u000a      Non-Euclidean statistics for covariance\u000a      matrices, with applications to diffusion tensor imaging. Annals of Applied\u000a      Statistics, 3, 1102-1123.\u000a      DOI:10.1214\/09-AOAS249\u000a    \u000a\u000a[A5] Dryden, I. L. (2003-2013). `shapes' package. Versions 1.0 to\u000a      1.1-8. R Foundation for\u000a      Statistical Computing, Vienna, Austria. Contributed package.\u000a      http:\/\/cran.r-project.org\/web\/packages\/shapes\/index.html\u000a    \u000aGrants:\u000a    [A6] EPSRC grant GR\/R55757\/0 `Identifying structure from shape and\u000a      image data' PI: Dryden. Co-\u000a      Is: Le and Wood. &#163;157,196, 2001-2004.\u000a    [A7] EPSRC grant EP\/K022547\/1 `Statistical Analysis of\u000a      Manifold-Valued Data' PI: Wood. Co-Is:\u000a      Le, Dryden and Preston. &#163;611,045, 2013-2016.\u000a    [A8] Royal Society Wolfson Research Merit Award `Object data\u000a      analysis, with applications to\u000a      medical images and molecular shapes' PI: Dryden. &#163;60,000, 2012-2017.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"}],"Sources":"\u000a    [B1] Letter from Ogglebox Sensory Computing, Charnwood Dynamics\u000a      Ltd, Leicester detailing work\u000a      on human movement modelling software (copy on file).\u000a    [B2] Mitchelson, J.R. (2013). MOSHFIT: Algorithms for\u000a      occlusion-tolerant mean shape and rigid\u000a      motion from 3D movement data. Journal of Biomechanics, 46 (13), 2326-2329.\u000a      http:\/\/www.jbiomech.com\/article\/S0021-9290(13)00262-5\/abstract\u000a      (copy also on file)\u000a    [B3] Letter from Ilmenau University of Technology detailing work\u000a      on fingerprint modelling of\u000a      adolescents with the German Federal Police (copy on file).\u000a    [B4] Gottschlich, C. et al. (2011). Modeling the growth of\u000a      fingerprints improves matching for\u000a      adolescents. IEEE Transactions on Information Forensics and Security, 6\u000a      (3), 1165-1169. (copy on\u000a      file or through DOI:\u000a        10.1109\/TIFS.2011.2143406).\u000a    [B5] Hotz, T. et al. (2011). Statistical Analyses of Fingerprint\u000a      Growth. BIOSIG 2011 - Proceedings,\u000a      Lecture Notes in Informatics, P-191, 11-20. (copy on file or through\u000a      http:\/\/subs.emis.de\/LNI\/Proceedings\/Proceedings191\/11.pdf).\u000a    [B6] Zelditch, M.L., Swiderski, D.L. and Sheets, D.H. (2012).\u000a      Geometric Morphometrics for\u000a      Biologists, Second Edition. On-line companion materials. ISBN:\u000a      9780123869036\u000a      http:\/\/booksite.elsevier.com\/9780123869036\/\u000a      (includes full pdf of workbook)\u000a    [B7] Chinthapalli, K. et al. (2012). Atypical face-shape and\u000a      genomic structural variants in epilepsy.\u000a      Brain: A Journal of Neurology, 135(10), 3101-3114. DOI:\u000a      10.1093\/brain\/aws232. (copy also on file)\u000a    [B8] Ishihara, S. and Ishihara, K., Morphometrics and Kansei\u000a      Engineering, in Proceedings of 10th\u000a      QMOD Conference. Quality Management and Organizational Development. Our\u000a      Dreams of\u000a      Excellence (Editors: Dahlgaard-Park, S. and Dahlgaard, J.), 18-20 June,\u000a      2007 in Helsingborg,\u000a      Sweden, Issue 026, No 142. www.ep.liu.se\/ecp\/026\/142\/ecp0726142.pdf\u000a      (copy also on file)\u000a    \u000a    ","Title":"\u000a    Improved movement and fingerprint analysis using statistical shape\u000a        analysis in computer vision\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Computer Vision has become a key enabling technology across a range of\u000a      industrial and medical\u000a      applications, including forensics, manufacturing, transport and disease\u000a      diagnosis. A recent Frost &amp;\u000a      Sullivan analysis reports the global market for Machine Vision (the\u000a      automated application of\u000a      Computer Vision techniques) as $4.5 billion in 2012, projected to reach\u000a      $6.75 billion by 2016 -\u000a      tinyurl.com\/o9f4jm7. A core\u000a      requirement in many of these applications is the ability to recognise\u000a      shapes; statistical shape analysis provides an important tool for\u000a      achieving accurate shape\u000a      recognition for difficult problems and under demanding operating\u000a      conditions.\u000a    Pioneering research undertaken at UoN by Dr Huiling Le (UoN, 1991 to\u000a      date, Associate Professor\u000a      and Reader in Probability, School of Mathematical Sciences) provided the\u000a      first detailed structure of\u000a      Euclidean shape spaces [A1-A3]. Expanding on earlier work on the\u000a      differential geometric structure\u000a      of the shape and size-and-shape spaces [Le &amp; Kendall, Annals of\u000a      Statistics, 1993], Le produced\u000a      detailed methodology for mean shape and mean size-and-shape estimation\u000a      [A2] in 1995 and\u000a      derivation of uniqueness conditions for the Fr&#233;chet mean [A2, A3] between\u000a      1995 and 2001. This\u000a      research is part of a large body of work in shape theory and shape\u000a      analysis conducted over the\u000a      past 20 years by Le with Professor Ian Dryden, Professor Andrew Wood and\u000a      colleagues (UoN,\u000a      Dryden 2000-2010 and 2012 to date, Wood 1999 to date, both Professors of\u000a      Statistics, School of\u000a      Mathematical Sciences), that extends to the analysis of more general\u000a      manifold valued data, e.g.\u000a      [A4] and recent support from [A7, A8]. Practical results from these\u000a      insights have been codified into\u000a      an open source statistical package [A5] by Dryden, allowing researchers\u000a      and practitioners access\u000a      to these and other shape analysis methods in applications that require,\u000a      and hence benefit from,\u000a      accurate shape analysis. The software package, `shapes', was first made\u000a      publicly available in 2003\u000a      as part of an EPSRC-funded grant [A6]. This has since been regularly\u000a      updated by Dryden.\u000a    A specific example of a key research insight developed at Nottingham is\u000a      the work on mean size-\u000a      and-shapes by Le. Size-and-shape analysis is carried out where objects are\u000a      compared with\u000a      rotation and translation invariance, but not scale invariance. In 1995, Le\u000a      [A2] gave definitive details\u000a      of estimation of the mean size-and-shape and outlined various important\u000a      properties including\u000a      uniqueness of the mean. A fundamental question of practical interest is\u000a      whether the population and\u000a      sample mean size-and-shapes are unique; Le [A2] gave conditions for\u000a      uniqueness which can be\u000a      readily checked, and for shape and size-and-shape estimation this requires\u000a      knowledge of the\u000a      sectional curvature of the size-and-shape space, which was originally\u000a      derived by Le &amp; Kendall in\u000a      1993. A practical implementation of the sample mean size-and-shape\u000a      estimate is available in\u000a      Dryden's `shapes' package [A5], which is substantially based upon the\u000a      research undertaken by the\u000a      Nottingham group, as well as exploiting early theory developed elsewhere.\u000a    "},{"CaseStudyId":"31110","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Aircraft icing consultancy company AeroTex was founded in 2002, as an SME\u000a      offshoot from QinetiQ. Since 2008, it has used UCL's research to underpin\u000a      its specialist icing work, ranging from ice accretion physics to the\u000a      design and certification of ice protection systems [A].\u000a    In 2010, the United States Federal Aviation Administration (FAA) proposed\u000a      new aircraft icing regulations. The European Aviation Safety Agency (EASA)\u000a      followed suit in 2011, proposing a similar update to their certification\u000a      specifications for large aeroplanes. Since then, aircraft manufacturers\u000a      have been working to ensure their designs meet these specifications, as\u000a      the proposals will come into force imminently.\u000a    These proposals followed a number of accidents, including the October\u000a      1994 crash of American Eagle Flight 4184, which rolled out of level flight\u000a      and crashed into a field in Indiana, killing all 68 people on board.\u000a      Investigations found that the accident was caused by a build-up of ice on\u000a      the wings during icing conditions that included freezing rain, or\u000a      supercooled large droplets (SLD). At the time, the FAA required aircraft\u000a      to be tested and certified for flight in icing conditions, but the rules\u000a      were aimed at much smaller cloud-droplet diameters. The importance of\u000a      SLDs, and their significant splash effects, was unknown, and was not even\u000a      considered when certifying aircraft. Accidents including Flight 4184 and\u000a      many since then highlighted the inadequacy of the icing certification\u000a      standards and led to lengthy, complex discussions between authorities,\u000a      manufacturers and designers about introducing updated regulations, and how\u000a      these would be prepared for. This then resulted in the FAA introducing its\u000a      proposals for new regulations that include SLD and ice crystals.\u000a    In 2002, the UCL group was asked by industry to conduct research into\u000a      SLD. UCL's research described in section 2 helps AeroTex to design new ice\u000a      protection systems that meet these regulatory requirements [A]. It also\u000a      impacts upon AeroTex's customers by enabling them to operate aircraft more\u000a      safely in icing conditions and to comply with improved safety legislation.\u000a      The research came to influence AeroTex through research collaborations,\u000a      including Research Associate placements, and meetings of the UK Icing\u000a      Group, of which UCL and AeroTex are both members. The owner of AeroTex\u000a      said: \"We have been able to use the knowledge generated within our\u000a      analysis phase so that there has certainly been a benefit to us. We flow\u000a      the knowledge generated by UCL into designs for ice protection systems for\u000a      commercial customers\" [B].\u000a    AeroTex makes use of UCL research when advising customers on whether ice\u000a      protection is needed and when designing new ice protection systems that\u000a      comply with future and current regulations. The UCL research findings are\u000a      an important part of a portfolio of methods and evidence used to make\u000a      decisions, and have directly influenced the course of development of\u000a      AeroTex's AID (Aircraft Icing Design) analysis tool, used in aircraft ice\u000a      protection system design. UCL research also helps AeroTex to stay ahead of\u000a      their competitive rivals; the company has become experienced at providing\u000a      consultancy for manufacturers who need to conform to the new certification\u000a      rules [A].\u000a    UCL's research has not only led to these important impacts on aircraft\u000a      safety, but also has commercial benefits for AeroTex. [text removed for\u000a      publication]\u000a    AeroTex's customers are confidential, but include several aircraft\u000a      manufacturers and Tier 1 (the top approved) equipment suppliers. New\u000a      sub-system designs have already been or are to be incorporated into\u000a      manufactured aircraft, and are estimated to be included on thousands of\u000a      aircraft over many years. UCL research has helped both AeroTex and their\u000a      customers to understand the effects of ice on planes, and the safety\u000a      implications associated with icing. It has helped AeroTex provide system\u000a      designs that comply with proposed new rules, and enabled them to compete\u000a      effectively against their rivals, supporting improved aircraft safety for\u000a      the future. The financial figures as far as the customers are concerned\u000a      are confidential but extend into the hundreds of thousands of pounds\u000a      sterling.\u000a    ","ImpactSummary":"\u000a    The consultancy company AeroTex makes use of UCL research findings to\u000a      design new and improved ice protection systems for fixed wing or rotor\u000a      aircraft. These new designs enable AeroTex's customers (aircraft\u000a      manufacturers and Tier 1 equipment suppliers) to comply with upcoming\u000a      changes that are raising aircraft certification standards and to operate\u000a      aircraft more safely in icing conditions. The increase in income to\u000a      AeroTex resulting from this work was approximately [text removed for\u000a      publication] per year between 2010 and 2013, representing around 15% of\u000a      AeroTex's annual turnover.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University College London (UCL)\u000a    ","Institutions":[{"AlternativeName":"University College London","InstitutionName":"University College London","PeerGroup":"A","Region":"London","UKPRN":10007784}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"4921868","Name":"Indiana"}],"References":"\u000a    \u000a[1] Air cushioning with a lubrication\/inviscid balance, F. T. Smith, L.\u000a      Li and G. X. Wu, J. Fluid Mech., 482, 291-318 (2003) doi:10.1017\/S0022112003004063\u000a    \u000a\u000a[2] Droplet impact on water layers: post-impact analysis and\u000a      computations, R. Purvis and F. T. Smith, Phil. Trans. R. Soc. A,\u000a      363, 1209-1221 (2005) doi:10.1098\/rsta.2005.1562\u000a    \u000a\u000a[3] Droplet impact on a thin fluid layer, S. D. Howison, J. R. Ockendon,\u000a      J. M. Oliver, R. Purvis and F. T. Smith, J. Fluid Mech., 542, 1-23\u000a      (2005) doi:10.1017\/S0022112005006282\u000a    \u000a\u000a[4] Trapping of air in impact between a body and shallow water, A. A.\u000a      Korobkin, A. S. Ellis and F. T. Smith, J. Fluid Mech., 611,\u000a      365-394 (2008) doi:10.1017\/S0022112008002899\u000a    \u000a\u000a[5] Skimming impacts and rebounds on shallow liquid layers, P. D. Hicks\u000a      and F. T. Smith, Proc. R. Soc. A, 467, 653-674 (2011) doi:10.1098\/rspa.2010.0303\u000a    \u000a\u000a[6] On interaction between falling bodies and the surrounding fluid, F.\u000a      T. Smith and A. S. Ellis, Mathematika, 56, 140-168 (2010) doi:10.1112\/S0025579309000473\u000a    \u000aReferences [1], [5] and [6] best indicate the quality of the\u000a        underpinning research.\u000a    Relevant research grants:\u000a    (i) Theory and computation in unsteady flow modelling (GR\/S35394\/01);\u000a      &#163;4,121; awarded to Professor Frank T. Smith; sponsor: EPSRC (CASE Award);\u000a      2003-2006\u000a    (ii) Faraday fast track proposal: droplet impact on water layers\u000a      (GR\/R91939\/01); &#163;103,456; awarded to Professor Frank T. Smith; sponsor:\u000a      EPSRC (RA support); 2002-2004\u000a    (iii) Air and surface effects on water droplet impact (EP\/D069335\/1);\u000a      &#163;257,779; awarded to Professor Frank T. Smith; sponsor: EPSRC (RA\u000a      support); 2006-2010\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"}],"Sources":"\u000a    [A] The Aircraft Icing Consultant at AeroTex UK can be contacted to\u000a      corroborate all of the claims and details about AeroTex, including what\u000a      the company uses the UCL research for and how it benefits from this, and\u000a      the details of income generated and projects conducted. Contact details\u000a      provided separately.\u000a    [B] Statement from the owner of AeroTex (contained within a document\u000a      about the EPSRC funded Knowledge Exchange Programme &#8212; see page 1) &#8212;\u000a      corroborates that the research is used by AeroTex and is beneficial to the\u000a      company. Document available on request. \u000a    ","Title":"\u000a    Improving aircraft safety in icing conditions\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    When an aircraft flies through cloud at or below freezing temperature,\u000a      ice can accrete on its forward-facing parts. This can lead to detrimental\u000a      performance, and has been a significant factor in a number of past\u000a      incidents and accidents, some of which resulted in the loss of life. Icing\u000a      occurs when supercooled water droplets suspended in the cloud impact on\u000a      the body of the aircraft and freeze on to it. The rate, amount and\u000a      location of ice accretion depend on parameters including temperature,\u000a      speed, aircraft surface shape and droplet size. Existing mathematical\u000a      models are reasonably accurate in predicting the shape and quantity of ice\u000a      produced when droplets are small (&#8804;40&#956;m). However, for\u000a      larger droplets (supercooled large droplets, or SLD) the models were\u000a      inadequate and tended to dramatically over-predict the amount of ice\u000a      produced and wrongly predict its location, partly because larger droplets\u000a      tend to splash. This resulted in errors in predicting aerodynamic\u000a      performance and safety.\u000a    Accurate modelling of aircraft icing is desirable as it can inform the\u000a      design of aircraft and ice protection systems, resulting in improved\u000a      aircraft safety. Between 2001 and 2013 researchers in UCL's Department of\u000a      Mathematics investigated and modelled various aspects of impacts of\u000a      relevance to aircraft icing, taking larger droplets into account and\u000a      ensuring that models are realistic. Work on asymptotic expansions and\u000a      matching, involving complex multi-phase fluids, irregular geometry,\u000a      air-water interactions, shallow-layer impacts and ice-skimming, produced\u000a      reduced-equation computations and code supported by comparisons with\u000a      real-world findings.\u000a    Early work (2001-03) involved the development of models that simulate how\u000a      a layer of air between a droplet and a water layer affects the impact of\u000a      the droplet [1]; the effect of air on impacts &#8212; vital for real-world\u000a      applications &#8212; had never been predicted before. Splashing of large\u000a      droplets impacting onto a layer of water was then investigated in 2002 to\u000a      2005 [2, 3], with parameters such as air flow, water depth and droplet\u000a      size being varied. Findings included the first-ever predictions of surface\u000a      roughness effects after impact and how much of the water layer is splashed\u000a      away. Some of this work [3] was written up jointly with a group from the\u000a      Mathematical Institute in Oxford, the University of Nottingham and the\u000a      University of East Anglia, who had arrived simultaneously at the same\u000a      research conclusions. This work was followed in 2006 to 2008 by modelling\u000a      of impacts involving a solid body approaching another solid body with two\u000a      fluids (air and water) between them [4]; in an aircraft icing scenario\u000a      this corresponds to an ice crystal impacting upon a solid aircraft surface\u000a      covered by a water layer.\u000a    A related research strand (2008-13) involved skimming impacts and\u000a      rebounds. A model was derived for a solid body (e.g. an ice crystal)\u000a      undergoing an oblique skimming impact with a shallow liquid layer and then\u000a      rebounding from it [5]; this work included explanations of both entries\u000a      into and exits from water. An extension of this model included fluid-body\u000a      interactions with multiple bodies and multiple impacts [6], of relevance\u000a      to wind-blown ice particles travelling along an aircraft wing.\u000a    The above research generated new and simpler computational methodology.\u000a      It also provided flexible mathematical predictions of the precise extent\u000a      of a splash, rebound duration, effects of surrounding air motion, and\u000a      shapes resulting from ice accretion or melting, which take into account\u000a      highly variable parameter values including droplet size, impact speed and\u000a      angle of incidence.\u000a    Site visits and discussions with QinetiQ and AeroTex, together with\u000a      complementary experimental input from Cranfield University, were important\u000a      for much of the above modelling work [1, 2, 3, 4].\u000a    Key UCL researchers: Frank Smith (Professor in Mathematics),\u000a      Richard Purvis (PDRA 2002-05), Andrew Ellis (PDRA 2006-08) and Peter Hicks\u000a      (PDRA 2010-11).\u000a    "},{"CaseStudyId":"31135","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"390903","Name":"Greece"},{"GeoNamesId":"2963597","Name":"Ireland"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"2661886","Name":"Sweden"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Monitoring of short-term outcomes using VLADs is now conducted within\u000d\u000a      many adult cardiac\u000d\u000a      surgery centres in the UK and other countries, including India (Sri\u000d\u000a      Jayadeva Institute of Cardiology,\u000d\u000a      Bangalore; since 2010), Singapore (National University Heart Centre; since\u000d\u000a      2009), Greece\u000d\u000a      (Onassis Cardiac Surgery Center, Athens; since 2011) and Sweden (&#214;rebro\u000d\u000a      and Linkoping\u000d\u000a      University Hospitals; since 2010). The technique has impacted on surgical\u000d\u000a      units as it allows them\u000d\u000a      to analyse and compare the performance of individual surgeons and ensures\u000d\u000a      that any appropriate\u000d\u000a      action relating to an unexpected increase in mortality can rapidly be\u000d\u000a      taken. One UK-based surgeon\u000d\u000a      informed us that VLADs are \"invaluable for quality assurance\" within his\u000d\u000a      cardiac unit and give him\u000d\u000a      \"great confidence in the overall performance of surgeons and the unit\"\u000d\u000a      [A]. In 2011, the Society for\u000d\u000a      Cardiothoracic Surgery in Great Britain &amp; Ireland (SCTS) reported a\u000d\u000a      \"50% reduction in risk-adjusted\u000d\u000a      mortality in the United Kingdom in recent years\" as a result of the\u000d\u000a      collection, analysis,\u000d\u000a      benchmarking and feeding back of robust data on clinical outcomes for the\u000d\u000a      purposes of quality\u000d\u000a      improvement [B], which is facilitated in part by the use of VLADs. The\u000d\u000a      SCTS also believes that\u000d\u000a      these improved processes are the cause of the reduction in recent years of\u000d\u000a      damaging cardiac\u000d\u000a      surgeon suspensions and restrictions of practice, as they lead to\u000d\u000a      \"detection of potential problems at\u000d\u000a      an early stage, allowing implementation of strategies to improve outcomes\u000d\u000a      before any restriction of\u000d\u000a      practice or suspension may be needed.\" [B]\u000d\u000a    In paediatric cardiac surgery, software developed by CORU (which uses\u000d\u000a      VLADs with the PRAiS\u000d\u000a      risk model) was sold under licence in 2013 to all 12 UK (NHS and private)\u000d\u000a      centres performing this\u000d\u000a      type of surgery, and is being used by them for routine monitoring of\u000d\u000a      outcomes. This use of VLADs\u000d\u000a      has been incorporated by NHS England into the quality assurance checklist\u000d\u000a      they developed for\u000d\u000a      commissioners of paediatric surgery services [C]. The relevant national\u000d\u000a      audit body, NICOR, has\u000d\u000a      also purchased the software and used it in their comparative analysis of\u000d\u000a      outcomes in the 10\u000d\u000a      English centres [D], which followed the suspension of paediatric cardiac\u000d\u000a      surgery at Leeds General\u000d\u000a      Infirmary in April 2013. Their analysis indicated that there were no\u000d\u000a      `safety' problems in any of the\u000d\u000a      centres [D]. Outside of cardiac surgery, VLADs have found use in the\u000d\u000a      monitoring of surgical wound\u000d\u000a      infection rates at University College Hospital in London, and the\u000d\u000a      monitoring of mortality rates within\u000d\u000a      the general adult Intensive Care Unit at Waikato Hospital in New Zealand.\u000d\u000a    NHS Blood and Transplant uses VLADs (together with CUSUM charts) on a\u000d\u000a      national level to\u000d\u000a      monitor early outcomes of all transplants undertaken in the UK's 23\u000d\u000a      kidney, 8 pancreas, 7\u000d\u000a      cardiothoracic and 7 liver transplantation units [E]. Each significant\u000d\u000a      change in the rate of mortality\u000d\u000a      or graft failure generates a signal that leads to an investigation. For\u000d\u000a      example, in 2011 monitoring\u000d\u000a      indicated that Royal Brompton &amp; Harefield NHS Foundation Trust had\u000d\u000a      experienced more deaths\u000d\u000a      than expected following heart transplants. This prompted an external\u000d\u000a      review conducted by two\u000d\u000a      senior clinicians, and the eight recommendations of this review have now\u000d\u000a      been implemented by the\u000d\u000a      trust [F]. These included developing \"a consensus approach to the\u000d\u000a      management of primary graft\u000d\u000a      dysfunction and failure\", and making sure that \"cardiothoracic retrieval\u000d\u000a      surgeons at the donor\u000d\u000a      operation are made aware of any need for delay so as to ensure minimised\u000d\u000a      ischaemic times\". In\u000d\u000a      the 2011 UK Liver Transplant Audit, VLAD charts revealed that no\u000d\u000a      significant deviation from\u000d\u000a      expected mortality had occurred in paediatric centres since 2008, but that\u000d\u000a      a significant change had\u000d\u000a      occurred in January 2011 in the Newcastle adult centre, which led to that\u000d\u000a      centre conducting an\u000d\u000a      internal review of its service [G].\u000d\u000a    Since March 2009, VLADs have been used by the Veterans Health\u000d\u000a      Administration (VHA), a\u000d\u000a      component of the United States Department of Veterans Affairs and the\u000d\u000a      USA's largest integrated\u000d\u000a      health care system, serving over 8.3 million veterans each year. VLAD\u000d\u000a      charts are incorporated into\u000d\u000a      the VHA's national quality improvement project to monitor mortality on\u000d\u000a      acute medical and surgical\u000d\u000a      units at 127 VHA centres. Charts are updated on a quarterly basis and made\u000d\u000a      available to\u000d\u000a      managers or analysts at centres as part of a quarterly report package. To\u000d\u000a      help these users\u000d\u000a      interpret the information, the VHA prepared educational materials in\u000d\u000a      November 2010 and has held\u000d\u000a      several training sessions since June 2009. The VLAD chart is \"well\u000d\u000a      perceived by managers for its\u000d\u000a      ease of use and its ability to alert users to investigate care process\u000d\u000a      during a specific period.\" The\u000d\u000a      VHA has informed UCL that it is not possible to isolate the contribution\u000d\u000a      of VLAD in improving\u000d\u000a      mortality since it is part of a national quality improvement program that\u000d\u000a      involves other tools and\u000d\u000a      improvement strategies, but that they have seen an \"improvement in\u000d\u000a      mortality over time and\u000d\u000a      consider VLAD an important tool that signals periods needing\u000d\u000a      investigation\" [H].\u000d\u000a    Since 2007, VLADs have been a component of the UK Care Quality\u000d\u000a      Commission's (CQC)\u000d\u000a      nationwide surveillance programme, in which they are used as a\u000d\u000a      presentational tool to guide\u000d\u000a      interpretation. Within this programme the CQC monitors a selection of\u000d\u000a      outcomes (including\u000d\u000a      maternity and emergency re-admissions indicators) across all 163 acute NHS\u000d\u000a      hospitals in England,\u000d\u000a      in addition to adverse events in other care sectors such as adult social\u000d\u000a      care and mental health. The\u000d\u000a      CQC has handled over 650 alerts under this programme; in recent years\u000d\u000a      60-70% of these alerts\u000d\u000a      have led to improvement plans being implemented in NHS trusts [I].\u000d\u000a      Improvement plans included\u000d\u000a      those for \"better management of patient fluid balance, the complete\u000d\u000a      redesign of patient pathways,\u000d\u000a      improved identification of early warning signs and more efficient links\u000d\u000a      with primary and community\u000d\u000a      care\" [I]. In one case, an alert identified high mortality among patients\u000d\u000a      admitted with a hip fracture.\u000d\u000a      The trust reviewed their care for these patients and identified remediable\u000d\u000a      problems at specific\u000d\u000a      points in patients' care; to address these they developed and shared an\u000d\u000a      improvement plan [I].\u000d\u000a    The enhanced approach to VLAD charting devised by Sherlaw-Johnson\u000d\u000a      (reference [3] above) was\u000d\u000a      adopted in 2007 by Queensland Government's Department of Health as part of\u000d\u000a      their clinical\u000d\u000a      governance framework; VLADs were introduced into the state's largest\u000d\u000a      public and private hospitals\u000d\u000a      as a major part of the Queensland Health Patient Safety and Quality\u000d\u000a      Improvement Service. This\u000d\u000a      was followed by a partnership between Queensland Health (QH) and the\u000d\u000a      software company Opus\u000d\u000a      5K to develop the VLAD Clinical Monitoring (VLAD CM) IT system, which\u000d\u000a      enabled QH to deploy\u000d\u000a      VLAD charting in over 64 Queensland hospitals in October 2009, where it is\u000d\u000a      currently used to\u000d\u000a      monitor 34 clinical indicators [J]. On-going rigorous reviews of\u000d\u000a      indicators are conducted by VLAD\u000d\u000a      Indicator Review Working Groups [J, K].\u000d\u000a    The Queensland Government's VLAD Policy (2012) [J] governs the use of\u000d\u000a      VLADs within QH and\u000d\u000a      details the following procedure: VLAD CM disseminates monthly VLAD charts\u000d\u000a      to hospitals,\u000d\u000a      indicating where predetermined levels of variation in patient outcomes are\u000d\u000a      exceeded and flagging\u000d\u000a      issues for further review. Hospitals are required to investigate why flags\u000d\u000a      have occurred and submit\u000d\u000a      a response within 30 days. In 2010-11, around 1,000 VLAD charts were\u000d\u000a      disseminated each month,\u000d\u000a      the Queensland Health Peak Safety and Quality Committee VLAD Subcommittee\u000d\u000a      reviewed 382\u000d\u000a      hospital investigation reports written in response to flags, and 300\u000d\u000a      clinical reviews by hospital staff\u000d\u000a      occurred as a result of VLADs [L]. The use of VLADs has resulted in the\u000d\u000a      implementation of\u000d\u000a      numerous quality initiatives within Queensland hospitals, leading to\u000d\u000a      improvements in areas such as\u000d\u000a      discharge processes, clinician documentation and resource allocation [K].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The Variable Life-Adjusted Display (VLAD) is a graphical tool for\u000d\u000a      monitoring clinical outcomes. It\u000d\u000a      has been widely adopted by UK cardiac surgery centres, and has helped a\u000d\u000a      shift in culture towards\u000d\u000a      more open outcome assessment in adult cardiac surgery, which has been\u000d\u000a      credited with reduced\u000d\u000a      mortality rates. VLAD is also being used for a broad range of other\u000d\u000a      clinical outcomes by regulatory\u000d\u000a      bodies worldwide. For example, Queensland Health uses VLAD as a major part\u000d\u000a      of its Patient\u000d\u000a      Safety and Quality Improvement Service to monitor 34 outcomes across 64\u000d\u000a      public hospitals, and\u000d\u000a      NHS Blood and Transplant uses VLAD to monitor early outcomes of all UK\u000d\u000a      transplants.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    University College London (UCL)\u000d\u000a    ","Institutions":[{"AlternativeName":"University College London","InstitutionName":"University College London","PeerGroup":"A","Region":"London","UKPRN":10007784}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"1880252","Name":"Singapore"},{"GeoNamesId":"2686657","Name":"Örebro"},{"GeoNamesId":"2694762","Name":"Linkoping"},{"GeoNamesId":"264371","Name":"Athens"},{"GeoNamesId":"1277333","Name":"Bangalore"}],"References":"\u000d\u000a    \u000a[1] Monitoring the results of cardiac surgery by variable life-adjusted\u000d\u000a      display, J. Lovegrove, O.\u000d\u000a      Valencia, T. Treasure, C. Sherlaw-Johnson and S. Gallivan, The Lancet,\u000d\u000a      350(9085), 1128-1130\u000d\u000a      (1997) doi:10\/dxvknj\u000d\u000a    \u000a\u000a[2] Computer tools to assist the monitoring of outcomes in surgery, C.\u000d\u000a      Sherlaw-Johnson, S.\u000d\u000a      Gallivan, T. Treasure and S. A. Nashef, Eur. J. Cardiothorac. Surg.,\u000d\u000a      26(5), 1032-1036 (2004)\u000d\u000a      doi:10\/bp92z2\u000d\u000a    \u000a\u000a[3] A method for detecting runs of good and bad clinical outcomes on\u000d\u000a      variable life-adjusted display\u000d\u000a      (VLAD) charts, C. Sherlaw-Johnson, Health Care Manag. Sci., 8(1),\u000d\u000a      61-65 (2005) doi:10\/dpvrft\u000d\u000a    \u000a\u000a[4] The development of and use of tools for monitoring the occurrence of\u000d\u000a      surgical wound infections,\u000d\u000a      C. Sherlaw-Johnson, P. Wilson and S. Gallivan, Journal of the\u000d\u000a        Operational Research Society, 58,\u000d\u000a      228-234 (2007) doi:10\/bpbnzn\u000d\u000a    \u000a\u000a[5] Automating the monitoring of surgical site infections using variable\u000d\u000a      life-adjusted display charts,\u000d\u000a      C. Vasilakis, A. P. R. Wilson and F. S. Haddad, J. Hosp. Infect.,\u000d\u000a      79, 119-124 (2011) doi:10\/d8qp6n\u000d\u000a    \u000a\u000a[6] Real time monitoring of risk-adjusted paediatric cardiac surgery\u000d\u000a      outcomes using variable life-adjusted\u000d\u000a      display: implementation in three UK centres, C. Pagel, M. Utley, S. Crowe,\u000d\u000a      T. Witter, D.\u000d\u000a      Anderson, R. Samson, A. McLean, V. Banks, V. Tsang and K. Brown, Heart,\u000d\u000a      99, 1445-1450 (2013)\u000d\u000a      doi:10\/n2g\u000d\u000a    \u000aReferences [1], [3] and [6] best indicate the quality of the\u000d\u000a        underpinning research.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"}],"Sources":"\u000d\u000a    [A] Supporting statement from a cardiac surgeon at Royal Victoria\u000d\u000a      Hospital, Belfast &#8212; corroborates\u000d\u000a      that cardiac surgery at this hospital is benefiting from VLADs. Available\u000d\u000a      on request.\u000d\u000a    [B] Maintaining Patients' Trust: Modern Medical Professionalism 2011,\u000d\u000a      available online\u000d\u000a      http:\/\/www.scts.org\/_userfiles\/resources\/634420268996790965_SCTS_Professionalism_FINAL.pdf\u000d\u000a      &#8212; corroborates\u000d\u000a      the SCTS's view that outcome monitoring has led to improvements.\u000d\u000a    [C] Supporting statement from Service Specialist at NHS England &#8212;\u000d\u000a      corroborates the incorporation\u000d\u000a      of VLADs into NHS England's quality assurance checklist. Available on\u000d\u000a      request.\u000d\u000a    [D] Investigation of mortality from Paediatric Cardiac Surgery in\u000d\u000a        England 2009-12, available online\u000d\u000a      http:\/\/www.england.nhs.uk\/wp-content\/uploads\/2013\/04\/finl-rep-mort-paed-card-surg-2009-12.pdf\u000d\u000a      &#8212; corroborates the use of VLADs and PRAiS by the national audit body in\u000d\u000a      their analysis.\u000d\u000a    [E] Supporting statement from the Associate Director of Statistics &amp;\u000d\u000a      Clinical Audit at NHS Blood\u000d\u000a      and Transplant &#8212; corroborates the numbers of transplant centres in which\u000d\u000a      VLADs are\u000d\u000a      implemented. Note that this statement refers to VLAD charts as O-E\u000d\u000a        charts. Evidence that these\u000d\u000a        are the same thing can be found in Collett et al. (2009) The UK Scheme\u000d\u000a        for Mandatory Continuous\u000d\u000a        Monitoring of Early Transplant Outcome in all Kidney Transplant Centers,\u000d\u000a        Transplantation, 88, 970-5\u000d\u000a        (page 971). Available on request.\u000d\u000a    [F] Royal Brompton &amp; Harefield NHS Foundation Trust Response to\u000d\u000a        NSCT External Review Report\u000d\u000a        of 29th December 2011, available online at\u000d\u000a      http:\/\/www.rbht.nhs.uk\/healthprofessionals\/clinical-departments\/transplant\/\u000d\u000a      &#8212; corroborates\u000d\u000a      the implementation of the recommendations by the trust.\u000d\u000a    [G] UK Liver Transplant Audit 2011 &#8212; corroborates the use of\u000d\u000a      VLADs, the findings of the audit and\u000d\u000a      the internal review at the Newcastle centre (e.g. see pages 9-10 and 57).\u000d\u000a      Pdf available on request.\u000d\u000a    [H] Supporting statement from the Innovations and Development Coordinator\u000d\u000a      at the VHA &#8212; corroborates\u000d\u000a      that VLAD charts are being used by the VHA to monitor outcomes and that it\u000d\u000a      finds\u000d\u000a      them beneficial. Available on request.\u000d\u000a    [I] Supporting statements from the Surveillance Manager at the CQC &#8212;\u000d\u000a      corroborates that VLADs\u000d\u000a      are used in the surveillance programme, and corroborates the details of\u000d\u000a      that programme and the\u000d\u000a      improvement plans. Available on request.\u000d\u000a    [J] Queensland Government VLAD website: http:\/\/www.health.qld.gov.au\/psu\/vlad\/default.asp\u000d\u000a      &#8212; corroborates\u000d\u000a      the VLAD Policy, indicators, and activity of Indicator Review Working\u000d\u000a      Groups.\u000d\u000a      [K] Using the quality improvement cycle on clinical indicators &#8212; improve\u000d\u000a      or remove?, K. M.\u000d\u000a      Sketcher-Baker, M. C. Kamp, J. A. Connors, D. J. Martin and J. E. Collins,\u000d\u000a      Med. J. Aust., 193,\u000d\u000a      S104-S106 (2010) http:\/\/bit.ly\/19mirG2\u000d\u000a      &#8212; corroborates the implementation of quality initiatives\u000d\u000a      leading to improvements.\u000d\u000a    [L] Patient Safety: from learning to action 2012, available\u000d\u000a      online\u000d\u000a      http:\/\/www.health.qld.gov.au\/psu\/reports\/docs\/lta5.pdf\u000d\u000a      &#8212; corroborates numbers of VLAD charts\u000d\u000a      disseminated, investigation reports reviewed, and clinical reviews\u000d\u000a      written. See page x (in the\u000d\u000a      executive summary) and page 58.\u000d\u000a    ","Title":"\u000d\u000a    Better clinical outcome monitoring and healthcare quality through the use\u000d\u000a      of\u000d\u000a      graphical methods\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"},{"GeoNamesId":"2641673","Name":"Newcastle-upon-Tyne"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    In the UK in the mid-1990s it was discovered that prolonged periods of\u000d\u000a      poor performance by\u000d\u000a      individual cardiac surgeons had been going undetected. This highlighted\u000d\u000a      the need for clinical\u000d\u000a      outcomes &#8212; such as the rate of mortality within 30 days after surgery &#8212; to\u000d\u000a      be routinely monitored.\u000d\u000a    Researchers in UCL's Clinical Operational Research Unit (CORU) &#8212; a team\u000d\u000a      dedicated to applying\u000d\u000a      operational research and mathematical modelling approaches to problems in\u000d\u000a      health care &#8212; collaborated\u000d\u000a      with cardiothoracic surgeon Tom Treasure to develop a monitoring tool that\u000d\u000a      clinicians\u000d\u000a      would find useful. As part of the engagement process necessary for\u000d\u000a      successful operational\u000d\u000a      research, the CORU team spent months attending seminars and meetings at St\u000d\u000a      George's Hospital,\u000d\u000a      where Professor Treasure worked at the time, to \"tune in\" to how surgical\u000d\u000a      teams discussed\u000d\u000a      outcomes and related to data. A key challenge was how to account for\u000d\u000a      differences in case-mix\u000d\u000a      (e.g. different severity of patients' heart disease) between centres, so\u000d\u000a      that meaningful comparisons\u000d\u000a      can be made and clinicians or hospitals that undertake more risky cases\u000d\u000a      are not unfairly penalised.\u000d\u000a    This research led to the development in 1997 of a novel graphical display\u000d\u000a      for outcome monitoring\u000d\u000a      called the Variable Life-Adjusted Display (VLAD) [1]. The VLAD is a plot\u000d\u000a      of the difference between\u000d\u000a      the cumulative expected mortality and the cumulative observed mortality as\u000d\u000a      a function of case\u000d\u000a      number (or, in later versions, time). The expected mortality takes into\u000d\u000a      account the risk associated\u000d\u000a      with each case, as estimated using an existing risk scoring system. For\u000d\u000a      each death within 30 days\u000d\u000a      the VLAD trace falls by the estimated probability of survival for that\u000d\u000a      case; for each survival within\u000d\u000a      30 days it rises by that case's estimated probability of death. This\u000d\u000a      simple, intuitive display was the\u000d\u000a      result of the engagement process, and repeated prototyping and discussion\u000d\u000a      between the CORU\u000d\u000a      team and the surgical author. In addition to the incorporation of\u000d\u000a      patient-to-patient differences, key\u000d\u000a      to the success of VLAD has been the explicit \"credit\" given to clinical\u000d\u000a      teams for runs of better than\u000d\u000a      expected outcomes.\u000d\u000a    In the mid-2000s, CORU extended the methodology to add flexibility and\u000d\u000a      aid interpretation of VLAD\u000d\u000a      charts. In 2004, a collaboration with Cambridge's Papworth Hospital and\u000d\u000a      Guy's and Thomas'\u000d\u000a      Hospital Medical School led to the addition of graphical tools, based on\u000d\u000a      exact analytical methods,\u000d\u000a      which allow the user to see how likely it is that deviations from expected\u000d\u000a      surgical outcomes occur\u000d\u000a      by chance [2]. A method was then devised in 2005 for augmenting the basic\u000d\u000a      VLAD chart with a\u000d\u000a      \"signalling\" function based on CUSUM analysis, adding information as to\u000d\u000a      whether an upwards or\u000d\u000a      downwards trend in clinical outcomes constitutes a statistically\u000d\u000a      significant deviation from expected\u000d\u000a      performance [3].\u000d\u000a    Although originally developed to monitor outcomes in adult cardiac\u000d\u000a      surgery, the VLAD technique\u000d\u000a      has since been applied in many other clinical settings. The CORU team has\u000d\u000a      been active in this\u000d\u000a      research area; for example, in collaboration with University College\u000d\u000a      Hospital (UCH), they adapted\u000d\u000a      the technique for monitoring the occurrence of surgical wound infections\u000d\u000a      in hospitals in 2007 [4],\u000d\u000a      and implemented it at UCH for this purpose in 2011 [5]. In 2010-12, CORU\u000d\u000a      also worked on\u000d\u000a      outcome monitoring using VLADs after paediatric cardiac surgery, first\u000d\u000a      helping to develop a\u000d\u000a      dedicated risk model (known as PRAiS) to adjust for case-mix differences\u000d\u000a      and then working with\u000d\u000a      three UK paediatric cardiac surgery centres (Great Ormond Street Hospital,\u000d\u000a      Evelina Children's\u000d\u000a      Hospital in London and The Royal Hospital for Sick Children in Glasgow) to\u000d\u000a      implement local routine\u000d\u000a      monitoring [6].\u000d\u000a    CORU's contribution to all the research above included engagement with\u000d\u000a      the clinical communities\u000d\u000a      to build a shared understanding of the clinical context and the purpose of\u000d\u000a      monitoring; data analysis\u000d\u000a      and model development; and design and implementation of graphical tools\u000d\u000a      and software.\u000d\u000a    Key UCL researchers: Jocelyn Lovegrove (Research Fellow; 1995-99),\u000d\u000a      Stephen Gallivan (Senior\u000d\u000a      Research Fellow to Professor, then Principal Research Fellow; 1985-2010),\u000d\u000a      Chris Sherlaw-Johnson\u000d\u000a      (Associate Research Assistant to Senior Research Fellow; 1990-2006),\u000d\u000a      Christina Pagel\u000d\u000a      (Research Fellow to Lecturer in Operational Research; 2005-current), Sonya\u000d\u000a      Crowe (Research\u000d\u000a      Associate to Health Foundation Improvement Science Research Fellow;\u000d\u000a      2009-current), Martin\u000d\u000a      Utley (Research Fellow to Professor of Operational Research;\u000d\u000a      1996-current).\u000d\u000a    "},{"CaseStudyId":"31905","Continent":[{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"1861060","Name":"Japan"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    Experimental impact: The impact of the Mathon and Umerski 2001\u000d\u000a      paper [3.1], predicting that MgO-based systems would exhibit very high MR\u000d\u000a      ratios, was immediate. The race to observe this effect experimentally,\u000d\u000a      with its obvious commercial application, ended in a dead heat in December\u000d\u000a      2004 when the Tsukuba group in Japan and the IBM group in the US\u000d\u000a      simultaneously reported MR ratios of 180-220% at room temperature in\u000d\u000a      Nature Materials [3.3, 3.4] [5.12]. Both these papers cite the predictions\u000d\u000a      of [3.1] and [3.2] as motivation. Moreover, the emphasis in [3.1] on the\u000d\u000a      importance of interface quality must have helped the experimentalists\u000d\u000a      achieve their goals.\u000d\u000a    Commercial developments: The motivation then moved to creating a\u000d\u000a      commercial product, principally by the Tsukuba group in Japan and the IBM\u000d\u000a      group in the US (see [5.7] and [5.8] for a review). The first TMR read\u000d\u000a      head reached the market in 2007. By 2009 all new hard disks were based on\u000d\u000a      this technology and this remains the case. This is confirmed by S.S.P.\u000d\u000a      Parkin (head of the IBM research group responsible for [3.4]), who states:\u000d\u000a      `The work of Mathon and Umerski clearly played an important role in the\u000d\u000a      development of these materials and their subsequent widespread application\u000d\u000a      to recording read heads in ~2007. All disk drives manufactured since about\u000d\u000a      2008-2009 use recording read heads based on magnetic tunnel junctions.'\u000d\u000a      [5.2]\u000d\u000a    Today, there are only three major manufacturers of HDDs: Western Digital,\u000d\u000a      Seagate and Toshiba [5.1]. In 2008 Western Digital reported that `the\u000d\u000a      industry has made the transition to tunnel-junction magneto resistive\u000d\u000a      (\"TMR\") technology for the head reader function' [5.3]. By 2009 it\u000d\u000a      reported that `[we] have completed the transition to PMR [Perpendicular\u000d\u000a      Magnetic Recording] and TMR across all product platforms' [5.4]. An\u000d\u000a      example of the use of TMR technology by Toshiba is given in its product\u000d\u000a      information for internal notebook hard drives, which `use proven state of\u000d\u000a      the art ... TMR Head Recording technology for increased capacity,\u000d\u000a      reliability and performance' [5.5].\u000d\u000a    To date, the increase in hard disk capacity as a result of the MgO-based\u000d\u000a      read head is about a factor of five [5.10] and for the near future the\u000d\u000a      MgO-based read head looks likely to remain. The paper [5.11] contains a\u000d\u000a      2010 report on the expected design and operation of a future 2TB\/in2 read\u000d\u000a      head, by researchers at Seagate, which they say `will likely use a higher\u000d\u000a      quality MgO tunneling giant magnetoresistance (TGMR) stack'. They conclude\u000d\u000a      by saying `[we] expect the MgO barrier technology to be continuously\u000d\u000a      improved to fulfil the industry's mid-term needs'.\u000d\u000a    Commercial impact: The hard disk industry has annual sales\u000d\u000a      exceeding $28 billion [5.1]. The huge commercial benefit of MgO-based read\u000d\u000a      heads to the industry is clearly demonstrated in the following claim of\u000d\u000a      industrial espionage. [5.6] is a link to the findings of the American\u000d\u000a      Arbitration Association in a five-year dispute between Seagate and Western\u000d\u000a      Digital regarding an employee (Dr Mao) who moved from Seagate to Western\u000d\u000a      Digital in September 2006, when HDD manufacturers were developing the new\u000d\u000a      MgO-based read heads. Page 4 of [5.6] states:\u000d\u000a    `Seagate claims that Dr Mao stole Seagate trade secrets and confidential\u000d\u000a      information regarding TMR technology and provided it to Western Digital,\u000d\u000a      which used trade secrets and confidential information to design and\u000d\u000a      manufacture an MgO TMR read head. As a result Seagate claims that Western\u000d\u000a      Digital was able to introduce products, incorporating an MgO TMR read\u000d\u000a      head, into the market many months ahead of when it would have been able to\u000d\u000a      do so without Seagate trade secrets and confidential information.'\u000d\u000a    In 2011 the American Arbitration Association ruled that for this\u000d\u000a      infringement Seagate was entitled to recover $525,000,000 plus pre-award\u000d\u000a      interest at 10% per annum [5.6, page 28].\u000d\u000a    Impact on society: The role of [3.1] is highlighted in reviews\u000d\u000a      [5.7, 5.8], and in the original experimental papers [3.3, 3.4]. The\u000d\u000a      significance of the industrial application of TMR technology and its\u000d\u000a      impact on society is emphasised in the citation [5.9] for the 20th Tsukuba\u000d\u000a      Prize, awarded to Drs Yuasa and Suzuki (of [3.3]) for `Giant tunnel\u000d\u000a      magnetoresistance in MgO-based magnetic tunnel junctions and its\u000d\u000a      industrial applications'. This citation acknowledges the role of the\u000d\u000a      earlier theoretical predictions about MgO and states:\u000d\u000a    `The giant TMR effect in MgO MTJs (magnetic-tunnel-junctions) is expected\u000d\u000a      to contribute to our society by significantly reducing the power\u000d\u000a      consumption of electronics devices and improving the performance and\u000d\u000a      security of computers.'\u000d\u000a    Summary: The predictions of Mathon and Umerski's 2001 publication\u000d\u000a      [3.1] have directly influenced the design of all hard disk read heads\u000d\u000a      commercially manufactured since 2009. This has led to more than a\u000d\u000a      five-fold increase in hard disk storage capacity in an industry with\u000d\u000a      annual sales exceeding $28 billion [5.1]. The publication has attracted\u000d\u000a      more than 500 citations and is regarded as a seminal paper in spintronics,\u000d\u000a      giving birth to the explosion of interest in MgO-based systems. Moreover,\u000d\u000a      such systems are also the basis of magnetic random access memory (MRAM), a\u000d\u000a      new type of non-volatile memory that is being actively developed and may\u000d\u000a      someday replace both hard disks and existing random access memory [5.7,\u000d\u000a      5.8].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Since 2009 the read heads of all hard disks have used a technology based\u000d\u000a      on magnesium oxide (MgO). The development of this technology can be partly\u000d\u000a      attributed to a 2001 publication [3.1] co-authored by Dr Andrey Umerski of\u000d\u000a      The Open University, which concluded that a system based on MgO would lead\u000d\u000a      to a huge increase in magnetoresistance, a physical property that\u000d\u000a      determines the efficiency of hard disk read heads.\u000d\u000a    In 2004 these theoretical predictions were confirmed experimentally; by\u000d\u000a      2008 the new type of read head based on MgO was manufactured commercially,\u000d\u000a      leading to significant increases in storage capacity, from GBs to TBs.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    The Open University\u000d\u000a    ","Institutions":[{"AlternativeName":"Open University","InstitutionName":"Open University","PeerGroup":"D","Region":"South East","UKPRN":10007773}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a3.1. Mathon, J. and Umerski, A. (2001) `Theory of tunneling\u000d\u000a        magnetoresistance of an epitaxial Fe\/MgO\/Fe(001) junction', Physical\u000d\u000a        Review B, vol. 63, 220403(R).\u000d\u000a    \u000a\u000a3.2. Butler, W.H., Zhang, X.G., Schulthess, T.C. and MacLaren, J.M.\u000d\u000a      (2001) `Spin-dependent tunneling conductance of Fe|MgO|Fe sandwiches' Physical\u000d\u000a        Review B, vol. 63, 054416.\u000d\u000a    \u000a\u000a3.3. Yuasa, S., Nagahama, T., Fukushima, A., Suzuki, Y. and Ando, K.\u000d\u000a      (2004) `Giant room-temperature magnetoresistance Fe\/MgO\/Fe magnetic tunnel\u000d\u000a      junctions', Nature Materials, vol. 3, pp. 868-71.\u000d\u000a    \u000a\u000a3.4. Parkin, S.S.P., Kaiser, C., Panchula, A., Rice, P.M., Hughes, B.,\u000d\u000a      Samant, M. and Yang, S.H. (2004) `Giant tunneling magnetoresistance at\u000d\u000a      room temperature with MgO (100) tunnel barriers', Nature Materials,\u000d\u000a      vol. 3, pp. 862-7.\u000d\u000a    \u000a\u000a3.5. Aut&#232;s, G., Mathon, J. and Umerski, A. (2010) `Strong enhancement of\u000d\u000a      the tunneling magnetoresistance by electron filtering in an\u000d\u000a      Fe\/MgO\/Fe\/GaAs(001) junction', Physical Review Letters, vol. 104,\u000d\u000a      no. 21, p. 217202.\u000d\u000a    \u000a\u000a3.6. Aut&#232;s, G., Mathon, J. and Umerski, A. (2011) `Theory of ultrahigh\u000d\u000a      magnetoresistance achieved by k-space filtering without a tunnel barrier',\u000d\u000a      Physical Review B, vol. 83, no. 5, p. 052403.\u000d\u000a    \u000aPapers [3.5] and [3.6] were supported by EPSRC grants EP\/F023472\/1 and\u000d\u000a      EP\/F022808\/1:\u000d\u000a    dates: 01\/01\/08 - 31\/12\/10, project titles: `Solving the fundamental\u000d\u000a      limitations for RT spintronics &#8212; the role of interfaces in electron spin\u000d\u000a      detection and injection', total value &#163;338,268, principal investigators\u000d\u000a      Dr. A. Umerski of The Open University and Prof. J. Mathon of City\u000d\u000a      University London, respectively.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"2","Level2":"99","Subject":"Other Physical Sciences"},{"Level1":"3","Level2":"2","Subject":"Inorganic Chemistry"},{"Level1":"9","Level2":"12","Subject":"Materials Engineering"}],"Sources":"\u000d\u000a    The Wikipedia article http:\/\/en.wikipedia.org\/wiki\/Tunnel_magnetoresistance\u000d\u000a      contains a brief overview of MgO-based tunnel magnetoresistance and its\u000d\u000a      applications. Reference [3.1] is cited.\u000d\u000a    5.1. Bizmology article: `Consolidation in the hard disk drive market:\u000d\u000a      then there were three' http:\/\/bizmology.hoovers.com\/2012\/03\/19\/consolidation-in-the-hdd-hard-disk-drive-market-then-there-were-three\/\u000d\u000a      (Archived by WebCite&#174; at http:\/\/www.webcitation.org\/6BhSl9Ylg)\u000d\u000a    5.2. Letter from Magnetoelectronics Manager, IBM Almaden Research Center,\u000d\u000a      confirming the important role of Umerski and Mathon in the development of\u000d\u000a      Fe\/MgO\/Fe TMR junctions (Sept 2012).\u000d\u000a    5.3. Western Digital 2008 Annual Report and Form 10-K\u000d\u000a      https:\/\/materials.proxyvote.com\/Approved\/958102\/20080917\/AR_27910\/images\/Western_Digital-AR2008.pdf.\u000d\u000a      (Archived by WebCite&#174; at http:\/\/www.webcitation.org\/6BX5o4wht)\u000d\u000a    5.4. Western Digital 2009 Annual Report and Form 10-K\u000d\u000a      https:\/\/materials.proxyvote.com\/Approved\/958102\/20090916\/AR_46224\/HTML2\/default.htm.\u000d\u000a      (Archived by WebCite&#174; at http:\/\/www.webcitation.org\/6BX5xyQ8a).\u000d\u000a    5.5. Toshiba Storage Products `Internal Notebook Hard Drives' product\u000d\u000a      details webpage\u000d\u000a      http:\/\/storage.toshiba.com\/storagesolutions\/archived-models\/internal-notebook-hard-drives.\u000d\u000a      (Archived by WebCite&#174; at http:\/\/www.webcitation.org\/6BX6Eic0N)\u000d\u000a    5.6. http:\/\/amlawdaily.typepad.com\/01302012western_interim.pdf\u000d\u000a        (Archived by WebCite&#174; at http:\/\/www.webcitation.org\/6DwPHDu1U).\u000d\u000a    5.7. Yuasa, S. and Djayaprawira, D.D. (2007) `Giant tunnel\u000d\u000a      magnetoresistance in magnetic tunnel junctions with a crystalline MgO(001)\u000d\u000a      barrier', Journal of Physics D-Applied Physics, vol. 40, no. 21,\u000d\u000a      p. R337-54. Particularly the conclusion in Section 7, which contains a\u000d\u000a      brief summary.\u000d\u000a    5.8. Ikeda, S., Hayakawa, J., Lee, Y.M., Matsukura, F., Ohno, Y., Hanyu,\u000d\u000a      T. and Ohno, H. (2007) `Magnetic tunnel junctions for spintronic memories\u000d\u000a      and beyond', IEEE Transactions on Electron Devices, vol. 54, no.\u000d\u000a      5, pp. 991-1002. See section 3A.\u000d\u000a    5.9. Citation for the 20th Tsukuba prize\u000d\u000a      http:\/\/www.suzukiylab.mp.es.osaka-u.ac.jp\/Top\/tsukuba_english.pdf (Archived by WebCite&#174; at\u000d\u000a      http:\/\/www.webcitation.org\/6BX6MlwDd).\u000d\u000a    5.10. Ignoring other factors like write density, this estimate is based\u000d\u000a      on the fact that, in 2005, just before the new read heads were\u000d\u000a      manufactured, Toshiba introduced a hard drive with a storage density of\u000d\u000a      179 Gbit\/in2. Whereas in March of 2012 Seagate demonstrated a\u000d\u000a      1TB\/in2 drive:\u000d\u000a      http:\/\/storageeffect.media.seagate.com\/2012\/03\/storage-effect\/paving-the-way-for-big-hard-drive-capacity-gains\/(Archived\u000a        by WebCite&#174; at http:\/\/www.webcitation.org\/6DzIQc2I7).\u000d\u000a    5.11 Chen, Y., Song, D., Qiu, J., Kolbo, P., Wang, L., He, Q., Covington,\u000d\u000a      M., Stokes, S., Sapozhnikov, V., Dimitrov, D., Gao, K. and Miller, B.\u000d\u000a      (2010) `2 Tbit\/in2 reader design outlook', IEEE\u000d\u000a        Transactions on Magnetics, vol. 46, no. 3, pp. 697-701.\u000d\u000a    5.12 Recent experiments with improved growth techniques have measured\u000d\u000a      magnetoresistances of about 1100% at low temperature, in agreement with\u000d\u000a      the original prediction of Mathon and Umerski: Ikeda, S., Hayakawa, J.,\u000d\u000a      Ashizawa, Y., Lee, Y.M., Miura, K., Hasegawa, H., Tsunoda, M., Matsukura,\u000d\u000a      F. and Ohno, H. (2008) `Tunnel magnetoresistance of 604% at 300K by\u000d\u000a      suppression of Ta diffusion in CoFeB\/MgO\/CoFeB pseudo-spin-valves annealed\u000d\u000a      at high temperature', Applied Physics Letters, vol. 93, no. 8,\u000d\u000a      082508. \u000d\u000a    ","Title":"\u000d\u000a    Hard disks based on tunneling magnetoresistance\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Key publication: The modern-day hard disk read head is the first\u000d\u000a      commercial device to use the spin of an electron rather than its charge.\u000d\u000a      It exploits a phenomenon called magnetoresistance, which is an important\u000d\u000a      part of a new multidisciplinary field of study called spintronics. Dr\u000d\u000a      Umerski was involved in the spintronics field from an early stage. From\u000d\u000a      1994 to 2000 he was a PDRA to Professor J. Mathon and Professor D.M.\u000d\u000a      Edwards (City University and Imperial College, respectively) before taking\u000d\u000a      up a lectureship at The Open University in January 2001.\u000d\u000a\u0009Umerski and\u000d\u000a      Mathon's 2001 publication on tunneling magnetoresistance in MgO [1], which\u000d\u000a      is the subject of the present case study, is regarded as a milestone in\u000d\u000a      the field of spintronics, gaining more than 500 citations. Since 2001,\u000d\u000a      Umerski and Mathon have continued to build on the work pioneered in [3.1]\u000d\u000a      with ten further publications exploring different aspects of MgO tunneling\u000d\u000a      junctions &#8212; these include: oscillatory and resonance effects (2003, 2004,\u000d\u000a      2005 and 2009), the effect of disorder (2003, 2004 and 2006), and more\u000d\u000a      recent work on strong enhancement of magnetoresistance discussed below.\u000d\u000a    Magnetoresistance: A material is said to exhibit magnetoresistance\u000d\u000a      (MR) if its electrical resistance changes when the direction of an\u000d\u000a      external magnetic field is varied. MR is characterised by a quantity\u000d\u000a      called the magnetoresistance ratio (MR ratio) &#8212; the maximum percentage\u000d\u000a      change in resistance as the direction of applied magnetic field is varied.\u000d\u000a      A read head with a large MR ratio can read smaller magnetic `bits' on the\u000d\u000a      hard disk, and hence a higher storage density.\u000d\u000a    Earlier research: In 1989 an MR of quantum mechanical origin,\u000d\u000a      which utilises the spin of the electron, was discovered by Albert Fert and\u000d\u000a      Peter Gr&#252;nberg, who later received the 2007 Nobel Prize for their finding.\u000d\u000a      This discovery was quickly developed commercially and, in 1997, IBM\u000d\u000a      brought out a hard disk drive (HDD) in which the read head used this\u000d\u000a      effect to sense the magnetic `bits' of the disk. By the late 1990s, all\u000d\u000a      HDD read heads were based on this form of MR, and this development is the\u000d\u000a      main cause of the huge increase in disk storage density from 0.1 to\u000d\u000a      100Gbit\/in2 between 1991 and 2003.\u000d\u000a    This early form of MR, called giant magnetoresistance (GMR), was based on\u000d\u000a      entirely metallic systems. The GMR based read head had MR ratios limited\u000d\u000a      to less than 50%. Another system, using an alumina insulating barrier,\u000d\u000a      produced a modest increase in MR ratios (70%) and was briefly developed\u000d\u000a      into a read head by Seagate in 2005. The physical mechanism behind both\u000d\u000a      these systems involves non-coherent scattering of the electrons.\u000d\u000a    The MgO idea: In 2001 Mathon and Umerski [3.1], simultaneously\u000d\u000a      with a group in the US [3.2], proposed an entirely new system using a\u000d\u000a      crystalline insulator, magnesium oxide (MgO). The underlying physics\u000d\u000a      relies on coherent, spin-dependent, quantum electron tunneling through the\u000d\u000a      crystalline MgO barrier and so is entirely different to the GMR and the\u000d\u000a      alumina systems. The theoretical calculations in [3.1] predicted that the\u000d\u000a      MR ratio of this novel tunneling device can exceed 1000%, some 15 times\u000d\u000a      higher than previously achieved. This MR effect is called tunneling\u000d\u000a      magnetoresistance (TMR), and the magnetoresistive MgO system is referred\u000d\u000a      to as an MgO tunnel junction.\u000d\u000a    Recent theoretical work: Recently Umerski and Mathon, together\u000d\u000a      with their EPSRC-funded PDRA Aut&#232;s, showed how MR ratios could be\u000d\u000a      massively enhanced (to more than 100,000%), both in MgO tunnel junctions\u000d\u000a      and in metallic GMR junctions [3.5, 3.6]. Experimentalists are currently\u000d\u000a      trying to confirm these predictions, which have the potential to lead to\u000d\u000a      the next generation of spintronic based devices.\u000d\u000a    "},{"CaseStudyId":"31915","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Wellcome Trust","Engineering and Physical Sciences Research Council","Medical Research Council","Royal Society"],"ImpactDetails":"\u000d\u000a    The SCCS method was widely used in 2008-2013 by health practitioners, and\u000d\u000a      has been recommended by the World Health Organisation and the European\u000d\u000a      Centre for Disease Prevention and Control [5.1]. Non-academic\u000d\u000a      practitioners using the method include public health officials at national\u000d\u000a      government-funded institutes (notably the US Centers for Disease Control\u000d\u000a      and Prevention (CDC)) and epidemiologists working in the private sector,\u000d\u000a      notably the healthcare and pharmaceutical industries [5.2]. The means by\u000d\u000a      which the research contributed to the impact is primarily the large number\u000d\u000a      of epidemiological studies using the SCCS method, alone or in combination\u000d\u000a      with other methods, and relating to pressing public health issues, which\u000d\u000a      were published in the medical literature over the period 2008-13.\u000d\u000a    The breadth of applications of the SCCS method demonstrates both its\u000d\u000a      reach and its impact. The following two paragraphs document\u000d\u000a      (non-exhaustively) the range of investigations using the SCCS method that\u000d\u000a      were published in the medical and epidemiological literature in 2008-13\u000d\u000a      [5.3].\u000d\u000a    In vaccine safety studies, the SCCS method was used to study the safety\u000d\u000a      of off-label vaccines in elderly populations; epilepsy and influenza\u000d\u000a      vaccine; Guillain-Barr&#233; syndrome and influenza vaccines; adverse events\u000d\u000a      after vaccination of premature children; febrile convulsions and childhood\u000d\u000a      vaccinations; multiple sclerosis relapse and influenza vaccination;\u000d\u000a      thrombocytopenic purpura and vaccinations; effectiveness of pandemic `flu\u000d\u000a      vaccine; emergency admissions and vaccinations; and metabolic disorders\u000d\u000a      and vaccines.\u000d\u000a    In non-vaccine epidemiology, the SCCS method was used to investigate\u000d\u000a      depression in patients with heart disease or diabetes; fracture and\u000d\u000a      hypertensive drugs; adverse events and proton pump inhibitors; risks\u000d\u000a      associated with antipsychotics; motor vehicle accidents and drugs; motor\u000d\u000a      vehicle accidents and medications; falls and antidepressants; vascular\u000d\u000a      events and invasive dental treatment; oral antibiotic prescribing and\u000d\u000a      pregnancy; fractures and thiazolidinediones; oral bisphosphonates and\u000d\u000a      heart problems; and antipsychotics and stroke. The method was also used to\u000d\u000a      study vascular events after infections and in chronic respiratory disease.\u000d\u000a    The benefits of using the SCCS method are:\u000d\u000a    \u000d\u000a      the ability to rapidly undertake low-cost, high-quality studies of\u000d\u000a        rare conditions, using computerised databases of clinical records\u000d\u000a      better control of time-invariant confounders than is normally possible\u000d\u000a        in other study designs such as cohort and case-control studies\u000d\u000a      to extend the range of study designs that can be carried out on the\u000d\u000a        same data set, which, owing to the contrasting assumptions they make,\u000d\u000a        can help throw new light on causal mechanisms.\u000d\u000a    \u000d\u000a    This has led to several positive comparative reviews and recommendations\u000d\u000a      involving non- academic practitioners [5.4].\u000d\u000a    The methodological advance represented by the SCCS method has helped to\u000d\u000a      improve the quality and versatility of statistical methods in\u000d\u000a      pharmacoepidemiology, resulting in better studies and well- informed\u000d\u000a      medical decisions. Two specific examples of how SCCS methodology has had\u000d\u000a      an impact on major public health issues are as follows.\u000d\u000a    \u000d\u000a      Between 2008 and 2013, the SCCS method was used by GlaxoSmithKline and\u000d\u000a        the CDC to study the safety of the new Rotarix vaccine against rotavirus\u000d\u000a        infection, following the withdrawal of the Wyeth RotaShield vaccine\u000d\u000a        (confirmed by evidence from a 2001 study involving the SCCS method). The\u000d\u000a        importance of this impact derives from the fact that, worldwide, it is\u000d\u000a        estimated that more than 500,000 children under 5 years old die annually\u000d\u000a        from rotavirus diarrhoea [5.5].\u000d\u000a      In 2008-2013, several SCCS studies were undertaken to investigate the\u000d\u000a        safety of influenza vaccination, notably in relation to Guillain-Barr&#233;\u000d\u000a        syndrome. The issue shot to prominence in 2009 with the advent of H1N1\u000d\u000a        influenza A (`swine flu'), and the SCCS method was used by several\u000d\u000a        agencies to investigate the safety of influenza vaccines against various\u000d\u000a        influenza strains [5.6].\u000d\u000a    \u000d\u000a    ","ImpactSummary":"\u000d\u000a    This research has profoundly influenced the practice of\u000d\u000a      pharmacoepidemiology in 2008-13. The self-controlled case series (SCCS)\u000d\u000a      method is particularly well-suited for working with computerised\u000d\u000a      databases, which are increasingly used in epidemiology. The method has\u000d\u000a      been recommended by international agencies (WHO, ECDC) and is now widely\u000d\u000a      used by health practitioners within national public health agencies,\u000d\u000a      including the CDC (USA), Public Health England (UK) and many other\u000d\u000a      national and regional public health bodies. It has influenced practice\u000d\u000a      within the private sector (notably the pharmaceutical and the healthcare\u000d\u000a      industries). Use of the SCCS method has impacted on health by reducing\u000d\u000a      costs, improving timeliness and improving the quality of evidence upon\u000d\u000a      which policy decisions are based.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    The Open University\u000d\u000a    ","Institutions":[{"AlternativeName":"Open University","InstitutionName":"Open University","PeerGroup":"D","Region":"South East","UKPRN":10007773}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a3.1. Farrington, C.P. (1995) `Relative incidence estimation from case\u000d\u000a      series for vaccine safety evaluation-, Biometrics, vol. 51, pp.\u000d\u000a      228-35.\u000d\u000a    \u000a\u000a3.2. Taylor, B., Miller, E., Farrington, C.P., Petropoulos, M.-C.,\u000d\u000a      Favot-Mayaud, I., Li, J. and Waight, P.A. (1999) `Autism and measles,\u000d\u000a      mumps and rubella vaccine: no epidemiological evidence for a causal\u000d\u000a      association', Lancet, vol. 353, pp. 202-9.\u000d\u000a    \u000a\u000a3.3. Farrington, C.P. and Whitaker, H.J. `Semiparametric analysis of case\u000d\u000a      series data (with discussion)', Journal of the Royal Statistical\u000d\u000a        Society, Series C, vol. 55, pp. 553-94.\u000d\u000a    \u000a\u000a3.4. Whitaker, H.J., Farrington, C.P., Spiessens, B. and Musonda, P.\u000d\u000a      (2006) `Tutorial in biostatistics: the self-controlled case series\u000d\u000a      method', Statistics in Medicine, vol. 25, pp. 1768-98.\u000d\u000a    \u000a\u000a3.5. Farrington, C.P., Whitaker, H.J. and Hocine, M.N. (2009) `Case\u000d\u000a      series analysis for censored, perturbed or curtailed post-event\u000d\u000a      exposures', Biostatistics, vol. 10, pp. 3-16.\u000d\u000a    \u000a\u000a3.6. Whitaker, H.J., Hocine, M.N. and Farrington, C.P. (2009) `The\u000d\u000a      methodology of self-controlled case series studies', Statistical\u000d\u000a        Methods in Medical Research, vol. 18, pp. 7-26.\u000d\u000a    \u000a\u000a3.7. Farrington, C.P., Anaya-Izquierdo, K., Whitaker, H.J., Hocine, M.N.,\u000d\u000a      Douglas, I. and Smeeth, L. (2011) `Self-controlled case series analysis\u000d\u000a      with event-dependent observation periods', Journal of the American\u000d\u000a        Statistical Association, vol. 106, pp. 417-26.\u000d\u000a    \u000aThis research was funded by six peer-reviewed grants over the period\u000d\u000a      2003-2013 awarded to C.P. Farrington (PI) totalling more than &#163;500,000,\u000d\u000a      from the Wellcome Trust, EPSRC, GlaxoSmithKline, MRC, EPSRC and Royal\u000d\u000a      Society.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"}],"Sources":"\u000d\u000a    The pervasive use of the SCCS method has resulted in a large number of\u000d\u000a      relevant sources, and a selection of these is given here.\u000d\u000a    5.1 Use of SCCS by international health agencies\u000d\u000a    World Health Organisation: Zuber, P.L.F. et al. (2009) `Global\u000d\u000a      safety of vaccines: strengthening systems for monitoring, management and\u000d\u000a      the role of GACVS', Expert Review of Vaccines, vol. 8, pp. 705-16.\u000d\u000a      See p. 708, column 2, paragraph 2.\u000d\u000a    European Centre for Disease Prevention and Control: Lopalco, P.L.\u000d\u000a      et al. (2010) `Monitoring and assessing vaccine safety: a European\u000d\u000a      perspective', Expert Review of Vaccines, vol. 9, pp. 371-80. See\u000d\u000a      p. 373, column 1, paragraphs 6-7.\u000d\u000a    5.2 Use of SCCS by public health institutes and private sector\u000d\u000a          companies\u000d\u000a    For examples of use by public health bodies in the USA (Centers for\u000d\u000a      Disease Control and Prevention), UK (Public Health England), Quebec\u000d\u000a      (Ministry of Health), and by Kaiser Permanente (private healthcare\u000d\u000a      provider) and GSK (pharmaceutical company), see Sections 5.5 and 5.6\u000d\u000a      below. Recommendations by several other public and private sector users\u000d\u000a      are listed in Section 5.4 below. Other public health bodies using the SCCS\u000d\u000a      method include the Robert Koch Institute, Berlin (Uphoff et al. (2011) PLoS\u000d\u000a        One, vol. 6, e19932) and Public Health Ontario (Hawken et al. (2012)\u000d\u000a      American Journal of Epidemiology, vol. 176, pp. 1035-42).\u000d\u000a    5.3 List of studies undertaken using SCCS\u000d\u000a    See the citations of paper [3.4] given in Section 2, from which all these\u000d\u000a      examples are drawn.\u000d\u000a    5.4 Recommendations from non-academic practitioners\u000d\u000a    Authors from Johnson &amp; Johnson, Kaiser Permanente, and BC Ministry\u000d\u000a        of Health Services: Gagne et al. (2012) Pharmacoepidemiology and\u000d\u000a        Drug Safety, vol. 21 (supplement 1), pp. 32-40.\u000d\u000a    Authors from Roche Products, Amgen and Novartis: Quartey et al.\u000d\u000a      (2011) Pharmaceutical Statistics, vol. 10, pp. 539-47.\u000d\u000a    Author from PHE (UK): Andrews (2012) Biologicals, vol. 40,\u000d\u000a      pp. 389-92.\u000d\u000a    Author from Institut National de la Sant&#233; et de la Recherche M&#233;dicale\u000d\u000a        (Paris): Hocine, M.N. and Chavance, M. (2010) Revue\u000d\u000a        d'Epid&#233;miologie et de Sant&#233; Publique, vol. 58, pp. 435-40.\u000d\u000a    Authors from the US Food and Drug Administration and Kaiser Permanente\u000d\u000a        (USA): Maclure et al. (2012) Pharmacoepidemiology and Drug\u000d\u000a        Safety, vol. 21 (supplement 1), pp. 50-61.\u000d\u000a    Authors from Denver Health and Kaiser Permanente: McClure et al.\u000d\u000a      (2008) Vaccine, vol. 26, pp.3341-5.\u000d\u000a    5.5 SCCS and new rotavirus vaccines, 2008-2013: studies by\u000d\u000a          practitioners Study by GlaxoSmithKline: Velasquez et al.\u000d\u000a      (2012) Pediatric Infectious Disease Journal, vol. 31, pp. 736-44.\u000d\u000a    Study by the CDC: Patel et al. (2011) New England Journal of\u000d\u000a        Medicine, vol. 364, pp. 2283-92.\u000d\u000a    5.6 SCCS and influenza vaccine safety, 2008-2013: studies by\u000d\u000a          practitioners Studies by the UK PHE: Stowe et al. (2008) American\u000d\u000a        Journal of Epidemiology, vol. 169, pp. 382-8; Andrews et al. (2011)\u000d\u000a      Vaccine, vol. 29, pp. 7878-82.\u000d\u000a    Study by Quebec Ministry of Health: De Wals et al. (2012) Journal\u000a        of the American Medical Association, vol. 308, pp. 175-86.\u000d\u000a    Study by the CDC: Tokars et al. (2012) Pharmacoepidemiology\u000d\u000a        and Drug Safety, vol. 21, pp. 546-52.\u000d\u000a    Study involving Kaiser Permanente: Greene et al. (2012) American\u000a        Journal of Epidemiology, vol. 175, pp. 1100-1109. \u000d\u000a    ","Title":"\u000d\u000a    The self-controlled case series method in pharmacoepidemiology\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The SCCS method was published in 1995 by Dr Farrington [3.1], who was\u000d\u000a      then a statistician at the Public Health Laboratory Service, part of what\u000d\u000a      is now Public Health England (PHE). Further methodological developments\u000d\u000a      since 1999 at The Open University, detailed below, have extended the SCCS\u000d\u000a      method's applicability and accessibility, and are central to its impact.\u000d\u000a    The method is unusual in that it requires only individuals who have\u000d\u000a      experienced the adverse event of interest and thus, unlike case-control\u000d\u000a      studies, does not require separate controls. Instead, each individual is\u000d\u000a      used as their own control. An important consequence of this self-matching\u000d\u000a      is that the method automatically controls for time-invariant confounders,\u000d\u000a      thus overcoming biases associated with the selection of individuals for\u000d\u000a      treatment, known as indication and channelling biases. This is\u000d\u000a      particularly important when analysing data from electronic databases where\u000d\u000a      covariate information is often inadequate, and which are therefore more\u000d\u000a      likely to suffer from confounding biases. Because it requires only cases,\u000d\u000a      the method is also simple, cheap and rapid to apply.\u000d\u000a    In 1999, the SCCS method was used to provide the first substantive\u000d\u000a      evidence that the claim of an association between MMR and autism was\u000d\u000a      unfounded [3.2]. The method subsequently gained greatly in prominence,\u000d\u000a      generating further research by Dr Farrington, who had moved in 1998 to a\u000d\u000a      Lectureship at The Open University's Department of Statistics (which later\u000d\u000a      merged with the Department of Mathematics). From 1999, a sustained\u000d\u000a      research programme by Dr Farrington (who was appointed Professor of\u000d\u000a      Statistics in 2004) and colleagues at The Open University, including Dr\u000d\u000a      Heather Whitaker (postdoc 2001-06, then Lecturer from 2006), Dr Karen\u000d\u000a      Vines (Lecturer), Dr Patrick Musonda (PhD student then postdoc, 2003-07),\u000d\u000a      Dr Mounia Hocine (postdoc, 2007-09) and Yonas Weldeselassie (PhD student,\u000d\u000a      2010-present) was undertaken to extend the method's applicability.\u000d\u000a    A key theme in this research has been to weaken the assumptions required\u000d\u000a      by the method. This was largely in response to interest from the wider\u000d\u000a      pharmacoepidemiology community, who sought to apply the method to\u000d\u000a      life-changing events and in contexts where some of the assumptions may be\u000d\u000a      violated [3.3, 3.5, 3.7]. A second theme of the research, led by Dr\u000d\u000a      Whitaker, has been to increase the accessibility of the method by making\u000d\u000a      it available in standard software, creating a dedicated website (http:\/\/statistics.open.ac.uk\/sccs)\u000d\u000a      and popularising the method through workshops and publications in leading\u000d\u000a      medical statistics journals [3.4, 3.6].\u000d\u000a    Since 2003, this research programme at The Open University has been\u000d\u000a      supported by six external research grants, and its outputs have been\u000d\u000a      published in leading statistics journals. As a result, the reach and\u000d\u000a      impact of the SCCS method have been greatly expanded. In 2011, Professor\u000d\u000a      Farrington was awarded a Royal Society Wolfson Research Merit Award, and\u000d\u000a      in 2013 he was awarded the Royal Statistical Society's Bradford Hill\u000d\u000a      medal, partly in recognition of his work on the SCCS method.\u000d\u000a    Important contextual aspects of the impact are the heightened public\u000d\u000a      profile of vaccine safety issues following the MMR and autism controversy,\u000d\u000a      and the large-scale use of pandemic influenza vaccines. These have focused\u000d\u000a      attention on the need for reliable yet rapid methods of safety evaluation\u000d\u000a      such as provided by the SCCS method.\u000d\u000a    "},{"CaseStudyId":"31921","Continent":[],"Country":[],"Funders":["Medical Research Council"],"ImpactDetails":"\u000d\u000a    Statistical methods for the analysis of single-patient data have largely\u000d\u000a      been developed in the\u000d\u000a      context of neuropsychology, so it is in this area where the impact of\u000d\u000a      Garthwaite's research is\u000d\u000a      greatest. Indeed, the results on deficit and dissociation found by\u000d\u000a      Crawford, Garthwaite and others,\u000d\u000a      and the associated software they have produced, have been used by a very\u000d\u000a      large number of\u000d\u000a      neuropsychologists.\u000d\u000a    1. Single-patient studies that have been reported in the literature\u000d\u000a      provide the best\u000d\u000a      documentation of the impact of Crawford and Garthwaite's methods. In 2012\u000d\u000a      their papers in\u000d\u000a      the field received more than 140 citations and papers [3.1-3.5] have\u000d\u000a      together been cited\u000d\u000a      more than 500 times. Also, because only a very small fraction of patients\u000d\u000a      seen clinically are\u000d\u000a      of sufficient theoretical interest to warrant a subsequent write-up as a\u000d\u000a      case report, it is safe\u000d\u000a      to assume that the methods are being used on many more cases in practice.\u000d\u000a    The citations mainly arise from their methods being used to analyse data,\u000d\u000a      rather than their\u000d\u000a      work simply being discussed. Studies that have used their methods include\u000d\u000a      papers in\u000d\u000a      Science [5.11] and Brain [5.3, 5.6, 5.8]. Much of the work\u000d\u000a      that uses the methods is aimed at\u000d\u000a      influencing clinical practice. Recent examples include the work of\u000d\u000a      McGibbon and Jansari\u000d\u000a      [5.9], which raises the prospect of a standard test to diagnose\u000d\u000a      accelerated long-term\u000d\u000a      forgetting in a single clinical visit rather than requiring\u000d\u000a      multiple visits. Similarly, Borchers et\u000d\u000a      al. [5.4] propose guidelines for the diagnosis of optic ataxis. Their\u000d\u000a      conclusions note that `[in]\u000d\u000a      a first screening ... the lower C &amp; G 0.05 threshold (13%) would be a\u000d\u000a      good choice', where\u000d\u000a      `C &amp; G' refers to a method given in [3.2] that was used to determine\u000d\u000a      the threshold.\u000d\u000a    2. Computer programs implementing the methods are available from the\u000d\u000a      website of Crawford,\u000d\u000a      who has received at least 400 emails from users between 2008 and July 2013\u000d\u000a      enquiring\u000d\u000a      about these quantitative methods and programs. The emails (email logs\u000d\u000a      available on\u000d\u000a      request) include numerous unsolicited comments testifying to their\u000d\u000a      clinical use, such as:\u000d\u000a    \u000d\u000a      `Re-visiting your site to download .exe's to my new computer. Thanks\u000d\u000a        for all the\u000d\u000a        stunning work. Most helpful.'\u000d\u000a      `I have been finding your website and resources absolutely fantastic,\u000d\u000a        and as a\u000d\u000a        clinician have recommended them to others. Thanks for all your wonderful\u000d\u000a        work, we\u000d\u000a        appreciate it down under!'\u000d\u000a      `I just wanted to take a minute to tell you how much I appreciate the\u000d\u000a        contribution you\u000d\u000a        have made to the field with your work on statistical analyses of\u000d\u000a        psychometric\u000d\u000a        change, especially as it applies to neuropsychology.'\u000d\u000a    \u000d\u000a    3. Many reviews of methods for inference in the individual case have\u000d\u000a      devoted considerable\u000d\u000a      space to setting out the methods of Crawford, Garthwaite et al. and\u000d\u000a      recommending their\u000d\u000a      use [5.1, 5.2, 5.5, 5.7, 5.10]. The recommendations have been very\u000d\u000a      positive. For example,\u000d\u000a      McIntosh and Brooks [5.10] note that the methods developed by Crawford,\u000d\u000a      Garthwaite and\u000d\u000a      colleagues `are now the tests of choice for single-case comparisons' and\u000d\u000a      that they `have\u000d\u000a      been adopted enthusiastically by the neuropsychological community' (p.\u000d\u000a      1155).\u000d\u000a    4. The work is used by practising neuropsychologists to evaluate their\u000d\u000a      patients and assess\u000d\u000a      whether they have abnormal deficits or dissociations. Quantifying this\u000d\u000a      uptake is hard.\u000d\u000a      However, the methods are linked to important psychological test batteries,\u000d\u000a      including the\u000d\u000a      Wechsler Adult Intelligence Scale, the Delis-Kaplan executive function\u000d\u000a      system and the\u000d\u000a      Repeatable Battery for the assessment of neuropsychological data. These\u000d\u000a      test batteries do\u000d\u000a      not routinely find their way into single-patient studies, but they are\u000d\u000a      commercially available\u000d\u000a      and used daily by clinicians.\u000d\u000a    Given the ease with which the methods can be used, the large number of\u000d\u000a      citations and the positive\u000d\u000a      reviews and unsolicited comments, it is clear that the methods are well\u000d\u000a      used in clinical practice.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Garthwaite has collaborated with Professor John Crawford, a\u000d\u000a      neuropsychologist at the University\u000d\u000a      of Aberdeen, to develop novel statistical methods for drawing inferences\u000d\u000a      on the performance of an\u000d\u000a      individual patient. The methods have become the standard way of analysing\u000d\u000a      single-patient studies\u000d\u000a      in neuropsychology and are widely used by clinicians to compare individual\u000d\u000a      patients with normative\u000d\u000a      data.\u000d\u000a    The methods have also been implemented in easy-to-use software, freely\u000d\u000a      accessible over the web,\u000d\u000a      and have been linked to databases containing the results of large\u000d\u000a      normative samples on\u000d\u000a      psychological test batteries. They have been the focus of review papers\u000d\u000a      for clinical practice that\u000d\u000a      have strongly recommended their use.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    The Open University\u000d\u000a    ","Institutions":[{"AlternativeName":"Open University","InstitutionName":"Open University","PeerGroup":"D","Region":"South East","UKPRN":10007773}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a3.1 Crawford, J.R. and Garthwaite, P.H. (2002) `Investigation of the\u000d\u000a      single case in\u000d\u000a      neuropsychology: confidence limits on the abnormality of test scores and\u000d\u000a      test score differences',\u000d\u000a      Neuropsychologia, vol. 40, pp. 1196-1208.\u000d\u000a    \u000a\u000a3.2 Crawford, J.R. and Garthwaite, P.H. (2005) `Testing for suspected\u000d\u000a      impairments and\u000d\u000a      dissociations in single-case studies in neuropsychology: evaluation of\u000d\u000a      alternatives using Monte\u000d\u000a      Carlo simulations and revised tests for dissociations', Neuropsychology,\u000d\u000a      vol. 19, pp. 318-31.\u000d\u000a    \u000a\u000a3.3 Crawford, J.R. and Garthwaite, P.H. (2007) `Comparison of a single\u000d\u000a      case to a control or\u000d\u000a      normative sample in neuropsychology: development of a Bayesian approach',\u000d\u000a      Cognitive\u000d\u000a        Neuropsychology, vol. 24, pp. 343-72.\u000d\u000a    \u000a\u000a3.4 Crawford, J.R., Garthwaite, P.H., Azzalini, A., Howell, D.C. and\u000d\u000a      Laws, K.R. (2006) `Testing\u000d\u000a      for a deficit in single-case studies: Effects of departures from\u000d\u000a      normality', Neuropsychologia, vol.\u000d\u000a      44, pp. 666-77.\u000d\u000a    \u000a\u000a3.5 Crawford, J.R., Garthwaite, P.H. and Gault, C.B. (2007) `Estimating\u000d\u000a      the percentage of the\u000d\u000a      population with abnormally low scores (or abnormally large score\u000d\u000a      differences) on standardized\u000d\u000a      neuropsychological test batteries: a generic method with applications', Neuropsychology,\u000d\u000a      vol.\u000d\u000a      21, pp. 419-30.\u000d\u000a    \u000a\u000a3.6 Crawford, J.R., Garthwaite, P.H., Sutherland, D. and Borland N.\u000d\u000a      (2011) `Some\u000d\u000a      supplementary methods for the analysis of the Delis-Kaplan Executive\u000d\u000a      Function System',\u000d\u000a      Psychological Assessment, vol. 23, pp. 888-98.\u000d\u000a    \u000a\u000a3.7 Garthwaite, P.H. and Crawford, J.R. (2004) `The distribution of the\u000d\u000a      difference between two\u000d\u000a      t-variates', Biometrika, vol. 91, pp. 987-94.\u000d\u000a    \u000a\u000a3.8 Garthwaite, P.H. and Crawford, J.R. (2011) `Confidence intervals for\u000d\u000a      a binomial proportion\u000d\u000a      in the presence of ties', Journal of Applied Statistics, vol. 38,\u000d\u000a      pp. 1915-34.\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    Citation information comes from the ISI Web of Knowledge.\u000d\u000a    5.1 Atzeni, T. (2009) `Statistiques appliqu&#233;es aux &#233;tudes de cas unique:\u000d\u000a      m&#233;thodes\u000d\u000a      usuelles et alternatives', Revue de Neuropsychologie Neurosciences\u000d\u000a        Cognitives et\u000d\u000a        Cliniques, vol. 1, pp. 343-51.\u000d\u000a    This (French) review of developments in how to make inferences\u000d\u000a        concerning the\u000d\u000a        performance of a single case is, in essence, solely concerned with\u000d\u000a        Crawford, Garthwaite\u000d\u000a        and colleagues' methods; seven of the ten equations presented are those\u000d\u000a        developed by\u000d\u000a        Crawford, Garthwaite and colleagues (two of the remaining three are\u000d\u000a        basic definitional\u000d\u000a        equations).\u000d\u000a    5.2 Balboni, G. and Cubelli, R. (2011) `How to use psychological tests\u000d\u000a      for functional\u000d\u000a      diagnosis: the case of assessment of learning disabilities', Advances\u000d\u000a        in Learning and\u000d\u000a        Behavioral Disabilities, vol. 24, pp. 79-92.\u000d\u000a    This (Italian) guide to assessment shows that Crawford and\u000d\u000a        Garthwaite's methods are now\u000d\u000a        also having an impact in the area of learning disabilities. It\u000d\u000a        recommends (and illustrates the\u000d\u000a        use of) four of Crawford, Garthwaite and colleagues' methods.\u000d\u000a    5.3 Bird, C.M., Castelli, F., Malik, O., Frith, U. and Husain, M. (2004)\u000d\u000a      `The impact of\u000d\u000a      extensive medial frontal lobe damage on `theory of mind' and cognition', Brain,\u000d\u000a      vol. 127, pp.\u000d\u000a      914-28.\u000d\u000a    5.4 Borchers, S., Muller, L., Synofzik, M. and Himmelbach, M. (2013)\u000d\u000a      `Guidelines and\u000d\u000a      quality measures for the diagnosis of optic ataxia', Frontiers in\u000d\u000a        Human Neuroscience, vol. 7,\u000d\u000a      article 324.\u000d\u000a    5.5 Brooks, B.L., Strauss, E., Sherman, E.M.S., Iverson, G.L. and Slick,\u000d\u000a      D.J. (2009)\u000d\u000a      `Developments in neuropsychological assessment: Refining psychometric and\u000d\u000a      clinical\u000d\u000a      interpretive methods', Canadian Psychology, vol. 50, pp. 196-209.\u000d\u000a    This (Canadian) review provides further evidence of the impact of\u000d\u000a        Crawford and colleagues'\u000d\u000a        work on assessment in clinical practice. It recommends (and illustrates\u000d\u000a        the use of) four of\u000d\u000a        Crawford, Garthwaite and colleagues' methods.\u000d\u000a    5.6 Fotopoulou, A., Pernigo, S., Maeda, R., Rudd, A. and Kopelman, M.A.\u000d\u000a      (2010)\u000d\u000a      `Implicit awareness inanasognosia for hemiplegia: unconscious interference\u000d\u000a      without\u000d\u000a      conscious re-representation', Brain, vol. 133, pp. 3564-77.\u000d\u000a    5.7 Hanson, R.K., Lloyd, C.D., Helmus, L. and Thornton, D. (2012)\u000d\u000a      `Developing non-arbitrary\u000d\u000a      metrics for risk communication: percentile ranks for the Static-99\/R and\u000d\u000a      Static-2002\/R\u000d\u000a      sexual offender risk tools', International Journal of Forensic Mental\u000d\u000a        Health, vol. 11,\u000d\u000a      pp. 9-23.\u000d\u000a    This recent (Canadian) paper illustrates that Crawford, Garthwaite and\u000d\u000a        colleagues' methods\u000d\u000a        are now also having an impact in the forensic area: the methods were\u000d\u000a        used to provide point\u000d\u000a        and interval estimates for risk assessment tools\u000d\u000a    5.8 Maguire, A.E., Nannery, R. and Spiers, H.J. (2006) `Navigation around\u000d\u000a      London by a\u000d\u000a      taxi driver with bilateral hippocampal lesions', Brain, vol. 129,\u000d\u000a      pp. 2894-907.\u000d\u000a    5.9 McGibbon, T. and Jansari, A.S. (2013) `Detecting the onset of\u000d\u000a      accelerated long-term\u000d\u000a      forgetting: Evidence from temporal lobe epilepsy', Neuropsychologia,\u000d\u000a      vol. 51, pp. 114-22.\u000d\u000a    5.10 McIntosh, R.D. and Brooks, J.L. (2011) `Current tests and trends in\u000d\u000a      single-case\u000d\u000a      neuropsychology', Cortex, vol. 47, pp. 1151-9.\u000d\u000a    This recent (UK) review of single case methods is focused almost\u000d\u000a        exclusively on reviewing\u000d\u000a        and recommending Crawford and colleagues' methods. It cites 13 of\u000d\u000a        Crawford, Garthwaite\u000d\u000a        and colleagues' papers.\u000d\u000a    5.11 Thiebaut de Schotten, M., Urbanski, M., Duffau, H., Voue, E., Levy,\u000d\u000a      R., Dubois, B.\u000d\u000a      and Bartolomeo, P. (2005) `Direct evidence of parietal-frontal pathway\u000d\u000a      subserving spatial\u000d\u000a      awareness in humans', Science, vol. 309, pp. 2226-8.\u000d\u000a    ","Title":"\u000d\u000a    Standard methods of analysing single-patient data in neuropsychology\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Paul Garthwaite has been Professor of Statistics at The Open University\u000d\u000a      since 2000. Since then he\u000d\u000a      has worked extensively with Professor John Crawford on single-patient\u000d\u000a      data, publishing 24 papers\u000d\u000a      on this subject since 2002. This account covers only a small part of their\u000d\u000a      work.\u000d\u000a    Their early work [3.1] developed classical inferential methods for\u000d\u000a      deficits; that is, cases in which a\u000d\u000a      patient's measurements are so unusual that the patient cannot be part of\u000d\u000a      the control population.\u000d\u000a      This work gave point and interval estimates for the percentage of the\u000d\u000a      normal population who would\u000d\u000a      have a more extreme score than the score obtained by the patient.\u000d\u000a      Assumptions underlying the\u000d\u000a      inferences have been examined [3.4] and asymptotic approximations were\u000d\u000a      developed [3.7] so that\u000d\u000a      a classical hypothesis test could be constructed for dissociations [3.2],\u000d\u000a      where a patient's\u000d\u000a      measurements on two similar tasks are unusually different.\u000d\u000a    Forming an interval estimate for dissociation proved impossible using\u000d\u000a      classical statistics, so\u000d\u000a      Bayesian methods for inference about both deficits and dissociations were\u000d\u000a      developed [3.3].\u000d\u000a      Bayesian and classical methods are equivalent for making inferences for\u000d\u000a      deficits but Bayesian\u000d\u000a      methods can give interval estimates for dissociations. Modifications to\u000d\u000a      the Bayesian methods have\u000d\u000a      been devised that give good frequentist properties, which is necessary if\u000d\u000a      the methods are to be\u000d\u000a      widely accepted by the neuropsychology community.\u000d\u000a    Often performance on a task is influenced by covariates such as a\u000d\u000a      person's age and number of\u000d\u000a      years of education, and methods have been developed that control\u000d\u000a      covariates. The above methods\u000d\u000a      work well with large samples but are designed to give accurate inferences\u000d\u000a      with small samples,\u000d\u000a      greatly increasing their usefulness. Methods have also been developed\u000d\u000a      specifically to compare a\u000d\u000a      patient's score with large databases [3.5, 3.6, 3.8]: complications can\u000d\u000a      arise because a patient's\u000d\u000a      score may be tied with many controls, in which case the usual method of\u000d\u000a      breaking ties may give\u000d\u000a      serious inaccuracy.\u000d\u000a    All the methods have been implemented in software that is freely\u000d\u000a      available (for example, from\u000d\u000a      Professor Crawford's website: http:\/\/homepages.abdn.ac.uk\/j.crawford\/pages\/dept\/)\u000d\u000a      and is easy to\u000d\u000a      use. The software can be run directly from the web without the need to\u000d\u000a      download and save it, and\u000d\u000a      a user need only type in simple summary data and the scores of the\u000d\u000a      patient. Relevant programs\u000d\u000a      also link to databases where the scores on common psychological tests from\u000d\u000a      large control samples\u000d\u000a      are stored. To use these programs a user need only specify a patient's\u000d\u000a      scores and the scales on\u000d\u000a      which they were measured.\u000d\u000a    The importance of this work to assessment of the individual patient has\u000d\u000a      been further recognised by\u000d\u000a      the award of a recent (2012-2015) MRC grant MR\/J013838\/1 (&#163;480,000 FEC)\u000d\u000a      for `Development of\u000d\u000a      statistical methods for the analysis of single patient data'. Garthwaite\u000d\u000a      is principal investigator on the\u000d\u000a      grant, and Jones (The Open University) and Crawford are co-investigators.\u000d\u000a    "},{"CaseStudyId":"33367","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000d\u000a    Nature of the Impact:\u000d\u000a    The dynamic inversion control law developed by Sasane and his co-authors\u000d\u000a      underpins an invention used and patented by Boeing in May 2012 [A] to\u000d\u000a      design flight control systems (Patent no. US 8, 185,255 B2) [B]. This\u000d\u000a      invention overcomes a challenge in newer aircraft which are designed with\u000d\u000a      sophisticated flight control systems to achieve greater agility (in the\u000d\u000a      case of military aircraft) or improved fuel efficiency (in commercial and\u000d\u000a      private aircraft) and need advanced control laws to maximize their\u000d\u000a      performance.\u000d\u000a    The in-flight control and manoeuvring of aerodynamic vehicles, such as\u000d\u000a      aircraft, is accomplished by positioning the aircraft's control surfaces,\u000d\u000a      the ailerons on the wings for example, to modify the airfb02ow across them\u000d\u000a      to affect an aerodynamic response from the aircraft. The control surfaces,\u000d\u000a      along with other force and moment producing devices, are referred to as\u000d\u000a      \"control effectors\".\u000d\u000a    New aircraft designs include an increasing number of control effectors,\u000d\u000a      to improve fuel efficiency and increase fb02ight safety. Advanced control\u000d\u000a      laws are needed to make use of the larger range of control effectors. The\u000d\u000a      fb02ight control system must allocate the execution of commands among\u000d\u000a      several control effectors. The allocation is determined by the type of\u000d\u000a      action commanded, the fb02ight conditions and the known responses of the\u000d\u000a      aircraft. The previous generation of fb02ight control systems, however,\u000d\u000a      used control allocation algorithms which assume linearity of the control\u000d\u000a      effector effects, and fail to account for the possibility of interactions\u000d\u000a      between control effectors.\u000d\u000a    The US patent invention overcomes this problem by using as its foundation\u000d\u000a      the dynamic inversion control law developed by Sasane, Hovakimyan and\u000d\u000a      Lavretsky [C]. This formulates a solution to the nonlinear problem\u000d\u000a      mentioned above by using an approximate dynamic inversion based on\u000d\u000a      time-scale principles.\u000d\u000a    The US patent covers a new method for the allocation of control authority\u000d\u000a      in real time among several control effectors of a controllable vehicle (in\u000d\u000a      particular an aircraft) in execution of a commanded manoeuvre. The\u000d\u000a      allocation takes account of possible nonlinear effects which those control\u000d\u000a      effectors may have on the vehicle and on each other in affecting such a\u000d\u000a      manoeuvre.\u000d\u000a    Wider Implications: The invention is applicable to any air, space,\u000d\u000a      sea, under-sea or ground vehicle whose dynamics are controlled via a\u000d\u000a      selected set of control effectors (Column 1, first paragraph, of [A]). The\u000d\u000a      results can include improved passenger safety and increased fuel\u000d\u000a      efficiency (and hence less environmental damage). The reach of the\u000d\u000a      invention is therefore considerable.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research by Amol Sasane and co-authors is the foundation of an invention\u000d\u000a      patented and used by the aerospace company Boeing to design flight control\u000d\u000a      systems. The invention is a method which aims to optimize aerodynamic\u000d\u000a      performance of aircraft, thereby improving fuel efficiency and flight\u000d\u000a      safety.\u000d\u000a    Sasane and his co-authors' research is explicitly mentioned as having\u000d\u000a      been used to overcome a problem in flight control &#8212; one that arises in\u000d\u000a      newer, more sophisticated aircraft designs &#8212; in Patent no. US 8, 185,255\u000d\u000a      B2, 'Robust control effector allocation'.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    London School of Economics and Political Science\u000d\u000a    ","Institutions":[{"AlternativeName":"London School of Economics and Political Science","InstitutionName":"London School of Economics & Political Science","PeerGroup":"C","Region":"London","UKPRN":10004063}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1. Naira Hovakimyan, Eugene Lavretsky and Amol Sasane. Dynamic inversion\u000d\u000a      for nonaffine-in-control systems via time-scale separation: Part I. Proceedings\u000a        of the American Control Conference, pages 3542-3547, Portland,\u000d\u000a      Oregon, June 2005.\u000d\u000a    \u000aURL1:\u000d\u000a      http:\/\/www.nt.ntnu.no\/users\/skoge\/prost\/proceedings\/acc05\/PDFs\/Papers\/0632_FrA05_1.pdf\u000d\u000a    URL 2: http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=1470522\u000d\u000a    \u000a2. Naira Hovakimyan, Eugene Lavretsky and Amol Sasane. Dynamic inversion\u000d\u000a      for nonaffine-in-Control systems via time-scale separation. I. Journal\u000d\u000a        of Dynamical and Control Systems, volume 13, pages 451-465, number\u000d\u000a      4, 2007.\u000d\u000a    \u000aURL: http:\/\/www.springerlink.com\/content\/w310861170h13557\/?MUD=MP\u000d\u000a    Evidence of quality: publication in peer-reviewed outlets.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"9","Level2":"13","Subject":"Mechanical Engineering"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000d\u000a    All sources listed below can also be seen at: https:\/\/apps.lse.ac.uk\/impact\/case-study\/view\/3\u000d\u000a    A. The US patent number 8,185,255 B2 is available in the public domain at\u000d\u000a      the following address:\u000d\u000a      URL: http:\/\/www.spacepatents.com\/patented_inventions\/pat8185255.pdf\u000d\u000a      https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1280\u000d\u000a    B. Private e-mail correspondence with a Technical Fellow at the Boeing\u000d\u000a      Corporation confb01rms that Boeing is using the patent to design flight\u000d\u000a      control systems for aerial platforms. This source is confidential.\u000d\u000a    C. Column 5, paragraph lines 40-50 in the patent:\u000d\u000a      \"The present invention overcomes this control allocation problem by using\u000d\u000a      as its foundation a dynamic inversion control law for nonaffine-in-control\u000d\u000a      dynamic systems that was earlier developed by Naira Hovakimyan, Eugene\u000d\u000a      Lavretsky, and Amol J. Sasane. This dynamic inversion control law\u000d\u000a      formulates a solution to the nonlinear and\/or non-monotonic problem by\u000d\u000a      using an approximate dynamic inversion based on time-scale separation\u000d\u000a      principles.\"\u000d\u000a      URL: http:\/\/www.spacepatents.com\/patented_inventions\/pat8185255.pdf\u000d\u000a      https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1281\u000d\u000a    ","Title":"\u000d\u000a    Improving the design of flight control systems\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Research Insights and Outputs:\u000d\u000a    Dr. Amol Sasane undertook research with Naira Hovakimyan (University of\u000d\u000a      Illinois) and Eugene Lavretsky (The Boeing Company) to solve the problem\u000d\u000a      of stabilizing a non-affine control system using time-scale separation.\u000d\u000a      Their fb01ndings fb01rst appeared in a paper for the American Control\u000d\u000a        Conference in 2005 [Reference 1]. A revised version of the paper was\u000d\u000a      published in 2007 [Reference 2]. Subsequently, the US patent for an\u000d\u000a      invention utilising their solution was applied for by Eugene Lavretsky,\u000d\u000a      Ryan Diecker and Joseph Brinker ('the inventors'), and assigned to the\u000d\u000a      Boeing Company. The patent was granted on 22 May, 2012.\u000d\u000a    Control theory is a branch of applied mathematics in which one considers\u000d\u000a      systems governed by equations of the form x'(t)=f(x(t),u(t)), where x and\u000d\u000a      u are vector-valued functions, and time t runs from an initial time ti\u000d\u000a      onwards. This equation is underdetermined, in the sense that the function\u000d\u000a      u, thought of as consisting of input variables, can be freely chosen. Once\u000d\u000a      a particular input function u has been chosen, then the corresponding\u000d\u000a      function x, given an initial condition x(ti), is uniquely\u000d\u000a      determined for all future times. If one changes the choice of u, then the\u000d\u000a      corresponding x changes too. Thus, there is a possibility of controlling\u000d\u000a      the behaviour of the x variables by a suitable choice of the control input\u000d\u000a      u. The aim in control theory is to find u which produces some desirable\u000d\u000a      effect on x.\u000d\u000a    Control theory is particularly useful in aerospace engineering, where the\u000d\u000a      models are typically nonlinear. Moreover, the control inputs appear in a\u000d\u000a      non-affine manner. Affine systems are ones in which the input u appears in\u000d\u000a      f in an affine manner, that is, f(x,u)=g(x)+h(x)u. Feedback linearization\u000d\u000a      is a popular method in control design for nonlinear systems that are\u000d\u000a      affine in the control inputs. However, an explicit feedback linearization\u000d\u000a      for non-affine control systems is not satisfactory as often a\u000d\u000a      transcendental equation appears when one attempts to fb01nd an appropriate\u000d\u000a      control.\u000d\u000a    The 2005 conference paper set out a new general method for feedback\u000d\u000a      linearization of non-affine nonlinear control systems. The method used\u000d\u000a      time-scale separation, where the control signal is sought as a solution of\u000d\u000a      fast dynamics, and is shown to asymptotically stabilize the original slow\u000d\u000a      non-affine control system.\u000d\u000a    The control allocation method in the patent invention utilises this\u000d\u000a      finding, allowing it to make real time allocation of control of pilot or\u000d\u000a      auto-pilot fb02ight commands among the aircraft's control effector\u000d\u000a      actuators despite possible nonlinear interactions which the control\u000d\u000a      effector displacements may have on the aircraft and on each other.\u000d\u000a    Key Researcher: Dr Sasane has been full-time at LSE since 2004.\u000d\u000a    "},{"CaseStudyId":"33443","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2658434","Name":"Switzerland"},{"GeoNamesId":"798544","Name":"Poland"}],"Funders":[],"ImpactDetails":"\u000a    BACKGROUND\u000a    Basel III, developed by the Basel Committee on Banking Supervision and\u000a      agreed in September 2010, is a set of comprehensive reform measures that\u000a      puts in place a global regulatory standard on bank capital adequacy,\u000a      stress testing and market liquidity. One of its mandatory requirements is\u000a      for all banks to conduct counterparty credit risk (CCR) model backtesting.\u000a      However, it leaves each bank to define its own backtesting methodology.\u000a    CCR backtesting is intended to ensure that the models provide more timely\u000a      and accurate information on a bank's exposure to the risk caused by a\u000a      counterparty by comparing the risk measures implied by the bank's pricing\u000a      models with the realised exposure based on the traded prices. The main\u000a      output of the backtesting is in the form of a \"traffic light\" system\u000a      whereby: \"green\" signals that there is no evidence against the pricing\u000a      models; \"amber\" signals that observed risk exposure is higher than that\u000a      implied by the pricing models but still within an acceptable tolerance\u000a      level; \"red\" indicates that the price models underestimate the risk and\u000a      need to be re-calibrated and the capital requirement adjusted. Ultimately,\u000a      backtesting results should demonstrate to the regulators (i.e. the Bank of\u000a      England in UK) the soundness and conservativeness of the reported exposure\u000a      to risk. Backtesting should also be able to identify where pricing models\u000a      are overly-conservative.\u000a    At the invitation of Barclays Bank PLC, Yao has been participating in the\u000a      CCR backtesting project undertaken by the Quantitative Analyst &#8212; Exposure\u000a      Group at Barclays since January 2012. Yao was invited because of his\u000a      relevant research expertise in handling dependence and nonstationarity in\u000a      data and his considerable experience in inference with conditional\u000a      distributions.\u000a    NATURE AND EXTENT OF THE IMPACT\u000a    Based on his previous work in [1], [2] and [3], and the newly developed\u000a      method in [4], Yao proposed several key statistical methods that were used\u000a      in the development of the backtesting methodology outlined in document\u000a      [A]; see also [B] and [C]. The methodology has been approved by Barclays\u000a      internal governance process and, from September 2013, it has been\u000a      implemented as a part of business operations at Barclays. The outputs of\u000a      the methodology are now being used by Barclays credit risk managers on a\u000a      daily basis to control model risks. The new methodology improves\u000a      substantially the CCR assessment and management at Barclays in the ways\u000a      described below, and puts the practice at Barclays in line with the Basel\u000a      III regulatory capital framework. The resulting improved information about\u000a      the bank's exposure to risk mitigates potential future losses and thus\u000a      also helps to stabilize the global financial market and protect economic\u000a      stability and individual welfare.\u000a    DETAILS OF HOW THE RESEARCH UNDERPINNED THE BACKTESTING METHODOLOGY AND\u000a      HOW RISK ASSESSMENT HAS BEEN IMPROVED\u000a    Yao contributed directly to Barclays backtesting methodology set out in\u000a      [A] and [B] (see also [C]). Indeed, Yao wrote the first version of [B].\u000a      The key ways in which his research underpinned the methodology and the\u000a      benefits arising from his research contributions can be summarised as\u000a      follows.\u000a    1. Yao's research fed into a conditional counter (a new metric for\u000a        backtesting) and a simulation-based testing method which result in more\u000a        reliable information on risk exposure.\u000a    A \"binomial counter\" had been used by Barclays for analysing\u000a      collateralized transactions for which the data from non-over-lapping time\u000a      intervals can be treated as independent. A stratified version was\u000a      introduced to deal with dependence in uncollateralized transactions.\u000a      However, this simple approach, although approved by the Financial Services\u000a      Authority, is inadequate where prices across different time horizons are\u000a      dependent on each other &#8212; which is commonplace. Indeed, there hardly\u000a      exists any effective metrics for backtesting with dependent data.\u000a    The conditional counter and simulation-based testing method, introduced\u000a      on the basis of Yao's research, fill this gap. The conditional counter\u000a      method specified in [A] and [B] applies a version of the nonparametric\u000a      estimation method proposed in [1]. It effectively looks at the extreme\u000a      values of conditional distributions instead of those of unconditional\u000a      distributions. The simulation method is more generic. It can be used to\u000a      test not only the extreme quantiles but also other features such as the\u000a      whole distribution. It can also be easily extended to test the sensitivity\u000a      to risk factors. This is significant because an important new requirement\u000a      of Basel III is to test various features of the distribution.\u000a    A simulation-based testing procedure for calculating p-values using the\u000a      bootstrap multiple comparison method of [2] is generic. It can be used for\u000a      testing, for example, a pricing model in relation to an observed trade\u000a      path over different time horizons based on any appropriate test statistic.\u000a      It takes into account the non-stationarity and the dependencies among\u000a      prices at different time horizons, or different price paths, in an\u000a      automatic manner (see [A]). This enables Barclays to extract more reliable\u000a      information on risk exposure in their daily operation. This test procedure\u000a      can also be adapted to identify whether or not pricing models are\u000a      overly-conservative, providing a sound scientific basis for Barclays to\u000a      adjust is capital reserve.\u000a    The proposed simulation-based testing method based on bootstrap\u000a      calibration represents the first such generic method to incorporate\u000a      nonstationarity and dependence among different trades and\/or different\u000a      time horizons in an almost automatic manner. This represents a big step\u000a      forward in the backtesting techniques used at Barclays.\u000a    2. Yao's research contributed some key steps to the methodology for\u000a        selecting representative portfolios , which result in the Basel III\u000a        standard being met more effectively.\u000a    Basel III allows banks to construct representative portfolios for each\u000a      counterparty consisting of, for example, a subset of the trades between\u000a      two banks. Banks are left to decide the number and trades to be included\u000a      in the portfolio, but they have to justify their choices to their\u000a      supervisors (the Bank of England in the UK). As the number of trades\u000a      between two major banks can easily be in the order of tens thousands or\u000a      more, a simple linear regression runs into the so-called \"large p and\u000a      small n\" problem, even after the initial screening and categorisation.\u000a      Furthermore, many trades are highly correlated in the sense that the\u000a      sparse representation is certainly not unique. Hence some popular\u000a      techniques such as LASSO or the Danzig algorithm are no longer applicable.\u000a      The procedure adopted by Barclays uses the method of [3], i.e. a stepwise\u000a      sweep coupled with the use of (modified) information criteria to form the\u000a      candidate set. To construct a representative portfolio from the trades in\u000a      the candidate set, the new Matching Quantiles Estimation (MQE) method\u000a      proposed in [4] is now employed. Barclays has also adopted a new measure\u000a      and a test proposed in [4] to check how well the distribution of a\u000a      selected portfolio matches the target distribution at all levels\u000a      simultaneously, as required by Basel III. This overcomes problems with\u000a      existing estimation methods, such as quantile regression, which can only\u000a      check the success of a match at one, or at most, a few fixed levels,\u000a      resulting in a representative portfolio that falls short of the Basel III\u000a      standard.\u000a    Construction of counterparty representative portfolios is a new mandated\u000a      requirement under Basel III. Some key steps in the way that Barclays has\u000a      formulated this construction in its new methodology are attributable to\u000a      Yao's research.\u000a    ","ImpactSummary":"\u000a    In response to the deficiencies in bank risk management revealed\u000a      following the 2008 financial crisis, one of the mandated requirements\u000a      under the Basel III regulatory framework is for banks to backtest the\u000a      internal models they use to price their assets and to calculate how much\u000a      capital they require should a counterparty default. Qiwei Yao worked with\u000a      the Quantitative Analyst &#8212; Exposure team at Barclays Bank, which is\u000a      responsible for constructing the Barclays Counterpart Credit Risk (CCR)\u000a      backtesting methodology. They made use of several statistical methods from\u000a      Yao's research to construct the newly developed backtesting methodology\u000a      which is now in operation at Barclays Bank. This puts the CCR assessment\u000a      and management at Barclays in line with the Basel III regulatory capital\u000a      framework.\u000a    ","ImpactType":"Economic","Institution":"\u000a    London School of Economics and Political Science\u000a    ","Institutions":[{"AlternativeName":"London School of Economics and Political Science","InstitutionName":"London School of Economics & Political Science","PeerGroup":"C","Region":"London","UKPRN":10004063}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"3099434","Name":"Gdansk"},{"GeoNamesId":"2661604","Name":"Basel"}],"References":"\u000a    \u000a[1] Cai, Z., Yao, Q. and Zhang, W. (2001). Smoothing for discrete-valued\u000a      time series. Journal of the Royal Statistical Society, Series B\u000a      (Statistical Methodology), 63 (2), pp. 357-375.\u000a      http:\/\/eprints.lse.ac.uk\/6095\/\u000a      DOI: 10.1111\/1467-9868.00290\u000a    \u000a\u000a[2] Fan, J., Hall, P. and Yao, Q. (2007). To how many simultaneous\u000a      hypothesis tests can normal student's t or bootstrap calibrations be\u000a      applied? Journal of the American Statistical Association, 102\u000a      (480), pp. 1282-1288. http:\/\/eprints.lse.ac.uk\/5399\/\u000a      DOI: 10.1198\/016214507000000969\u000a    \u000a\u000a[3] An, H.Z., Huang, D., Yao, Q. and Zhang C.H. (2008). Stepwise\u000a      searching for feature variables in high-dimensional linear regression. LSE\u000a      preprint:\u000a\u0009  http:\/\/stats.lse.ac.uk\/q.yao\/qyao.links\/paper\/ahyz08.pdf\u000a    \u000a\u000a[4] Sgouropoulos, N., Yao, Q. and Yastremiz, C. (2013). Matching a\u000a      distribution function by matching quantiles estimation.LSE preprint:\u000a\u0009  http:\/\/stats.lse.ac.uk\/q.yao\/qyao.links\/paper\/mqe.pdf\u000a    \u000aEvidence of quality: (1) and (2) are in top, peer-reviewed journals.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000a    [A] Barclays QA Exposure Analytics (2012). (This source is confidential.\u000a      Please see [C])\u000a    [B] Barclays QA Exposure Analytics (2012). (This source is confidential.\u000a      Please see [C])\u000a    [C] A letter from the Director of the Barclays QA Exposure Analytics on\u000a      Yao's contribution on the CCR backtesting. This source is confidential.\u000a    Yao received the initial invitation from Barclays to participate this\u000a      project for 3 months (January &#8212; March 2012). The invitation has been\u000a      subsequently extended to December 2013.\u000a    ","Title":"\u000a    Improving Barclays Bank's management of its exposure to Counterparty\u000a        Credit Risk\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Research Insights and Outputs: The Barclays CCR backtesting\u000a      methodology, upon which Qiwei Yao's research had an impact, integrates a\u000a      number of statistical methods, underpinned by four pieces of Yao's\u000a      research. Each of these four research contributions arose as part of his\u000a      long-term focus on statistical inference for time series.\u000a    In an effort to reveal dynamic structure beyond linear autocorrelation,\u000a      Yao has made substantial advances in developing methods for modelling and\u000a      forecasting the future conditional on current and past status, which\u000a      reveals various interesting features that are relevant to but absent from\u000a      conventional linear time series models. For example, prediction errors for\u000a      the future depend on current position and errors are nonlinearly amplified\u000a      over time. Yao's work in this area involves a number of co-authors. For\u000a      the method used for the CCR project, of particular relevance was his joint\u000a      work [1] published in 2001 with Zongwu Cai from the University of North\u000a      Carolina at Charlotte and Wenyang Zhang, then a postdoctoral research\u000a      officer at LSE and now Professor in the Department of Mathematics at the\u000a      University of York. In terms of technical tools, Yao and his co-authors\u000a      have developed several nonparametric and semiparametric methods, including\u000a      a method in [1] for estimating conditional forecasters in the form of\u000a      point forecasts, forecast sets and forecasting distributions. To assess\u000a      the accuracy of those forecasters, some resampling techniques are used. A\u000a      distinguishing feature is to make the resampling adaptive to the\u000a      dependence in the data. This is achieved by either reproducing the\u000a      dependence in the resampled data or resampling the pre-whitened data.\u000a    Yao's second underpinning research contribution in [2] in 2000 was joint\u000a      work with two LSE Visiting Professors, Jianqing Fan and Peter Hall, on a\u000a      project investigating the use of traditional Z-tests and t-tests and also\u000a      bootstrap calibration in the context of simultaneous hypothesis testing.\u000a      One of the interesting findings in [2] was that with bootstrap methods the\u000a      number of simultaneous tests can be substantially larger than the sample\u000a      size. In fact, log(v) can be as large as the square-root of the sample\u000a      size, where v denotes the number of simultaneous tests. This justifies the\u000a      use of bootstrap methods when traditional Z-tests and t-test are\u000a      inapplicable.\u000a    In the third more recent contribution, Yao, jointly with Hongzhi An\u000a      (Chinese Academy of Sciences), Da Huang (Yao's PhD student at the time)\u000a      and Cun-Hui Zhang (Rutgers University), revisited the classic stepwise\u000a      selection methods for regression in the setting of \"large p and small n\",\u000a      see [3]. They showed that model selection consistency can be achieved by\u000a      using stepwise selection coupled with appropriately modified information\u000a      criteria. They also demonstrated by simulation that their method provides\u000a      a much more robust performance than some popular procedures such as LASSO.\u000a      This method provides a stable initial screening among a large number of\u000a      trades for selecting a counterparty representative portfolio.\u000a    Finally the matching quantiles estimation method proposed in [4] directly\u000a      resulted from solving the counterparty representative portfolio selection\u000a      problem. A new measure and a new statistical test were also proposed in\u000a      [4] to measure the goodness of match. It was joint work with Nikolaos\u000a      Sgouropoulos at QA Exposure of Barclays Bank and Claudia Yastremiz of Bank\u000a      of England.\u000a    Key Researcher: Professor Yao has been at LSE since 2000.\u000a    "},{"CaseStudyId":"33460","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Natural Environment Research Council"],"ImpactDetails":"\u000a    Impacts on public policy and services\u000a      Smith and Stainforth's research has stimulated and informed policy debate\u000a      on climate change\u000a      since 2007. The Intergovernmental Panel on Climate Change (IPCC) Fourth\u000a      Assessment Report\u000a      cites Smith (Section 3, reference 2) as the sole article in noting the\u000a      significant impact of structural\u000a      model error on its \"probability\" distributions [A: pg. 797].\u000a    Their direct engagement in the policy development process is evidenced by\u000a      their involvement with\u000a      the pre-release criticism of UK Climate Projections 2007 (UKCP07). In\u000a      response to a request from\u000a      Defra for more information, Smith and Stainforth expressed concerns\u000a      regarding the fidelity of\u000a      UKCP07 [B]. Their subsequent involvement in post-study pre-release\u000a      interactions with the Defra\u000a      Chief Scientist and Sir Brian Hoskins contributed to the formation of an\u000a      international review of\u000a      UKCP07. There are two notable outcomes, firstly, the details of the review\u000a      of UKCP07 have never\u000a      been released, and secondly, UKCP07 became UKCP09. Smith's research and\u000a      views were quoted\u000a      extensively in post-release criticism in both scientific and mainstream\u000a      press [C].\u000a    The UK government leads the world in the search for climate forecasts at\u000a      scales relevant for\u000a      adaptation decisions. UK Climate Projections 2009 (UKCP09) initially\u000a      claimed to provide detailed\u000a      predictions (\"post code\" in space, daily in time) for this century. UKCP09\u000a      probabilities, described as\u000a      best-available and dubbed \"Bayesian\", provided core information which\u000a      underlies the 2012 Climate\u000a      Change Risk Assessment (CCRA).\u000a    The Royal Commission on Environmental Pollution, led by Sir John Lawton,\u000a      invited both Smith and\u000a      Stainforth to give evidence for their report [D]. Smith and Stainforth\u000a      also contributed to the\u000a      Treasury's underpinning research on the economics of climate change,\u000a      altering the framing of the\u000a      Stern Review [E].\u000a    Impacts on the environment (policy debate on the environment have\u000a          been stimulated or\u000a        informed by research and research evidence)\u000a      Smith and Stainforth's improved interpretation of climate-model\u000a      simulations has contributed to\u000a      changes in how the UKCP09 is presented and government policy on climate\u000a      change, allowing for\u000a      better deployment of government funding. For example, the Climate Change\u000a      Act 2008 committed\u000a      the government to significant reductions in greenhouse gas emissions.\u000a      Their work also affected the\u000a      2012 Climate Change Risk Assessment (CCRA) which represents a key part of\u000a      the Government's\u000a      response to the Climate Change Act 2008, itself the first of a series of\u000a      regular assessments\u000a      required by law.\u000a    Having been severe critics of UKCP09, Smith and Stainforth were, at\u000a      Defra's request, both\u000a      members of the initial framing meeting before beginning the CCRA, as well\u000a      as reviewers of the\u000a      draft report. They objected to the violation of methodological\u000a      restrictions on the use of UKCP09\u000a      probabilities in the CCRA that had been agreed at the initial meeting,\u000a      illustrating the intense\u000a      pressure to over-interpret the output of climate models.\u000a    CATS's distinct attention to the shift in the rational interpretation of\u000a      climate predictions away from\u000a      an optimization approach to a risk based approach founded on broad\u000a      scientific insights and known\u000a      vulnerabilities can be seen in the disclaimers of several significant\u000a      publications (IPCC, UKCP09\u000a      program, the UKCP09 user guidance and other government reports), the focus\u000a      on the Dutch\u000a      alternative approaches to climate risk management, and the Treasury's\u000a      approach to the Stern\u000a      Review [E].\u000a    Consequently, national meteorological services wishing to avoid\u000a      UKCP09-like approaches within\u000a      their own borders have been more effective at pressing alternative\u000a      approaches. For example,\u000a      Smith and Stainforth provided information to the Dutch meteorological\u000a      office and Dutch\u000a      government scientists [F] in support of their successful attempt to avoid\u000a      a similar process to\u000a      UKCP09. More recently, members of CATS were invited by the Dutch\u000a      Government to a closed\u000a      door meeting on the presentation of uncertainty in the 2013 IPCC Report\u000a      [G].\u000a    Smith and Stainforth continue to engage with Defra, DECC, and a now\u000a      independent UKCIP to\u000a      improve how the level and coherence of UK climate information is evidenced\u000a      and acknowledged.\u000a    Economic impacts (where performance of an existing business has\u000a          been improved)\u000a      The procurement of climate change research by industry provides further\u000a      evidence of reach\u000a      beyond public sector. CATS' research has been used to clarify the limits\u000a      on climate simulation for\u000a      decision making and to improve the performance of EDF, EON, and NG. Smith\u000a      provided a review\u000a      of a UK Met Office commercial project purporting to provide high\u000a      resolution meteorological\u000a      information for \"climate proofing\" new long-lived energy infrastructure.\u000a      In addition to written advice,\u000a      Smith also represented industry in discussions with the Met Office to\u000a      clarify the assumptions\u000a      underlying the proposed study. Smith's continued engagement with industry\u000a      partners, Munich Re\u000a      and Lloyd's in particular, have allowed them to better interpret climate\u000a      information [H, I].\u000a    International Reach\u000a      Smith's involvement in the Baker Committee [J] for the US Central\u000a      Intelligence Agency (CIA),\u000a      focused on the extent to which the risks of climate change can be\u000a      quantified. As a key participant\u000a      in a series of meetings and reviews that led to the preparation of the\u000a      Harvard Report, Professor\u000a      Smith contributed his expertise, provided short turn-around calculations\u000a      on the impact of station\u000a      distribution, provided an in-depth review of the report and is named as\u000a      one of the major reviewers.\u000a      Dr James Baker states, \"Professor Smith understands better than most\u000a      climate scientists what the\u000a      limitations of the science are and how to use statistical and physical\u000a      analysis to draw robust\u000a      conclusions for policy makers\" [K].\u000a    Smith has represented the American Statistical Association at all three\u000a      annual American\u000a      Association for the Advancement of Science's \"Climate Day on Capitol\u000a      Hill\"; engaged with\u000a      individual Senators' and Congressmen's offices (eight\/year), and was\u000a      invited to assist in\u000a      developing both the American Statistical Association position on climate\u000a      change and redrafting the\u000a      American Geophysical Union's position statement.\u000a    Wider Implications. Quantifying the financial value of this\u000a      case study is nontrivial given the long\u000a      forward shadow that today's decisions on climate mitigation and adaptation\u000a      will cast. Industrial\u000a      sectors with large-scale infrastructure decisions (energy and ports in\u000a      particular), and national\u000a      security agencies familiar with unquantified risk (the US Central\u000a      Intelligence Agency) have\u000a      reconsidered their view of the fidelity and robustness of model-based\u000a      projections, significantly\u000a      reducing the likelihood of maladaptation through overconfidence of\u000a      quantitative predictions. Direct\u000a      costs\/spend of particular studies reflect far too low a value (hundreds of\u000a      thousands to millions of\u000a      pounds). While the value at risk in the longer term is truly immense,\u000a      attribution of any fraction of it\u000a      to our actions is arbitrary. It is estimated that close to &#163;11m was spent\u000a      on UKCP09, which Smith\u000a      and Stainforth show to be fundamentally flawed, furthermore, the\u000a      ill-advised use of products from\u000a      UKCP09 could cost orders of magnitude more.\u000a    Climate change is perhaps the greatest risk that humans will face in this\u000a      century and the next, and\u000a      alongside intervention, Smith contributes to public discourse [L]. Smith\u000a      has publicly argued that\u000a      effective adaptation to climate change costs a fraction of GDP with long\u000a      term savings significantly\u000a      greater. He and colleagues have established that the \"probabilities\" of\u000a      UKCP09 are not a reliable\u000a      foundation, either for adaptation planning or for risk assessment. The\u000a      value at risk dwarfs the\u000a      &#163;10m spent on the studies themselves and the inopportune exposure of the\u000a      weakness in the\u000a      science base of UK adaptation plans risks a loss of public credibility in\u000a      science based policy.\u000a      UKCP09 and the CCRA are stronger than they would have been without the\u000a      LSE's impact. Other\u000a      countries including the Netherlands and the USA are more clearly aware of\u000a      the mathematical\u000a      shortcomings. The Netherlands have rejected the UK methodology and adopted\u000a      an entirely\u000a      different approach.\u000a    ","ImpactSummary":"\u000a    As the realities of climate change have become more widely accepted over\u000a      the last decade,\u000a      decision makers have requested projections of future changes and impacts.\u000a      Founded in 2002, the\u000a      Centre for Analysis of Time Series (CATS) has conducted research revealing\u000a      how the limited\u000a      fidelity of climate models reduces the relevance of cost-benefit style\u000a      management in this context:\u000a      actions based on ill-founded projections (including probabilistic\u000a      projections) can lead to\u000a      maladaptation and poor policy choice. CATS' conclusions were noted in the\u000a      Fourth Assessment\u000a      Report of the Intergovernmental Panel on Climate Change (IPCC) report and\u000a      led in turn to the\u000a      toning down of the UK Climate Projections 2009 and the 2012 UK Climate\u000a      Change Risk\u000a      Assessment. Members of the insurance sector, energy sector, national\u000a      security agencies, scientific\u000a      bodies and governments have modified their approaches to climate risk\u000a      management as a direct\u000a      result of understanding CATS' research. Attempts to reinterpret climate\u000a      model output and design\u000a      computer experiments for more effective decision support have also\u000a      resulted.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    London School of Economics and Political Science\u000a    ","Institutions":[{"AlternativeName":"London School of Economics and Political Science","InstitutionName":"London School of Economics & Political Science","PeerGroup":"C","Region":"London","UKPRN":10004063}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a1. K Judd &amp; LA Smith (2004) Indistinguishable States II: The\u000a      Imperfect Model Scenario.\u000a      Physica D 196: 224-242. DOI: 10.1016\/j.physd.2004.03.020 http:\/\/eprints.lse.ac.uk\/22175\/\u000a    \u000a\u000a2. LA Smith, (2002) What Might We Learn from Climate Forecasts? Proc.\u000a        National Acad. Sci.\u000a        USA 4 (99): 2487-2492. http:\/\/eprints.lse.ac.uk\/16905\/\u000a    \u000a\u000a3. DA Stainforth, et al (2005) Uncertainty in the Predictions of the\u000a      Climate Response to Rising\u000a      Levels of Greenhouse Gases Nature 433 (7024): 403-406. DOI:\u000a      10.1038\/nature03301\u000a    \u000a\u000a4. DA Stainforth, MR Allen, ER Tredger &amp; LA Smith (2007) Confidence,\u000a      uncertainty and\u000a      decision-support relevance in climate predictions, Phil. Trans. R.\u000a        Soc. A, 365, 2145-2161.\u000a      DOI:10.1098\/rsta.2007.2074 http:\/\/eprints.lse.ac.uk\/22222\/\u000a    \u000aEvidence of Quality: publications in top-ranked journals, plus\u000a      research grants as follows:\u000a    &#8226; EC Marie Curie Postdoctoral Fellowship &#8212; Dr Antje Weisheimer,\u000a      EVK2-CT-2001-50012, Feb\u000a      02 - Jul 03.\u000a    &#8226; Climate Variability. Funded by University of California, San Diego.\u000a      Grant # 10255373.\u000a      Grant holder: Prof. L. Smith. &#163;16,026. 01\/11\/2005 - 30\/06\/2006.\u000a    &#8226; Climateprediction.net: A practical platform for ensemble Earth System\u000a      Modelling. NERC\u000a      grant # NE\/C515747\/. Grant holder: Myles Allen, Oxford University.\u000a      Co-Investigator: Prof\u000a      L. Smith. ~&#163;283k\u000a    &#8226; Ensemble-based Predictions of Climate Changes and their Impacts\u000a      (ENSEMBLES). EU\u000a      6th framework programme \/ Integrated project. Grant # GOCE-CT-2003-505539-\u000a      ENSEMBLES. Grant holder: Prof L. Smith &#163;108,306. 01\/09\/2004 - 31\/12\/2009.\u000a    &#8226; Integrated Ocean Observing Systems (IOOS), NOAA, October 2007-October\u000a      2013.\u000a      (~$500,000).\u000a    &#8226; Evaluating the economics of climate risks and opportunities in the\u000a      insurance sector, Munich\u000a      Re, October 2008 - September 2013. (&#163;2.9M) PI: Prof L. Smith\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"4","Level2":"1","Subject":"Atmospheric Sciences"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000a    All Sources listed below can also be seen at: https:\/\/apps.lse.ac.uk\/impact\/case-study\/view\/7\u000a    A. Solomon, S., D. Qin, M. Manning, Z. Chen, M. Marquis, K.B. Averyt, M.\u000a      Tignor and H.L. Miller\u000a      (eds.). 2007. Contribution of Working Group I to the Fourth Assessment\u000a      Report of the\u000a      Intergovernmental Panel on Climate Change. http:\/\/www.ipcc.ch\/pdf\/assessment-report\/ar4\/wg1\/ar4_wg1_full_report.pdf\u000a    B. Testimonial from Defra. This source is confidential.\u000a    C. Professor Leonard Smith quoted in analysis by Pallab Ghosh, BBC News,\u000a      18 June 2009:\u000a      https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1556\u000a    D. Royal Commission on Environmental Pollution. (2010) Adapting\u000a      Institutions to Climate Change,\u000a      Twenty-eighth report. David Stainforth is acknowledged in the report as a\u000a      key contributor.\u000a      http:\/\/webarchive.nationalarchives.gov.uk\/20110112040753\/http:\/\/www.rcep.org.uk\/reports\/28-adaptation\/documents\/adaptation_final_report.pdf\u000a      https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1557\u000a    E. HMT. (2010) The Stern Review. http:\/\/webarchive.nationalarchives.gov.uk\/+\/http:\/www.hm-treasury.gov.uk\/sternreview_index.htm\u000a      https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1558\u000a    F. Testimonial from PBL Netherlands. This source is confidential\u000a    G. Invitation from IPCC. https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1561\u000a    H. https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1562\u000a      https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1564\u000a    I. Munich Re, press release research collaboration.\u000a      https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1565\u000a    J. Testimonial from Head of Exposure management and Reinsurance\u000a      performance Management,\u000a      Lloyd's. This source is confidential\u000a    J. Harvard Climate Extremes Report (Baker Committee Report for the CIA,\u000a      2011-2013)\u000a      http:\/\/environment.harvard.edu\/sites\/default\/files\/climate_extremes_report_2012-12-04.pdf\u000a      https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1566\u000a    K. Testimonial from Director of the Global Carbon Measurement Program of\u000a      the Clinton\u000a      Foundation. This source is confidential\u000a    L. New Scientist. https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1568\u000a    ","Title":"\u000a    Improved climate policy and planning via realistic evaluation of model\u000a        projections\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Research Insights and Outputs:\u000a      The LSE's climate simulation research derives in large part from a broader\u000a      research program on\u000a      nonlinear dynamical systems, a key focus of CATS.\u000a    Mathematical research in Judd and Smith [1] established consequences of\u000a      model imperfections for\u000a      probability forecasting from noisy observations and models of chaotic\u000a      systems. These notions\u000a      were further developed in the context of climate models [See 2, 3, 4] and\u000a      underpin the impact\u000a      detailed here, specifically:\u000a    \u000a      The first stresses the fundamental limitations on interpreting the\u000a        output from collections of\u000a        today's climate models as if they reflected the probability of future\u000a        climate change [2];\u000a      The second applies the insights of the first in the interpretation of\u000a        what was then the largest\u000a        ensemble of simulations with a complex climate model; this ensemble was\u000a        generated by the\u000a        climateprediction.net project of which Stainforth was co-founder and\u000a        Smith Co-Investigator and\u000a        a key player in its conceptual design [3];\u000a      The third provides additional analysis while illustrating the\u000a        practical and conceptual limitations\u000a        in the probabilistic interpretation of such ensembles [4].\u000a    \u000a    The basic insight is that the limited fidelity of a generation of models\u000a      places an a priori cut-off on\u000a      the quantitative informativeness of models, and therefore ensembles of\u000a      models, from that\u000a      generation. Simulation models are, of course, qualitatively different from\u000a      the real-world system they\u000a      attempt to represent. This is a particular problem in climate studies\u000a      where the object of interest (the\u000a      future state of the climate system) is expected to be qualitatively\u000a      different to the state of the system\u000a      for which we have some, limited, observations with which to assess our\u000a      models. Technological and\u000a      knowledge constraints impose shared weaknesses on all today's models which\u000a      limit the lead time\u000a      and spatial scales on which simulations are realistic, or can be made\u000a      informative by statistical post-processing. There is no statistical fix, just as a collection of\u000a      simulations using Newtonian physics\u000a      cannot be expected to account for non-Newtonian phenomena (like the orbit\u000a      of Mercury) that\u000a      require knowledge of general relativity, unless the information on those\u000a      phenomena are in the\u000a      observational data. Inasmuch as climate is an extrapolation problem,\u000a      historical data are of limited\u000a      use. The research provided a foundation for resisting the oversell of\u000a      climate projections and\u000a      protecting the credibility of science-based policy and decision making.\u000a    Key researchers:\u000a      Professor Leonard Smith has been at LSE since 2000, Dr David Stainforth\u000a      has been at LSE since\u000a      2009. ER Tredger, a graduate student at LSE, 2006-2009, Ana Lopez,\u000a      2009-present, and Erica\u000a      Thompson October 2012-present.\u000a    "},{"CaseStudyId":"33500","Continent":[{"GeoNamesId":"6255146","Name":"Africa"},{"GeoNamesId":"6255150","Name":"South America"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"3686110","Name":"Colombia"},{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"192950","Name":"Kenya"},{"GeoNamesId":"3573345","Name":"Bermuda"},{"GeoNamesId":"3865483","Name":"Argentina"},{"GeoNamesId":"953987","Name":"South Africa"}],"Funders":["Engineering and Physical Sciences Research Council","Natural Environment Research Council","Royal Society"],"ImpactDetails":"\u000d\u000a    Weather has an impact on people, businesses and economies every single\u000d\u000a      day. Even fairly common weather, with no extreme meteorological elements\u000d\u000a      whatsoever, can produce costly and stressful disruptions &#8212; to power and\u000d\u000a      water supplies, food production and distribution networks, travel systems,\u000d\u000a      communication networks (e.g. GPS) and other infrastructure necessary to\u000d\u000a      the smooth functioning of families, communities and societies. And changes\u000d\u000a      in forecasts themselves, not the weather at all, can have huge impacts on\u000d\u000a      prices and the provision of (cancellation of) services. There is therefore\u000d\u000a      an imperative for tools which enhance the use of modern (probabilistic)\u000d\u000a      weather forecast in addition to the need for weather predictions that are\u000d\u000a      timely, accurate and enable users with varying needs and levels of\u000d\u000a      understanding to plan for and manage their responses to weather conditions\u000d\u000a      and events. There are three ways in which Professor Smith and his\u000d\u000a      colleagues at CATS have responded to this need: 1) advancing the state of\u000d\u000a      the art in probabilistic forecasting in the weather industry; 2) advancing\u000d\u000a      the state of practice in weather forecasting; and 3) advancing\u000d\u000a      understanding and application amongst various types of users in business\u000d\u000a      and industry.\u000d\u000a    Advancing the state of the art in the weather industry\u000d\u000a    Professor Smith and CATS have significant and often longstanding\u000d\u000a      relationships focused on transferring knowledge and advancing the state of\u000d\u000a      the art with a number of key institutions involved in weather forecasting.\u000d\u000a      The relationship with the European Centre for Medium Range Weather\u000d\u000a      Forecasts (ECMWF), \"considered the worldwide leader in global,\u000d\u000a      medium-range, monthly and seasonal predictions\", dates back to the 1994\u000d\u000a      Royal Society of Meteorologists meeting on predictability [A].\u000d\u000a      Longstanding partners also include: the UK Met Office [B]; the US Naval\u000d\u000a      Research Laboratory (NRL), where one of Professor Smith's former students\u000d\u000a      now leads a group forecasting hurricanes in the Pacific; and the US\u000d\u000a      National Centre for Atmospheric Research (NCAR), where Dr. Du is sharing\u000d\u000a      methods and providing a test bed for data assimilations. More recent\u000d\u000a      partnerships include: the International Research Institute for Climate and\u000d\u000a      Society (IRI) [C], which focuses on climate service development, primarily\u000d\u000a      within developing countries; the Risk Prediction Initiative (RPI) of the\u000d\u000a      Bermuda Institute of Ocean Sciences (BIOS) [D]; and the Industrial\u000d\u000a      Mathematics Knowledge Transfer Network of the Smith Institute. These\u000d\u000a      relationships focus on two critical areas of weather forecasting:\u000d\u000a      predictability and verification.\u000d\u000a    According to the head of ECMWF's Predictability Division, Roberto Buizza,\u000d\u000a      \"During the past 15 years, Prof. Lenny Smith has contributed to a range of\u000d\u000a      key subjects in the area of predictability, which had an impact on the\u000d\u000a      design of the operational systems of ECMWF, and in the use of\u000d\u000a      ensemble-based, probabilistic forecasts...The work we did on the\u000d\u000a      estimation of the impact of non-linearity ...provided estimates on the\u000d\u000a      time when non-linearity impacted on forecast accuracy, thus helping us\u000d\u000a      refining our techniques and methodologies. Prof. Smith and his group's\u000d\u000a      contribution in this area continued throughout the years, and the systems\u000d\u000a      operational today are still benefitting from his research done years\u000d\u000a      ago...and supported their gradual extension of the forecast range to the\u000d\u000a      monthly, seasonal and decadal time scale\" [A]. IRI's Chief Climate\u000d\u000a      Scientist, Dr. Simon Mason, likewise has been using Professor Smith's work\u000d\u000a      on second-order uncertainty and intractability in \"communicating forecast\u000d\u000a      uncertainty honestly at all times scales\" and relying on Smith's advice in\u000d\u000a      contributing to the World Climate Research Programme Working Group on\u000d\u000a      Regional Climate, \"where issues of estimating forecast uncertainty are\u000d\u000a      paramount\" [C].\u000d\u000a    ECMWF, IRI and BIOS have all been influenced by the work of Professor\u000d\u000a      Smith and CATS on verification, i.e. metrics to evaluate the accuracy of\u000d\u000a      weather forecasts. For ECMWF, \"his work on the assessment of probabilistic\u000d\u000a      forecasts using information theory provided a new and extremely valuable\u000d\u000a      measure of the quality of a probabilistic forecasting scheme. Based on the\u000d\u000a      amount of a data compression it allows, called ignorance (Roulston &amp;\u000d\u000a      Smith 2002), this measure has been used routinely to assess the quality of\u000d\u000a      ECMWF operational products\" [A]. IRI's Dr. Mason was also influenced by\u000d\u000a      the underpinning research [e.g. 6] and by extensive discussions with\u000d\u000a      Professor Smith on the properties of verification scores, which is\u000d\u000a      reflected in guidance and training that Dr. Mason has produced and\u000d\u000a      delivered for the World Meteorology Organisation (WMO) [C] (see next\u000d\u000a      section). Professor Smith has worked with BIOS on evaluating the skill of\u000d\u000a      seasonal forecasts of sea surface temperatures in the tropical Atlantic\u000d\u000a      and the equatorial Pacific Ocean where such temperatures play a role in\u000d\u000a      the formation of hurricanes. This knowledge has been transferred through\u000d\u000a      papers and through two workshops with members of BIOS' Risk Prediction\u000d\u000a      Initiative [D].\u000d\u000a    Advancing practice in probabilistic weather forecasting\u000d\u000a    The advance of practice is where the reach of the impact has perhaps been\u000d\u000a      greatest, through tools, guidance, training, software and products based\u000d\u000a      on the underpinning research that have influenced weather forecasters\u000d\u000a      around the world. What has proved to be highly influential is a conceptual\u000d\u000a      framework called Weather Roulette [6] that allows weather predictors to\u000d\u000a      more easily communicate the skill and value of forecasts to customers and\u000d\u000a      users by translating the probabilities into effective daily interest\u000d\u000a      rates, which is particularly useful in situations where small\u000d\u000a      probabilities can lead to large costs or benefits. Weather Roulette has\u000d\u000a      been incorporated into ECMWF's Ensemble Verification Training Course since\u000d\u000a      at least 2010 [E]. It is also an essential component of a verification\u000d\u000a      training course run by the WMO's Commission on Climatology over the past\u000d\u000a      five years to train trainers from regional climate centres, who then\u000d\u000a      assist in training seasonal forecasters in their regions. To date\u000d\u000a      trainings have been held in China (2x), Trinidad, South Africa, Colombia,\u000d\u000a      Argentina, Kenya (2x) and the US, involving over 150 trainers [C].\u000d\u000a    Weather Roulette has also been incorporated into the Climate\u000d\u000a      Predictability Tool, a software package being used by all meteorological\u000d\u000a      services in South America and extensively in Central America, the\u000d\u000a      Caribbean, Africa, South and East Asia, and elsewhere for the production\u000d\u000a      of seasonal forecasts [C]. Equally significant, effective interest rates\u000d\u000a      (i.e. weather roulette) [6] and ignorance scores [1] have been included as\u000d\u000a      two of the seven recommended verification scores and procedures in the Guidance\u000d\u000a        on Verification of Seasonal Climate Forecasts being officially\u000d\u000a      disseminated to the 191 member countries and territories of the WMO to\u000d\u000a      guide seasonal forecasting by their National Climate Centres and regional\u000d\u000a      and local forecasters [C,F]. This guidance document is also expected to\u000d\u000a      influence the approaches utilised by both governmental and commercial\u000d\u000a      seasonal forecasters around the globe.\u000d\u000a    In addition, the UK Met Office has used the concepts of kernel dressing\u000d\u000a      and blending climatology from the underpinning research to redesign its\u000d\u000a      3-month Outlook product, which is constantly available on behalf of the\u000d\u000a      Cabinet Office to assist contingency planners across the public and\u000d\u000a      private sectors to prepare for extreme weather events and potential\u000d\u000a      emergencies [B,G].\u000d\u000a    Advancing the application of probabilistic weather forecasting\u000d\u000a        (industry)\u000d\u000a    Professor Smith and CATS have a long history of partnership with industry\u000d\u000a      in embedding the research findings in contexts where real-time forecasting\u000d\u000a      is helpful in managing uncertainty and risk. Particular attention has been\u000d\u000a      given to the challenges of constructing actionable information from raw\u000d\u000a      ensemble weather forecasts and framing this information in a format useful\u000d\u000a      for specific users. The wide range of applications has included: road\u000d\u000a      gritting, food sales, horse race-track conditions, crop forecasting,\u000d\u000a      insurance brokerage, energy trading, wind power production, hydrocarbon\u000d\u000a      exploration and production (oil reservoirs and flows), and electricity\u000d\u000a      generation. About one-third of Professor Smith's students and postdocs now\u000d\u000a      continue this applied work in organisations such as the Bank of England,\u000d\u000a      Royal Dutch Shell, the Met Office and Risk Management Solutions. Current\u000d\u000a      CATS partnerships are under way with the UK Department of Energy and\u000d\u000a      Climate Change (DECC) and the Royal National Lifeboat Institute.\u000d\u000a    The application impact has been most obvious in the energy sector. Altalo\u000d\u000a      and Smith (2002) \"promoted the use of probabilistic weather forecasts in\u000d\u000a      business, thus increasing the return on investments in weather forecasting\u000d\u000a      and reducing energy production costs. The benefit of this work is still\u000d\u000a      felt today, which sees the energy sector (producers and energy traders) as\u000d\u000a      one of the main users of weather forecasts\" [A]. More specifically, work\u000d\u000a      was done with EDF Energy to develop and implement improved methods for\u000d\u000a      electricity demand forecasting, which helped to reduce risk, avoid u-turn\u000d\u000a      trades, manage supply and improve performance. Metra, the global\u000d\u000a      commercial arm of the New Zealand meteorological service, also partnered\u000d\u000a      with CATS on the design and marketing of a product called Vantage to help\u000d\u000a      energy managers and traders in managing weather-related opportunity and\u000d\u000a      risk and in making operational decisions [H], which has led to a new\u000d\u000a      generation of Metra products and services in use worldwide [I].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research by Professor Leonard Smith and the LSE Centre for the Analysis\u000d\u000a      of Time Series (CATS) on forecasting in non-linear and often chaotic\u000d\u000a      systems, with particular attention to weather, has led to advances in\u000d\u000a      three areas: 1) national and international weather industry products and\u000d\u000a      services that are built upon state-of-the-art research and knowledge, 2)\u000d\u000a      dissemination of state-of-the-art practice in forecast production and\u000d\u000a      verification to national, regional and local weather centres around the\u000d\u000a      world, and 3) the introduction of, and new applications in,\u000d\u000a      state-of-the-art forecasting methods in industries facing high uncertainty\u000d\u000a      and risk, e.g. insurance and energy.\u000d\u000a    ","ImpactType":"Environmental","Institution":"\u000d\u000a    London School of Economics and Political Science\u000d\u000a    ","Institutions":[{"AlternativeName":"London School of Economics and Political Science","InstitutionName":"London School of Economics & Political Science","PeerGroup":"C","Region":"London","UKPRN":10004063}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1. MS Roulston &amp; LA Smith (2002) Evaluating probabilistic forecasts\u000d\u000a      using information theory, Monthly Weather Review 130 6: 1653. DOI:\u000d\u000a      10.1175\/1520\u000d\u000a    \u000a\u000a2. MS Roulston, DT Kaplan, J Hardenberg &amp; LA Smith (2003) Using\u000d\u000a      Medium Range Weather Forecasts to Improve the Value of Wind Energy\u000d\u000a      Production, Renewable Energy 29 (4) April 585.DOI:\u000d\u000a      10.1016\/S0960-1481(02)00054-X\u000d\u000a    \u000a\u000a3. MS Roulston, J Ellepola &amp; LA Smith (2005) Forecasting Wave Height\u000d\u000a      Probabilities with Numerical Weather Prediction Models, Ocean\u000d\u000a        Engineering 32 (14-15), 1841. DOI:10.1016\/j.oceaneng.2004.11.012\u000d\u000a    \u000a\u000a4. J Br&#246;cker &amp; LA Smith (2007) Scoring Probabilistic Forecasts: The\u000d\u000a      Importance of Being Proper Weather and Forecasting, 22 (2), 382.\u000d\u000a      DOI: 10.1175\/WAF966.1\u000d\u000a    \u000a\u000a5. J Br&#246;cker &amp; LA Smith (2008) From Ensemble Forecasts to Predictive\u000d\u000a      Distribution Functions Tellus A 60(4): 663. DOI:\u000d\u000a      10.1111\/j.1600-0870.2008.00333.x\u000d\u000a    \u000a\u000a6. R Hagedorn &amp; LA Smith (2009) Communicating the value of\u000d\u000a      probabilistic forecasts with weather roulette. Meteorological\u000d\u000a        Applications 16 (2): 143. DOI: 10.1002\/met.92\u000d\u000a    \u000aEvidence of quality: Publications appeared in peer-reviewed\u000d\u000a      journals; Professor Smith received the FitzRoy Prize of the Royal\u000d\u000a      Meteorological Society for distinguished work in applied meteorology in\u000d\u000a      2002; and the research received grant funding that included:\u000d\u000a    &#8226; Direct &amp; Inverse Modelling in End-to-End Environmental Estimation\u000d\u000a      (DIME) EPSRC, 2003-2005 (&#163;94,360), GR\/R92363\/01.\u000d\u000a    &#8226; Real-time Modelling of Nonlinear Datastreams (REMIND). EPSRC, 2003-2005. (&#163;85,827, plus industrial in-kind support from National Grid Company\u000d\u000a      and Intertec), GR\/R92271\/01\u000d\u000a    &#8226; Nonlinear Analysis &amp; Prediction Statistics from Timeseries &amp;\u000d\u000a      Ensemble-forecast Realizations (NAPSTER) NERC, 2005 - 2008. (&#163;152,481),\u000d\u000a      NE\/D00120X\/1.\u000d\u000a    &#8226; ENSEMBLE-based Predictions of Climate Changes and their Impacts\u000d\u000a      (ENSEMBLES), EU FP6, 2004-2009. (&#163;112,926), GOCE-CT-2003-505539.\u000d\u000a    &#8226; Towards Identifying and Increasing the Socio-Economic Value of\u000d\u000a      High-Impact Weather Forecasts, US NOAA, 2003-2004 (&#163;94,538).\u000d\u000a    &#8226; BIOS RPI Grant for evaluation of seasonal forecasts for the insurance\u000d\u000a      industry.($50k) 2012.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"4","Level2":"1","Subject":"Atmospheric Sciences"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a    All Sources listed below can also be seen at: https:\/\/apps.lse.ac.uk\/impact\/case-study\/view\/4\u000d\u000a    A. Letter from Head of Predictability Division, ECMWF. This source is\u000d\u000a      confidential.\u000d\u000a    B. Letter from UK Met Office. This source is confidential.\u000d\u000a    C. Letter from Chief Climate Scientist, IRI. This source is confidential.\u000d\u000a    D. Letter from former manager of BIOS Risk Prediction Initiative. This\u000d\u000a      source is confidential.\u000d\u000a    E. ECMWF training: https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1573\u000d\u000a    F. IRI Guidance on Verification of Seasonal Climate Forecasts\u000d\u000a      https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1575\u000d\u000a    G. UK Met Office 3-month outlook (as example of impact):\u000d\u000a      https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1576\u000d\u000a    H. Metra Vantage marketing brochure https:\/\/apps.lse.ac.uk\/impact\/download\/file\/1577\u000d\u000a    I. Letter from Business Services Manager, Metra. This source is\u000d\u000a      confidential.\u000d\u000a    ","Title":"\u000d\u000a    Better risk management through improved weather forecasting\u000d\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Research Insights and Outputs: From its beginnings in 2002, CATS\u000d\u000a      has advanced the generation and interpretation of state-of-the-art weather\u000d\u000a      forecasts in support of decision-making. Professor Leonard Smith's\u000d\u000a      research over the past 25 years has focused on forecasting nonlinear,\u000d\u000a      often chaotic, systems, including weather forecasting. In 2003, Professor\u000d\u000a      Smith was awarded two of the ten Department of Trade and Industry Faraday\u000d\u000a      Maths grants (REMIND and DIME). One focused on improving the\u000d\u000a      interpretation and evaluation of ensembles of simulations, the other on\u000d\u000a      their application in a variety of industrial settings. Modern operational\u000d\u000a      weather forecasts take an ensemble (Monte Carlo) approach to account for\u000d\u000a      uncertainty (in the initial condition); scientific, statistical and\u000d\u000a      philosophical questions still remain as to how to interpret an ensemble of\u000d\u000a      model-trajectories as a probability forecast for the future of the system.\u000d\u000a      The research involved translating a sample of about 51 points (the\u000d\u000a      ensemble forecast) in a 10,000,000 dimensional space into information\u000d\u000a      regarding the reliability of a single, high resolution forecast or into a\u000d\u000a      probability forecast for a similar target variable (which was sometimes a\u000d\u000a      nonlinear combination of several meteorological variables). A key aspect\u000d\u000a      is that, since the weather model is nontrivially imperfect, on some days\u000d\u000a      the ensemble is seen, in hindsight, to capture the evolution of the\u000d\u000a      weather, while on other days the model is unable to shadow the events due\u000d\u000a      to model error. The research [5] combined (a) kernel dressing the ensemble\u000d\u000a      members (turning simulations into probabilities), (b) blending with the\u000d\u000a      climatological distribution (a prior based upon historical observations\u000d\u000a      for the relevant phase of the seasonal cycle), (c) an empirically-driven\u000d\u000a      recognition of fundamental limitations for the method itself and (d) input\u000d\u000a      from energy traders in terms of presentation and tolerances, in order to\u000d\u000a      achieve a robust, actionable probabilistic tool.\u000d\u000a    In 2005 the applied aspects of this research were extended under the NERC\u000d\u000a      grant NAPSTER and both theory and application under the EU FP6 ENSEMBLES\u000d\u000a      project. Following IJ Good's work on quantifying skill in the 1950s, CATS\u000d\u000a      [1,4] enabled the improved tuning of models and the communication of\u000d\u000a      probabilistic skill [6], alternatively in \"bits\" for those familiar with\u000d\u000a      information theory and as an \"effective interest rate\" for those more\u000d\u000a      familiar with financial work (traders in the energy sector; managers in\u000d\u000a      the weather sector). This work was performed jointly with Hagedorn of the\u000d\u000a      European Centre for Medium-range Weather Forecasts, which hired Smith as a\u000d\u000a      consultant.\u000d\u000a    The key research insight here was that a sample of simulations from one\u000d\u000a      (or more) imperfect physics-based simulation models can provide more\u000d\u000a      information if they are not treated as a sample drawn from some target\u000d\u000a      probability density function. CATS led research development of kernel\u000d\u000a      dressing of individual ensemble members and blending with the background\u000d\u000a      climatological distribution. While this method is superficially similar to\u000d\u000a      Bayesian Model Averaging techniques in weather forecasting, which appeared\u000d\u000a      shortly after (and cite) the CATS work, the epistemological framework is\u000d\u000a      rather different. The novel use of a prior distribution (the\u000d\u000a      \"climatology\") to lessen the impact of model error was important. Breadth\u000d\u000a      of applicability was shown in additional research papers on off-shore wave\u000d\u000a      height with Royal Dutch Shell [3] and on wind energy production [2].\u000d\u000a    Key researchers: Professor Leonard Smith, Reader\/Professor of\u000d\u000a      Statistics, LSE, March 2000-\u000d\u000a      Dr Jochen Broecker, Postdoctoral Research Officer, LSE March 2003-April\u000d\u000a      2007\u000d\u000a      Dr Liam Clarke, Postdoctoral Research Officer, LSE March 2003-May 2008\u000d\u000a      Dr Renate Hagedorn, Scientific Researcher, ECMWF, ~2000-2011\u000d\u000a      Dr Hailiang Du, Graduate Student\/Postdoctoral Research Officer, LSE,\u000d\u000a      2004-present\u000d\u000a    "},{"CaseStudyId":"35275","Continent":[{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6251999","Name":"Canada"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Engineering and Physical Sciences Research Council","Natural Environment Research Council","Royal Society"],"ImpactDetails":"\u000a    Population dynamics modelling\u000a    Our framework for modelling wildlife population dynamics, as detailed in\u000a      section 2, has been applied to inform a range of real-world management\u000a      scenarios involving multi-million pound industries, including red deer\u000a      (UK), pacific salmon (USA) and the red grouse-hen harrier system (UK).\u000a      Buckland was invited to participate in a Royal Society of Edinburgh\u000a      inquiry into the future of the Scottish fishing industry; the resulting\u000a      report[S3] made a number of recommendations, several of which\u000a      have since been implemented. However, we focus on applications to grey\u000a      seals, where our methods form the basis for management of most of the\u000a      world's populations. In the UK, grey seals are a controversial\u000a      conservation success story: they were the first mammal given statutory\u000a      protection (in 1914) following historical over-harvesting; numbers have\u000a      increased substantially and now support a large eco-tourism industry, but\u000a      this has also brought conflict with both the fishing and the fish-farming\u000a      industries. Management is led by an independent panel of scientists\u000a      convened by NERC, the Special Committee on Seals. They meet annually and\u000a      provide management recommendations, as well as answering specific\u000a      questions posed by UK and Scottish government[S4]. Estimates of\u000a      population size, trajectory and other management-relevant parameters come\u000a      from a population dynamics model developed within CREEM, updated annually\u000a      (including throughout 2008-2013) with new survey information. The other\u000a      globally significant population occurs in Eastern Canada; there the\u000a      methods developed for UK seals were adapted by members of CREEM to fit the\u000a      different population dynamics and survey methods. These methods are used\u000a      by the management agency (Canadian Department of Fisheries and Oceans) for\u000a      population assessment, and also to determine sustainable levels of\u000a      harvest, should a commercial harvest for this species be re-started.[S5]\u000a    The Deputy Chief Scientific Adviser and Head of Marine Evidence at Defra\u000a      writes: \"Under the 1970 Conservation of Seals Act, the Natural Environment\u000a      Research Council has a statutory obligation to provide the UK government\u000a      with `...scientific advice on matters related to the management of seal\u000a      populations'. This advice is provided annually by a panel of experts &#8212; the\u000a      Special Committee on Seals. A major component of the advice is up-to-date\u000a      information on the size and distribution of UK seal populations &#8212;\u000a      information provided each year by the University of St Andrews Sea Mammal\u000a      Research Unit in collaboration with CREEM. The Bayesian state-space\u000a      modelling methods developed by CREEM ... are instrumental in providing an\u000a      estimate of total population size from annual survey data. They represent\u000a      the state-of-the art in the field ... Outputs from the models are viewed\u000a      with confidence by all stakeholders and in our view are a unique and\u000a      integral component of the advice to the Scottish Executive Environment and\u000a      Rural Affairs Department (SEERAD) and the Department for Environment Food\u000a      and Rural Affairs (Defra). Overall the advanced population dynamics\u000a      modelling methods developed at CREEM have made a very considerable\u000a      contribution to Defra's ability to determine the population status of UK\u000a      grey seal populations, and to quantify uncertainty in these\u000a      determinations. This has, in turn, contributed to assessing `Favourable\u000a      Conservation Status' for important seal populations &#8212; an EU requirement\u000a      under the Habitats Directive.\" [S1]\u000a    State-space models are being used with increasing frequency to\u000a      characterise the population dynamics of salmon, delta smelt, and other\u000a      fish species in the western United States, and to provide guidance for\u000a      assessing the effects of management actions. Methods developed at St\u000a      Andrews have allowed more realistic, and hence more reliable, modelling to\u000a      be conducted. The US Fish and Wildlife Service used our methods in\u000a      2008-2010 to develop improved life cycle models for Chinook salmon, and to\u000a      assess the effects of management actions (particularly the effects of\u000a      water exports, and reductions in these exports) on delta smelt populations[S6].\u000a    Monitoring the impact of renewable energy developments\u000a    Our spatial modelling (and associated) methods have had particular impact\u000a      within the marine renewables industry. Offshore wind, tidal and wave\u000a      energy is intended to produce 20% of UK electricity by 2020. However, the\u000a      development and operation of energy installations has the potential to\u000a      impact wild animal populations in the area, and developers are required to\u000a      conduct environmental assessments as part of the permitting process, as\u000a      well as ongoing monitoring. We have formulated UK-wide acceptable practice\u000a      for survey design and analysis in this area based on work commissioned by\u000a      Marine Scotland. We have also advised government regulators, advisory\u000a      bodies, energy development companies and environmental consultants. We\u000a      delivered a half-day workshop to representatives of the windfarm industry\u000a      in London in November 2010, developed an EPSRC-funded 4-day workshop on\u000a      impact assessment in offshore renewable energy development in June 2011,\u000a      attended by 33 individuals, and offered a training workshop in St Andrews\u000a      in September 2013, attended by 30 individuals. Attendees represent\u000a      regulators (e.g. Marine Scotland, JNCC, SNH), conservation bodies (e.g.\u000a      RSPB, BTO), consultancy companies and power companies.\u000a    The influence of our work on decisions of whether to license offshore\u000a      renewable energy developments is indicated in a letter from the Marine\u000a      Renewable Energy Programme Manager at Marine Scotland (Scottish Government\u000a      body), which states[S2]: `We scrutinise licence applications\u000a      for evidence that energy developers ... have provided robust estimates of\u000a      abundance of seabirds and\/or cetaceans. Marine Scotland commissioned CREEM\u000a      to provide a guidance document on best practice for the design and\u000a      analysis of baseline surveys of the distributions of birds and mammals and\u000a      subsequent environmental impact assessments ... of marine renewable energy\u000a      developments. As a result, CREEM-based research outputs now form a central\u000a      part of the recommended statistical analysis for impact assessment in the\u000a      Scottish marine renewables sector.... We consider that the CREEM group is\u000a      an authoritative source of advice on marine survey and data analysis in\u000a      support of renewable energy developments .... Robust data analysis is\u000a      providing sound foundations for both licensing decisions and for the\u000a      definition of impact monitoring programmes.'\u000a    Two UK companies use methods, developed in collaboration with us during\u000a      2008-2010, for surveying seabirds using high-resolution imagery: HiDef\u000a      (who use high-resolution video) and APEM (who use high-resolution stills).\u000a      Both companies now routinely use the methods to quantify seabird abundance\u000a      in areas proposed for large-scale offshore wind farms. Thaxter and Burton[S7]\u000a      report on the Carmarthen Bay study, designed and analysed by us, and in\u000a      which both companies participated, together with WWT Consulting, to\u000a      compare and evaluate different survey methodologies.\u000a    ","ImpactSummary":"\u000a    Researchers at the University of St Andrews have changed the way\u000a      environmental monitoring and impact assessment data are collected and\u000a      analysed, particularly in the marine environment. We have developed new\u000a      statistical models of wildlife population dynamics that, for example, form\u000a      the basis for population assessment of most of the world's grey seals,\u000a      allowing the UK and Canadian governments to implement effective management\u000a      of the populations. Other research carried out by us has led to\u000a      reformulation of the recommended standard statistical practice for impact\u000a      assessment in the UK marine renewables industry, enabling marine\u000a      regulators such as Marine Scotland to make better-informed licensing\u000a      decisions concerning large-scale offshore renewable energy developments.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University of St Andrews\u000a    ","Institutions":[{"AlternativeName":"St Andrews (University of)","InstitutionName":"University of St Andrews","PeerGroup":"B","Region":"Scotland","UKPRN":10007803}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2643743","Name":"London"}],"References":"\u000a    \u000a1Buckland, S.T., Newman, K.B., Fern&#225;ndez, C., Thomas, L. and\u000a      Harwood, J. 2007. Embedding population dynamics models in inference. Statistical\u000a        Science 22, 44-58. DOI: 10.1214\/088342306000000673.\u000a    \u000aThis output was submitted to RAE2008 under UoA22, for which the unit\u000a      scored 2.65 overall for publications, with 95% of outputs scored at 2* or\u000a      greater.\u000a    \u000a2Newman, K.B., Fern&#225;ndez, C., Thomas, L. and Buckland, S.T.\u000a      2009. Monte Carlo inference for state-space models of wild animal\u000a      populations. Biometrics 65, 572-583. DOI: 10.1111\/j.1541-0420.2008.01073.x\u000a    \u000a\u000a3King, R., Morgan, B.J.T., Gimenez, O. and Brooks, S.P. 2010.\u000a      Bayesian Analysis for Population Ecology. CRC Press, Boca Raton.\u000a      ISBN: 9781439811870. Available from the University library.\u000a    \u000a\u000a4Scott Hayward, L.A.S., MacKenzie, M.L., Donovan, C.R.,\u000a      Walker, C.G. and Ashe, E. 2013. Complex Region Spatial Smoother (CReSS).\u000a      Journal of Computational and Graphical Statistics. DOI: 10.1080\/10618600.2012.762920.\u000a      Posted online 23 Jan 2013.\u000a    \u000a\u000a5Buckland, S.T., Burt, M.L., Rexstad, E.A., Mellor, M.,\u000a      Williams, A.E. and Woodward, R. 2012. Aerial surveys of seabirds: the\u000a      advent of digital methods. J. App. Ecol., 49, 960-967.\u000a      DOI: 10.1111\/j.1365-2664.2012.02150.x.\u000a    \u000a\u000a6Marques, T.A, L. Thomas, S.W. Martin, D.K. Mellinger, J.A.\u000a      Ward, D.J. Moretti, D. Harris and P.L. Tyack. 2013. Estimating animal\u000a      population density using passive acoustics. Biological Reviews 88,\u000a        287-309. DOI: 10.1111\/brv.12001.\u000a    \u000aOutputs 2, 3 and 4 best indicate the quality of the research.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    [S1]Letter on file from the Deputy Chief Scientific Adviser to\u000a      Defra.\u000a    [S2]Letter on file from the Marine Renewable Energy Programme\u000a      Manager at Marine Scotland.\u000a    [S3]RSE press release. 2014. Independent inquiry makes key\u000a      recommendations for the sustainable future of the Scottish fishing\u000a      industry. See\u000a      http:\/\/www.royalsoced.org.uk\/134_IndependentInquirymakeskeyrecommendationsfortheSustainableFutureoftheScottishFishingIndustry.html\u000a    This press release clarifies the importance of the inquiry\u000a      recommendations for the future of the Scottish Fishing industry.\u000a    [S4]Special Committee on Seals. 2012. Scientific advice on\u000a      matters related to the management of seal populations: 2012. See http:\/\/www.scotland.gov.uk\/Topics\/marine\/marine-\u000a        environment\/species\/19887\/20814\/22139 for information on SCOS and\u000a\u0009\u0009http:\/\/www.smru.st-andrews.ac.uk\/documents\/1199.pdf for the 2012 report.\u000a    Confirms contribution of our modelling in shaping advice to the UK and\u000a      Scottish governments.\u000a    [S5]Department of Fisheries and Oceans. 2011. Stock assessment\u000a      of Northwest Atlantic grey seals (Halichoerus grypus). DFO Can.\u000a      Sci. Advis. Sec. Sci. Advis. Rep. 2010\/091. See\u000a      http:\/\/www.dfo-mpo.gc.ca\/CSAS\/Csas\/publications\/sar-as\/2010\/2010_091_e.pdf\u000a    Confirms contribution of our modelling in shaping advice to the Canadian\u000a      government.\u000a    [S6]Maunder, M.N., and Deriso, R.B. 2011. A state-space\u000a      multistage life cycle model to evaluate population impacts in the presence\u000a      of density dependence: illustrated with application to delta smelt. Can.\u000a        J. Fish. Aquat. Sci. 068, 1285-1306. DOI: 10.1139\/F2011-071.\u000a    Confirms use of our methods for assessing delta smout populations in\u000a      California.\u000a    [S7]Thaxter, C.B. and Burton, N.H.K. 2009. High Definition\u000a      Imagery for Surveying Seabirds and Marine Mammals: A Review of Recent\u000a      Trials and Development of Protocols. COWRIE\/BTO report, available at\u000a      http:\/\/www.coastalkent.net\/data\/news\/downloads\/COWRIE%20High%20Definition%20Imagery%20\u000a       Final%20Report%2020091130.pdf.\u000a    Confirms our input to methods adopted by HiDef and APEM.\u000a    ","Title":"\u000a    New statistical methods result in better marine environmental\u000a        monitoring and impact assessment\u000a    ","UKLocation":[{"GeoNamesId":"2650225","Name":"Edinburgh"}],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    At the University of St Andrews, the Centre for Research into Ecological\u000a      and Environmental Modelling (CREEM) has been at the forefront of\u000a      developing realistic \"hidden process models\" for animal population\u000a      dynamics, which allow the major sources of uncertainty to be incorporated\u000a      in inference (e.g.1,2,3). Work in this area started in 1993,\u000a      when Prof ST Buckland was appointed. This resulted in deer management\u000a      models, developed in collaboration with BioSS and the Macaulay Land Use\u000a      Research Inst (now the James Hutton Inst), used to guide Scottish deer\u000a      managers on culling levels. Dr KB Newman (Senior Lecturer, 2001-06, now US\u000a      Fish and Wildlife Service) brought considerable expertise in this field,\u000a      and at the same time, Dr L Thomas (Reader, appointed 1997) first became\u000a      involved in modelling the dynamics of grey seal populations in work\u000a      commissioned by Defra.\u000a    Prior to our work, realistic population models could be built, but not\u000a      fitted to data in a rigorous manner; alternatively, models could be fitted\u000a      to data, but they were necessarily too simple to be realistic. Our\u000a      framework allows multiple diverse sources of information to be\u000a      incorporated in a consistent manner, including expert opinion, which is\u000a      vital when management decisions must be made about species for which\u000a      little concrete information exists. The models (largely) use Bayesian\u000a      inference; we have developed fitting methods based on Markov chain Monte\u000a      Carlo3 and on particle filtering1,2, and have\u000a      compared the two2. Key challenges that have been overcome,\u000a      after the initial framework was developed, include developing general but\u000a      reasonably fast fitting algorithms, extensions to allow model selection,\u000a      and incorporating animal dispersal.\u000a    Since 2008, we have built a team to develop improved methods for\u000a      assessing marine environmental impact. The key people in the team are Dr\u000a      ML Mackenzie (Lecturer, appointed 2003), Dr EA Rexstad (Research Fellow,\u000a      appointed 2005), Buckland and Thomas.\u000a    Assessment of environmental impact at marine renewable sites involves the\u000a      analysis of survey data to look for evidence of either overall declines or\u000a      redistribution of animals in the area (or both). Therefore, reliable\u000a      quantification of any environmental impacts requires statistically-sound\u000a      surface-fitting methods that accurately describe both the temporal\u000a      magnitude and spatial range of impacts. Challenges include the requirement\u000a      to account for missed animals during the surveying, small sample size,\u000a      poor survey design, and the fact that sites designated for marine\u000a      renewables (such as undersea turbines and wind farms) often have complex\u000a      topography with abrupt local changes in animal density. Motivated by this\u000a      and other applications, a research group led by Mackenzie developed during\u000a      the REF period spatial smoothing methods4 that respect complex\u000a      study region boundaries, being based on geodesic (\"as the animal swims\")\u000a      distance between points, rather than Euclidean distances. These methods\u000a      allow the amount of smoothing to vary spatially, making them more flexible\u000a      than standard approaches.\u000a    Until recently, the only reliable approach for collecting environmental\u000a      impact assessment data was a visual shipboard or aerial survey along\u000a      random transect lines &#8212; both of which are expensive to undertake. We have\u000a      evaluated the use of digital survey methods, in which high-resolution\u000a      digital images are obtained from aircraft flying at higher altitude than\u000a      is possible for visual surveys. These methods are now replacing visual\u000a      survey methods for seabirds affected by offshore wind farm developments5.\u000a      We have also explored the potential contribution of passive acoustics in\u000a      these sites6, which offer greater cost-effectiveness. This work\u000a      has been led by Buckland and Thomas.\u000a    "},{"CaseStudyId":"35276","Continent":[{"GeoNamesId":"6255150","Name":"South America"},{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"3469034","Name":"Brazil"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"2017370","Name":"Russia"}],"Funders":[],"ImpactDetails":"\u000a    The History of\u000a        Mathematics Archive (at http:\/\/www-history.mcs.st-and.ac.uk)\u000a      is arguably the most widely consulted mathematics information resource in\u000a      the world and one of the most accessed scientific websites. It has helped\u000a      to popularise mathematics and science to the public in the UK and\u000a      internationally, as well as to inspire future scientists. Its reach and\u000a      significance have been considerable in stimulating public interest,\u000a      engagement and understanding of mathematics and science across a\u000a      world-wide arena that includes schools, colleges and the general public,\u000a      contributing to a stated aim of government and other public and private\u000a      bodies of raising public awareness of mathematics and science.\u000a    The Archive includes detailed biographies of 2740 mathematicians and over\u000a      2000 other pages of essays and supporting material. It is underpinned by a\u000a      great deal of research including that described in section 2, was produced\u000a      entirely by members of the University of St Andrews School of Mathematics\u000a      and is located on the School's webserver. It is a direct and highly\u000a      visible result of the School's research in the History of Mathematics,\u000a      incorporating both research published in learned journals and material\u000a      researched specifically for the Archive.\u000a    The reach of the web Archive is vast in terms of the number of hits and\u000a      the countries from which they originate. The web\u000a        statistics [3] show typically 2 million pages of downloads (about\u000a      4.5 Gigabytes of information) by about 150,000 users each week\u000a      from 1st January 2008 onwards. For example, there were 11.6 million hits\u000a      during May 2012. Whilst many hits on the site are from UK, USA and Russia,\u000a      in a typical month hits are recorded from virtually every country in the\u000a      world, with India, Brazil and China near the top of the list. A high\u000a      proportion of downloads are from non- university users (e.g. with over 31%\u000a      from .com domains).\u000a    The Archive is widely recognized as a major online educational resource;\u000a      for example, it is included in MERLOT,\u000a      California State University's Multimedia Educational Resource for\u000a        Learning and Online Teaching [4]. A recent UK report History\u000a        of Mathematics in the Higher Education Curriculum [5] by a working\u000a      group set up jointly by the HEA MSOR Network, Mathematical Sciences Strand\u000a      of National STEM Programme &amp; BSHM refers to the Archive as \"A\u000a        large and ever increasing repository of entries on a whole range of\u000a        people and topics within the history of mathematics. An excellent and,\u000a        in terms of search engine ranking, almost inevitable first port of\u000a        call.\" A recent article in MSOR\u000a          Connections states \"There are many valuable resources on\u000a        the history of mathematics, and in particular the MacTutor history of\u000a        mathematics website is widely used by academics and students.\" [6].\u000a      Many leading universities offer courses that depend heavily on the\u000a      Archive, for example Teaching\u000a        Math with a Historical Perspective at Harvard [7] and Mathematics\u000a        and its History at the University of Florida [8]. Moreover, many\u000a      lecturers use the Archive for supporting material to provide historical\u000a      background when delivering mathematics and science courses; see [9] for an\u000a      example of a mathematics\u000a        course website with a direct link to the Archive.\u000a    The Archive is widely used for high school teaching and projects. The\u000a        University of Chicago &amp; Chicago Public Schools Internet Project\u000a      states \"This is the most comprehensive site on the web for the history\u000a        of math, biographies of mathematicians, and famous curves. It can be\u000a        used as a companion piece for research and for helping students\u000a        understand mathematical concepts.\" [10] There are direct links to\u000a      the Archive from the teaching and learning resource web pages of numerous\u000a      high schools across the world, see [11] for one\u000a        of many examples.\u000a    Social networking now plays a role in disseminating the Archive. The\u000a      twitter feed @mathshistory\u000a      mainly provides a daily tweet linking to a mathematician of the day on the\u000a      Archive, and the number of followers has rapidly grown to over 16,000 at\u000a      the time of writing [12].\u000a    The Archive is frequently used as a source for popular scientific\u000a      writers, lecturers and broadcasters. Best-selling writer Ian Stewart\u000a      writes \"As a writer of popular science, specialising in mathematics, I\u000a        have found the MacTutor archive to be of great value when researching or\u000a        checking historical events and people. Since 2008 I have made extensive\u000a        use of it when writing the following 8 books...\" [1]. The Archive is\u000a      cited as a web-reference in many popular science books, e.g. A Very\u000a        Short Introduction to Numbers by Peter Higgins (OUP, 2011). It is a\u000a      resource for media presentations, such as Melvyn Bragg's Radio 4 In\u000a        Our Time series which has featured Roney-Dougal on several occasions\u000a      [13]. Indeed, as a result of the Archive, Research Group members are\u000a      approached virtually every month to advise for radio and television\u000a      programmes [2] as well as for journal and magazine articles.\u000a    MacTutor has received many internet and other awards, including in 2012\u000a      the Comenius Medal of the Societas Comeniana Hungarica (The Hungarian\u000a      Comenius Society) for \"contributions to education particularly for the\u000a        MacTutor History of Mathematics Web Archive\".\u000a    The development of the web Archive is a dynamic and on-going process. It\u000a      is updated in minor or major ways almost daily and additional biographies\u000a      and other material are added regularly, with feedback and input from users\u000a      and mathematicians contributing to its breadth and efficacy. This provides\u000a      the impetus for further research and ensures that the vitality, utility\u000a      and impact of the Archive will continue into the future.\u000a    ","ImpactSummary":"\u000a    The MacTutor History of\u000a        Mathematics Web Archive at the University of St Andrews is one of\u000a      the most accessed resources worldwide for mathematics and its history. The\u000a      archive includes detailed biographies of 2740 mathematicians and over 2000\u000a      other pages of essays on specific topics and supporting material,\u000a      presented in a readily searchable form which engages and informs. It has\u000a      had great influence on popularising and communicating the essence and\u000a      importance of mathematics, inspiring a broad audience across the world, as\u000a      well as being a vast educational resource. The site has sustained an\u000a      average of two million hits per week over the last six years. It has been\u000a      the basis for college courses worldwide and numerous student and school\u000a      projects on mathematics and its history, and it has served as a seminal\u000a      resource for many popular science, reference, and academic books, TV and\u000a      radio broadcasts and lectures. The Archive continues to grow, with new\u000a      material continually being researched and added.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of St Andrews\u000a    ","Institutions":[{"AlternativeName":"St Andrews (University of)","InstitutionName":"University of St Andrews","PeerGroup":"B","Region":"Scotland","UKPRN":10007803}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"4155751","Name":"Florida"}],"References":"\u000a    \u000a[R1] Craik, A. D. D. Calculus and analysis in early 19th-century Britain:\u000a      the work of William Wallace. Historia Math. 26 (1999) 239-267.\u000a      DOI:10.1006\/hmat.1999.2250\u000a    \u000a\u000a[R2] Craik, A. D. D. Geometry versus analysis in early 19th-century\u000a      Scotland: John Leslie, William Wallace, and Thomas Carlyle, Historia\u000a        Math. 27 (2000), 133-163. DOI:10.1006\/hmat.1999.2264\u000a    \u000a\u000a[R3] Craik, A. D. D. &amp; O'Connor, J. J. Some unknown documents\u000a      associated with William Wallace. BSHM Bull., 26 (1) (2011), 17-28.\u000a      DOI:10.1080\/17498430.2010.503555\u000a    \u000a\u000a[R4] *Craik, A. D. D. Polylogarithms, functional equations and more: The\u000a      elusive essays of William Spence (1777-1815). Historia Math.\u000a      Online 18 July 2013. DOI:10.1016\/j.hm.2013.06.002\u000a    \u000a\u000a[R5] *Craik, A. D. D. Mr Hopkins' Men. Cambridge Reform and British\u000a        Mathematics in the 19th Century. Springer-Verlag London, Ltd.,\u000a      London, 2007. xiv+405 pp. ISBN 978-1-84628-791-6. Available from the\u000a      University library. [London Mathematical Society Newsletter review states,\u000a      \"This is an unusual, well-written and well-researched book. So favourably\u000a      has it been received that a paperback edition appeared within just a few\u000a      months of the original publication.\"\u000a    \u000a\u000a[R6] *Craik, A. D. D. The origins of water wave theory. Annu. Rev.\u000a        Fluid Mech. 36 (2004), 1-28. DOI:10.1146\/annurev.fluid.36.050802.122118\u000a      [highly cited paper]\u000a    \u000a*Three publications that best indicate the quality of the research.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"20","Level2":"5","Subject":"Literary Studies"},{"Level1":"21","Level2":"3","Subject":"Historical Studies"}],"Sources":"\u000a    [1] Letter on file from a well-known popular mathematics writer.\u000a      (Corroborates use of the Archive by popular science writers.)\u000a    [2] Request on file from a history editor, BBC, 30 March 2010.\u000a      (Corroborates media requests for information as a result of the Archive.)\u000a    [3] Statistics\u000a        on web usage: http:\/\/turnbull.mcs.st-and.ac.uk\/analog_res.html\u000a      (Corroborates the website access figures quoted.)\u000a    [4]MERLOT\u000a      &#8212; Multimedia Educational Resource for Learning and Online Teaching:\u000a      http:\/\/www.merlot.org\/merlot\/viewMaterial.htm?id=89142\u000a      (Corroborates the use of the Archive as an online higher educational\u000a      resource in the U.S.)\u000a    [5] History\u000a          of Mathematics in the Higher Education Curriculum Report by a\u000a      working group HEAMSOR Network, the Mathematical Sciences Strand of\u000a      National STEM Programme &amp; BSHM, May 2012 http:\/\/mathcentre.ac.uk\/resources\/uploaded\/historyofmaths.pdf\u000a      (Corroborates the visibility and quality of the Archive in Higher\u000a      Education.)\u000a    [6] Mathematical\u000a        Motivators: Using the history of mathematics to enrich the curriculum\u000a      M. McCartney, N.-A. Bradshaw and T. Mann, MSOR Connections,11\u000a      No.2 (2011) 14-16\u000a      http:\/\/journals.heacademy.ac.uk\/doi\/abs\/10.11120\/msor.2011.11020014(Corroborates\u000a      the widespread use of the Archive by students.)\u000a    [7] Teaching\u000a        Math with a Historical Perspective, Harvard University\u000a      http:\/\/www.math.harvard.edu\/~knill\/teaching\/mathe320_2013\/links.html\u000a      (An example of the Archive as a resource for a university history of\u000a      mathematics course.)\u000a    [8] MA6932\u000a        Mathematics and its History, University of Florida\u000a      http:\/\/www.math.ufl.edu\/~kees\/MAT6932History.html\u000a      (An example of the Archive as a resource for a university history of\u000a      mathematics course.)\u000a    [9] Lectures\u000a        on Classical Mechanics, University of Cambridge\u000a      http:\/\/www.damtp.cam.ac.uk\/user\/tong\/dynamics.htm\u000a      (An example of a mathematics course web page with a link to the Archive to\u000a      provide biographical information.)\u000a    [10] University\u000a        of Chicago &amp; Chicago Public Schools Internet Project\u000a      http:\/\/cuip.uchicago.edu\/websift\/math\/mactutorhistory.html(Corroborates\u000a      the use of the Archive as a resource in high school teaching.)\u000a    [11] Christopher\u000a        Columbus High School, Miami, Teaching and learning resource page\u000a      http:\/\/www.columbushs.com\/page.aspx?pid=410\u000a      (An example of a high school web page with a prominent link to the\u000a      Archive.)\u000a    [12] Maths History on Twitter\u000a      https:\/\/twitter.com\/mathshistory\u000a      (Corroborates the popularity of the Archive by twitter users.)\u000a    [13] Radio 4 In Our Time programmes featuring Roney-Dougal: 29\u000a      May 2008 (Probability &#8212; Heads or Tails?), 11 February 2010 (The Unintended\u000a      Consequences of Mathematics),13 January 2011 (Random and Pseudorandom).\u000a      (Corroborates the participation of History Group members in popular\u000a      science broadcasts.)\u000a    ","Title":"\u000a    Improving Public Awareness of Mathematics and its History &#8212; The\u000a        MacTutor History of Mathematics Archive\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    A Research Group in the History of Mathematics was set up in 1995 by\u000a      members of the University of St Andrews' Mathematics Department. Prof Alex\u000a      Craik (1968- retired 2003, Emeritus Professor since 2003), Prof Edmund\u000a      Robertson (1968- retired 2008, Emeritus Professor since 2008) and Dr John\u000a      O'Connor (1970- retired 2010, Honorary Reader since 2010) were founding\u000a      Group members, and more recently the Group has involved Dr Colva\u000a      Roney-Dougal (2003- Senior Lecturer from 2010) and Dr Colin Bleak (2010-\u000a      ). These staff, all researchers in different fields of mathematics, are\u000a      fascinated by the historical side of their subject and the evolution of\u000a      mathematics. Some 6 research students have also made major contributions\u000a      to the Group over the past 10 years. Since 1995 members of the Group have\u000a      published over 30 research papers in highly-regarded peer-reviewed History\u000a      of Mathematics journals, such as the British Society of the History of\u000a      Mathematics Bulletin and Historia Mathematica, as well as invited\u000a      historical papers in mathematics journals, obituaries and many other\u000a      popular articles. Primary sources for the research include mathematical\u000a      papers going back many years, contemporary accounts, and archival material\u000a      from libraries and mathematical societies, for example, minutes of the\u000a      Edinburgh Mathematical Society from 1883 (which are reproduced on the MacTutor History of\u000a        Mathematics Archive) and recently discovered manuscripts of William\u000a      Wallace.\u000a    Much of the research relates to UK, and in particular Scottish,\u000a      mathematicians. For example, since 1999, Craik and O'Connor have authored\u000a      a series of papers including [R1-R3] on William Wallace and his colleagues\u000a      and contemporaries. The paper [R1] discusses the significance of Wallace's\u000a      lengthy articles on `Fluxions' in the early 19th century in the\u000a      Encyclopaedia Britannica and the Edinburgh Encyclopaedia. These include\u000a      the first complete English language account of the calculus using the\u000a      differential notation which is now universally adopted, and which included\u000a      what was, with hindsight, a very perceptive discussion on the nature of\u000a      limits. The role of Wallace and other mathematicians, notably Leslie and\u000a      Carlyle, in the introduction of modern analysis to Britain is examined in\u000a      [R2], and some newly discovered manuscripts of Wallace are presented and\u000a      critically analysed in [R3]. Craik's recent paper [R4] presents\u000a      substantial research over several years on the mathematics of the\u000a      little-known Scottish analyst William Spence, one of the earliest British\u000a      mathematicians to become familiar with continental mathematics at the turn\u000a      of the 19th century. Craik's book `Mr Hopkins' Men' [R5] published in 2007\u000a      is the culmination of research on the many mathematicians taught or\u000a      advised for the Cambridge Mathematical Tripos by William Hopkins in the\u000a      mid-19th century, including Green, Adams, Stokes, Cayley and Kelvin, and\u000a      considers their education and its impact on their later careers and their\u000a      influence on mathematics and the scientific community.\u000a    On the more applied side, Craik's 2004 paper on the origins of water wave\u000a      theory [R6], which describes the contributions of the many mathematicians\u000a      that set the scene for Stokes' work, has attracted 53 citations (WoS)\u000a      mostly from papers on contemporary fluids research.\u000a    Papers by research students supervised by Group staff also range across a\u000a      wide area of history of mathematics. For example, Ian Duncan presents new\u000a      insights into Eddington's search for a theory of quantum gravity, a BSHM\u000a      paper by Elizabeth Lewis considers early work of P. G. Tait, and in\u000a      another BSHM paper Stefanie Eminger examines the importance of the first\u000a      ICMS in Zurich in 1897 as a landmark in international mathematical\u000a      collaboration.\u000a    These are samples from the considerable body of published research by the\u000a      Group that has fed into the Archive. The Group has also undertaken a great\u000a      deal of research specifically for the Archive. Researching new biographies\u000a      and articles and refining existing ones, with input from active\u000a      mathematicians, is an on-going process, to ensure that the material\u000a      reaches the high standards expected by the mathematics and history of\u000a      mathematics communities for dissemination to the wider public.\u000a    "},{"CaseStudyId":"35277","Continent":[{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"1269750","Name":"India"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    GAP is used in universities all over the world to teach courses in\u000d\u000a      mathematics. From user feedback and continued, increasingly versatile use,\u000d\u000a      we know that this is resulting in better teaching, more thorough\u000d\u000a      understanding of the mathematics taught, and to improved learning\u000d\u000a      opportunities for students. It helps students acquire key additional\u000d\u000a      skills applicable to a broad range of chosen career paths. The reach and\u000d\u000a      significance are considerable and below we elaborate on this widespread\u000d\u000a      positive impact on the education of students.\u000d\u000a    GAP is used widely in teaching mathematics. In February 2012, we surveyed\u000d\u000a      the GAP community via the GAP-Forum (a mailing list with 1055 members),\u000d\u000a      about the use of GAP in teaching. From the replies from all over the world\u000d\u000a      (including major universities in Europe, US, South America, Asia and\u000d\u000a      Australia {2}); the results are summarised in our exposition below. Here\u000d\u000a      is a typical statement {2} from a colleague in the academic community (in\u000d\u000a      Montgomery, Alabama, USA):\u000d\u000a    \"GAP has played such a large part in my research and my understanding\u000d\u000a        of group theory. I am forever in debt to its many authors. I take every\u000d\u000a        opportunity to share it with my students.\"\u000d\u000a\u0009\u0009A professor at Colorado\u000d\u000a      State University at Fort Collins in the USA writes {1}:\u000d\u000a    \"I have been using GAP for many years in my undergraduate and graduate\u000d\u000a        classes in algebra and combinatorics [...]. I have found the system an\u000d\u000a        indispensible tool for illustrating phenomena that are beyond simple\u000d\u000a        pencil-and-paper methods [...]. It also has been most useful as a\u000d\u000a        laboratory environment for students to investigate algebraic structures\u000d\u000a        [...]. [T]his first-hand investigation gives students a much better\u000d\u000a        understanding of what these algebraic structures are, and how their\u000d\u000a        elements behave, than they would get by the traditional examples\u000d\u000a        presented in a board-lecture situation. This more concrete approach to\u000d\u000a        abstract algebra has been received well by students, in particular by\u000d\u000a        students whose major (or study aim) has not been graduate school in\u000d\u000a        mathematics.''\u000d\u000a    Our user respondents mention at least 10 different mathematical areas in\u000d\u000a      which they use GAP for teaching, in courses at all levels from second year\u000d\u000a      up to PhD and beyond. For example, in the University of Porto, GAP has\u000d\u000a      been used to teach number theory, computational algebra and cryptography\u000d\u000a      to around 140 students per year since 2009 {2}. Furthermore, there are\u000d\u000a      summer schools for graduate students and PDRs in which GAP was used for\u000d\u000a      the practical exercises ({3}, {4}). For example, the summer school \"Groups\u000d\u000a      Actions Computations 2010\" at the Harish-Chandra Research Institute in\u000d\u000a      Allahabad (India) used GAP for the practical tutorials on computer\u000d\u000a      algebra. In summary, we know from our survey that worldwide hundreds of\u000d\u000a      students every year take part in university courses that use GAP. Some of\u000d\u000a      these activities have been going on for many years (for example in Aachen\u000d\u000a      for 15 years), while many only started in or after 2008, and the numbers\u000d\u000a      have grown year on year. The real numbers are likely to be considerably\u000d\u000a      higher than what our quick survey suggests, and certainly run into many\u000d\u000a      thousands if uses through Sage are counted (see below).\u000d\u000a    GAP complements the traditional methods of teaching in many different\u000d\u000a      ways. Some use it to pose computer exercises to their students, either to\u000d\u000a      look at mathematical examples inaccessible to conventional methods, or to\u000d\u000a      introduce mathematical programming (e.g. in Fort Collins, {5}). Others\u000d\u000a      (e.g. Aachen, Braunschweig, Perth) use GAP to give the students access to\u000d\u000a      large data collections, e.g. the library of finite groups. In many courses\u000d\u000a      the lecturer uses GAP to show non-trivial computations in class. There are\u000d\u000a      mathematical teaching publications relying on GAP (e.g. \"Adventures in\u000d\u000a      Group Theory: Rubik's Cube, Merlin's Machine and Other Mathematical Toys\"\u000d\u000a      by Joyner (2008), \"Contemporary Abstract Algebra\" by Gallian (2010) and\u000d\u000a      \"Representations of Groups: A Computational Approach\" by Lux and Pahlings\u000d\u000a      (2010)), and GAP is integrated into the Sage system, which itself\u000d\u000a      is used extensively in teaching {6}. Sage is a system which\u000d\u000a      endeavours to incorporate as much free mathematical software as possible\u000d\u000a      to create a general purpose tool for mathematicians. This means that\u000d\u000a      students are exposed to GAP throughout their entire higher education.\u000d\u000a    We now proceed to describe different ways in which using GAP is improving\u000d\u000a      and broadening students' education.\u000d\u000a    Using computers in class or for homework vastly increases the size of\u000d\u000a      mathematical structures one is able to analyse. Interesting phenomena\u000d\u000a      often occur only in examples that are beyond hand calculations, which\u000d\u000a      means that, through the use of GAP, students can gain mathematical\u000d\u000a      insights to which they did not have any access hitherto. For example in\u000d\u000a      group theory, with pen and paper one can only practically compute with\u000d\u000a      groups containing some dozens of elements, whereas with the right software\u000d\u000a      handling groups with &gt; 109 elements is easy and convenient.\u000d\u000a      Even if the size of the examples is not too large, the GAP system with its\u000d\u000a      huge data-bases of interesting mathematical structures provides a plethora\u000d\u000a      of examples from which a lecturer can simply select, or indeed quickly run\u000d\u000a      through them automatically. Examples of such collections are: the library\u000d\u000a      of small groups up to the order 2000 (SmallGroups package,\u000d\u000a      published in 2002 by O'Brien (Auckland), Eick (Braunschweig) and Besche\u000d\u000a      (Braunschweig)), the library of all semigroups with up to 10 elements (Smallsemi\u000d\u000a      package, published in 2012 by Distler (PhD student at St Andrews at the\u000d\u000a      publication time) and Mitchell) or the library of primitive groups up to\u000d\u000a      degree 2500 (published in 2005 by Roney-Dougal with the GAP distribution).\u000d\u000a    This ready access to mathematical structures opens up the possibility of\u000d\u000a      a more 'experimental' approach in abstract mathematics. Students can even\u000d\u000a      conduct such experiments themselves, in a lab or at home -- all that is\u000d\u000a      needed is a PC with a free GAP installation. This in turn leads to a more\u000d\u000a      grounded familiarity with the contents, and, more importantly the students\u000d\u000a      develop a different, more independent, attitude towards mathematics. By\u000d\u000a      degrees they learn to come up with their own conjectures, to verify or\u000d\u000a      disprove them; in a word, they begin \"doing\" mathematics, rather than\u000d\u000a      simply \"absorbing\" it, which prepares them for independent creative\u000d\u000a      thinking, problem solving and a career in research or industry.\u000d\u000a    In the course of this activity, students will be led naturally towards an\u000d\u000a      efficient usage of computers and the need to write simple programs. This\u000d\u000a      is clearly a valuable transferable skills in its own right, and can be\u000d\u000a      developed into ability to write correct and efficient programs, and even\u000d\u000a      mathematically prove their correctness.\u000d\u000a    Finally, computational methods in (discrete) mathematics can itself be a\u000d\u000a      subject of study. The open source nature of GAP allows lecturers and\u000d\u000a      students to examine GAP and its algorithms. In particular, students can\u000d\u000a      develop new methods on their own, thereby expanding GAP and contributing\u000d\u000a      to its capabilities. Further down the line, PhD students can become\u000d\u000a      members of the development team, build their own packages, and base\u000d\u000a      publications on these. This is not only good for their education in\u000d\u000a      computational mathematics, but they benefit in a much broader way from\u000d\u000a      gaining expertise in professional software development.\u000d\u000a    In summary, the use of GAP in mathematical teaching improves the\u000d\u000a      education of thousands of students worldwide every year, making them the\u000d\u000a      main beneficiaries. The lecturers themselves also benefit, because the\u000d\u000a      deployment of new teaching methods can enrich their views on both their\u000d\u000a      subject area and education. In particular they can lead their students\u000d\u000a      towards creative mathematical thinking and problem solving. The society\u000d\u000a      benefits from a stream of well qualified professionals who bring together\u000d\u000a      mathematical knowledge, numerical- and analytic-problem solving skills,\u000d\u000a      and broad computational expertise.\u000d\u000a    The impact is significant since these skill-sets are crucial for\u000d\u000a      functioning of our society, and is far-reaching because GAP is freely\u000d\u000a      available and GAP-based instruction takes place in dozens of leading\u000d\u000a      universities worldwide.\u000d\u000a    St Andrews research work is crucial for GAP in all its aspects, and in\u000d\u000a      particular for its educational applications. Each of the thousands of GAP\u000d\u000a      installations worldwide incorporates years of research effort by St\u000d\u000a      Andrews scientists.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The School of Mathematics and Statistics at St Andrews is leading the\u000d\u000a      development and implementation of new efficient algorithms for the GAP\u000d\u000a      (Groups, Algorithms and Programming) free, open-source system for\u000d\u000a      computational discrete algebra. Although it is primarily a research tool,\u000d\u000a      GAP is widely used in education. Therefore lecturers, as well as students\u000d\u000a      in class and beyond, benefit from a whole new range of educational\u000d\u000a      possibilities, for example being able to investigate considerably larger\u000d\u000a      abstract mathematical structures than hitherto. This new, hands-on\u000d\u000a      approach is radically changing the way mathematics is taught in\u000d\u000a      universities worldwide, and is deepening the learning and understanding.\u000d\u000a      The pioneering work of St Andrews researchers has shaped GAP at all levels\u000d\u000a      for 20 years, from discovering and incorporating state-of-the-art\u000d\u000a      algorithms, to its unique design, which is an educational feature in its\u000d\u000a      own right.\u000d\u000a    ","ImpactType":"Societal","Institution":"\u000d\u000a    University of St Andrews\u000d\u000a    ","Institutions":[{"AlternativeName":"St Andrews (University of)","InstitutionName":"University of St Andrews","PeerGroup":"B","Region":"Scotland","UKPRN":10007803}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2945024","Name":"Braunschweig"},{"GeoNamesId":"2063523","Name":"Perth"},{"GeoNamesId":"1278994","Name":"Allahabad"},{"GeoNamesId":"2193733","Name":"Auckland"}],"References":"\u000d\u000a    All mentioned publications have appeared in major mathematical peer\u000d\u000a      reviewed journals or top international computer science conferences, and\u000d\u000a      the research conducted at the University of St Andrews is clearly\u000d\u000a      world-leading in terms of originality, significance and rigour.\u000d\u000a    \u000a[1] S.A. Linton, G. Pfeiffer, E.F. Robertson, and N. Ruskuc. Groups and\u000d\u000a      actions in transformation semigroups. Math. Z., 228(3):435--450, 1998,\u000d\u000a      DOI: 10.1007\/PL00004628.\u000d\u000a    \u000a\u000a[2] S.A. Linton, G. Pfeiffer, E.F. Robertson, and N. Ruskuc. Computing\u000d\u000a      transformation semigroups. J. Symbolic Comput., 33(2):145--162, 2002, DOI:\u000d\u000a      10.1006\/jsco.2000.0406.\u000d\u000a    \u000a\u000a[3] J. Ara&#250;jo, P.V. B&#252;nau, J.D. Mitchell, and M. Neunh&#246;ffer. Computing\u000d\u000a      automorphisms of semigroups. J. Symbolic Comput., 45(3):373--392, 2010,\u000d\u000a      DOI: 10.1016\/j.jsc.2009.10.001.\u000d\u000a    \u000a\u000a[4] M. Neunh&#246;ffer and C.E. Praeger. Computing minimal polynomials of\u000d\u000a      matrices. LMS J. Comput. Math., 11:252--279, 2008, DOI: 10.1112\/S1461157000000590.\u000d\u000a    \u000a\u000a[5] J.F. Carlson, M. Neunh&#246;ffer, and C.M. Roney-Dougal. A polynomial-time\u000d\u000a      reduction algorithm for groups of semilinear or subfield class. J.\u000d\u000a      Algebra, 322(3):613--637, 2009, DOI: 10.1016\/j.jalgebra.2009.04.022.\u000d\u000a    \u000a\u000a[6] R. Behrends, A. Konovalov, S. Linton, F. L&#252;beck, and M. Neunh&#246;ffer.\u000d\u000a      Towards high-performance computational algebra with GAP. In Komei Fukuda,\u000d\u000a      Joris van der Hoeven, Michael Joswig, and Nobuki Takayama, editors, ICMS,\u000d\u000a      volume 6327 of Lecture Notes in Computer Science, pages 58--61. Springer,\u000d\u000a      2010, DOI:10.1007\/978-3-642-15582-6_12.\u000d\u000a    \u000aThe articles [1, 4, 5] indicate the quality of the underpinning research\u000d\u000a      best. Their acceptance in major mathematical journals indicates\u000d\u000a      theoretical depth whilst they at the same time led to an important\u000d\u000a      improvement of the algorithms in GAP and practical computations.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"8","Level2":"3","Subject":"Computer Software"}],"Sources":"\u000d\u000a    {1} Statement from a professor at the Colorado State University at Fort\u000d\u000a      Collins in the USA.\u000d\u000a    {2} Feedback from our survey on the GAP-Forum: 28 emails by colleagues\u000d\u000a      from all over the world, available in a repository at St Andrews for\u000d\u000a      auditing purposes.\u000d\u000a    {3}Summer school \"Groups Actions Computations 2010\" in Allahabad, India.\u000d\u000a      http:\/\/www.icts.res.in\/program\/details\/177\/.\u000d\u000a      This supports the claim that GAP has been used in summer schools.\u000d\u000a    {4} Summer school on computational group theory in Kirchberg\/Hunsr&#252;ck. http:\/\/www.math.rwth-aachen.de\/~Graduiertenkolleg\/schools\/2011\/.\u000d\u000a      This supports the claim that GAP has been used in summer schools.\u000d\u000a    {5} Alexander Hulpke with contributions by Kenneth Monks and Ellen\u000d\u000a      Ziliak: \"Abstract Algebra in GAP\", 2008--2011. http:\/\/www.math.colostate.edu\/~hulpke\/CGT\/howtogap.pdf\u000d\u000a      This shows that GAP has been used for teaching.\u000d\u000a    {6} The Sage Development Team: http:\/\/wiki.sagemath.org\/Teaching_with_SAGE\u000d\u000a      This page underpins the claim that GAP has been used in teaching through\u000d\u000a      SAGE.\u000d\u000a    ","Title":"\u000d\u000a    Innovations in mathematics teaching using GAP\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    GAP (Groups, Algorithms and Programming) is a worldwide collaborative\u000d\u000a      effort in open source software. Over the decades it has grown into a large\u000d\u000a      computer algebra system comprising over 0.6 million lines of code plus\u000d\u000a      another 1.1 million in over 100 contributed packages. Its core\u000d\u000a      functionality lies in computational group and semigroup theory. Since 1997\u000d\u000a      The University of St Andrews has been a leading centre of GAP development,\u000d\u000a      currently one of four world-wide, and the only one in the UK. We host the\u000d\u000a      source code repository for core system and some packages, mailing lists\u000d\u000a      and archives, centralized testing services, issue tracker, release\u000d\u000a      management, as well as the GAP website. In the following, we highlight a\u000d\u000a      few characteristic examples of the many contributions to GAP by\u000d\u000a      researchers in St Andrews.\u000d\u000a    Prof. Steve Linton (from 1994), Dr James Mitchell (from 2004), Prof.\u000d\u000a      Edmund Robertson (retired 2008) and Prof. Nik Ruskuc (from 1995) have\u000d\u000a      played pivotal roles in the invention of computational methods for\u000d\u000a      semigroups (see for example [1, 2, 3]). This research activity has been\u000d\u000a      ongoing at the University of St Andrews since 1997 and has led to\u000d\u000a      efficient implementations in the GAP library as well as in GAP packages\u000d\u000a      (e.g. Monoid and Citrus published in 2012 by Mitchell). Achieving this aim\u000d\u000a      involved mathematical research of various flavours, including setting up\u000d\u000a      underlying theory, the invention and analysis of algorithms and extending\u000d\u000a      to the development of high quality and well-tested implementations with\u000d\u000a      good documentation. Today, every time a user asks GAP a question about a\u000d\u000a      semigroup, these algorithms run in the background, and it is only due to\u000d\u000a      the ongoing efforts of researchers at St Andrews that one can work with\u000d\u000a      semigroups in GAP at all.\u000d\u000a    Computing minimal polynomials is a fundamental problem for matrices. In\u000d\u000a      2008, Prof.Cheryl Praeger (University of Western Australia) and Dr Max\u000d\u000a      Neunh&#246;ffer (University of St Andrews, from 2007) developed [4] and\u000d\u000a      implemented (in the cvec GAP package by Neunh&#246;ffer) a new\u000d\u000a      efficient algorithm for this over finite fields. For example, whenever GAP\u000d\u000a      is asked to compute the order of a matrix, this algorithm is used. Such\u000d\u000a      order computations play an important role for applications in coding\u000d\u000a      theory, cryptography and group theory, which are standard subjects of\u000d\u000a      university teaching.\u000d\u000a    Algorithms for matrix groups over finite fields have been a major\u000d\u000a      research area for the past 20 years. Efficient implementations of\u000d\u000a      algorithms to recognise such groups constructively have only been\u000d\u000a      published in the past few years (e.g. the recog GAP package by\u000d\u000a      Neunh&#246;ffer and Seress (Ohio State, Columbus, USA) was first published in\u000d\u000a      2009). Dr Max Neunh&#246;ffer and Dr Colva Roney-Dougal (from 2002) with J.\u000d\u000a      Carlson (Georgia, Athens, USA), developed an important algorithm for\u000d\u000a      matrix groups which fits into the framework of the recog package [5]. This\u000d\u000a      is now used every time a GAP user works with a matrix group. Most of the\u000d\u000a      work for this was done by members of staff at the University of St\u000d\u000a      Andrews.\u000d\u000a    Since 2009, the EPSRC-funded HPCGAP project has been developing the GAP\u000d\u000a      system into a parallel programming platform for discrete mathematics [6].\u000d\u000a      The GAP development within this project is done entirely by University of\u000d\u000a      St Andrews staff (Dr Reimer Behrends (from 2009), Dr Vladimir Janjic (from\u000d\u000a      2010), Dr Alexander Konovalov (from 2007), Prof. Steve Linton and Dr Max\u000d\u000a      Neunh&#246;ffer). Public alpha releases were published in 2011, 2012 and August\u000d\u000a      2013. This new parallel version maintains GAP's status as a\u000d\u000a      state-of-the-art system and adapts it to modern hardware. Its advent\u000d\u000a      enables lecturers to introduce students to the paradigm of parallel\u000d\u000a      programming in discrete mathematics.\u000d\u000a    "},{"CaseStudyId":"35278","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6251999","Name":"Canada"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The distance sampling methods developed at the University of St Andrews\u000d\u000a      are widely used for\u000d\u000a      managing the catch or cull of natural resources (e.g. fisheries, deer),\u000d\u000a      for monitoring the status of\u000d\u000a      populations of conservation concern (e.g. elephants, apes, whales, polar\u000d\u000a      bears), and for routine,\u000d\u000a      often statutory, monitoring (e.g. UK Breeding Bird Survey, which feeds\u000d\u000a      into the Wild Bird Indicator,\u000d\u000a      one of 15 headline Quality of Life Indicators adopted by Defra).\u000d\u000a      Organisations that have\u000d\u000a      sponsored the development of our software Distance include the US\u000d\u000a      Navy (US$455K during 2008-13),\u000d\u000a      and, prior to 2008, the US Office of Naval Research, US National Park\u000d\u000a      Service, Fisheries and\u000d\u000a      Oceans Canada, and the Wildlife Conservation Society. Distance is\u000d\u000a      the global industry standard\u000d\u000a      with over 30,000 registered users from around 115 countries.[S6]\u000d\u000a    The Distance team at St Andrews has disseminated its work through a\u000d\u000a      multi-pronged strategy:\u000d\u000a      1. Publish methodological developments in top journals. 2. Publish\u000d\u000a      introductory and advanced\u000d\u000a      texts. 3. Provide user-friendly software Distance. 4. Provide\u000d\u000a      training workshops (42 workshops\u000d\u000a      during 1999-2012, attended by 833 participants &#8212; mostly non-academic &#8212;\u000d\u000a      from 73 countries). 5.\u000d\u000a      Publish papers in ecology and taxon-specific journals, targeting wildlife\u000d\u000a      managers, that promote\u000d\u000a      best practice, e.g. for dung and nest surveys (primarily used to assess\u000d\u000a      deer and ape populations\u000d\u000a      respectively, two papers in J App Ecol), primate surveys (two papers in\u000d\u000a      Int J Primatology), bird\u000d\u000a      surveys (two papers in The Auk and one in Bird Conservation\u000d\u000a      International), whale surveys in\u000d\u000a      geographically-complex regions (J Cetacean Res &amp; Mgt), aerial surveys\u000d\u000a      of seabirds (J App Ecol),\u000d\u000a      and acoustic surveys (Biological Reviews).\u000d\u000a    As a result of this extensive dissemination, our Distance\u000d\u000a      software and distance sampling methods\u000d\u000a      are used for a wide range of surveys. In an editorial in J Appl Ecol[S7],\u000d\u000a      the large number of citations\u000d\u000a      to the paper describing Distance software (Thomas et al., 2010) is\u000d\u000a      noted, and the editors state:\u000d\u000a      \"This academic impact is likely to translate into improved assessment of\u000d\u000a      population densities by\u000d\u000a      scientists worldwide and thence to better management decision-making.\"\u000d\u000a      Here, we list just a few\u000d\u000a      surveys that use our methods and software. For cetaceans, these include\u000d\u000a      ongoing cetacean\u000d\u000a      surveys conducted by NOAA in North America (e.g.[S8]) and under\u000d\u000a      the auspices of the International\u000d\u000a      Whaling Commission. Examples of surveys of endangered populations, for\u000d\u000a      which abundance\u000d\u000a      estimates are needed both to assess the risk of extinction and to monitor\u000d\u000a      the success or otherwise\u000d\u000a      of management action, and for which we developed tailor-made methods,\u000d\u000a      include cotton-top\u000d\u000a      tamarins (first large-scale surveys, results published in Nature\u000d\u000a        Communications in 2010); passive\u000d\u000a      acoustic surveys of North Pacific right whales (published in Endangered\u000d\u000a        Species Research in\u000d\u000a      2011); and Key Largo woodrat surveys (published in Methods in Ecology\u000d\u000a        and Evolution in 2012).\u000d\u000a      Large-scale terrestrial surveys include the Pan Africa Great Ape Program\u000d\u000a      (launched in 2010) and\u000d\u000a      the ongoing Monitoring the Illegal Killing of Elephants Project. In the\u000d\u000a      UK, the ongoing national\u000d\u000a      Breeding Bird Survey (http:\/\/www.bto.org\/volunteer-surveys\/bbs)\u000d\u000a      is analysed using our methods[S9].\u000d\u000a    Acoustic distance sampling methods are beginning to see wide use &#8212; e.g.\u000d\u000a      the US$1.5m DECAF\u000d\u000a      project (completed 2011) was jointly funded by the US government\u000d\u000a      environmental regulation\u000d\u000a      agency NOAA and by the International Association of Oil and Gas\u000d\u000a      Industries, as the methods are\u000d\u000a      needed for monitoring seismic exploration and oil production fields. Our\u000d\u000a      methods also form the\u000d\u000a      basis of the &#8364;4.2 million EU-Life funded SAMBAH project (started 2010),\u000d\u000a      which aims to use a grid\u000d\u000a      of 300 static acoustic monitoring devices to estimate, for the first time,\u000d\u000a      density and distribution of\u000d\u000a      the endangered Baltic harbour porpoise population.\u000d\u000a    The US Office of Naval Research has sponsored the Distance\u000d\u000a      software, and continues to fund the\u000d\u000a      development of acoustic survey methods. The Head, Marine Science Branch,\u000d\u000a      Energy and\u000d\u000a      Environmental Readiness Division, US Navy[S1] comments: \"The\u000d\u000a      CREEM group's work on survey\u000d\u000a      design and analysis has found widespread application in addressing\u000d\u000a      important research and\u000d\u000a      environmental stewardship issues by several US federal Government\u000d\u000a      agencies, including the Navy... a sign of the strength and merit of Distance\u000d\u000a      is the adaptability of distance methods to the\u000d\u000a      assessment of environmental risk from a wide range of human activities,\u000d\u000a      including naval training\u000d\u000a      and exercise. ... The CREEM group's clever and innovative adaptations of\u000d\u000a      distance methods to\u000d\u000a      passive acoustic sensing has opened an entirely new and highly exciting\u000d\u000a      field of research and\u000d\u000a      environmental monitoring that will pay huge dividends for decades to come.\u000d\u000a      ... Thank you for this\u000d\u000a      opportunity to document the tremendous impacts that distance methods and\u000d\u000a      the combined\u000d\u000a      expertise of the CREEM group have had on the way the US Navy, and many\u000d\u000a      others, now address\u000d\u000a      their environmental stewardship responsibilities ...\"\u000d\u000a    The Chief Science Advisor and Director of Scientific Programs at the US\u000d\u000a      National Marine Fisheries\u000d\u000a      Service (NOAA)[S2] confirms the importance of our work in\u000d\u000a      enabling them to complete mandatory\u000d\u000a      assessments: \"Under the US Marine Mammal Protection Act, NOAA Fisheries is\u000d\u000a      mandated to\u000d\u000a      maintain marine mammal populations ... You ... have had a profound impact\u000d\u000a      on our ability to fulfil\u000d\u000a      our mandates through your research, software development and support, and\u000d\u000a      training. The\u000d\u000a      software Distance is used throughout our organization ... Your\u000d\u000a      research on acoustic applications of\u000d\u000a      distance sampling and double-observer surveys has been particularly\u000d\u000a      important ...\"\u000d\u000a    The Head of Science, International Whaling Commission[S3],\u000d\u000a      confirms the impact of our work on the\u000d\u000a      conservation and management of cetacean populations: \"Key developments by\u000d\u000a      CREEM\u000d\u000a      scientists, together with incorporation of these developments into later\u000d\u000a      versions of your software,\u000d\u000a      have ensured that abundance estimation for most stocks is now relatively\u000d\u000a      uncontroversial. ... the\u000d\u000a      work of CREEM on matters related to cetacean abundance estimation using\u000d\u000a      distance sampling\u000d\u000a      techniques has been of immeasurable value to our work and cetacean\u000d\u000a      conservation. The\u000d\u000a      theoretical and practical developments that have arisen from CREEM\u000d\u000a      scientists represent a\u000d\u000a      remarkable degree of innovation from a single group. In my opinion this is\u000d\u000a      unrivalled by any other\u000d\u000a      group working in the field. The impact on the conservation and management\u000d\u000a      has been profound\u000d\u000a      and I look forward to continued collaboration between us in the future.\"\u000d\u000a    The Director, Conservation Support at the Wildlife Conservation Society[S4],\u000d\u000a      notes that they use our\u000d\u000a      methods and software to assess diverse populations, including elephants,\u000d\u000a      great apes and other\u000d\u000a      species at risk of poaching in Central Africa, primates, ungulates and\u000d\u000a      cranes in Asia, and\u000d\u000a      cetaceans in Africa and Asia. He concludes: \"We pride ourselves in using\u000d\u000a      rigorous science to\u000d\u000a      inform our conservation work. The continuously improving wildlife\u000d\u000a      estimation techniques and\u000d\u000a      associated software that results from the research done by you and your\u000d\u000a      colleagues at St Andrews\u000d\u000a      helps us to do this well.\"\u000d\u000a    The Scientific Secretary, North Atlantic Marine Mammal Commission[S5],\u000d\u000a      comments: \"... work\u000d\u000a      carried out at CREEM has had a significant impact on the efficiency of\u000d\u000a      stock management within\u000d\u000a      the NAMMCO countries. Reliable and improved methods for providing\u000d\u000a      estimates of abundance ...\u000d\u000a      form the essential tool using which NAMMCO scientists provide management\u000d\u000a      advice on the stocks\u000d\u000a      under NAMMCO jurisdiction.\"\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Reliable estimates of the size of natural populations are required by\u000d\u000a      national and regional\u000d\u000a      governments for management and conservation, by international commissions\u000d\u000a      that manage natural\u000d\u000a      resources, and by NGOs. Distance sampling, in which distances of animals\u000d\u000a      from a line or point are\u000d\u000a      sampled, is the most widely-applicable technique for obtaining such\u000d\u000a      estimates. Statisticians at St\u000d\u000a      Andrews are the acknowledged world-leaders in the development and\u000d\u000a      dissemination of distance\u000d\u000a      sampling survey methods. Their software Distance is the industry\u000d\u000a      standard and has over 30,000\u000d\u000a      registered users from around 115 countries. The methodological\u000d\u000a      developments and associated\u000d\u000a      software have allowed better-informed decisions to be made in the\u000d\u000a      management and conservation\u000d\u000a      of populations as diverse as whales, seals, fish, elephants, apes, deer,\u000d\u000a      birds, ants, trees and\u000d\u000a      flowering plants.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of St Andrews\u000d\u000a    ","Institutions":[{"AlternativeName":"St Andrews (University of)","InstitutionName":"University of St Andrews","PeerGroup":"B","Region":"Scotland","UKPRN":10007803}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1Thomas, L., Buckland, S.T., Rexstad, E.A., Laake, J.L.,\u000d\u000a      Strindberg, S., Hedley, S.L., Bishop,\u000d\u000a      J.R.B., Marques, T.A. and Burnham, K.P. 2010. Distance software:\u000d\u000a      design and analysis of\u000d\u000a      distance sampling surveys for estimating population size. J. App. Ecol.\u000d\u000a      47, 5-14.\u000d\u000a      378 citations in Google Scholar (August 2013). DOI: 10.1111\/j.1365-2664.2009.01737.x\u000d\u000a    \u000a\u000a2Buckland, S.T., Anderson, D.R., Burnham, K.P., Laake, J.L.,\u000d\u000a      Borchers, D.L. and Thomas, L. (eds)\u000d\u000a      2004. Advanced Distance Sampling. Oxford University Press, Oxford.\u000d\u000a      419 citations in Google\u000d\u000a      Scholar (August 2013); the 2001 introductory book had 2323 citations.\u000d\u000a    \u000a\u000a3Hedley, S.L. and Buckland, S.T. 2004. Spatial models for line\u000d\u000a      transect sampling. Journal of\u000d\u000a        Agricultural, Biological and Environmental Statistics 9,\u000d\u000a      181-199. DOI: 10.1198\/1085711043578\u000d\u000a      Selected as best JABES paper, 2004-05.\u000d\u000a    \u000a\u000a4Borchers, D.L., Laake, J.L., Southwell, C. and Paxton, C.G.M.\u000d\u000a      2006. Accommodating unmodeled\u000d\u000a      heterogeneity in double-observer distance sampling surveys. Biometrics\u000d\u000a      62, 372-378. DOI:\u000d\u000a      10.1111\/j.1541-0420.2005.00493.x\u000d\u000a    \u000a\u000a5Buckland, S.T., Laake, J.L. and Borchers, D.L. 2010.\u000d\u000a      Double-observer line transect methods:\u000d\u000a      levels of independence. Biometrics 66, 169-177. DOI: 10.1111\/j.1541-0420.2009.01239.x\u000d\u000a    \u000a\u000a6Langrock, R., Borchers, D.L. and Skaug, H. Markov-modulated\u000d\u000a      nonhomogeneous Poisson\u000d\u000a      processes for modeling detections in surveys of marine mammal abundance.\u000d\u000a      2013. Journal of the\u000d\u000a        American Statistical Association. DOI: 10.1080\/01621459.2013.797356\u000d\u000a    \u000aOutputs 2, 3 and 4 were submitted to RAE2008 under UoA22, for which the\u000d\u000a      unit scored 2.65\u000d\u000a      overall for publications, with 95% of outputs scored at 2* or greater.\u000d\u000a      Outputs 3, 4 and 6 best\u000d\u000a      indicate the quality of the underpinning research.\u000d\u000a    \u000d\u000a    \u000d\u000a    \u000d\u000a    \u000d\u000a    \u000d\u000a    \u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    [S1] Letter on file from Head, Marine Science Branch, Energy\u000d\u000a      and Environmental Readiness\u000d\u000a      Division, US Navy, Pentagon.\u000d\u000a    [S2] Letter on file from Chief Science Advisor and Director of\u000d\u000a      Scientific Programs at the US National\u000d\u000a      Marine Fisheries Service (NOAA).\u000d\u000a    [S3] Letter on file from Head of Science, International\u000d\u000a      Whaling Commission.\u000d\u000a    [S4] Letter on file from Director, Conservation Support at the\u000d\u000a      Wildlife Conservation Society.\u000d\u000a    [S5] Letter on file from Scientific Secretary, North Atlantic\u000d\u000a      Marine Mammal Commission.\u000d\u000a    [S6] Distance home page. http:\/\/www.ruwpa.st-and.ac.uk\/distance\/.\u000d\u000a      Confirms number of registered\u000d\u000a      users as over 30,000. See http:\/\/www.ruwpa.st-and.ac.uk\/distance\/distanceusers.html\u000d\u000a      for a\u000d\u000a      summary of use and http:\/\/www.ruwpa.st-and.ac.uk\/distance\/distancelist.html\u000d\u000a      for the distance\u000d\u000a      sampling listserver, with over 800 members.\u000d\u000a    [S7] Milner-Gulland, E.J., Barlow, J., Cadotte, M.W., Hulme,\u000d\u000a      P.E., Kerby, G. and Whittingham, M.J.\u000d\u000a      (2012) Ensuring applied ecology has impact. Journal of Applied Ecology\u000d\u000a      49, 1-5. DOI:\u000d\u000a      10.1111\/j.1365-2664.2011.02102.x\u000d\u000a      Confirms that our methods lead to better decision-making in\u000d\u000a      the management of wild animal populations.\u000d\u000a    [S8] Gerrodette, T., Taylor, B.L., Swift, R., Rankin, S.,\u000d\u000a      Jaramillo-Legorreta, A.M. and Rojas-Bracho,\u000d\u000a      L. (2011) A combined visual and acoustic estimate of 2008 abundance, and\u000d\u000a      change in abundance\u000d\u000a      since 1997, for the vaquita, Phocoena sinus. Marine Mammal\u000d\u000a        Science 27, E79-E100.\u000d\u000a      DOI: 10.1111\/j.1748-7692.2010.00438.x\u000d\u000a      Confirms use of our methods in NOAA surveys to help\u000d\u000a      manage marine mammal populations.\u000d\u000a    [S9] Newson, S.E., Evans, K.L., Noble, D.G., Greenwood, J.J.D.\u000d\u000a      and Gaston, K.J. (2008) Use of\u000d\u000a      distance sampling to improve estimates of national population sizes for\u000d\u000a      common and widespread\u000d\u000a      breeding birds in the UK. Journal of Applied Ecology 45,\u000d\u000a      1330-1338. DOI: 10.1111\/j.1365-2664.2008.01480.x\u000d\u000a      Confirms use of our methods in the UK Breeding Bird Survey.\u000d\u000a    ","Title":"\u000d\u000a    Distance sampling surveys: enabling better decision-making by wildlife\u000d\u000a        managers\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Distance sampling is a suite of techniques for estimating the size and\/or\u000d\u000a      spatial density of animal\u000d\u000a      and plant populations from transect surveys. In the early 1990s, a\u000d\u000a      `Distance team' was\u000d\u000a      established, comprising Prof ST Buckland (then at BioSS in Aberdeen),\u000d\u000a      Anderson, Burnham and\u000d\u000a      Laake (Colorado), to develop software and establish good practice. After\u000d\u000a      Buckland's move to St\u000d\u000a      Andrews in October 1993, Dr DL Borchers (Reader) and Dr L Thomas (Reader)\u000d\u000a      were added to the\u000d\u000a      team in December 1993 and April 1997, respectively, with Thomas taking\u000d\u000a      responsibility for\u000d\u000a      software development1. Since 2001, with the exception of\u000d\u000a      ongoing contributions by Laake (mostly\u000d\u000a      in the area of double-platform methods) and Fewster (Auckland, but a PhD\u000d\u000a      student at St Andrews\u000d\u000a      1995-98), the active members of the team have all been at St Andrews. The\u000d\u000a      key researchers are\u000d\u000a      Borchers, Buckland, Thomas and Dr EA Rexstad (Research Fellow since 2005).\u000d\u000a    We have actively extended the applicability of distance sampling so that\u000d\u000a      populations that violate\u000d\u000a      the standard assumptions (perfect detection on the transect, no movement,\u000d\u000a      distances measured\u000d\u000a      without error, lines placed independently or animal locations), or are\u000d\u000a      prohibitively expensive to\u000d\u000a      survey by standard methods, can be reliably assessed. Since 2001, we have\u000d\u000a      published\u000d\u000a      methodological advances in 2 OUP books and in a wide range of statistical\u000d\u000a      and biological journals:\u000d\u000a      1 paper in JASA, 7 in Biometrics, 4 in JABES, 4 in J Appl Ecol, 4 in J\u000d\u000a      Acoustical Soc of America,\u000d\u000a      and 1 each in Applied Statistics and J Ornith. We pick out a few\u000d\u000a      highlights here.\u000d\u000a    Buckland recruited 3 research students (1 in 1995 and 2 in 1996) to\u000d\u000a      develop 3 aspects of distance\u000d\u000a      sampling: multiple-covariate distance sampling, spatial distance sampling\u000d\u000a      and automated survey\u000d\u000a      design. In parallel with their work, an introductory distance sampling\u000d\u000a      book was prepared, and was\u000d\u000a      published by OUP in 2001. It set out standards for conventional distance\u000d\u000a      sampling, based on\u000d\u000a      research conducted in the 1980s and 1990s. Subsequent developments of the\u000d\u000a      team appeared in\u000d\u000a      an advanced book in 20042. The work on spatial distance\u000d\u000a      sampling methods3 has sparked much\u000d\u000a      interest, and several groups have published papers developing the approach\u000d\u000a      further. The methods\u000d\u000a      allow density of animals to be related to geographical covariates that\u000d\u000a      quantify habitat, topography,\u000d\u000a      management practices, etc. Meanwhile, Borchers developed in a series of\u000d\u000a      papers methods for\u000d\u000a      when animals (such as whales) on the transect line are not certain to be\u000d\u000a      detected, culminating in a\u000d\u000a      comprehensive methodological framework for mark-recapture distance\u000d\u000a      sampling4. The concept of\u000d\u000a      `point independence', covered in detail in that paper, was extended to\u000d\u000a      that of `limiting\u000d\u000a      independence' subsequently5. Borchers' work was further\u000d\u000a      extended to accommodate stochastic\u000d\u000a      animal availability by embedding a Markov-modulated Poisson process model\u000d\u000a      for availability into\u000d\u000a      the distance sampling detection process model6.\u000d\u000a    "},{"CaseStudyId":"35388","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000d\u000a    Reporting directly to the DoH Advisory Committee on Resource Allocation\u000d\u000a      (ACRA), Bailey and\u000d\u000a      team successfully developed and implemented two different modelling\u000d\u000a      frameworks during the\u000d\u000a      feasibility study [4] referred to in Section 2. The first was aimed at\u000d\u000a      acute specialities and illustrated\u000d\u000a      in the project report [see evidence item a] through generation of\u000d\u000a      GP practice based estimates of\u000d\u000a      resource needs across England for the treatment of each of cardiovascular\u000d\u000a      disease,\u000d\u000a      endocrine\/metabolic disease and diabetes. It involved modelling the\u000d\u000a      log-odds of self-reported\u000d\u000a      longstanding illness (LSI) in specific illness categories from individuals\u000d\u000a      included in the Health\u000d\u000a      Survey for England (HSfE) over a number of years. To do this Binomial\u000d\u000a      multi-level Bayesian\u000d\u000a      models including random effects and a range of individual,\u000d\u000a      socio-demographic, socio-economic\u000d\u000a      and geographical variables were employed. A computationally intensive\u000d\u000a      micro-simulation of sub\u000d\u000a      populations in each GP practice nationally was then generated using\u000d\u000a      iterative proportional fitting\u000d\u000a      applied to available OPCS small area census data tables. An\u000d\u000a      age\/sex\/illness specific resource\u000d\u000a      need distribution was then sampled for each sub-population from national\u000d\u000a      historic costs drawn from\u000d\u000a      the `Hospital Episodes Statistics' (HES) dataset. Results were combined to\u000d\u000a      derive an estimate of\u000d\u000a      the total resource required to treat each condition within each GP\u000d\u000a      practice.\u000d\u000a    The second approach pioneered by Bailey and the modelling team in the\u000d\u000a      feasibility project study\u000d\u000a      report [see evidence item b], and then subsequently refined in the\u000d\u000a      second research project [5]\u000d\u000a      referred to in Section 2 and its associated report [c], was aimed\u000d\u000a      specifically at the difficult area of\u000d\u000a      adult mental health. It sought to determine the probability that\u000d\u000a      particular individuals will fall into one\u000d\u000a      of a number of case mix categories based upon a classification of adult\u000d\u000a      mental health care\u000d\u000a      combining clinically agreed and coherent treatment pathways with\u000d\u000a      iso-resource patient groups (i.e.\u000d\u000a      groups of patients that make similar resource demands on the NHS). In\u000d\u000a      order to implement this\u000d\u000a      approach, adults in the HSfE are classified as having needs in five\u000d\u000a      casemix mental health groups.\u000d\u000a      Then, paralleling the modelling of the approach for acute specialties\u000d\u000a      (except that a multinomial\u000d\u000a      rather than binomial Bayesian hierarchical model is fitted), individuals'\u000d\u000a      socio-demographic and\u000d\u000a      other characteristics are related to their casemix category. This\u000d\u000a      multinomial model is then applied\u000d\u000a      to micro simulated sub-populations to obtain estimates of the number of\u000d\u000a      people in each casemix\u000d\u000a      category in each practice. Relative resource needs are attached to\u000d\u000a      practices on the basis of\u000d\u000a      national per capita costs within the casemix.\u000d\u000a    In 2009\/10 primary care trusts in the UK spent &#163;8.08 billion on\u000d\u000a      secondary care mental health and\u000d\u000a      an additional &#163;8.37 billion in 2010\/11 through PCB. The mental\u000d\u000a      health component of PCB includes\u000d\u000a      the Community Health Services budget for adult mental illness, child and\u000d\u000a      adolescent psychiatry,\u000d\u000a      forensic psychiatry and old age psychiatry. Services provided under the\u000d\u000a      mental illness component\u000d\u000a      include; continuing care, crisis teams, early psychosis intervention and\u000d\u000a      hospitalisation.\u000d\u000a    In 2006, the department of health (DoH) released the PBC toolkit which\u000d\u000a      was recommended for use\u000d\u000a      in 2007 and is used by GPs and medical professionals to allocate resources\u000d\u000a      to practices in the UK.\u000d\u000a      In the period 2007-2009 results of the research described above\u000d\u000a      were presented by Bailey and\u000d\u000a      Asthana to the DoH Advisory Committee on Resource Allocation (ACRA) and\u000d\u000a      influenced thinking in\u000d\u000a      this group. ACRA was established in September 1997 as the successor body\u000d\u000a      to the Resource\u000d\u000a      Allocation Group (RAG). ACRA directly advises the Secretary of State for\u000d\u000a      Health on the\u000d\u000a      distribution of resources across primary and secondary care to ensure that\u000d\u000a      these fully reflect local\u000d\u000a      population need and operate as fairly as possible. Members include\u000d\u000a      academics, NHS senior\u000d\u000a      managers and GPs.\u000d\u000a    In 2009\/10 the DoH implemented new changes to the toolkit [a]\u000d\u000a      which included an entirely new\u000d\u000a      methodology for the mental health component of the toolkit developed by\u000d\u000a      Bailey and Asthana [see\u000d\u000a      evidence item b and evidence item c]. It was described by\u000d\u000a      the DoH in their PBC budget guidance\u000d\u000a      for 2009\/10 as `The new methodology has undergone extensive\u000d\u000a        testing by researchers and the\u000d\u000a        department of health and we believe it provides a step change\u000d\u000a        improvement in the way we model\u000d\u000a        mental health need'. In 2009\/10 the model was used by PCTs,\u000d\u000a      GPs and medical professionals\u000d\u000a      responsible for the practice based resource allocation to distribute &#163;8.08\u000d\u000a      billion pounds worth of\u000d\u000a      services for mental health.\u000d\u000a    In 2010\/11 an 'improved' version of the methodology was\u000d\u000a      implemented in the Practice-based\u000d\u000a        commissioning budget guidance for 2010\/11 [evidence item d]\u000d\u000a      which utilises a full multilevel model\u000d\u000a      that separately captures individual and area-level effects. The new\u000d\u000a      methodology was extensively\u000d\u000a      tested by the Department of Health and was overseen by the advisory\u000d\u000a      committee on Resource\u000d\u000a      Allocations. According to the NHS `Capturing both these effects makes\u000d\u000a        the estimates more\u000d\u000a        responsive to the needs of each practices population'. Similarly in\u000d\u000a      2010\/11, the model was used to\u000d\u000a      distribute &#163;8.37 billion of primary care services for mental health. The\u000d\u000a      National Audit Office's\u000d\u000a      Cross-government landscape: Formula funding of local public services\u000d\u000a      references the project\u000d\u000a      [evidence item e] as having `so far informed practice-based\u000d\u000a        commissioning, and may also form the\u000d\u000a        basis for allocations to clinical commissioning groups in the future'.\u000d\u000a    According to the Chief Economist\/Deputy Chief Analyst of the Department\u000d\u000a      of Health, who is a\u000d\u000a      member of ACRA [evidence item f]:\u000d\u000a    `It was Trevor Bailey who helped operationalize the new approach that\u000d\u000a        did not rely on past\u000d\u000a        utilization of service. This was particularly important for mental\u000d\u000a        health where utilization data was\u000d\u000a        patchy and represented the \"old model\" of service provision with over\u000d\u000a        reliance on hospital\u000d\u000a        inpatients. Trevor Bailey implemented an innovative approach based on\u000d\u000a        directly observed\u000d\u000a        morbidity indicators at individual level, rather than utilization at\u000d\u000a        area level. This was known as\u000d\u000a        Person Based resource Allocation (PBRA) and the technique was piloted in\u000d\u000a        2007 and successfully\u000d\u000a        implemented in 2007\/8. The highly computational technique using 40,000\u000d\u000a        Health Survey for\u000d\u000a        England data records led to a new formula being used to set target\u000d\u000a        allocations for &#163;8bn of funding\u000d\u000a        for mental health services for General Practice in 2009\/10 and 2010\/11,\u000d\u000a        The new method,\u000d\u000a        implemented by Trevor, proved to be practicable and had the added\u000d\u000a        advantage that the resulting\u000d\u000a        estimates of need included \"confidence intervals\" for different sized\u000d\u000a        populations.'\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Professor Trevor Bailey of the University of Exeter led the\u000d\u000a      methodological and computational\u000d\u000a      development of new improved mathematical models to more fairly allocate\u000d\u000a      resources, and\u000d\u000a      particularly mental health resources, to GP practices in the UK within an\u000d\u000a      interdisciplinary research\u000d\u000a      team from the universities of Plymouth, Southampton and St Andrews. The\u000d\u000a      mental health services\u000d\u000a      component of NHS Practice based commissioning (PBC) introduced by the\u000d\u000a      Department of Health\u000d\u000a      (DoH) from 2007 onwards, deals with resource allocation for specialist\u000d\u000a      healthcare for some\u000d\u000a      400,000 patients with severe mental illness. From 2009 to 2011, the team's\u000d\u000a      mental health\u000d\u000a      estimates, based upon the modelling efforts of Bailey, were used to set\u000d\u000a      practice-level PBC budgets\u000d\u000a      accounting for around &#163;8 billion of NHS funding, the DoH describing this\u000d\u000a      as a `step-change\u000d\u000a      improvement' in how mental health needs are modelled.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    University of Exeter\u000d\u000a    ","Institutions":[{"AlternativeName":"Exeter (University of)","InstitutionName":"University of Exeter","PeerGroup":"B","Region":"South West","UKPRN":10007792}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    Evidence of the quality of the research that underpins this case study is\u000d\u000a      provided through the\u000d\u000a      following peer-reviewed publications and grants secured through\u000d\u000a      competitive funding sources.\u000d\u000a    \u000a[1]**Asthana S, Gibson A, Hewson P, Bailey T, Dibben C. (2011).\u000d\u000a      General practitioner\u000d\u000a      commissioning consortia and budgetary risk: evidence from the modelling of\u000d\u000a      'fair share'\u000d\u000a      practice budgets for mental health, J Health Serv Res Policy, vol.\u000d\u000a      16, no. 2, 95-101.\u000d\u000a    \u000a\u000a[2]**Asthana S, Gibson A, Moon G, Brigham P. (2003). Allocating\u000d\u000a      resources for health and\u000d\u000a      social care: the significance of rurality, Health and Social Care in the\u000d\u000a      Community, vol. 11, no 6,\u000d\u000a      486-493.\u000d\u000a    \u000a\u000a[3]**Asthana S, Gibson A, Parsons E. (1999). The geography of\u000d\u000a      fundholding in southwest\u000d\u000a      England: implications for the evolution of primary care groups, Health\u000d\u000a      &amp; Place, vol. 5, 271-278.\u000d\u000a    \u000aKey Supporting Grants\u000d\u000a    [4] S. Asthana, A. Gibson, T. Bailey, C. Dibbens. The\u000d\u000a        feasibility of developing an approach to\u000d\u000a        Person Based Resource Allocation (PBRA) based on epidemiological data.\u000d\u000a      National Institute\u000d\u000a      for Health Research (Policy Research Programme), 2007, &#163;121,269.\u000d\u000a    [5] S. Asthana, A. Gibson, T. Bailey, C. Dibbens. Developing\u000d\u000a        a resource allocation formula at\u000d\u000a        General Practice level based on individual patient characteristics\u000d\u000a        (Person-Based Resource\u000d\u000a        Allocation): Mental Health. National Institute for Health Research\u000d\u000a      (Policy Research\u000d\u000a      Programme), 2008, &#163;191,216.\u000d\u000a    ** Papers that best indicate quality of underpinning research.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    a. Department of Health Practice-based commissioning budget\u000d\u000a      guidance for 2009\/10\u000d\u000a      `Methodological changes and toolkit guide' References the research p.10.\u000d\u000a      http:\/\/www.dh.gov.uk\/en\/Publicationsandstatistics\/Publications\/PublicationsPolicyAndGuidance\/DH_094364\u000d\u000a    b. Asthana, S., Gibson, A., Bailey, T., Dibben, C., Hewson, P.,\u000d\u000a      Economou, T., Batchelor, D.,\u000d\u000a      Eastham, J., Craig, R., Scholes, S., Flowers, J., Jenner, D. Person\u000d\u000a        Based Resource Allocation\u000d\u000a        (PBRA): The Feasibility of Developing a Need-Based Approach to PBRA.\u000d\u000a      Report to the\u000d\u000a      Department of Health (Policy Research Programme). 2008. University of\u000d\u000a      Plymouth. 118pp.\u000d\u000a    c. Asthana, S., Gibson, A., Bailey, T., Dibben, C., Hewson, P.\u000d\u000a      Developing a Person Based\u000d\u000a      Resource Allocation Formula for Setting Practice Level Mental Health\u000d\u000a      Budgets: 2009\/10 and\u000d\u000a      2010\/11. Final Report April 2009. Universities of Plymouth, Exeter and St\u000d\u000a      Andrews.\u000d\u000a    d. Department of Health Practice-based commissioning budget\u000d\u000a      guidance for 2010\/11\u000d\u000a      `Methodological changes and toolkit guide' References the research p.10\u000d\u000a      http:\/\/www.dh.gov.uk\/en\/Publicationsandstatistics\/Publications\/PublicationsPolicyAndGuidance\/DH_111057\u000d\u000a    e. The National Audit Office. Cross-government landscape: Formula\u000d\u000a      funding of local public\u000d\u000a      services, July 2010 references research p.31 http:\/\/www.official-documents.gov.uk\/document\/hc1012\/hc10\/1090\/1090.pdf\u000d\u000a    f. Letter of corroboration from Chief Economist\/Deputy Chief\u000d\u000a      Analyst, Department of Health who\u000d\u000a      is a member of ACRA.\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Development of mathematical models for Practice based Commissioning\u000d\u000a      budgets for adult mental health in the UK\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2637487","Name":"Southampton"},{"GeoNamesId":"2640194","Name":"Plymouth"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Resource allocation in the UK NHS since 2007 has been progressively\u000d\u000a      determined via Practice\u000d\u000a      based commissioning (PBC), a reform introduced by the Department of Health\u000d\u000a      (DoH) aimed at\u000d\u000a      more engagement of GP practices, clinicians and health professionals in\u000d\u000a      local funding decisions.\u000d\u000a      PBC groups now determine and design effective responses to local patient\u000d\u000a      needs and allocate\u000d\u000a      resources against competing service priorities. However, at a national\u000d\u000a      level, the setting of PBC\u000d\u000a      budgets must still accord with the core NHS principle that `individuals in\u000d\u000a      equal need should have\u000d\u000a      equal access to care, irrespective of where they live'. This is\u000d\u000a      problematic because sufficient\u000d\u000a      information is not collected to directly measure need for healthcare in\u000d\u000a      different local areas for\u000d\u000a      different conditions (as opposed to demand or utilisation of those\u000d\u000a      services). Traditionally, the basic\u000d\u000a      assumption underlying NHS resource allocation has therefore been that\u000d\u000a      utilisation of healthcare\u000d\u000a      services can be used to determine need indirectly by modelling the\u000d\u000a      relationship between utilisation\u000d\u000a      and socio-economic variables whilst correcting for supply factors.\u000d\u000a      Information on the level of such\u000d\u000a      socio-economic variables in small areas (such as practices) can then be\u000d\u000a      used to allocate resources\u000d\u000a      in a way that reflects needs.\u000d\u000a    However, despite the sophistication of some of the modelling involved,\u000d\u000a      the use of local utilisation\u000d\u000a      data as a proxy for need clearly runs the risk of reflecting and\u000d\u000a      reinforcing existing inequalities in the\u000d\u000a      relationship between underlying healthcare needs and the resources\u000d\u000a      allocated to address them.\u000d\u000a      Various published studies, including work [2,3] by Dr Alex Gibson\u000d\u000a      (University of Exeter until 2005),\u000d\u000a      have presented evidence for such inequalities in NHS resource allocation\u000d\u000a      in England and\u000d\u000a      questioned whether existing utilisation is an appropriate basis upon which\u000d\u000a      to set fair target\u000d\u000a      allocations commensurate with `equal opportunity of access for equal\u000d\u000a      needs'.\u000d\u000a    Against this background and building on Gibson's earlier work at Exeter,\u000d\u000a      in 2007 Trevor Bailey\u000d\u000a      (Professor of Computational Statistics, University of Exeter since 1986)\u000d\u000a      collaborated with\u000d\u000a      Plymouths Professor Sheena Asthana (Health Policy), Dr Alex Gibson and Dr\u000d\u000a      Paul Hewson (also\u000d\u000a      University of Exeter 2000-2005), and others from the Universities of\u000d\u000a      Southampton and St Andrews,\u000d\u000a      to pioneer an innovative approach to resource allocation based on\u000d\u000a      morbidity (epidemiological\u000d\u000a      evidence) rather than solely utilisation data. Bailey and his Exeter team\u000d\u000a      led on formulation and\u000d\u000a      development of the statistical modelling and computational aspects\u000d\u000a      underpinning the work and\u000d\u000a      Asthana and others involved in the team led on coordinating the health\u000d\u000a      policy and data handling\u000d\u000a      dimensions.\u000d\u000a    This team successfully won a tender in 2007 by DoH and National Institute\u000d\u000a      for Health Services\u000d\u000a      Research (NIHR) to undertake a feasibility study examining whether direct\u000d\u000a      epidemiological\u000d\u000a      evidence could be used as a basis for setting health care capitations [4].\u000d\u000a      A key advantage of their\u000d\u000a      novel approach to Person Based Resource Allocation (PBRA) was that it\u000d\u000a      proposed directly\u000d\u000a      modelling the relationship between morbidity and personal characteristics\u000d\u000a      such as age, gender,\u000d\u000a      ethnicity and socio-economic status, so largely circumventing the\u000d\u000a      difficulty faced by utilisation-based\u000d\u000a      approaches of having to disentangle legitimate need factors from\u000d\u000a      illegitimate drivers of need\u000d\u000a      associated with unmet need and supply-side factors. The need-based\u000d\u000a      approach to PBRA\u000d\u000a      developed during the feasibility study rested on a modelling framework\u000d\u000a      formulated by Bailey which\u000d\u000a      merged two separate methodologies. First, Bayesian modelling and\u000d\u000a      population micro simulation\u000d\u000a      techniques were used to generate estimates of the number of individuals\u000d\u000a      experiencing designated\u000d\u000a      categories of morbidity within the units to which resources are to be\u000d\u000a      allocated. Second, estimates\u000d\u000a      of the resource required by each of those individuals in order to meet\u000d\u000a      their health care needs were\u000d\u000a      developed either on the basis of national average historic use, or\u000d\u000a      normative tariffs. A probabilistic\u000d\u000a      approach was adopted throughout. Thus, all estimates produced were\u000d\u000a      expressed not in terms of\u000d\u000a      \"averages\" but in terms of 95% or 99% \"Credible Intervals\" whereby all of\u000d\u000a      the uncertainty in the\u000d\u000a      modelling was retained and made explicit. More detail on the results is\u000d\u000a      given in Section 4.\u000d\u000a    Having shown their need-based PBRA approach to be feasible, the team was\u000d\u000a      then commissioned\u000d\u000a      in a subsequently funded research project to use their casemix modelling\u000d\u000a      approach on more recent\u000d\u000a      data to develop the PBC formula for the difficult area of Mental Health\u000d\u000a      [5]. In a highly computational\u000d\u000a      study using multi-processor MCMC algorithms devised by Bailey and Hewson,\u000d\u000a      the probability of\u000d\u000a      individuals falling into six mental health casemix groups was estimated\u000d\u000a      from multinomial, multi-level\u000d\u000a      Bayesian models fitted to some 40,000 HSfE data records and then applied\u000d\u000a      to several million\u000d\u000a      micro-simulated sub-populations within some 8,500 GP practices nationally\u000d\u000a      The resulting needs\u000d\u000a      estimates were then combined with resource needs for each group and\u000d\u000a      aggregated to give the\u000d\u000a      resource allocation for adult mental health for each individual practice.\u000d\u000a    These results were then used by the DoH in national resource allocation\u000d\u000a      for mental health services\u000d\u000a      in 2009-2011 as described more fully in Section 4. An account of the work\u000d\u000a      was published in the\u000d\u000a      Journal of Health Services Research Policy in 2011 [1].\u000d\u000a    "},{"CaseStudyId":"35389","Continent":[],"Country":[],"Funders":["Natural Environment Research Council"],"ImpactDetails":"\u000a    Exeter's storm risk research has had a clear impact on risk\u000a      quantification practices in the insurance sector. Notable impacts include:\u000a    1. Improved mathematical tools which have been adopted by industry\u000a          for better pricing of storm insurance.\u000a\u0009Research publications [1-3] led in 2009 to storm clustering being\u000a        incorporated for the first time into the Integrated Financial Platform\u000a        (iFM) at Willis. Willis is a major global insurance and reinsurance\u000a        broker that employs around 20,000 people worldwide and arranges\u000a        protection for around US$5 trillion of exposed risk every year. The\u000a        clustering methodology accounts for the increased clustering of more\u000a        extreme storms [3] and is routinely used by the Chief Actuary and other\u000a        brokers at Willis to design more robust reinsurance contracts and to\u000a        better price windstorm risk. Local spatial calibration of tropical\u000a        cyclones simulated by climate models was also successfully implemented\u000a        on the platform in 2009 and provides Willis with a useful tool for\u000a        exploring the tropical cyclone risk especially in regions where there\u000a        have been few cyclone observations (e.g. western Pacific near E. Asia).\u000a        Willis Re have quoted Stephenson's research `....as the most\u000a          significant contributor of of any WRN [Willis Research Network]\u000a          partner in terms of applicable research' they further add that `the\u000a          studies of clustering of clustering that they [Stephenson's group]\u000a          produced allowed Willis to lead the market in providing better\u000a          quantification of the additional risk, and demonstrate to our clients\u000a          that we are the leading insurance broker' and `clustering has\u000a          now been built into commercially availible models for windstorm risk\u000a          following the Exeter work' [evidence item i].\u000a      2. Raised awareness in storm clustering that has stimulated\u000a          improvements in the commercial vendor catastrophe models used by the\u000a          insurance industry.\u000a      Exeter's storm risk research has raised awareness in the importance of\u000a        storm clustering throughout the insurance industry. Three major\u000a        catastrophe modelling companies (AIR, EQECAT and RMS) have now\u000a        explicitly incorporated clustering in their most recent windstorm models\u000a        for Europe [see evidence items a-d]. Thanks in part to Exeter's\u000a        research, storm clustering has also become a subject of much research\u000a        and development at major reinsurance companies, e.g. SCOR [evidence item\u000a          f]. Exeter's research on storm clustering is often cited and\u000a        quoted in company technical reports (see evidence item e and\u000a        evidence item g).\u000a      3. Expert dialogue on storm risk with global insurers and insurance\u000a          brokers\u000a      As world-leading experts in storm risk, Professor Stephenson and his\u000a        team are in frequent demand to provide expert advice to insurance on\u000a        storms. For example,\u000a      \u000a         The PhD research [2] supervised by Professor Stephenson led to the\u000a          creation in 2006 of the Willis Research Network (WRN) &#8212; the world's\u000a          largest partnership between academia and the insurance industry\u000a          (www.willisresearchnetwork.com). The WRN is a unique open forum for\u000a          the advancement of the science of extreme catastrophic events through\u000a          close collaboration between universities, insurers, reinsurers,\u000a          catastrophe modelling companies, government research institutions and\u000a          non-governmental organisations. As one of the world's major brokers,\u000a          Willis is able to use the expert advice to better inform its clients,\u000a          which include most of the world's major insurance companies and\u000a          several national governments. Exeter continues to play a pivotal role\u000a          in the success of this network [evidence item i].\u000a         In 2010, Professor Stephenson was invited to serve as the windstorm\u000a          expert on the Natural Catastrophe Advisory Council of Zurich &#8212; one of\u000a          the world's largest insurance companies. He provides expert advice at\u000a          annual council meetings;\u000a         The work of Prof. Stephenson and Dr. Economou supported by the AXA\u000a          research fund has featured heavily in AXA's outreach and 2013 brand\u000a          campaign [evidence items h].\u000a      \u000a    ","ImpactSummary":"\u000a    Statistical modelling of storms by Professor David Stephenson and\u000a      co-workers in the mathematics institute at the U. of Exeter, has improved\u000a      the understanding and thereby the pricing of insurance risk due to\u000a      European windstorms and tropical cyclones. Temporal clustering in these\u000a      catastrophic natural hazards has been quantified using novel process-based\u000a      statistical models, which have then been implemented by industry to\u000a      improve insurance pricing, e.g. on the integrated financial platform used\u000a      by Willis actuaries to provide a more reliable view of risk as required by\u000a      EU solvency 2 regulation. This research has also raised awareness in the\u000a      industry about storm clustering, and has stimulated significant\u000a      improvements in the main vendor catastrophe models, which are the main\u000a      tools used by insurance companies to price European windstorm insurance.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University of Exeter\u000a    ","Institutions":[{"AlternativeName":"Exeter (University of)","InstitutionName":"University of Exeter","PeerGroup":"B","Region":"South West","UKPRN":10007792}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Evidence of the quality of the research that underpins this case study is\u000a      provided through the following peer-reviewed publications and grants\u000a      secured through competitive funding sources.\u000a    \u000a[1]** Kvamsto, N-G., Y. Song, I. Seierstad, A. Sorteberg, &amp; D. B.\u000a      Stephenson. (2008) Clustering of cyclones in the ARPEGE general\u000a      circulation model. Tellus A, 60 (3), pp. 547-556. (IF:\u000a      2.062; 5-year IF: 2.320)\u000a    \u000a\u000a[2]** Vitolo, R., D. B. Stephenson, I. M. Cook, &amp; K.\u000a      Mitchell-Wallace. (2009) Serial clustering of intense European storms. Meteorologische\u000a        Zeitschrift, 2009, 18, pp. 411 - 424. DOI:\u000a      10.1127\/0941-2948\/2009\/0393. (Impact factor from Journal Citation Reports\u000a      2010: 1.402; 5-year IF: 1.530; Short-listed for the Lloyd's research\u000a        project prize in 2010)\u000a    \u000a\u000a[3]** Mumby, P. J., R. Vitolo, &amp; D. B. Stephenson. (2011) Temporal\u000a      clustering of tropical cyclones and its ecosystem impacts, Proceedings\u000a        of the National Academy of Science, volume 108, pages 17626-17630.\u000a      DOI: 10.1073\/pnas.1100436108 (Impact factor from Journal Citation Reports\u000a      2010: 9.771; 5-year IF: 10.591)\u000a    \u000a\u000a[4] Villarini G, Smith JA, Baeck ML, Vitolo R, Stephenson DB, Krajewski\u000a      WF. (2011) On the frequency of heavy rainfall for the Midwest of the\u000a      United States, Journal of Hydrology, volume 400, no. 1-2, pages\u000a      103-120.\u000a    \u000a\u000a[5] Villarini, G., Smith, J.A., Vitolo, R., and Stephenson, D.B., (2012)\u000a      \"On the temporal clustering of US floods and its relationship to climate\u000a      teleconnection patterns, International Journal of Climatology,\u000a      doi:10.1002\/joc.3458.\u000a    \u000a** Papers that best indicate quality of underpinning research.\u000a    Key Supporting Grants\u000a    &#8226; Willis Research Network: Funding of research fellow, 1 Jan 2008 &#8212;\u000a      ongoing, &#163;50,000\/year\u000a    &#8226; AXA Research Fund Research Grant: Regional Assessment of Climate Change\u000a      Impact on European Windstorms: Track clustering and multi-peril dependency\u000a      (RACEWIN), 1\/10\/2010-31\/9\/2013, &#163;241,231.\u000a    &#8226; NERC Doctoral Training Grant (Open CASE with Willis): Trends in extreme\u000a      extratropical cyclones and non-indemnity insurance, 1\/10\/2010-31\/9\/2013,\u000a      &#163;93,764\u000a    &#8226; NERC Doctoral Training Grant Project TEMPEST: Testing and Evaluating\u000a      Model Predictions of European Storms, 1\/9\/2010-31\/8\/2013, &#163;93,764\u000a    &#8226; NERC PURE (Probability, Uncertainty and Risk in the Environment)\u000a      consortium project CREDIBLE, 1\/10\/2012-30\/09\/2016 &#163;212,453. PhD student\u000a      (Laura Dawkins; start 1\/10\/2012) and postdoctoral researcher to start in\u000a      10\/2013.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"4","Level2":"1","Subject":"Atmospheric Sciences"},{"Level1":"4","Level2":"5","Subject":"Oceanography"}],"Sources":"\u000a    a. AIR (Applied Insurance Research inc.): Dr Gerhard Zuba and Dr\u000a      Milan Simic: \"European Windstorms: Implications of Storm Clustering on\u000a      Definitions of Occurrence Losses\", September 20, 2010, http:\/\/www.air-worldwide.com\/Publications\/AIR-Currents\/2010\/European-Windstorms--Implications-of-Storm-Clustering-on-Definitions-of-Occurrence-Losses\/\u000a      (References Stephenson)\u000a    b. Verisk Analytics `Air Worldwide updates European Wind and\u000a      Earthquake models', 12th September 2011. http:\/\/www.verisk.com\/Press-Releases\/2011\/AIR-Worldwide-Updates-European-Wind-and-Earthquake-Catastrophe-Models.html\u000a      (corroborates claim that Air has updated their model to include\u000a        clustering)\u000a    c. EQECAT: Global Reinsurance Special Report &#8212; European Windstorm\u000a      Special Edition, September 2010\u000a      http:\/\/www.eqecat.com\/pdfs\/global-reinsurance-report-windstorm-sep09.pdf\u000a      (References Stephenson p.8)\u000a    d. RMS `Ten Years After &#8212; How Windstorm Modelling has Matured\u000a      since the 1999 European Storms, 22nd December 2009\u000a      https:\/\/support.rms.com\/Publications\/1999_European_Storms_10_Year_Retrospective.asp\u000a    e. Activity of Catastrophic Windstorm Events in Europe in the 21st\u000a      century, May 31st 2010. http:\/\/www.google.co.uk\/url?q=http:\/\/www.eqecat.com\/pdfs\/activity-catastrophic-windstorm-events-europe-2010.pdf&amp;sa=U&amp;ei=8dt3UZO4N8bJ0QX_54DgDA&amp;ved=0CBsQFjAA&amp;usg=AFQjCNF27W7vzp GZ79UVsX80xIpb9gZGBg\u000a      (References Stephenson p.33)\u000a    f. SCOR: Mitchell-Wallace, Kirsten and Alvarez-Diaz, Teresa: \"The\u000a      impact of clustering of extreme European windstorm events on (re)insurance\u000a      market portfolios\", presentation at EGU 2010, http:\/\/meetingorganizer.copernicus.org\/EGU2010\/EGU2010-12446.pdf\u000a    g. Lloyds: \"Storm clouds gather over Europe\" Tuesday 30th\u000a      November 2010\u000a      http:\/\/www.lloyds.com\/News-and-Insight\/News-and-Features\/Environment\/Environment-2010\/Storm-clouds-gather-over-Europe\u000a      (Article references Exeter's research)\u000a    h. AXA research fund advertising using storm risk work by Prof.\u000a      Stephenson and Dr. Theodorus Economou:\u000a    - AXA \"Born to Protect\" brand web documentary, 1 July 2013 http:\/\/www.borntoprotect.com\/en_EN\u000a    - AXA \"Born to Protect\" brand advert aired on major television channels\u000a      across Europe, 12 June 2013 http:\/\/www.youtube.com\/watch?v=JQ1WkX63uMk\u000a    - Tracking future windstorms in Europe, 2nd August 2011http:\/\/www.axa-research.org\/david-stephenson-tracking-future-windstorms-in-europe\u000a    i. Supporting Letter, Executive Director, Willis Re. \u000a    ","Title":"\u000a    Improved pricing of European natural catastrophe insurance by statistical\u000a      modelling of storm clustering\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Clustered windstorms in Europe are a major source of societal risk. They\u000a      lead to substantial aggregate insurance losses comparable to a US\u000a      hurricane e.g. clusters of European storms in December 1999 and in early\u000a      1990, both resulted in insured losses exceeding US$10 billion. Insurance\u000a      companies price this risk using stochastic simulation models known as\u000a      catastrophe models. However, such models are based on rather ad hoc\u000a      assumptions about clustering that leads to large uncertainties in\u000a      aggregate loss estimates.\u000a    The storm risk team, established by Professor Stephenson after his\u000a      appointment in mathematics at Exeter in April 2007, have deepened\u000a      knowledge in storm clustering by developing and applying statistical\u000a      models of storms based on process understanding. Using storm track data\u000a      simulated by a climate model [1], they confirmed the empirical clustering\u000a      results from the earlier PhD research supervised by Professor Stephenson.\u000a      They found significantly more variance in observed European storm counts\u000a      than expected by chance sampling of independent events. Furthermore, they\u000a      showed that this overdispersion (a measure of clustering) could be\u000a      accounted for by time variations in the large-scale background flow [1].\u000a      Discussions with insurers raised the important question as to whether the\u000a      more extreme storms were also clustered. New research then revealed that\u000a      non-intuitively there is actually more clustering for the more extreme\u000a      storms [2]. This has since led to the discovery in 2012 of a positive\u000a      correlation between storm frequency and storm intensity over N. Europe,\u000a      which is likely to substantially increase extreme aggregate losses (work\u000a      to be submitted by the end of 2013). The team have also modelled\u000a      clustering in tropical cyclones and have used it to show that storm\u000a      clustering provides valuable recovery time e.g. for Carribean coral\u000a      ecosystems [3]. In addition, local spatial calibration of tropical\u000a      cyclones simulated by climate models has been successfully implemented on\u000a      the risk simulation platform at Willis to provide a useful tool for\u000a      exploring the tropical cyclone risk especially in Asia (unpublished work).\u000a      The team has also successfully collaborated with U.S. hydrologists on the\u000a      clustering of extreme rainfall and flooding [4,5].\u000a    The storm risk team is unique in working at the interface between\u000a      environmental statistics, actuarial\/financial mathematics, and atmospheric\u000a      science. The team makes novel application of statistical concepts such as\u000a      compound Poisson processes, Generalized Linear Models, and Extreme Value\u000a      Theory. Under the guidance of Professor Stephenson, young statisticians in\u000a      the team learn valuable interdisciplinary skills relevant to real-world\u000a      risk quantification and decision-making. Team members include postdoctoral\u000a      fellows (Willis-funded: Dr Ben Youngman 10\/2011-now, Dr Renato Vitolo\u000a      1\/2008-9\/2010; AXA-funded: Dr Theo Economou 10\/2010-now) and NERC funded\u000a      PhD students (Laura Dawkins 10\/2011-now; Phil Sansom 03\/2011-now; Alasdair\u000a      Hunter 10\/2010-now). In addition to strong working links with the\u000a      reinsurance industry, the team also collaborates closely with storm\u000a      scientists at the nearby Met Office (e.g. joint biweekly meetings).\u000a    "},{"CaseStudyId":"35390","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    The public and a wide range of UK services and industries benefit from\u000d\u000a      weather forecasts provided\u000d\u000a      by the Met Office, including agriculture, aviation, construction, energy,\u000d\u000a      retail, and transport [see\u000d\u000a      evidence item a]. The economic value of the forecasts has been\u000d\u000a      estimated to be in excess of\u000d\u000a      &#163;600M pa, and perhaps much more [evidence item b]. Forecasts of\u000d\u000a      severe weather have a huge\u000d\u000a      value in terms of public safety. Although it would not be meaningful to\u000d\u000a      put a financial value on any\u000d\u000a      one component of the forecasting system, the accuracy and value of the\u000d\u000a      forecasts clearly depend\u000d\u000a      crucially on the accuracy, as well as efficiency, of the forecast model\u000d\u000a      dynamical core.\u000d\u000a    Climate change, forced by anthropogenic greenhouse gas emissions, will\u000d\u000a      have major impacts on\u000d\u000a      society in the UK and worldwide. Both mitigation and adaptation measures\u000d\u000a      will have huge costs,\u000d\u000a      from the level of individual businesses and organizations up to national\u000d\u000a      government level [evidence\u000d\u000a      item c]. Policies on mitigation and adaptation therefore require\u000d\u000a      the best possible predictions of the\u000d\u000a      future evolution of the climate, and our best available tools for making\u000d\u000a      those predictions are\u000d\u000a      sophisticated numerical models such as the Met Office \/ Hadley Centre\u000d\u000a      climate model (a variant of\u000d\u000a      the Met Office weather prediction model). Again, the accuracy and value of\u000d\u000a      the predictions depend\u000d\u000a      crucially on the accuracy of the model and its dynamical core.\u000d\u000a    The newest generation of the Met Office dynamical core, known as ENDGame,\u000d\u000a      is designed to\u000d\u000a      retain the advantages of the current operational dynamical core, while\u000d\u000a      making several\u000d\u000a      improvements. Prof Thuburn's group at the University of Exeter has made\u000d\u000a      several key\u000d\u000a      contributions to the formulation and development of ENDGame, some of which\u000d\u000a      are documented in\u000d\u000a      section 2, and which have impacted on its robustness, accuracy, and\u000d\u000a      efficiency.\u000d\u000a    Robustness, stability and convergence. For operational use, the\u000d\u000a      dynamical core must be robust\u000d\u000a      and free from numerical instability within its planned operating regime.\u000d\u000a      Prof Thuburn made key\u000d\u000a      contributions to obtaining a stable formulation (a) through improving the\u000d\u000a      calculation of semi-Lagrangian\u000d\u000a      departure points (reference [6] above) and (b) through analysis showing\u000d\u000a      the need for a\u000d\u000a      space-and-time-dependent reference state for the iterative solver\u000d\u000a      [evidence item d]. Tests have\u000d\u000a      shown ENDGame to be more stable and robust than the current operational\u000d\u000a      dynamical core [see\u000d\u000a      evidence item f]:\u000d\u000a    \"...work by Prof Thuburn helped to improve our understanding of the\u000d\u000a        ENDGame iterative solver\u000d\u000a        and other aspects of the formulation, which eventually led to a\u000d\u000a        breakthrough in achieving\u000d\u000a        numerically robust solutions. This specific aspect is of crucial\u000d\u000a        importance to our operational\u000d\u000a        forecasters for whom model failure causes major disruption, leading to\u000d\u000a        delays in delivery of\u000d\u000a        products to our customers.'' [evidence item f]\u000d\u000a    Accuracy. ENDGame combines accurate wave propagation with an\u000d\u000a      optional conservative semi-Lagrangian\u000d\u000a      transport of mass and trace species, considered essential for some climate\u000d\u000a      simulations\u000d\u000a      (references [1], [2] and [3] above). In addition, the improved stability\u000d\u000a      and robustness imply reduced\u000d\u000a      dependence on ad hoc scale-selective dissipation. These improvements have\u000d\u000a      led to several\u000d\u000a      measurable gains in model accuracy and realism in trials, including better\u000d\u000a      maintaining the\u000d\u000a      atmospheric eddy kinetic energy, which helps to increase the spread in\u000d\u000a      ensemble forecasts (a\u000d\u000a      desirable property) and to maintain the natural variability in climate\u000d\u000a      simulations, more realistic\u000d\u000a      stratospheric gravity wave activity, and improved climate precipitation\u000d\u000a      fields [see evidence item e\u000d\u000a      and evidence item f ].\u000d\u000a    \"The performance of ENDGame in pre-operational trials suggests that it\u000d\u000a        delivers significant\u000d\u000a        improvements in accuracy over the current operational dynamical core. An\u000d\u000a        important element of\u000d\u000a        this improvement is that the new model has much better variability. This\u000d\u000a        leads, for example, to\u000d\u000a        improved tropical cyclone tracks and intensity, improved summer Asian\u000d\u000a        monsoon rainfall and\u000d\u000a        improved wind speed forecasts.'' [evidence item f]\u000d\u000a    Efficiency. Prof Thuburn's group made a key contribution to the\u000d\u000a      efficiency of the iterative solver\u000d\u000a      through understanding the effect of different back-substitution strategies\u000d\u000a      on error growth\/decay.\u000d\u000a      The improvement helped enable ENDGame to run in approximately the same\u000d\u000a      time as its\u000d\u000a      predecessor, despite the greater complexity of its algorithm. Indeed,\u000d\u000a      ENDGame is slightly more\u000d\u000a      efficient on large numbers of processors. This efficiency was a crucial\u000d\u000a      factor in the decision to take\u000d\u000a      ENDGame forward to operational use [see evidence item e and\u000d\u000a      evidence item f].\u000d\u000a    \"The improved understanding of the iterative solver also allowed a\u000d\u000a        reformulation that\u000d\u000a        significantly reduced the number of iterations needed, and hence the\u000d\u000a        cost of ENDGame. This\u000d\u000a        has been critical in enabling ENDGame to run within the same time window\u000d\u000a        as the current\u000d\u000a        operational dynamical core. Indeed, the formulation of ENDGame achieves\u000d\u000a        better scaling on\u000d\u000a        large numbers of processors and, when ENDGame goes operational, this\u000d\u000a        will allow us to\u000d\u000a        increase the resolution of the forecasts (from 25km to 17km) beyond what\u000d\u000a        we would be able to\u000d\u000a        do with the current model.'' [evidence item f]\u000d\u000a    (Further improvements in efficiency may be possible using an\u000d\u000a      `incremental' version of the solver\u000d\u000a      proposed and tested by Prof Thuburn, and by using the longer timesteps\u000d\u000a      permitted by the\u000d\u000a      improved stability of ENDGame.)\u000d\u000a    ENDGame will be used operationally from early 2014. At that point the\u000d\u000a      impact of this research will\u000d\u000a      begin to reach the public, policy makers, and businesses and services that\u000d\u000a      use Met Office weather\u000d\u000a      and climate forecasts.Prof Thuburn engaged with the ENDGame project at an\u000d\u000a      early stage (2005)\u000d\u000a      by providing an independent review of a draft ENDGame formulation. The\u000d\u000a      pull through of the\u000d\u000a      research into pre-operational development has been facilitated by frequent\u000d\u000a      close contact between\u000d\u000a      Professor Thuburn and the Dynamics Research group at the Met Office; since\u000d\u000a      2005 he has\u000d\u000a      typically spent one day per week visiting the Met Office. He has\u000d\u000a      co-authored several papers with\u000d\u000a      the Met Office Dynamics Research Group (Section 2).\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research by Professor John Thuburn and his group at the University of\u000d\u000a      Exeter has made several\u000d\u000a      key contributions to the formulation and development of ENDGame, the new\u000d\u000a      dynamical core of the\u000d\u000a      Met Office weather and climate prediction model. ENDGame has been shown to\u000d\u000a      deliver improved\u000d\u000a      accuracy and better computational performance at high processor counts\u000d\u000a      compared to the current\u000d\u000a      operational dynamical core, directly impacting the technological tools\u000d\u000a      available to the Met Office.\u000d\u000a      These improvements will benefit users when ENDGame becomes operational in\u000d\u000a      early 2014: the\u000d\u000a      economic value to the UK of the weather forecasts produced by the Met\u000d\u000a      Office has been estimated\u000d\u000a      to be in excess of &#163;600M pa, while climate change projections inform\u000d\u000a      policy decisions on mitigation\u000d\u000a      and adaptation with huge economic implications.\u000d\u000a    ","ImpactType":"Environmental","Institution":"\u000d\u000a    University of Exeter\u000d\u000a    ","Institutions":[{"AlternativeName":"Exeter (University of)","InstitutionName":"University of Exeter","PeerGroup":"B","Region":"South West","UKPRN":10007792}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    Evidence of the quality of the research that underpins this case study is\u000d\u000a      provided through the\u000d\u000a      following peer-reviewed publications and grants secured through\u000d\u000a      competitive funding sources.\u000d\u000a    \u000a[1]** Thuburn, J. Vertical discretizations giving optimal representation\u000d\u000a      of normal modes: Sensitivity\u000d\u000a      to the form of the pressure gradient term. Quarterly Journal of the\u000d\u000a        Royal Meteorological Society,\u000d\u000a      2006, 132, 2809-2825.\u000d\u000a    \u000a\u000a[2] Thuburn, J. Rossby wave propagation on the C-grid. Atmos. Science\u000d\u000a        Letters, 2007, 8, 37-42.\u000d\u000a    \u000a\u000a[3]** Thuburn, J., M. Zerroukat, N. Wood, A. Staniforth. Coupling a mass\u000d\u000a      conserving semi-Lagrangian\u000d\u000a      scheme (SLICE) to a semi-implicit discretization of the shallow-water\u000d\u000a      equations:\u000d\u000a      minimizing the dependence on a reference atmosphere. Quarterly Journal\u000d\u000a        of the Royal\u000d\u000a        Meteorological Society, 2010, 136, 146-154.\u000d\u000a    \u000a\u000a[4] Thuburn, J. Some conservation issues for the dynamical cores of NWP\u000d\u000a      and climate models.\u000d\u000a      Journal of Computational Physics, 2008, 227, 3715-3730.\u000d\u000a    \u000a\u000a[5] Kent, J., J. Thuburn, N. Wood. Assessing implicit large eddy\u000d\u000a      simulation for two-dimensional\u000d\u000a      flow. Quarterly Journal of the Royal Meteorological Society, 2012,\u000d\u000a      138, 365-376.\u000d\u000a    \u000a\u000a[6]** Thuburn, J., A. A. White. A geometrical view of the\u000d\u000a      shallow-atmosphere approximation, with\u000d\u000a      application to the semi-Lagrangian departure point calculation. Quarterly\u000d\u000a        Journal of the Royal\u000d\u000a        Meteorological Society 2013, 139, 261-268.\u000d\u000a    \u000a** Papers that best indicate the quality of the underpinning research.\u000d\u000a    Key Supporting Grants\u000d\u000a    &#8226; Met Office Joint Chair in Geophysical Fluid Dynamics, 1 Jan 2005 -\u000d\u000a      ongoing.\u000d\u000a    &#8226; Numerical Methods for Weather and Climate Models I: Coupling resolved\u000d\u000a      scales to subgrid\u000d\u000a      models. Great Western Research \/ Met office studentship, Oct 2006 - Sept\u000d\u000a      2009, &#163;55,000.\u000d\u000a    &#8226; Physics-Dynamics Coupling for Weather and Climate Prediction Models.\u000d\u000a      EPSRC Mathematical\u000d\u000a      Sciences CASE studentship (with the Met Office), Oct 2006 - March 2010,\u000d\u000a      &#163;60,864 + &#163;16,950.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"9","Level2":"11","Subject":"Maritime Engineering"},{"Level1":"4","Level2":"5","Subject":"Oceanography"}],"Sources":"\u000d\u000a    a. Met Office 2010: http:\/\/www.metoffice.gov.uk\/services\u000d\u000a    b. (A summary of some of the UK services and industries benefiting from\u000d\u000a      Met Office weather\u000d\u000a      forecasts and climate services.)\u000d\u000a    c. PA Consulting Group 2007: The Public Weather Service's Contribution to\u000d\u000a      the UK Economy\u000d\u000a    d. http:\/\/www.metoffice.gov.uk\/about-us\/what\/pws\/value\u000d\u000a    e. Stern, N. (2010): Stern Review on the Economics of Climate Change\u000d\u000a    f. http:\/\/webarchive.nationalarchives.gov.uk\/+\/http:\/\/www.hm-treasury.gov.uk\/sternreview_index.htm\u000d\u000a    g. These statements may be corroborated by Head of Dynamics Research\u000d\u000a      group, Met Office.\u000d\u000a    h. Met Office Scientific Advisory Committee 2012 (MOSAC-17), paper 17.3:\u000d\u000a      Foundation Science\u000d\u000a      (A Brown). http:\/\/www.metoffice.gov.uk\/media\/pdf\/b\/p\/MOSAC_17.3_Brown.pdf\u000d\u000a    i. Letter from Met Office Director of Science to Prof Nick Talbot (Deputy\u000d\u000a      Vice Chancellor,\u000d\u000a      University of Exeter).\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Improving the Met Office Weather and Climate Prediction Model\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2649808","Name":"Exeter"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Operational weather and climate prediction are carried out using\u000d\u000a      enormously complex systems, at\u000d\u000a      the heart of which is a numerical model. The model component that solves\u000d\u000a      the equations of\u000d\u000a      atmospheric dynamics and thermodynamics, on the scales resolved by the\u000d\u000a      model grid, is called the\u000d\u000a      `dynamical core'. Of the order of 50 person years of effort are required\u000d\u000a      to research and develop a\u000d\u000a      state-of-the-art dynamical core. Over the last decade the Met Office has\u000d\u000a      developed a new\u000d\u000a      dynamical core, known as ENDGame. Through close collaboration with the Met\u000d\u000a      Office Dynamics\u000d\u000a      Research group, Prof John Thuburn (appointed 2005) and his group at the\u000d\u000a      University of Exeter\u000d\u000a      have made several key contributions to the formulation and development of\u000d\u000a      ENDGame.\u000d\u000a    One major contribution has been to understand how to represent wave\u000d\u000a      propagation as accurately\u000d\u000a      as possible in the model. Accurate wave propagation is essential for an\u000d\u000a      accurate representation of\u000d\u000a      large-scale dynamical balance in the atmosphere. Building on his previous\u000d\u000a      work, Thuburn (2006)\u000d\u000a      [1] showed that by an appropriate choice of predicted variables and\u000d\u000a      vertical grid staggering,\u000d\u000a      combined with an appropriate formulation of the pressure gradient term, an\u000d\u000a      optimal representation\u000d\u000a      of wave propagation could be achieved for all families of atmospheric\u000d\u000a      waves while predicting\u000d\u000a      density, allowing a mass conserving formulation. This work has recently\u000d\u000a      been extended under an\u000d\u000a      EPSRC Mathematical Sciences CASE PhD studentship (D. R. E. Holdaway),\u000d\u000a      showing that the\u000d\u000a      optimal configuration remains optimal when the dynamical core is coupled\u000d\u000a      to an eddy diffusivity\u000d\u000a      model of the atmospheric boundary layer of the sort used operationally.\u000d\u000a    An important factor affecting the accuracy of wave propagation on the\u000d\u000a      kind of staggered grid used\u000d\u000a      by ENDGame is the formulation of the Coriolis terms (associated with the\u000d\u000a      Earth's rotation).\u000d\u000a      Thuburn (2007) [2] showed how to formulate the Coriolis terms so as to\u000d\u000a      improve the accuracy of\u000d\u000a      Rossby wave propagation and ensure that balanced flows can be represented\u000d\u000a      accurately, while\u000d\u000a      respecting energy conservation.\u000d\u000a    The coupling of a conservative semi-Lagrangian advection scheme for the\u000d\u000a      mass variable with a\u000d\u000a      semi-implicit treatment of fast waves is a novel feature of the ENDGame\u000d\u000a      formulation. Thuburn et\u000d\u000a      al. (2010) [3] demonstrated how to improve the accuracy of this coupling\u000d\u000a      through an accurate\u000d\u000a      representation of the trajectory-averaged flow divergence in a prototype\u000d\u000a      shallow-water model.\u000d\u000a      Conservation properties of weather and climate model dynamical cores are a\u000d\u000a      controversial subject,\u000d\u000a      with no consensus on which is most important or desirable. The review by\u000d\u000a      Thuburn [4] was\u000d\u000a      prompted by discussion with the Met Office Dynamics Research group. The\u000d\u000a      handling of marginally-resolved\u000d\u000a      scales and the exchange of quantities like energy and potential enstrophy\u000d\u000a      between\u000d\u000a      resolved and unresolved scales is of particular interest. A PhD project\u000d\u000a      co-funded by the Met Office\u000d\u000a      under the Great Western Research scheme [5] has improved our understanding\u000d\u000a      and provided\u000d\u000a      some theoretical justification for the ENDGame formulation in this\u000d\u000a      respect.\u000d\u000a    Finally, the ENDGame formulation requires the iterative solution of a\u000d\u000a      stiff and nonlinear coupled\u000d\u000a      system of equations at every time step. Efficiency and stability of the\u000d\u000a      model depend on ensuring\u000d\u000a      that the iterative solver converges quickly. During 2010\/2011 significant\u000d\u000a      progress was made in\u000d\u000a      improving the convergence, stability, and efficiency of ENDGame, one\u000d\u000a      aspect of which is\u000d\u000a      documented in [6].\u000d\u000a    "},{"CaseStudyId":"35391","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"3017382","Name":"France"},{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"2510769","Name":"Spain"},{"GeoNamesId":"3175395","Name":"Italy"}],"Funders":["UK Space Agency"],"ImpactDetails":"\u000d\u000a    The European space industry has an annual turnover of approximately &#8364;8B\u000d\u000a      and employs over 35,000 people [evidence item c]. The UK space\u000d\u000a      industry has an upstream turnover of &#163;930M and a downstream turnover of\u000d\u000a      &#163;6.6B per annum, and directly employs over 8,000 people, mostly in highly\u000d\u000a      skilled occupations [evidence item d]. Over the past number of\u000d\u000a      years Dr Menon and his group have, uniquely for a UK-based mathematics\u000d\u000a      research group, worked with almost every major European company in this\u000d\u000a      sector, including Astrium (UK\/France\/Germany, turnover &#8364;5B), Thales\/Alenia\u000d\u000a      Space (Italy, turnover &#8364;2B), GMV Space (Spain, turnover &#8364;50M), SCISYS PLC\u000d\u000a      (UK, &#163;43M), Deimos Space (Spain, turnover &#8364;20M). Total research income to\u000d\u000a      Dr Menon's group from these contracts has exceeded &#8364;400K and has supported\u000d\u000a      4 full-time PhD students, a level of ESA funding that is unmatched by any\u000d\u000a      other maths-based research group in Europe. As a result of these\u000d\u000a      collaborations, the WCAT software tool developed by Dr Menon's group has\u000d\u000a      transformed the way that the European space industry performs V&amp;V. Use\u000d\u000a      of the tool by European Space companies is now actively encouraged, and\u000d\u000a      increasingly mandated, by the European Space Agency, through specific\u000d\u000a      requirements written into Statements of Work (SoW) [see evidence item\u000d\u000a        e and evidence item f] for recent and future ESA projects.\u000d\u000a    By leveraging the power of advanced global optimisation techniques, and\u000d\u000a      packaging these methods in a user-friendly software environment, WCAT\u000d\u000a      allows industrial companies to make significant reductions in the cost of\u000d\u000a      the V&amp;V process for advanced space control systems. This potential has\u000d\u000a      now been demonstrated in a number of different ESA-funded projects, across\u000d\u000a      several different application platforms. The results of the application of\u000d\u000a      the WCAT to an autonomous satellite rendezvous system developed by the\u000d\u000a      Spanish company GMV, where a \"chaser\" satellite is required to rendezvous\u000d\u000a      with and capture a small canister containing samples from the surface of\u000d\u000a      Mars. While 1000 Monte Carlo simulations (taking more than a week of\u000d\u000a      computations) uncovered no failure cases, the WCAT found a case for which\u000d\u000a      the chaser failed to capture the canister in only 339 simulations [1]\u000d\u000a      representing a saving of 60% in the cost of this stage of the V&amp;V\u000d\u000a      process.\u000d\u000a    In another recent study [2], use of the WCAT for the V&amp;V of an\u000d\u000a      attitude control system for a flexible telecoms satellite revealed\u000d\u000a      significant shortcomings in the traditional Monte Carlo simulation\u000d\u000a      approach. For a range of satellite fuel-tank filling ratios (FR), two\u000d\u000a      hybrid optimisation algorithms encoded in the WCAT (HGA and HDE) were able\u000d\u000a      to identify higher values than Monte Carlo simulation (MC) of a\u000d\u000a      sensitivity function (S) reflecting the fragility of the attitude\u000d\u000a      controller to uncertainties in the bandwidth of the satellite's flexible\u000d\u000a      modes. This more accurate evaluation of the controller's robustness was\u000d\u000a      achieved in each case with fewer simulations than the 1000 typically\u000d\u000a      required to ensure adequate statistical confidence intervals in MC\u000d\u000a      simulation campaigns.\u000d\u000a    Based on the demonstrated successes of the WCAT tool on several recent\u000d\u000a      projects, Dr Menon's group were approached by ESA in 2012 to produce an\u000d\u000a      updated version of the tool [evidence item e]. This work, funded\u000d\u000a      directly by ESA via a &#8364;60K research contract directly with Exeter\u000d\u000a      University (ESA Contract No. 4000104541) will leverage recent theoretical\u000d\u000a      work by Dr Menon's team on surrogate modelling and robust safety-margin\u000d\u000a      analysis to produce an enhanced and expanded version of WCAT for\u000d\u000a      application to a number of future ESA missions [4,5].\u000d\u000a    Head of Guidance and Control at ESA [see evidence item a]\u000d\u000a    `Our team at ESA and other key industrial partners significantly\u000d\u000a        benefit from the advanced verification and validation tools developed at\u000d\u000a        Exeter. The benefits are in terms of the reduction of the time to market\u000d\u000a        and associated development cost while achieving sufficiently reliable\u000d\u000a        validation and verification results for multiple projects. Especially\u000d\u000a        the methodology and tools developed at Exeter is capable of determining\u000d\u000a        the worst case performance and safety margins of designs and missions in\u000d\u000a        an efficient and consistent manner.'\u000d\u000a    Dr Menon was keynote speakers at an international workshop on \"Worst Case\u000d\u000a      Analysis Tools For Guidance Navigation &amp; Control Systems\" organized by\u000d\u000a      ESA at ESTEC headquarters in Noordwijk on November 13th &amp;\u000d\u000a      14th 2012 [evidence item g]. This workshop, which included\u000d\u000a      participants from all the major European space companies, research\u000d\u000a      organizations (CNES-France, DLR-Germany) and leading US\/EU universities\u000d\u000a      (Berkeley, Minnesota, Stuttgart, Exeter) has effectively set the agenda\u000d\u000a      for ESA-supported research and development in this field for the next five\u000d\u000a      years. In February 2013, Dr Menon's group initiated a new collaboration\u000d\u000a      with the Advanced Studies Group at Astrium UK in Stevenage, with the aim\u000d\u000a      of further integrating the WCAT tool into Astrium's design process for a\u000d\u000a      range of current and future missions [evidence item b]. In October\u000d\u000a      2013, Dr Menon's group was awarded &#163;50K, by esteemed CREST 2 &#8212; UK Space\u000d\u000a      Agency initiative of significant national importance [evidence item h], to\u000d\u000a      integrate the Worst Case Analysis Tool for developing a rapid auto tuning\u000d\u000a      tool for the Generic Rover Dynamics Model framework with Astrium Ltd.,\u000d\u000a      Stevenage, which also aim towards final integration and testing at ESA\u000d\u000a      Harwell Centre, Harwell, Oxford.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The difficulty of certifying the safety (often termed Verification and\u000d\u000a      Validation &#8212; V&amp;V) of increasingly complex and more autonomous\u000d\u000a      Guidance, Navigation and Control (GNC) systems is now widely accepted to\u000d\u000a      be a serious threat to the success of future space missions. In response\u000d\u000a      to this threat, the European Space Agency has funded Dr Prathyush P Menon\u000d\u000a      and his team to develop a suite of mathematical tools for the V&amp;V of\u000d\u000a      advanced GNC systems. These tools have now been widely adopted throughout\u000d\u000a      the European Space industry, and have been successfully applied by major\u000d\u000a      companies such as Astrium, Thales-Alenia and GMV to systems ranging from\u000d\u000a      flexible and autonomous satellites, to launch vehicles and hypersonic\u000d\u000a      re-entry vehicles.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Exeter\u000d\u000a    ","Institutions":[{"AlternativeName":"Exeter (University of)","InstitutionName":"University of Exeter","PeerGroup":"B","Region":"South West","UKPRN":10007792}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2749812","Name":"Noordwijk-Binnen"},{"GeoNamesId":"5037779","Name":"Minnesota"}],"References":"\u000d\u000a    Evidence of the quality of the research that underpins this case study is\u000d\u000a      provided through the following peer-reviewed publications and grants\u000d\u000a      secured through competitive funding sources.\u000d\u000a    \u000a[1] Verification and Validation of Autonomous Rendezvous Systems in the\u000d\u000a      Terminal Phase, W. Wang, P.P. Menon, D.G. Bates, N. M. Gomes Paulino, E.\u000d\u000a      Di Sotto, A. Bidaux, A. Kron S. Salehi and S. Bennani, Provisionally\u000d\u000a      accepted to the AIAA Journal of Spacecraft and Rockets, 2013 (preliminary\u000d\u000a      results in Proceedings of the AIAA Conference on Guidance, Navigation and\u000d\u000a      Control, Minneapolis, Minnesota, USA, 2012.)\u000d\u000a    \u000a\u000a[2] An Integrated Analytical\/Numerical Framework for Verification and\u000d\u000a      Validation of Attitude Control Systems for Flexible Satellites, W. Wang,\u000d\u000a      P.P. Menon, D.G. Bates, A. Bidaux, A. Garus, A. Kron, C. Charbonnel, F.\u000d\u000a      Ankersen, S. Bennani, in Proceedings of the AIAA Conference on Guidance,\u000d\u000a      Navigation and Control, Minneapolis, Minnesota, USA, 2012.\u000d\u000a    \u000a\u000a[3]** Integrated Structure\/Control Optimization Applied to the BIOMASS\u000d\u000a      Earth Observation Mission, M. Watt, M. Yu, A. Falcoz, A. Kron, P.P. Menon,\u000d\u000a      F. Ankersen, L. Massotti, in Proceedings of the AIAA Conference on\u000d\u000a      Guidance, Navigation and Control, Boston, USA, 2013.\u000d\u000a    \u000a\u000a[4]** Robust Safety Margin Assessment and Constrained Worst-Case Analysis\u000d\u000a      of a Launcher Vehicle, A. Kamath, P.P. Menon, D.G. Bates, M.\u000d\u000a      Ganet-Schoeller, S. Bennani, in Proceedings of the IFAC Symposium on\u000d\u000a      Robust Control Design, Aalborg, Denmark, 2012.\u000d\u000a    \u000a\u000a[5] Worst Case Analysis of a Launcher Vehicle Using Surrogate Models, A.\u000d\u000a      Kamath, P.P. Menon, M. Ganet-Schoeller, M. Guillaume, S. Bennani, D.G.\u000d\u000a      Bates, in Proceedings of the IFAC Symposium on Robust Control Design,\u000d\u000a      Aalborg, Denmark, 2012.\u000d\u000a    \u000a\u000a[6]** Robustness Analysis of Attitude and Orbit Control Systems for\u000d\u000a      Flexible Satellites, W. Wang, P.P. Menon, D.G. Bates, S. Bennani, IET\u000d\u000a        Control Theory and Applications, 2010, 4 (12), pp. 2958-2970\u000d\u000a    \u000a** Papers that best indicate quality of underpinning research.\u000d\u000a    Key Supporting Grants:\u000d\u000a    &#8226; Maturation of the Worst-Case Analysis Tool, 2012-2013, European Space\u000d\u000a      Agency, &#8364;60K.\u000d\u000a    &#8226; Integrated Guidance Navigation and Control for Mars Sample Return,\u000d\u000a      2011-2013, European Space Agency, (with GMV-Spain, Thales-France), &#8364;55K.\u000d\u000a    &#8226; Scalable Autonomous GNC for Entry Descent and Landing, 2011-2013,\u000d\u000a      European Space Agency, (with SciSys-UK, Thales Alenia-France, NGC-Canada),\u000d\u000a      &#8364;35K.\u000d\u000a    &#8226; Modern Satellite Attitude Control, 2011-2012, European Space Agency,\u000d\u000a      (with Astrium-UK), &#8364;20K.\u000d\u000a    &#8226; Robust Flight Control System Design Verification and Validation\u000d\u000a      Framework, 2011-2012, European Space Agency, (with Astrium Space\u000d\u000a      Transportation-France), &#8364;75K\u000d\u000a    &#8226; Generic Rover Dynamics Model Framework for Autonomous Capability\u000d\u000a      Development, Verification and Validation, Oct 2013 - 2014, CREST2 &#8212; UK\u000d\u000a      Space Agency, (with Astrium-UK), &#163;50K\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"1","Subject":"Aerospace Engineering"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    a. Letter of corroboration from Head of Guidance Navigation &amp;\u000d\u000a      Control Systems European Space Agency\u000d\u000a    b. Letter of corroboration from Head of AOCS\/GNC &amp; Flight\u000d\u000a      Dynamics, EADS Astrium\u000d\u000a    c. Eurospace Annual Report, 2011\u000d\u000a    d. UK Space Agency Report, 2011\u000d\u000a    e. SoW for WCAT-II Maturation of the Worst Case Analysis Tool,\u000d\u000a      TEC-ECN-SoW-SB-02, Dt. 28-06-2011.\u000d\u000a    f. SoW for \"Scalable EDL GNC &amp; Avionics System Demonstrator\"\u000d\u000a      Appendix 1 to ITT AO AO\/1-5966\/08\/NL\/BJ -ESA issue 2 rev 3\u000d\u000a    g. ESA-CNES-DLR Workshop on Worst Case Analysis Tools for\u000d\u000a      Guidance, Navigation and Control Systems, ESA-ESTEC, Noordwijk, 13-14 Nov.\u000d\u000a      2012.\u000d\u000a      (http:\/\/space-env.esa.int\/indico\/confLogin.py?confId=18)\u000d\u000a    h. UK Space Agency CREST Initiative: http:\/\/www.bis.gov.uk\/assets\/ukspaceagency\/docs\/crest-guidelines.pdf\u000d\u000a    ","Title":"\u000d\u000a    Changing the way the European space industry verifies the safety of\u000d\u000a      complex systems\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2636940","Name":"Stevenage"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    In the current V&amp;V process for mission-critical space control\u000d\u000a      systems, worst-case deviations from expected system performance due to\u000d\u000a      uncertainties and variations in the system and its environment must be\u000d\u000a      calculated in order to provide confidence that the mission objectives will\u000d\u000a      be met. The European space industry has to date relied almost completely\u000d\u000a      on Monte Carlo (MC) simulations for the V&amp;V of advanced control\u000d\u000a      systems. In this approach, high-fidelity spacecraft computer simulation\u000d\u000a      models are run repeatedly with their parameters randomly scattered to\u000d\u000a      reflect likely sources of uncertainty. However, MC techniques require a\u000d\u000a      large number of simulations to accurately represent the statistical\u000d\u000a      distributions characterizing variations in performance. As a result, MC\u000d\u000a      simulations are one of the largest consumers of time and computing\u000d\u000a      resources for industrial companies involved in ESA missions, with\u000d\u000a      thousands of simulation runs taking weeks of computing time for critical\u000d\u000a      mission phases such as spacecraft rendezvous, planetary entry, descent and\u000d\u000a      landing (EDL). This translates directly into increased development costs,\u000d\u000a      via increased computational and person-hour requirements. There is\u000d\u000a      increasing concern throughout the European space industry that the cost of\u000d\u000a      V&amp;V for future more autonomous and complex systems will become\u000d\u000a      prohibitive using current approaches.\u000d\u000a    Since his appointment in 2010, Dr Menon and his team at Exeter, which\u000d\u000a      includes Dr Wenfei Wang (Research Fellow), and three industry-funded PhD\u000d\u000a      students, have been working with their industrial collaborators to develop\u000d\u000a      techniques for the V&amp;V of the next generation of space systems. This\u000d\u000a      research [1-6] has investigated how advanced optimisation algorithms could\u000d\u000a      be used to more efficiently search for worst-case scenarios that violate\u000d\u000a      mission-critical performance specifications. This research has resulted in\u000d\u000a      the creation of the worst-case analysis tool (WCAT), a suite of software\u000d\u000a      incorporating advanced probabilistic and deterministic optimisation\u000d\u000a      methods that allow worst-case deviations from multiple performance\u000d\u000a      objectives over any particular phase of the mission to be computed. In\u000d\u000a      collaboration with our industrial partners, the WCAT tool has now been\u000d\u000a      applied on a wide range of different space systems, from flexible\u000d\u000a      telecommunication satellites [2], Earth Observation Satellites [3] and\u000d\u000a      autonomous rendezvous systems [1], to rocket launchers and hypersonic\u000d\u000a      re-entry vehicles [4,5]. These studies have demonstrated the ability of\u000d\u000a      WCAT to deliver more accurate and reliable analysis results (in terms of\u000d\u000a      uncovering true worst-case behaviour of the control system), at\u000d\u000a      significantly lower computational cost, than those produced by current\u000d\u000a      industrial practice [1]. This translates directly into reduced costs for\u000d\u000a      industry via reductions in development timeframes. Results of these\u000d\u000a      studies have been published in leading international journals and\u000d\u000a      conferences [1-6], including specialist ESA-organised workshops, and\u000d\u000a      several of the leading European space companies, including Astrium, GMV,\u000d\u000a      Thales Alenia, SCISYS and NGC (see list of key supporting grants), have\u000d\u000a      initiated collaborations with Dr Menon's group in order to incorporate the\u000d\u000a      WCAT into their in-house V&amp;V process (see evidence item a and\u000d\u000a      evidence item b in Section 5).\u000d\u000a    "},{"CaseStudyId":"35830","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Biotechnology and Biological Sciences Research Council","Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000a    Improved clinical practice and patient health outcomes at H. Lee\u000a        Moffitt Cancer Center\u000a      [Dates Impact Occurred: September 2008 - present]\u000a    The Moffitt Cancer Center (MCC) is a not-for-profit health institute in\u000a      Tampa, FL, USA. It opened in 1986 with a mission to \"contribute to the\u000a        prevention and cure of cancer\". Today it has over 4200 staff, treats\u000a      over 8900 patients per year, with over 4600 inpatient surgeries being\u000a      carried out. It is a nationally ranked hospital in the USA for cancer\u000a      treatment.\u000a    In 2008 the MCC set up the Institute for Mathematical Oncology (IMO),\u000a      with three researchers from the Mathematical Biology research group (Dr.\u000a      Alexander Anderson, 1996-2008; Dr. Kasia Rejniak, 2003-2008; Dr. David\u000a      Basanta, 2006-2008) recruited specifically for their specialist expertise\u000a      in cancer modelling. The success of the initial recruitment has\u000a      subsequently resulted in the further investment of $3Million by the MCC in\u000a      the IMO which has grown to five staff.\u000a    The cancer modelling techniques developed in Dundee have already impacted\u000a      on the clinical culture at MCC by changing clinical practice. Modelling\u000a      work in collaboration with Drs. Susan Minton (Clinical Director of Breast\u000a      Medical Oncology), Julio Pow-Sang (Chair of Genitourinary Oncology) and\u000a      Damon Reed (Medical Director of the Sarcoma Department) has had an impact\u000a      on their clinical practice and consequently on patient health.\u000a      Specifically, the mathematical modelling developed in Dundee and carried\u000a      out by the IMO personnel has led to an improved ability to sub-classify\u000a      patients into potential responders or non-responders for a given therapy\u000a      as well as aid in the early identification of aggressive cancers that need\u000a      to be treated immediately as opposed to non-aggressive cancers that can be\u000a      monitored. Mathematical modelling has also helped to optimise treatment\u000a      scheduling in a patient-specific manner by tailoring scheduling, dosing\u000a      and drug combinations for individual patients [FS1]. In this way, patient\u000a      health outcomes have been improved through the mathematical and\u000a      computational modelling work of the IMO department.\u000a    \"...via the initiation of extraordinarily productive collaborations with\u000a      clinicians, researchers (who originated at the UoA) have successfully\u000a      integrated mathematics into a wide range of subjects in tumour biology and\u000a      oncology. Via the routine integration of mathematics and empirical work,\u000a      their work is having world-wide impact.\" [FS2]\u000a    Development of a new anti-cancer drug at Cyclacel\u000a      [Dates Impact Occurred: September 2008 - December 2010]\u000a    Founded in 1996, Cyclacel Pharmaceuticals Inc. is a biopharmaceutical\u000a      company that develops cell-cycle specific oral therapies for the treatment\u000a      of cancer and other serious diseases. As a consequence of the\u000a      fundamentally nonlinear nature of the cell cycle, the efficacy of\u000a      cell-cycle-specific drugs cannot (typically) be well-understood using\u000a      verbal (linear) reasoning. Hence Cyclacel, in collaboration with the UoA,\u000a      have developed mathematical models of key aspects of the cell-cycle\u000a      pertinent to the drug pathways they wish to target with pharmaceutical\u000a      compounds. The mathematical models have been used to predict the behavior\u000a      of potential anti-cancer drugs that are now undergoing Phase 1 trials\u000a      [FS3]. Specifically, the models were used directly in the selection of\u000a      drug doses for trials, the prediction of toxicity effects and the analysis\u000a      of biomarkers.\u000a    \"The research (at the UoA) developed novel mathematical models of Aurora\u000a      kinase inhibitors (potential anticancer drugs) .. that were of direct\u000a      relevance to the development of Cyclacel's early-stage drugs.\u000a      Specifically, the drug CYC116 (an Aurora Kinase inhibitor) is in a Phase 1\u000a      trial in patients with solid tumors. The pharmacokinetic\/ pharmacodynamic\u000a      (PK\/PD) model of CYC116 drug action was a useful tool in selecting\u000a      starting doses for clinical trials, in predicting possible toxicity, and\u000a      in selecting sampling times for biomarkers. ... the PK\/PD model may assist\u000a      in design of later-stage trials, in design of combination protocols, and\u000a      in individualising treatment protocols to optimise response against\u000a      tumours with defined gene expression patterns.\" [FS4]\u000a    Performance improvement at AstraZeneca\u000a      [Dates Impact Occurred: December 2008 - November 2012]\u000a    AstraZeneca is a multinational biopharmaceutical company that specialises\u000a      in the discovery, development and manufacture of prescription medicines.\u000a      Their largest research and development site is at Alderley Park, UK, and\u000a      employs 2,900 people and hosts their global lead centre for cancer\u000a      research.\u000a    Working in their Systems Biology group in Alderley Park, Dr. Hitesh\u000a      Mistry brought mathematical modelling and simulation skills developed at\u000a      the UoA to processes involved in the industrial design of pharmaceuticals.\u000a      Dr. Mistry's research had an impact in a number of anti-cancer drug\u000a      projects and he was awarded a prestigious AstraZeneca Oncology Award in\u000a      recognition of his unique and highly valued role.\u000a    \"Whilst here at AstraZeneca Dr. Mistry's work was seen as so valuable to\u000a      the organisation that he was awarded a prestigious `Oncology Award' in\u000a      recognition of his unique and highly valued role. Such is the recognition\u000a      of the work that this area will be expanded and will be used for focus for\u000a      financial development. The hypothesis that was generated undoubtedly saved\u000a      months of development time and indirectly affected patients lives.\" [FS5]\u000a    ","ImpactSummary":"\u000a    A dedicated specialist mathematical modelling unit in the H. Lee Moffitt\u000a      Cancer Center, Tampa, FL, USA, was set up &#8212; the Integrated Mathematical\u000a      Oncology Unit (IMO) &#8212; through the movement of three staff with expertise\u000a      in cancer modelling from the UoA's Mathematical Biology (MB) research\u000a      group. Clinical practice has been changed and patient treatment improved\u000a      through the work of IMO.\u000a    Modelling by members of the MB research group and the move of a former\u000a      PDRA led to Cyclacel Ltd. and AstraZeneca obtaining a better understanding\u000a      of the link between drug-dose and drug-efficacy in a class of\u000a      cell-cycle-specific anti-tumour drugs called Aurora kinase inhibitors and\u000a      has led to enhanced business performance.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Dundee\u000a    ","Institutions":[{"AlternativeName":"Dundee (University of)","InstitutionName":"University of Dundee","PeerGroup":"B","Region":"Scotland","UKPRN":10007852}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] Anderson ARA, Chaplain MAJ (1998)\u000a\u0009Continuous and discrete mathematical models of tumour-induced angiogenesis.\u000a\u0009Bull. Math. Biol. 60, 857-899. http:\/\/link.springer.com\/article\/10.1006\/bulm.1998.0042\u000a    \u000a\u000a[2] Anderson ARA, Chaplain MAJ, Newman EL, Steele RJC, Thompson AM (2000)\u000a      Mathematical modelling of tumour invasion and metastasis. J. Theor. Med. 2, 129-154.\u000a\u0009  http:\/\/dx.doi.org\/10.1080\/10273660008833042\u000a    \u000a\u000a[3] Anderson AR, Weaver AM, Cummings PT, Quaranta V (2006)\u000a\u0009Tumor morphology and phenotypic evolution driven by selective pressure from the\u000a      microenvironment. Cell 127, 905-15.\u000a      doi:10.1016\/j.cell.2006.09.042\u000a    \u000a\u000a[4] McDougall S, Anderson ARA, Chaplain MAJ (2006)\u000a\u0009Mathematical Modelling of Dynamic Adaptive Tumour-Induced Angiogenesis: Clinical Implications and\u000a      Therapeutic Targeting Strategies.\u000a\u0009  J. Theor. Biol. 241,\u000a      564-589. http:\/\/dx.doi.org\/10.1016\/j.jtbi.2005.12.022\u000a    \u000a\u000a[5] Mistry HB, MacCallum DE, Jackson RC, Chaplain MAJ, Davidson FA (2008)\u000a      Modelling the Role of Aurora Kinases in Centrosome Separation and the\u000a      Spindle Assembly Checkpoint, PNAS 105 (51), 20215-20220.\u000a      doi:10.1073\/pnas.0810706106\u000a    \u000a\u000a[6] Kamei H, Jackson RC, Zheleva D, Davidson FA (2010)\u000a\u0009An integrated\u000a      pharmacokinetic-pharmacodynamic model for an Aurora kinase inhibitor. J.\u000a        Pharmacokinetics and Pharmacodynamics 37, 404-434.\u000a      doi:10.1007\/s10928-010-9166-0\u000a    \u000aEvidence of Research Quality:\u000a      (i) Grants:\u000a      [7] PI - Prof. M.A.J. Chaplain (co-I - Prof. S.L. Schor, Dr. A.M. Schor):\u000a      \"The mathematical modelling, simulation and prediction of matrix\u000a      modulation of angiogenesis\". BBSRC\/EPSRC Mathematical Biology Initiative\u000a      (grant 94\/MMI09008). 36 months, October 1997 - September 2000; &#163;223,876.\u000a    [8] PI - Dr. A.R.A. Anderson: \"Multi-scale mathematical modelling of\u000a      cancer invasion\". National Institutes of Health (NIH, USA). 60 months,\u000a      January 2003 - December 2007; $500,000 (c. &#163;330,461).\u000a    [9] PI - Prof. M.A.J. Chaplain: \"From mutations to metastases: Multiscale\u000a      mathematical modelling of cancer growth and spread.\" European Research\u000a      Council Advanced Investigator Grant (ERC AdG). 60 months, September 2009 -\u000a      August 2014; &#8364;1.68Million.\u000a    [10] PI - Prof. M.A.J. Chaplain: \"Mathematical modelling of cell-cycle\u000a      dependent anti-cancer drugs.\" EPSRC CASE PhD Studentship with Cyclacel\u000a      Ltd. 36 months, October 2003 - October 2006; c.&#163;40,000.\u000a    [11] PI - Dr. F. Davidson; co-I - Prof. M.A.J. Chaplain: \"The modelling\u000a      and analysis of the pharmacodynamics of anti-cancer drugs.\" EPSRC\u000a      \"Mathematics for Business\" initiative (grant EP\/D043859\/1). 36 Months,\u000a      October 2006 - September 2009; &#163;152,908.\u000a    [12] PI - Prof. M.A.J. Chaplain: \"Mathematical modelling of Drug\u000a      Metabolism: Using in silico Techniques to Investigate the Cytochrome P450\u000a      Enzyme System in Hepatic Reductase Null Mice.\" EPSRC CASE PhD Studentship\u000a      with CXR Biosciences. 36 months, October 2006 - October 2009; c.&#163;50,000.\u000a    (ii) Prizes and Awards:\u000a      [13] Award of London Mathematical Society Whitehead Prize to Professor M.\u000a      Chaplain for research work on the mathematical modelling of cancer growth,\u000a      July 2000.\u000a    [14] Professor M. Chaplain elected Fellow of the Royal Society of\u000a      Edinburgh, March 2003\u000a    [15] Professor M. Chaplain awarded a Leverhulme Personal Research\u000a      Fellowship, April 2007\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"11","Level2":"12","Subject":"Oncology and Carcinogenesis"}],"Sources":"\u000a    Factual Statements have been obtained from:\u000a    [FS1] Chair, Department of Integrated Mathematical Oncology, H. Lee\u000a      Moffitt Cancer Center, Tampa, FL, USA..\u000a    [FS2] Chair, Radiology, H. Lee Moffitt Cancer Center, Tampa, FL, USA.\u000a    [FS3] Programme Manager, Research and Development, Cyclacel Ltd.\u000a    [FS4] Director, Pharmacometrics Ltd.\u000a    [FS5] Therapeutic Area Pharmacometrics Expert, Clinical Oncology and\u000a      Infection, Clinical Pharmacology and Pharmacometrics, AstraZeneca UK Ltd.\u000a    \u000a    ","Title":"\u000a    Using Mathematical Modelling to Improve Cancer Treatment\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Led by Professor M. Chaplain, one of the main areas of research of the\u000a      UoA is the mathematical modelling of all aspects of cancer &#8212; growth,\u000a      spread and treatment. Since the late 1990s, the Mathematical Biology\u000a      research group has been at the forefront of developing quantitative and\u000a      predictive mathematical models in this area.\u000a    A seminal paper by Anderson and Chaplain in 1998 developed a novel hybrid\u000a      modelling technique (continuum-discrete, deterministic-stochastic) to\u000a      study tumour-induced angiogenesis, the process by which a tumour develops\u000a      its own blood supply [1]. Subsequent refinement of this hybrid technique\u000a      in collaboration with clinical colleagues at Ninewells Hospital led to a\u000a      mathematical model of tumour invasion and metastasis [2, 3], the process\u000a      by which tumour cells break off the main tumour mass and spread throughout\u000a      the body. The key idea behind the technique involves using a discretized\u000a      form of a system of nonlinear partial differential equations to develop a\u000a      biased random-walk model which enables the tracking of individual cells.\u000a      This naturally introduced stochasticity and a multi-scale aspect to the\u000a      model. This work led to the award of a London Mathematical Society\u000a      Whitehead Prize to Chaplain in June 2000:\u000a    \"His research establishes a framework in which clinical treatments can\u000a        be tested and has brought him international recognition amongst the\u000a        mathematical biology community... He and his group have developed this\u000a        area of research, are at its forefront, and its results could lead to a\u000a        massive advance in the treatment and control of malignant cancers.\"\u000a      [13]\u000a    Drawing on the stochastic and multi-scale nature of the modelling\u000a      approach, this initial research resulted in the subsequent development of\u000a      models on how best to target tumours via blood-borne drugs [4] leading to\u000a      new research into anti-cancer drug modelling. EPSRC funding to Chaplain\u000a      and Davidson through the Mathematics for Business scheme and Mathematics\u000a      CASE PhD Studentships [10, 11, 12] connected the modelling to the\u000a      pharmaceutical industry via the biotech companies Cyclacel Ltd. and CXR\u000a      Biosciences. The modelling undertaken here investigated the role played by\u000a      Aurora B kinase which is overexpressed in a subset of cancers and is\u000a      required for mitosis, making it an attractive anti-cancer target. Members\u000a      of the Mathematical Biology research group, along with colleagues from\u000a      Cyclacel Ltd., developed a novel stochastic mathematical model of the\u000a      spindle assembly checkpoint to incorporate all signaling\u000a      kinetochores within a cell rather than just one, and the role of Aurora B\u000a      within the resulting model [5, 6]. Computational simulation results of the\u000a      model showed that when Aurora B inhibition is considered, for a certain\u000a      range of inhibitor concentrations, a prolonged prometaphase\/metaphase is\u000a      observed.\u000a    The hybrid modelling of angiogenesis and cancer invasion was carried out\u000a      by Prof. M. Chaplain (then Chair in Mathematical Biology, now Ivory Chair\u000a      of Applied Mathematics), Dr. A. Anderson (Senior Lecturer), Dr. K. Rejniak\u000a      (PDRA), Dr. D. Basanta (PDRA) during the period 1996 - 2008. Drs.\u000a      Anderson, Rejniak and Basanta left Dundee in August 2008 to set up IMO at\u000a      Moffitt Cancer Center.\u000a    The Aurora B kinase modelling was carried out by Dr. F. Davidson (Reader\u000a      in Mathematical Biology), Prof. M. Chaplain (as above) and Dr. H. Mistry\u000a      (PDRA) during the period 2003 - 2009 [11]. Dr. Mistry left Dundee in 2008\u000a      to work in AstraZeneca.\u000a    "},{"CaseStudyId":"35831","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3144096","Name":"Norway"},{"GeoNamesId":"660013","Name":"Finland"},{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000a    Optimization problems are ubiquitous in industry, for example, how to\u000a      schedule airline crew, how to\u000a      improve drug design, how to minimize production costs, how to maximize\u000a      power output, how to find\u000a      the lowest energy configuration, and how to find the best combination of\u000a      medicines for a particular\u000a      patient.\u000a    The invention of the filter method by Professor Fletcher and co-workers\u000a      meant that a broad range\u000a      of previously computationally unfeasible optimization problems could be\u000a      efficiently solved.\u000a      Arguably the most challenging of the optimization problem types is MINLP,\u000a      in which some of the\u000a      variables are constrained to take only discrete values. Until fairly\u000a      recently it was regarded that such\u000a      problems were intractable unless the number of variables was small.\u000a      Professor Fletcher and co-workers\u000a      showed that using filterSQP to solve the many NLP sub-problems\u000a      generated in the Branch\u000a      and Bound method for MINLP, has considerably extended the range of\u000a      problems that can be\u000a      solved. A production software product MINLP_BB has been made\u000a      available and has attracted\u000a      many users. Since 1997, the University of Dundee has issued close to 100\u000a      licenses for Professor\u000a      Fletcher's optimization software algorithms to a range of end-users\u000a      including numerous\u000a      international-scale industrial companies e.g. those referred to in Section\u000a      1.\u000a    Four examples of where Professor Fletcher's filter method has made direct\u000a      impact are:\u000a    (i) Use of the filter method in the Design Explore\u000a      software utilized by The Boeing Company.\u000a      [Dates Impact Occurred: January 2008 - present]\u000a    Boeing is the world's leading aerospace company and the largest\u000a      manufacturer of commercial\u000a      jetliners and military aircraft. It employs over 170,000 people worldwide\u000a      with a total revenue in\u000a      2012 of $82 billion. Design Explore is the primary tool for design\u000a      optimization used by the Boeing\u000a      company. The filter method invented by Professor Fletcher and co-workers\u000a      has been at the heart\u000a      of this software.\u000a    \"...it is difficult to imagine how different life at Boeing would\u000a      be without the optimization tools....\u000a      whose underlying filter-based methods were inspired by the idea of Roger\u000a      Fletcher and Sven\u000a      Leyffer...no constrained optimization problem is ever solved in Design\u000a        Explore without making use\u000a      of the [Fletcher] filter\", [FS1].\u000a    The filter algorithm continues to be used extensively in engineering\u000a      projects throughout the\u000a      company. Examples that can be corroborated in open literature are the\u000a      joined wing sensorcraft\u000a      (LeDuox et al. AIAA2008-7190), helicopter rotor blade design (Hirsh et al.\u000a      AIAA2007-1252) and\u000a      hypersonic vehicle design (Bowcutt et al., AIAA2008-2591).\u000a    (ii) Use of the filter method in IPOPT core optimization software in\u000a        the IBM circuit tuning tool\u000a        EinsTuner.\u000a      [Dates Impact Occurred: January 2008 - present]\u000a    IBM is an international IT company employing over 430,000 worldwide (over\u000a      20,000 in the UK) with\u000a      a total revenue in 2012 of $105 billion dollars. The EinsTuner software is\u000a      used to determine the\u000a      optimal transistor sizes in commercial digital circuits. The filter\u000a      concept is central to the IPOPT\u000a      software, which is at the core of EinsTuner. The advantage offered by this\u000a      software has been\u000a      valued by IBM at \"millions of dollars\" and IPOPT is \"extremely\u000a      widely used\". \"The filter idea...has\u000a      extraordinary impact, both at IBM and ..[other] industry\" [FS2].\u000a    \"At IBM Research, we also used Prof. Fletcher's filterSQP\u000a      and bqpd optimization codes within\u000a      algorithms for the solution of MINLPs. ...Prof. Fletcher's filterSQP\u000a      and bqpd solvers are used to\u000a      solve continuous subproblems, the number of which can be in the millions\u000a      during the solution of a\u000a      single MINLP. In this context, the reliability and efficiency of the filterSQP\u000a      and bqpd software\u000a      packages have been essential for the good performance of the MINLP\u000a      solvers.\" [FS3].\u000a    (iii) Use of the filter method in European Space Agency activities.\u000a      [Dates Impact Occurred: January 2008 - present]\u000a    The ESA is a joint venture between 20 European Member States dedicated to\u000a      the exploration of\u000a      space. It employs over 2000 people and has an annual budget of near\u000a      &#8364;5Billion. A fundamental\u000a      requirement for ESA is to solve optimization problems related to space\u000a      vehicle manoeuvre\u000a      planning, trajectory optimization, vehicle performance optimization and\u000a      optimal vehicle design.\u000a      ESA has used filterSQP as a benchmarking tool for all their\u000a      optimization developments. Two\u000a      projects (totalling approx. &#8364;400K) were initiated to foster the\u000a      utilization of filter techniques in\u000a      European space industry. The impact of Prof. Fletcher's filter method in\u000a      the work of ESA is best\u000a      summarised by the following direct quotes:\u000a      \"[Fletcher's] input proved indispensable for achieving the targeted\u000a      quality of work\"\u000a      \"filter techniques.... reduced the computation time by a factor of\u000a      two\"\u000a      \"Fletcher .. has advanced the way numerical optimization is carried\u000a      out at the European Space\u000a      Agency\". Dr Sven Erb TEC-ECN \/ GNC Systems Engineer\u000a    (iv) Use of the filter method in TOMLAB.\u000a      [Dates Impact Occurred: January 2008 - present]\u000a    TOMLAB is the premier developer and distributer of large scale\u000a      optimization software for use in\u000a      conjunction with MATLAB. Since 2000, TOMLAB have offered Prof. Fletcher's\u000a      algorithms (bqpd,\u000a      filter SQP, miqpBB and minlpBB) as part of their software suite.\u000a      Commercial clients selecting these\u000a      codes include Honeywell International, General Motors, International\u000a      Atomic Energy Authority and\u000a      Draper Laboratories amongst others [FS4].\u000a    Prof. Fletcher's QP solvers are also utilized in a wide range of other\u000a      industries for example in the\u000a      forestry management systems GAYA and SIMO used by the Norwegian and\u000a      Finnish industries.\u000a      These industries contribute a significant component to the GDP of the\u000a      Nordic countries [FS5].\u000a    As well as software tools, Prof. Fletcher has provided vital advice to a\u000a      wide range of industrial end-users\u000a      of his code including a consultancy agreement with the European Space\u000a      Agency.\u000a    ","ImpactSummary":"\u000a    Research led by Professor Roger Fletcher has resulted in the development\u000a      of a suite of algorithms\u000a      that are now widely used throughout industry. An algorithm of fundamental\u000a      importance\u000a      constructed by Fletcher and co-workers is the filter method &#8212; a\u000a      radically different approach to\u000a      solving large and complex nonlinear optimization problems typical of those\u000a      faced by industry. This\u000a      algorithm was developed with the principal aim of providing a\u000a      computationally reliable and effective\u000a      method for solving such problems. The filter method is now utilised by a\u000a      variety of high-profile\u000a      industry end-users including IBM, Schlumberger, Lucent, EXXON, Boeing, The\u000a      Ford Motor\u000a      Company, QuantiSci and Thomson CSF. The use of the filter method has had a\u000a      significant\u000a      economic and developmental impact in these companies through enhanced\u000a      business performance\u000a      and cost savings.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Dundee\u000a    ","Institutions":[{"AlternativeName":"Dundee (University of)","InstitutionName":"University of Dundee","PeerGroup":"B","Region":"Scotland","UKPRN":10007852}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] Fletcher R. and Leyffer S.,\u000a      Nonlinear Programming Without a Penalty Function, Mathematical\u000a      Programming, 91, 2002, pp.\u000a      239-269. doi: 10.1007\/s101070100244\u000a    \u000a\u000a[2] Fletcher, R., Leyffer, S. and Toint, Ph. L.\u000a      On the Global Convergence of a Filter-SQP Algorithm, SIAM J. Optimization,\u000a      13, 2002, pp. 44-59.\u000a      doi: 10.1137\/S105262340038081X\u000a    \u000a\u000a[3] Chin C.M. and Fletcher R., On the Global Convergence of an SLP-Filter\u000a      Algorithm that takes\u000a      EQP steps, Mathematical Programming, 96, 2003, pp. 161-177. doi:\u000a      10.1007\/s10107-003-0378-6\u000a    \u000aEvidence of Research Quality:\u000a      Prizes and Awards to Professor Roger Fletcher:\u000a    [4] Awarded the 1997 George B Dantzig Prize by the Society for Industrial\u000a      and Applied\u000a      Mathematics\u000a    [5] Elected Fellow of the Royal Society of London, 2003\u000a    [6] Awarded the 2006 Lagrange Prize in Continuous Optimization by The\u000a      Mathematical\u000a      Programming Society and the Society for Industrial and Applied Mathematics\u000a    [7] Awarded the 2008 Royal Gold Medal from the Royal Society of Edinburgh\u000a    [8] Elected SIAM Fellow, 2009\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000a    Factual Statements have been obtained from:\u000a    [FS1] Senior Mathematician, The Boeing Company, P.O. Box 3707, Seattle,\u000a      WA 98124-2207,\u000a      USA.\u000a    [FS2] Research Relationship Manager, USA (Chemicals), IBM Research\u000a      Division.\u000a    [FS3] Senior Member, Nonlinear Optimization Research Group, IBM Research\u000a      Division (until\u000a      2012); currently Associate Professor at Northwestern University, USA.\u000a    [FS4] CEO, TOMLAB Optimization Inc.\u000a    [FS5] Senior Researcher, Finnish Forest Research Institute.\u000a    Licence details available from Research and Innovation Services,\u000a      University of Dundee.\u000a    \u000a    ","Title":"\u000a    The Development of Commercial Optimization Software\u000a    ","UKLocation":[{"GeoNamesId":"2650752","Name":"Dundee"}],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research emanates from the team led by Professor Roger\u000a      Fletcher, a world-leading\u000a      researcher in optimization (see prizes and awards). Professor Fletcher has\u000a      developed\u000a      methodology and theory for a wide range of constrained optimization\u000a      problem types such as Linear\u000a      Programming, Quadratic Programming (QP), Nonlinear Programming (NLP) and\u000a      Mixed Integer\u000a      Nonlinear Programming (MINLP), in increasing order of complexity.\u000a    NLP concerns the minimization of an objective function of many\u000a      variables, subject to nonlinear\u000a      equality and inequality constraints on the values that can be\u000a      taken by the variables. A well-known\u000a      method for NLP is the Sequential Quadratic Programming (SQP) method, which\u000a      involves the\u000a      iterated solution of a sequence of QP problems.\u000a    Professor Fletcher is widely respected for his many contributions over\u000a      the years to these areas\u000a      (being the recipient of a number of awards [4]-[8]), including for the\u000a      introduction of so-called Active\u000a      Set Methods for QP, and classic optimization methods such as the widely\u000a      used Davidon-Fletcher-Powell,\u000a      Fletcher-Reeves and Broyden-Fletcher-Goldfarb-Shanno (BFGS) methods.\u000a    Most relevant to this Impact Case, is Professor Fletcher's invention of\u000a      the filter globalization\u000a        scheme in 1996 (UoD Numerical Analysis Technical Report NA171, see\u000a      also ref. [1]). This method\u000a      has won particular praise for its novelty and effectiveness (Lagrange\u000a      Prize 2006). Up to that time,\u000a      the usual procedure for solving an NLP problem was to use a 'penalty\u000a      function' to induce\u000a      convergence of e.g. the SQP method, and to require iterations of the SQP\u000a      method to continuously\u000a      improve the value of the penalty function. Often this requirement could\u000a      considerably slow down the\u000a      speed of convergence of the SQP method. In the filter method, the\u000a      objective function and the\u000a      constraint violation function are regarded as two distinct functions, and\u000a      iterations can be accepted\u000a      if they improve either one of these functions. This is a much less\u000a      restrictive condition, and allows\u000a      faster convergence of the NLP solver. The generality of the filter method\u000a      allows for its use in many\u000a      contexts, such as trust region, line search, and interior point methods.\u000a    In [1], Professor Fletcher and post-doctoral research assistant Dr Sven\u000a      Leyffer (UoD 1993 - 2002,\u000a      including EPSRC grant GR\/K51204) introduced the implementation of the\u000a      filter concept in the\u000a      context of the SQP method for NLP, with suitable heuristics to induce\u000a      global convergence. A wide\u000a      variety of test results for large scale NLP showed its effectiveness and\u000a      robustness in comparison to\u000a      another state-of-the-art code. Production quality codes, filterSQP\u000a      for NLP, and the QP solver bqpd\u000a      with novel sparse matrix and other user-friendly facilities, were made\u000a      available for distribution.\u000a    In follow-up papers [2, 3], Professor Fletcher was able to refine the\u000a      heuristics in the filter methods,\u000a      and used novel arguments so as to provide a mathematically exact proof of\u000a      convergence.\u000a    The introduction of the filter idea has evoked considerable interest in\u000a      optimization circles, and\u000a      many mini-symposia and workshops have since been devoted to its\u000a      implications. The new\u000a      approach to balancing optimality and feasibility has been shown to have\u000a      applications in many\u000a      areas of optimization and has been utilized by researchers in diverse\u000a      areas such as constrained\u000a      and unconstrained optimization, solving systems of nonlinear equations\u000a      (the idea of a multi-filter),\u000a      and in derivative-free optimization (references [1, 2] have been cited\u000a      over 1000 times).\u000a    The research was carried out by Professor Roger Fletcher, Baxter Chair of\u000a      Mathematics and post-doctoral\u000a      research assistant Dr Sven Leyffer between 1993 and 2006. Dr. Leyffer left\u000a      in 2002 to\u000a      take up a position at the Argonne National Laboratory, Argonne, IL USA.\u000a    "},{"CaseStudyId":"36084","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council","Royal Academy of Engineering"],"ImpactDetails":"\u000d\u000a    We now present evidence to show that the research cited in &#167;2 and &#167;3\u000d\u000a      above has changed the\u000d\u000a      understanding and awareness of major industrial companies of the\u000d\u000a      advantages of BDD and has\u000d\u000a      resulted in its integration into major industrial trials and proprietary\u000d\u000a      software products.\u000d\u000a    BDD is now routinely used as an analysis tool for certification of\u000d\u000a      engineering designs, i.e. before\u000d\u000a      implementation. Much more significantly, BDD can also be used to support\u000d\u000a      decision-making on\u000d\u000a      system utilisation in real-time operation. For example, as\u000d\u000a      component faults are reported or\u000d\u000a      environmental conditions change fast BDD calculations can be performed to\u000d\u000a      update the prediction\u000d\u000a      of the probability of mission failure; when this reaches some preset\u000d\u000a      critical threshold appropriate\u000d\u000a      action can be taken. BDD is a major development with wide and significant\u000d\u000a      applications. In\u000d\u000a      particular, recognising BDD's value, BAE Systems have adopted it for use\u000d\u000a      in unmanned aerial\u000d\u000a      vehicles (UAVs).\u000d\u000a    BDD's are used for many different aerospace applications such as by NASA\u000d\u000a      (http:\/\/www.hq.nasa.gov\/office\/codeq\/doctree\/fthb.pdf)\u000d\u000a      and by the group of Antoine Rauzy at the\u000d\u000a      Ecole Polytechnique in Paris [5.5].\u000d\u000a    Example 1\u000d\u000a    The development of BDD led to a large collaborative project with BAE\u000d\u000a      Systems as Project\u000d\u000a      Partners, called NECTISE (Network Enabled Capability Through Innovative\u000d\u000a      Systems Engineering).\u000d\u000a      It was also supported by EPSRC and worth &#163;8.4 million\u000d\u000a      (1\/11\/2005-30\/4\/2009) with about half\u000d\u000a      being industrial funding. The research involved ten universities, led by\u000d\u000a      Loughborough University.\u000d\u000a      Prof John Andrews was the principle investigator of the submission, and\u000d\u000a      Prof Michael Henshaw\u000d\u000a      (Loughborough University) later led the programme. The impact of BDD on\u000d\u000a      this work and its\u000d\u000a      outcome for BAE can be verified by [5.1]. The key aspect being\u000d\u000a      that the BDD decision-making\u000d\u000a      capability in the area of real-time diagnostics was fundamental to a\u000d\u000a      number of BAE Systems\u000d\u000a      research themes and changed the understanding and awareness as to the\u000d\u000a      operational advantages\u000d\u000a      conferred by BDD. In particular the work led to the integration of BDD's\u000d\u000a      into major industrial\u000d\u000a      research programmes: Autonomous Systems and Integrated Vehicle Health\u000d\u000a      Management (IVHM).\u000d\u000a    Example 2\u000d\u000a    The development of BDD at Loughborough University led to it being\u000d\u000a      thoroughly tested and its\u000d\u000a      viability demonstrated for an entirely different application, firstly in\u000d\u000a      BAE Systems-led projects\u000d\u000a      ASTRAEA I (2006-8) and then in ASTRAEA II (2009-2013) concerning UAVs. It\u000d\u000a      is predicted that\u000d\u000a      following the Tornado, no future UK\/European military aircraft\u000d\u000a      will have onboard pilots, and for\u000d\u000a      such aircraft UAVs will be the norm. BAE's Advanced Technical Centres have\u000d\u000a      carried out\u000d\u000a      experimental trials and BAE's Military Air &amp; Information Group is (in\u000d\u000a      2012-13) undertaking UAV\u000d\u000a      flight trials. BDD is an essential part of this work and the impact of\u000d\u000a      this research will be felt\u000d\u000a      wherever UAVs are deployed &#8212; such as earth observation, monitoring of\u000d\u000a      pipelines and power lines,\u000d\u000a      detecting and controlling forest fires, law enforcement, border control\u000d\u000a      and coastguarding. The UAV\u000d\u000a      must have the ability to respond to changing conditions. Such changes can\u000d\u000a      occur due to\u000d\u000a      component failures causing loss of functionality or reduced redundancy,\u000d\u000a      changing weather\u000d\u000a      conditions, or the emergence of a threat such as another aircraft in the\u000d\u000a      locality. When these\u000d\u000a      conditions are reported the mission success likelihood is re-evaluated\u000d\u000a      using the BDD approach,\u000d\u000a      accounting for the new conditions.\u000d\u000a    Mission failures can be considered in two ways: catastrophic failure\u000d\u000a      where the vehicle will be lost,\u000d\u000a      and mission failure where the mission objectives are not accomplished but\u000d\u000a      the vehicle lands safely.\u000d\u000a      When the predicted likelihood that the vehicle will successfully perform\u000d\u000a      its intended task becomes\u000d\u000a      unacceptably low, action is required to mitigate this situation. This\u000d\u000a      action takes the form of mission\u000d\u000a      reconfiguration. Mission reconfiguration for a UAV selects a new route,\u000d\u000a      new mission objectives, or\u000d\u000a      can be implemented to abort the mission and make an emergency landing. It\u000d\u000a      is this mission\u000d\u000a      reconfiguration process through the use of BDD's which provides the main\u000d\u000a      impact in UAV design.\u000d\u000a    Within the ASTRAEA project there have been a number of projects (or\u000d\u000a      themes). Specifically,\u000d\u000a      ASTRAEA's Theme T7 `Health Management System Design' addressed the issue\u000d\u000a      of `maintaining\u000d\u000a      real-time system awareness of the UAV'. This was a collaborative project\u000d\u000a      between BAE Systems,\u000d\u000a      LU and Aberystwyth University. According to a BAE Systems member, `BDD was\u000d\u000a      an essential\u000d\u000a      underpinning methodology' [5.1].\u000d\u000a    The significance of ASTRAEA I's Theme 7 (referred to above) led to its\u000d\u000a      being selected as a finalist\u000d\u000a      (one of three) in the 2009 Institution of Engineering and Technology\u000d\u000a      Innovation Awards, a further\u000d\u000a      indicator of the impact of the BDD method [5.2, 5.3].\u000d\u000a    The research has therefore been integrated into experimental trials in\u000d\u000a      one of the world's largest\u000d\u000a      manufacturing companies in an area of vital importance to the future of UK\u000d\u000a      security and economic\u000d\u000a      development.\u000d\u000a    A further indicator of the impact of BDD comes from the fact that\u000d\u000a      commercial software companies\u000d\u000a      such as ISOGRAPH, ARALIA and ITEM SOFTWARE now incorporate BDD in their\u000d\u000a      proprietary\u000d\u000a      packages for Fault Tree Analysis [5.4]. The research therefore\u000d\u000a      changed the awareness of another\u000d\u000a      industry leading to new products and services.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Led by Professor Andrews, a computational method for real time mission\u000d\u000a      planning, based on\u000d\u000a      Binary Decision Diagrams (BDD), was developed in the Mathematical Sciences\u000d\u000a      Department at\u000d\u000a      Loughborough University (LU) from 1993-2003. This is fast and accurate and\u000d\u000a      can be used to\u000d\u000a      support decision-making on system utilisation in real-time operation,\u000d\u000a      which has led to the ability\u000d\u000a      to diagnose in flight faults for unmanned aerial vehicle (UAV)\u000d\u000a      applications.\u000d\u000a    The research has changed the understanding and awareness of the\u000d\u000a      advantages of BDD, resulting\u000d\u000a      in integration into major industrial trials and proprietary software\u000d\u000a      products, including at BAE\u000d\u000a      Systems, one of the world's largest companies in an area of vital\u000d\u000a      importance to UK security and\u000d\u000a      economic development. The methodology has attracted significant research\u000d\u000a      funding in\u000d\u000a      collaborative programmes with industry.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    Loughborough University\u000d\u000a    ","Institutions":[{"AlternativeName":"Loughborough University","InstitutionName":"Loughborough University","PeerGroup":"B","Region":"East Midlands","UKPRN":10004113}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2988507","Name":"Paris"}],"References":"\u000d\u000a    \u000a3.1. Sinnamon, R.M. and Andrews, J.D., (1996), Fault Tree\u000d\u000a      Analysis and Binary Decision\u000d\u000a      Diagrams, Proc. IEEE Reliability and Maintainability Symposium,\u000d\u000a      215-222, DOI:\u000d\u000a      10.1109\/RAMS.1996.500665\u000d\u000a    \u000a\u000a3.2. Sinnamon, R.M. and Andrews, J.D., (1997), New Approaches to\u000d\u000a        Evaluating Fault Trees,\u000d\u000a        Journal of Reliability Engineering and System Safety, 58,\u000d\u000a        89-96, DOI: 10.1016\/S0951-8320(96)00036-1\u000d\u000a    \u000a\u000a3.3. Bartlett, L.M. and Andrews, J.D., (2001), An Ordering\u000d\u000a      Heuristic to Develop the Binary\u000d\u000a      Decision Diagram Based on Structural Importance, Reliability\u000d\u000a        Engineering and System\u000d\u000a        Safety, 72, 31-38, DOI: 10.1016\/S0951-8320(00)00103-4\u000d\u000a    \u000a\u000a3.4. Reay, K. and Andrews, J.D., (2002), A Fault Tree Analysis\u000d\u000a        Strategy Using Binary\u000d\u000a        Decision Diagrams, Reliability Engineering and System Safety,\u000d\u000a      78, 45-56, DOI:\u000d\u000a        10.1016\/S0951-8320(02)00107-2\u000d\u000a    \u000a\u000a3.5. Andrews, J.D. and Beeson, S., (2003), Birnbaum's measure of\u000d\u000a        component importance\u000d\u000a        for non-coherent systems, IEEE Transactions on Reliability,\u000d\u000a      52, 213-219, DOI: 10.1109\/TR.2003.809656\u000d\u000a    \u000a\u000a3.6. Beeson, S. and Andrews, J.D., (2003), Importance\u000d\u000a      measures for non-coherent-system\u000d\u000a      analysis, IEEE Transactions on Reliability, 52, 301-310, DOI:\u000d\u000a      10.1109\/TR.2003.816397\u000d\u000a    \u000aGrants\u000d\u000a    EP\/D505461\/1, A Research Proposal in Systems Engineering Addressing\u000d\u000a        the Question: Are you\u000d\u000a        prepared for NEC? &#163;8,397,350, 01\/11\/05-30\/04\/09. This was\u000d\u000a      co-ordinated by Prof. Andrews who\u000d\u000a      was also PI before moving to Nottingham\u000d\u000a    ASTRAEA I consisted of 16 Projects [Ref www.astraea.aero],\u000d\u000a      value &#163;32 million. ASTRAEA II which\u000d\u000a      followed consisted of 6 Projects, value &#163;30 million. Total funding for\u000d\u000a      ASTRAEA &#163;62 million.\u000d\u000a    [Ref: http:\/\/www.uavs.org\/astraea].\u000d\u000a      The projects involved 7 major companies and 5 universities and\u000d\u000a      about 12 SME's and subcontractors and was led at Loughborough University\u000d\u000a      by Prof. Paul Chung\u000d\u000a      (now Dean of Science at LU).\u000d\u000a    The research quality of the work, in terms of originality, rigour and\u000d\u000a      significance is evidenced by the\u000d\u000a      above papers and the large amount of external funding obtained, including\u000d\u000a      industrial funding, in\u000d\u000a      addition to the prizes referred to in Section 2 above.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"},{"Level1":"8","Level2":"6","Subject":"Information Systems"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000d\u000a    The following sources of corroboration can be made available at request:\u000d\u000a    5.1. Email from Control Systems Design &#8212; Trent 1000, Aero Engine\u000d\u000a      Controls &#8212; Rolls-Royce plc\u000d\u000a      Group Sin C-6, PO Box 31, Derby, DE24 8BJ\u000d\u000a    5.2. Email from BAE &#8212; Warton &#8212; Military Air &amp; Information\u000d\u000a    5.3. The Institution of Engineering and Technology (IET FINALIST)\u000d\u000a      &#8212; Highly commended award\u000d\u000a      made 25 November 2009.\u000d\u000a    5.4. ARALIA &#8212; \"Seminal work on the use of BDD to assess\u000d\u000a        importance factors has been done by\u000d\u000a        J. Andrews and his students ....\" A quotation taken from a chapter\u000d\u000a      entitled BDD for Reliability\u000d\u000a      Studies in K.B. Misra ed., Handbook of Performability Engineering.\u000d\u000a      Elsevier. pp 381-396,\u000d\u000a      2008. Available as PDF at:\u000d\u000a      http:\/\/www.lix.polytechnique.fr\/~rauzy\/publications\/publications.html\u000d\u000a    ","Title":"\u000d\u000a    Fast binary decision algorithms to enable real time diagnosis of\u000d\u000a        in-flight\u000d\u000a        faults in Unmanned Aerial Vehicles\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Fault Trees provide a diagrammatic description of the various causes of a\u000d\u000a      specified system failure\u000d\u000a      in terms of the failure of its components. Fault Tree Analysis\u000d\u000a      (FTA) is used to predict the failure\u000d\u000a      likelihood of complex engineering systems (safety critical systems) and is\u000d\u000a      widely used in industry\u000d\u000a      (Nuclear, Petro-Chemical, Aircraft, etc.). However, FTA is such a\u000d\u000a      computationally intensive method\u000d\u000a      that for large-scale systems approximations are required, i.e. FTA has to\u000d\u000a      be truncated. The Binary\u000d\u000a      Decision Diagrams (BDD) method, developed at Loughborough University\u000d\u000a      overcomes this inherent\u000d\u000a      weakness by performing the calculations exactly, and, being much\u000d\u000a        quicker, offers considerable\u000d\u000a      advantages in efficiency enabling the technique to be used in new\u000d\u000a        contexts.\u000d\u000a    The development of the BDD system was undertaken at LU by Prof John\u000d\u000a      Andrews, primarily whilst\u000d\u000a      supervising five research students: Ros Sinnamon (PhD 1996), Lisa Bartlett\u000d\u000a      (PhD 2000), Karen\u000d\u000a      Reay (PhD 2002), Sally Beeson (PhD 2002), Rasa Remenyte-Prescott (PhD\u000d\u000a      2008). The four key\u000d\u000a      aspects researched and reported in LU PhD theses were:\u000d\u000a    (1) Developing efficient BDD methodology. [3.1, 3.2]\u000d\u000a    (2) Structuring the logic to identify the critical components and produce\u000d\u000a      an efficient ordering for\u000d\u000a      BDD computation. [3.3]\u000d\u000a    (3) Modularising BDD for increased efficiency and extended applications.\u000d\u000a      [3.4]\u000d\u000a    (4) Developing measures for non-coherent fault trees (i.e. where both the\u000d\u000a      occurrence of an event\u000d\u000a      or its non-occurrence may contribute to failure). [3.5, 3.6]\u000d\u000a    The fundamental research outcomes were published in the above PhD theses\u000d\u000a      and in a number of\u000d\u000a      high quality journal papers and conference proceedings, a number of which\u000d\u000a      won awards. Some of\u000d\u000a      these are listed in Section 3. Presentations at conferences were attended\u000d\u000a      by academics and\u000d\u000a      industrialists in about equal numbers.\u000d\u000a    The underlying work led to an EPSRC funded project, with BAE Systems as\u000d\u000a      Project Partners,\u000d\u000a      called NECTISE (Network Enabled Capability Through Innovative Systems\u000d\u000a      Engineering) worth\u000d\u000a      &#163;8.4 million (1\/11\/2005-30\/4\/2009). Its aims were `to investigate\u000d\u000a      through-life systems management\u000d\u000a      for defence capability' (i.e. to enable defence suppliers such as BAE to\u000d\u000a      develop systems capable\u000d\u000a      of responding to changing requirements in complex dynamically connected\u000d\u000a      networks of supplier-customer\u000d\u000a      organisations). This project involved 11 Universities and BAE Systems.\u000d\u000a      Three of the\u000d\u000a      twelve Investigators and four of the sixteen RAs were based at LU. Prof\u000d\u000a      Andrews was the\u000d\u000a      instigator and principal investigator of this project.\u000d\u000a    The recognised importance of the BDD method led to its inclusion in the\u000d\u000a      highly significant\u000d\u000a      ASTRAEA (Autonomous Systems Technology Related Airborne Evaluation &amp;\u000d\u000a      Assessment) project\u000d\u000a      (2006-2013). ASTRAEA I (2006-2008) focussed on the technologies, systems,\u000d\u000a      facilities,\u000d\u000a      procedures and regulations to allow unmanned aerial vehicles (UAVs) to\u000d\u000a      operate safely and\u000d\u000a      routinely in civil airspace over the United Kingdom. Following thorough\u000d\u000a      evaluation, BDD has\u000d\u000a      continued to feature in ASTRAEA II (2009-2013).\u000d\u000a    ASTRAEA I and II have been funded by the public sector (DTI then TSB, CAA\u000d\u000a      and others) and\u000d\u000a      industry (BAE, QinetiQ, Rolls Royce and others) with total funding\u000d\u000a      reported to be &#163;62 million. [Ref:\u000d\u000a      Flight Global http:\/\/www.flightglobal.com\/news\/articles\/uk-starts-47-million-astraea-ii-uav-project-340792\/\u000d\u000a      26 April 2010, Ref: http:\/\/www.uavs.org\/astraea\u000d\u000a      July 2012]. In July 2012 the ASTREA\u000d\u000a      project had reached its first goal of combining all the associated\u000d\u000a      research into the design of a UAV\u000d\u000a      which had its maiden flight, see [Ref. http:\/\/www.unmannedvehicles.co.uk\/uav-news\/bae-systems-unveils-astraea-unmanned-aerial-vehicle\/].\u000aFor\u000d\u000a      all the partners involved in the ASTRAEA project\u000d\u000a      see: [Ref. http:\/\/www.astraea.aero\/partners-and-associates.html]\u000d\u000a    The principle researcher Prof J D Andrews (LU Maths. 1988-2003, LU\u000d\u000a      Systems and Aeronautical\u000d\u000a      and Automotive Engineering 2003-2009) moved to Nottingham University in\u000d\u000a      2009 to take up the\u000d\u000a      post of Royal Academy of Engineering and Network Rail Professor of\u000d\u000a      Infrastructure Asset\u000d\u000a      Management. This resulted in widening the impact of the BDD method, for\u000d\u000a      rail applications.\u000d\u000a    Indicators of the significance and reach of papers [3.5] and [3.6]\u000d\u000a      are the two prestigious awards for\u000d\u000a      conference presentations based on the research's application to UAVs, as\u000d\u000a      follows:\u000d\u000a    (a) Best paper award at the 26th International\u000d\u000a      System Safety Conference, Vancouver, Canada,\u000d\u000a      August 2008, Remenyte-Prescott, R., Andrews, J.D., Downes, C.G.,\u000d\u000a      Reliability Analysis in\u000d\u000a      Responsive Mission Planning for Autonomous Vehicles.\u000d\u000a    (b) The Donald Julius Groen Prize 2009 by the Institution\u000d\u000a      of Mechanical Engineers for a paper:\u000d\u000a      Remenyte-Prescott, R. and Andrews, J.D., (2008), Analysis of Non-coherent\u000d\u000a      Fault Trees Using\u000d\u000a      Ternary Decision Diagrams, Proceedings of the Institution of\u000d\u000a        Mechanical Engineers, Part O:\u000d\u000a        Journal of Risk and Reliability, 222, 27-138, DOI:\u000d\u000a      10.1243\/1748006XJRR154.\u000d\u000a    Following Prof Andrews' move to Nottingham, utilising BDD has continued\u000d\u000a      at Loughborough\u000d\u000a      University in the School of Aeronautical &amp; Automotive Engineering\u000d\u000a      directed by senior lecturer Dr\u000d\u000a      Lisa Jackson (n&#233;e Bartlett) employed in Mathematical Sciences at LU from\u000d\u000a      2001-2003 and lecturer\u000d\u000a      Dr Sarah Dunnett employed in Mathematical Sciences at LU from 1996-2003.\u000d\u000a      Furthermore, the\u000d\u000a      impact of BDD is now seen in that BDD features as a tool, which is\u000d\u000a      heavily used in collaborative\u000d\u000a      work with BAE who have continued to support this research at Loughborough\u000d\u000a      University through\u000d\u000a      Case Study awards [5.1, 5.2].\u000d\u000a    "},{"CaseStudyId":"36085","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    The underpinning research detailed in section 2 and as evidenced by the\u000d\u000a      publications and funding detailed in section 3 has had impact in a number\u000d\u000a      of areas. In the commentary below we detail the impact that this research\u000d\u000a      has made in five different areas.\u000d\u000a    The research [3.1, 3.2] has changed awareness and understanding\u000d\u000a      within Pilkington and Applied Multilayers (subsequently Power Vision) [5.1]\u000d\u000a      of the optimized production conditions for depositing optical thin film\u000d\u000a      coatings with the best crystallinity and in the development of new\u000d\u000a      transparent conducting oxide materials. This has led to cost savings as it\u000d\u000a      is difficult to vary growth conditions and assess film quality\u000d\u000a      experimentally, but through computer simulation it is possible to predict\u000d\u000a      the type of growth that will occur for many different choices of parameter\u000d\u000a      values, and thus optimise the growth process. The research is further\u000d\u000a      supported by funding from Asahi Glass.\u000d\u000a    The work has also been taken up by the nuclear industry following\u000d\u000a      investigation of long time evolution of irradiated materials and dose\u000d\u000a      effects in radiation damage studies [3.3]. The impact in the civil\u000d\u000a      nuclear programme has been through an improved prediction of ageing\u000d\u000a      effects in irrradiated structural materials used in nuclear reactors and\u000d\u000a      the behaviour of nuclear fuels. This has also led to funding to optimise\u000d\u000a      the safe disposal of nuclear waste.\u000d\u000a    The Bader charge algorithm developed within this research [3.5]\u000d\u000a      has been incorporated into standard quantum chemistry packages such as\u000d\u000a      VASP and GAUSSIAN [5.2]; these packages are used by thousands of\u000d\u000a      research scientists internationally. The algorithm developed has\u000d\u000a      completely replaced the techniques that were previously used for this\u000d\u000a      analysis.\u000d\u000a    Prof. Smith was invited by EPSRC to attend two meetings (one in London\u000d\u000a      and one in Oxford) with politicians and representatives of India's\u000d\u000a      Department of Atomic Energy. These meetings resulted in a change of UK\u000d\u000a      government policy towards nuclear collaboration with India with funding\u000d\u000a      being made available for joint projects [5.3].\u000d\u000a    The research has also changed the operational models that are used by AWE\u000d\u000a      to understanding ageing in the nuclear weapons stockpile as evidenced by\u000d\u000a      the supporting letter [5.4]. The measure of AWE's interest in this\u000d\u000a      work is recognised through their continuing funding of research at\u000d\u000a      Loughborough for the last 7 years.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This study describes two atomistic methods that have been used to explain\u000d\u000a      better the behaviour and improve performance of materials. The research at\u000d\u000a      Loughborough University from 2006-2013 has led to improved awareness and\u000d\u000a      understanding in the areas of thin film growth and in irradiated\u000d\u000a      structural materials for nuclear power. It has also led to changes in the\u000d\u000a      operational models that Atomic Weapons Establishment (AWE) use. One of the\u000d\u000a      algorithms developed has been incorporated into standard quantum chemistry\u000d\u000a      packages, due to its increased accuracy and efficiency. The outcomes of\u000d\u000a      the research have also contributed to changing UK government policy with\u000d\u000a      regards to working with India in the area of nuclear research.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    Loughborough University\u000d\u000a    ","Institutions":[{"AlternativeName":"Loughborough University","InstitutionName":"Loughborough University","PeerGroup":"B","Region":"East Midlands","UKPRN":10004113}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    Journal Publications\u000d\u000a    \u000a3.1. Blackwell, S., Kenny, S.D., Smith, R. and Walls J.M., (2012),\u000d\u000a      Modeling evaporation, ion-beam assist, and magnetron sputtering of thin\u000d\u000a      metal films over realistic time scales, Physical Review B, 86,\u000d\u000a      035416, DOI: 10.1103\/PhysRevB.86.035416\u000d\u000a    \u000a\u000a3.2. Scott, C., Blackwell, S., Vernon, L., Kenny, S.D., Walls, J. M.,\u000d\u000a        and Smith, R., (2011), Atomistic surface erosion and thin film growth\u000d\u000a        modelled over realistic time scales, The Journal of Chemical Physics,\u000d\u000a        135, 174706, DOI: 10.1063\/1.3657436\u000d\u000a    \u000a\u000a3.3. Robinson, M., Kenny, S.D., Smith, R. and Storr, M.T., (2012),\u000d\u000a      Point defect formation and migration in Ga stabilised f064-Pu, Journal\u000d\u000a        of Nuclear Materials, 423, 16-21, DOI:\u000d\u000a      10.1016\/j.jnucmat.2011.11.046\u000d\u000a    \u000a\u000a3.4. Uberuaga, B.P., Bacorisen, D., Smith, R., Ball, R.J.A., Grimes,\u000d\u000a        R.W., Voter, A.F. and Sickafus, K.E., (2007), Defect kinetics in\u000d\u000a        spinels: Long-time simulations of MgAl2O4, MgGa2O4,\u000d\u000a        and MgIn2O4, Physical Review B, 75,\u000d\u000a        104116, DOI: 10.1103\/PhysRevB.75.104116\u000d\u000a    \u000a\u000a3.5. Sanville, E., Kenny, S.D., Smith, R., and Henkelman, G., (2007),\u000d\u000a        Improved Grid-based algorithm for Bader charge allocation, Journal\u000a          of Computational Chemistry, 28, 899-908, DOI:\u000d\u000a        10.1002\/jcc.20575\u000d\u000a    \u000a\u000a3.6. Tang, W., Sanville, E. and Henkelman, G., (2009), A\u000d\u000a      grid-based Bader analysis algorithm without lattice bias, Journal of\u000d\u000a        Physics: Condensed Matter, 21, 084204, DOI:\u000d\u000a      10.1088\/0953-8984\/21\/8\/084204\u000d\u000a    \u000aGrants\u000d\u000a    EP\/C524322\/1 A Multiscale Modelling Approach to Engineering Functional\u000d\u000a      Coatings, 10\/2005 - 09\/2009, &#163;413,532, R. Smith and S.D. Kenny.\u000d\u000a    EU project PERFORM60, 1\/2010-9\/2013 &#163;50,000.\u000d\u000a    EP\/I003150\/1 Performance and Reliability of Metallic Materials for\u000d\u000a      Nuclear Fission Power Generation, 12\/2010-11\/2014, &#163;106,955, R. Smith and\u000d\u000a      S.D. Kenny.\u000d\u000a    AWE Modelling Radiation Damage and He Bubble Formation in Plutonium and\u000d\u000a      the Effect of H Solubility and Diffusivity, 1\/2012-12\/2012, &#163;95,735.\u000d\u000a    AWE Modelling Radiation Damage and He Bubble Formation in Plutonium and\u000d\u000a      the Effect of H Solubility and Diffusivity, 1\/2013-6\/2014, &#163;147,456.\u000d\u000a    The quality of this work is evidenced by the fact that [3.5] has over 400\u000d\u000a      citations and that work using the methodologies in [3.1] was the subject\u000d\u000a      of an invited publication due to a presentation at the Materials Research\u000d\u000a      Society meeting. Work in this area has also received grants totalling over\u000d\u000a      &#163;800k.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"2","Level2":"2","Subject":"Atomic, Molecular, Nuclear, Particle and Plasma Physics"}],"Sources":"\u000d\u000a    The following sources of corroboration can be made available at request:\u000d\u000a    5.1. Letter from Power Vision Ltd, Herald Park, Crewe, Cheshire,\u000d\u000a      CW1 6EA\u000d\u000a    5.2. Software downloadable from VASP Tools: Bader Charge Analysis\u000d\u000a      http:\/\/theory.cm.utexas.edu\/vtsttools\/bader\/\u000d\u000a    5.3. Letter from Chief Scientific Advisor to the Foreign and\u000d\u000a      Commonwealth Office\u000d\u000a    5.4. Letter from Modelling Team, AWE, Aldermaston UK\u000d\u000a    ","Title":"\u000d\u000a    Quantum and classical atomistic methods to enable improved processing\u000d\u000a        and performance of materials\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Multi-timescale algorithm\u000d\u000a    This computational method involves an extension of the molecular dynamics\u000d\u000a      (MD) method to long timescales [3.1, 3.2]. A major problem with MD\u000d\u000a      is the limited timescales that can be modelled. This is at most of the\u000d\u000a      order of microseconds, because the numerical integration time step cannot\u000d\u000a      exceed ~ 10-15s. However, a typical deposition rate for thin\u000d\u000a      film growth is 1 monolayer per second so even to model the growth of a few\u000d\u000a      layers would be computationally excessive using MD alone. To extend the\u000d\u000a      time scales, a new method has been devised in which the fast processes are\u000d\u000a      calculated by MD and then the slower transitions are calculated adaptively\u000d\u000a      using a parallelised saddle point finding algorithm. This method has been\u000d\u000a      applied to the growth of thin films, giving guidance to industrialists as\u000d\u000a      to the optimum conditions for growth [3.1, 3.2]. Extended time\u000d\u000a      scale methods have also been applied to the modelling of radiation effects\u000d\u000a      with applications to the nuclear industry [3.3, 3.4]. This work\u000d\u000a      was carried out at Loughborough University from 2006-2013.\u000d\u000a    Bader Charge Algorithm\u000d\u000a    Bader charge analysis is a way of dividing molecules up into atoms when\u000d\u000a      solving Schr&#246;dinger's equation. The methodology solves two outstanding\u000d\u000a      problems relating classical and quantum descriptions of a solid.\u000d\u000a    The first application of the Bader method relates to the way in\u000d\u000a      which charge is allocated to individual atoms in a quantum mechanical\u000d\u000a      system. In the classical description of the electron the charge is\u000d\u000a      well-defined, but in a quantum mechanical description of a solid what is\u000d\u000a      calculated is a charge density. The way to allocate charge density to\u000d\u000a      atoms was developed by Richard Bader [Atoms in Molecules OUP, 1994]\u000d\u000a      but to do this in an accurate and numerically efficient way, processing\u000d\u000a      the output of quantum chemistry packages, has in the past been highly\u000d\u000a      inefficient and inaccurate. 3.5 and 3.6 describe the\u000d\u000a      numerical method that not only does this accurately but also scales\u000d\u000a      linearly with the number of interatomic surfaces in the system, so now it\u000d\u000a      is feasible on a small computer. This improved method is currently used in\u000d\u000a      conjunction with quantum chemistry packages such as VASP and GAUSSIAN. The\u000d\u000a      outline algorithm was initially devised by Henkelman but corrected and\u000d\u000a      improved at Loughborough University.\u000d\u000a    The second application of the Bader method allows potential energy\u000d\u000a      in a quantum system to be allocated to an atom. In a quantum description\u000d\u000a      of a solid, the concept of a potential energy per atom is not defined. The\u000d\u000a      paper [3.5] uses the Bader charge allocation to define a potential\u000d\u000a      energy per atom in a quantum system in a unique way, which can then be\u000d\u000a      used accurately to parameterise classical potential functions, thus\u000d\u000a      allowing computation of large systems of atoms beyond the scope of\u000d\u000a      numerical quantum calculations.\u000d\u000a    This work was carried out at Loughborough University from 2006-2009.\u000d\u000a    Key Researchers:\u000d\u000a    Smith, R. (Lecturer and Professor of Mathematical Engineering: 1971 -),\u000d\u000a      Kenny, S.D. (Senior Lecturer and Reader: 2000 -), Sanville, E. (PDRA:\u000d\u000a      2006-2009), Scott, C. (PDRA: 2011-2013).\u000d\u000a    Research Students:\u000d\u000a    Vernon, L. (PhD: 2006-10) Funded by EPSRC grant on modelling functional\u000d\u000a      coatings.\u000d\u000a    Robinson, M. (PhD: 2006-10) Partially funded by the Atomic Weapons\u000d\u000a      Establishment (AWE) on long time radiation effects in plutonium.\u000d\u000a    Scott, C. (PhD: 2008-11) Partially funded by Los Alamos National\u000d\u000a      Laboratory.\u000d\u000a    Kittiratanawasin, L. (PhD: 2007-11) Partially funded by Los Alamos\u000d\u000a      National Laboratory.\u000d\u000a    Bacorisen, D. (PhD: 2003-2007) Partially funded by Los Alamos National\u000d\u000a      Laboratory.\u000d\u000a    Blackwell S. (PhD: 2009-2012) Partially funded by CREST (Renewable Energy\u000d\u000a      Centre) at Loughborough\u000d\u000a    The work was also funded by the EPSRC Materials Modelling Grant\u000d\u000a      EP\/C524322\/1 for a consortium of five universities, led by Loughborough.\u000d\u000a      Project partners were Applied Multilayers and Pilkington.\u000d\u000a    "},{"CaseStudyId":"36086","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000a    The original and rigorous research at Loughborough University by\u000a      Professor Grimshaw and colleagues since 2000 has resulted in a new\u000a      modelling paradigm for non-linear oceanic waves. This research has had\u000a      impact not only for fundamental science, but also for the marine engineers\u000a      and ship operators worldwide who are concerned with the effect of these\u000a      waves on offshore structures and submersibles. This has given tools to\u000a      experimentalists and numerical modellers to find theoretical\u000a      interpretations of their observations and results.\u000a    The research means that the KdV model, with its various extensions and\u000a    modifications, is now routinely used as the basic paradigm for the dynamical\u000a    understanding of the nonlinear internal waves commonly found in the coastal\u000a    oceans. The large number of keynote presentations by Grimshaw, for example [5.1-5.3],\u000a    provide evidence of the widespread dissemination of this research to end\u000a    users and the interest the work has attracted.\u000a    This major change extends to end-users such as marine and oil industry\u000a      engineers, and to the navy scientists concerned with submersibles and\u000a      acoustic signalling [5.4]. Examples of applications of our\u000a      research comes from funded experimental work from the Office of Naval\u000a      Research (ONR). Professor Louis Goodman, now at the School for Marine\u000a      Science at the University of Massachusetts has carried out considerable\u000a      work for the ONR using the theories of Grimshaw, [5.5]. Oil\u000a      companies, such as BP, Exxon Mobil, StatOil, have also benefitted from\u000a      this work as working offshore needs the capacity to predict the occurrence\u000a      of these large waves, and to assess the impact of waves on structures and\u000a      pipe lines. Submersibles routinely communicate using sound signals, which\u000a      are hugely distorted by internal waves. For this reason the US Navy\u000a      through ONR has conducted several major ocean experiments, utilising\u000a      theory first developed at Loughborough University.\u000a    The benefits to these users include the better prediction of the\u000a      occurrence of large waves leading to an improved assessment of the impact\u000a      of the waves on under-sea structures and pipelines; and improved\u000a      communication for submersibles whose communication is largely disrupted.\u000a    ","ImpactSummary":"\u000a    Large-amplitude horizontally propagating internal solitary waves commonly\u000a      occur in the interior of the ocean. This case study presents evidence to\u000a      demonstrate the impact of research conducted by Professor Grimshaw at\u000a      Loughborough University on the development and utilisation of Korteweg- de\u000a      Vries (KdV) models of these waves, which has formed the paradigm for the\u000a      theoretical modelling and practical prediction of these waves.\u000a    These waves are highly significant for sediment transport, continental\u000a      shelf biology and interior ocean mixing, while their associated currents\u000a      cause strong forces on marine platforms, underwater pipelines and\u000a      submersibles, and the strong distortion of the density field has a severe\u000a      impact on acoustic signalling.\u000a    The theory developed at Loughborough University has had substantial\u000a      impact on the strategies developed by marine and naval engineers and\u000a      scientists in dealing with these issues.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    Loughborough University\u000a    ","Institutions":[{"AlternativeName":"Loughborough University","InstitutionName":"Loughborough University","PeerGroup":"B","Region":"East Midlands","UKPRN":10004113}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"6254926","Name":"Massachusetts"}],"References":"\u000a    \u000a3.1. Grimshaw R.H.J., Zhang D-H. and Chow K.W., (2007),\u000a      Generation of solitary waves by transcritical flow over a step, Journal\u000a        of Fluid Mechanics, 587, 235-254, DOI: 10.1017\/S0022112007007355\u000a    \u000a\u000a3.2. El G.A., Grimshaw R.H.J. and Smyth N.F., (2009), Transcritical\u000a        shallow-water flow past topography: finite-amplitude theory, Journal\u000a          of Fluid Mechanics, 640, 187-214, DOI:\u000a        10.1017\/S0022112009991315\u000a    \u000a\u000a3.3. Griffiths S.D. and Grimshaw R.H.J. (2007), Internal tide\u000a      generation at the continental shelf modeled using a modal decomposition:\u000a      two-dimensional results, Journal of Physical Oceanography, 37,\u000a      428-451, DOI: 10.1175\/JPO3068.1\u000a    \u000a\u000a3.4. Grimshaw, R., Pelinovsky, E., Talipova, T. and Kurkina, A. (2010)\u000a        Internal solitary waves: propagation, deformation and disintegration,\u000a      Nonlinear Processes in Geophysics, 17, 633-649. http:\/\/www.nonlin-processes-geophys.net\/17\/633\/2010\/npg-17-633-2010.pdf\u000a    \u000a\u000a3.5. El G.A., Grimshaw R.H.J. and Kamchatnov A.M., (2007),\u000a      Evolution of solitary waves and undular bores in shallow-water flows over\u000a      a gradual slope with bottom friction, Journal of Fluid Mechanics,\u000a      585, 213-244, DOI: 10.1017\/S0022112007006817\u000a    \u000a\u000a3.6. Grimshaw, R. and Helfrich, K.R. (2012). The effect of rotation on\u000a        internal solitary waves, IMA Journal of Applied Mathematics, 77,\u000a        326-339, DOI: 10.1093\/imamat\/hxs024\u000a    \u000aGrants to Professor Grimshaw:\u000a    EPSRC GR\/N63642\/01, Dynamics of finite-amplitude internal and\u000a        inertial waves, Grimshaw R.H.J., 01\/ 2001-01\/2004, &#163;128,521.\u000a    EPSRC EP\/D003342\/1,The effect of friction on undular bores,\u000a      Grimshaw R.H.J., 01\/2005-08\/2005, &#163;4,200.\u000a    EPSRC EP\/C530586\/1, Nonlinear internal gravity wave beams,\u000a      02\/2005-05\/2005, &#163;5,600\u000a    EPSRC EP\/I007180\/1, Interaction of oceanic vortices with steep\u000a        topography, 09\/2010-11\/2010, &#163;9,794.\u000a    EPSRC, Interaction of vortices with topography, 2009, &#163;15,700.\u000a    Office of Naval Research, Generation of the Internal Tide,\u000a      2003-5, US$180,000.\u000a    Office of Naval Research, Internal Solitary Waves, 2000-2,\u000a      US$110,000.\u000a    Royal Society, Large amplitude waves with trapped cores, 2011,\u000a      &#163;4000.\u000a    Royal Society, Nonlinear waves in coastal seas, 2008, &#163;8,200.\u000a    Royal Society, Solitary waves propagating over rough topography,\u000a      2006, &#163;2,480.\u000a    Royal Society, Analytical theory of frictional undular bores,\u000a      2004, &#163;2,360.\u000a    Leverhulme Visiting Professor (Georgi Sutyrin), 2003, &#163;34,000.\u000a    Leverhulme Visiting Professor (Efim Pelinovsky), 2008, &#163;67,500.\u000a    This research has originality, rigour and significance, indicated by its\u000a      publication in premier refereed journals, external funding of a total of\u000a      &#163;463,605 since 2002, and the external collaborations with two world class\u000a      institutes, viz. Woods Hole Oceanographic Institute, USA and Institute of\u000a      Applied Physics, Russia.\u000a    ","ResearchSubjectAreas":[{"Level1":"4","Level2":"5","Subject":"Oceanography"},{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"9","Level2":"11","Subject":"Maritime Engineering"}],"Sources":"\u000a    The following sources of corroboration can be made available at request:\u000a    5.1. Invitation to be a speaker at Pacific Institute for the\u000a      Mathematical Sciences (PIMS) Workshop \"Waves in the Atmosphere and Ocean\",\u000a      Vancouver, 2008: http:\/\/www.math.wisc.edu\/~milewski\/CRG-SFU\/Home.html\u000a    5.2. Invitation to be a principal lecturer at Geophysical Fluid\u000a      Dynamics (GFD) Summer School at Woods Hole Oceanographic Institution, on\u000a      Nonlinear Waves, 2009: http:\/\/www.whoi.edu\/fileserver.do?id=52147&amp;pt=2&amp;p=19387\u000a    5.3. Invitation to be a plenary lecturer at the Third DNVA-RSE\u000a      Norway-Scotland Waves Symposium, Oslo, 2013:http:\/\/www.mn.uio.no\/math\/english\/research\/groups\/fluid-mechanics\/events\/3rd-norway-scotland-waves-symposium.html\u000a    5.4. Email from Woods Hole Oceanographic Institution, USA\u000a    5.5. The Office of Naval Research in the USA uses Grimshaw's work:\u000a      http:\/\/www.onr.navy.mil\/Search.aspx?q=Grimshaw \u000a    ","Title":"\u000a    Modelling oceanic internal waves to enhance marine and naval\u000a        predictions and practices\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Large-amplitude internal solitary waves are usually generated by the\u000a      interaction of the barotropic tide with the shelf break, topographic sill\u000a      or other prominent bottom features. This leads to the formation of an\u000a      internal tide, which then deforms and evolves into a train of very\u000a      large-amplitude internal waves, with associated large pycnocline\u000a      displacements and strong currents.\u000a    Research on internal solitary waves in the coastal ocean is a very active\u000a      area, conducted worldwide by physical oceanographers and marine engineers.\u000a      Intensive observations have been obtained in several locations by\u000a      ship-based experiments, supplemented by global satellite observations.\u000a      Theoretical modelling has been based either on numerical simulations, or\u000a      on the development of analytical models. It is in this latter area where\u000a      the research at Loughborough University (LU) has made a major\u000a      contribution, to the extent that the theoretical models developed by\u000a      oceanographers worldwide now form the basic paradigms for interpreting and\u000a      predicting the dynamics of internal solitary waves.\u000a    The special focus on large amplitude internal waves, which are common and\u000a      a robust feature of the coastal ocean, and also in the atmospheric\u000a      boundary layer, began with the arrival of Professor Grimshaw to\u000a      Loughborough in 2000, and the subsequent establishment of his research\u000a      group consisting of Dr. El (appointed 2005) and Dr. Khusnutdinova\u000a      (appointed 2003), together with several post-doctoral fellows and\u000a      post-graduate students, and has continued through to 2013. The group has\u000a      collaborated internationally, notably with Pelinovsky (Institute of\u000a      Applied Physics, Russia) and Helfrich (Woods Hole Oceanographic Institute,\u000a      USA).\u000a    Research has focussed on four main areas: generation mechanisms, the\u000a      structure of internal wave trains, the effect of bottom topography, the\u000a      role of the earth's rotation. One of the most effective generation\u000a      mechanisms is transcritical flow over topography where the basic model is\u000a      the forced KdV equation, [3.1, 3.2]. The basic feature of the\u000a      forced KdV equation is the appearance of undular bores, which are\u000a      generated both upstream and downstream of the topography, linked by a\u000a      quasi-steady structure over the topography. In [3.1] it is\u000a      demonstrated, essentially for the first time, that the width and polarity\u000a      of the obstacle are important parameters. Then in [3.2] the forced\u000a      KdV model is extended to finite-amplitude, initially in the surface wave\u000a      context. One of the significant current issues is the observation that in\u000a      the generation region, often several internal wave modes are observed. In\u000a      [3.3] this topic is addressed in a novel theoretical model, which\u000a      is currently being implemented on a global scale, by Dr. Stephen Griffiths\u000a      at Leeds University and collaborators.\u000a    The standard KdV model has only quadratic nonlinearity, whereas observed\u000a      internal solitary waves often have very large amplitudes. Hence it has\u000a      become common to include a cubic nonlinear term, leading to an extended\u000a      KdV (Gardner) equation, which is now the standard model [3.4] and\u000a      can be used for arbitrary density stratification and background current\u000a      fields. One of the main thrusts of the Loughborough University research\u000a      has been the investigation of how internal solitary waves deform, and\u000a      possibly even disintegrate, as they propagate over variable topography.\u000a      This involves a detailed study of the properties of the\u000a      variable-coefficient extended KdV equation, and these have been applied to\u000a      actual oceanic locations, to demonstrate the wide variety of outcomes [3.4].\u000a      In [3.5] this approach was extended to a study of how undular\u000a      bores deform over a slope which focussed on how tsunamis deform over the\u000a      continental slope.\u000a    Recently, it has been shown that the earth's background rotation has a\u000a      marked effect on the long- time development of internal solitary waves,\u000a      and significant new results are described in [3.6]. Importantly\u000a      and surprisingly, it transpires that the long-time outcome is the\u000a      formation of nonlinear wave packets, and this has profound implications\u000a      for inter alia the interpretation of satellite observations.\u000a    "},{"CaseStudyId":"36087","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000a    OrbitrapTM is a brand name for a series of high-precision\u000a      Fourier Transform Mass Spectrometer\u000a      systems produced by Thermo Fisher Scientific. The electrostatic orbital\u000a      ion trap with synchronous\u000a      oscillation properties was invented in 1996 by Dr. A. Makarov, who\u000a      received the American Society\u000a      for Mass Spectrometry Distinguished Contribution in Mass Spectrometry\u000a      Award in 2008 for this\u000a      development. Currently, the Thermo Scientific OrbitrapTM mass\u000a      spectrometer has been produced in\u000a      several versions, with an average price about US$0.5 million for one\u000a      system. (Further details are\u000a      commercially sensitive.) Many OrbitrapTM mass analysers have\u000a      been sold to the hospitals and\u000a      laboratories worldwide, but the precise number is commercially\u000a      confidential.\u000a    A recent picture and description of an OrbitrapTM mass\u000a      spectrometer is shown below\u000a      (www.thermofisher.com).\u000a    \u000a    New Thermo Fisher Scientific High Performance\u000a      Benchtop Quadrupole-Orbitrap Mass Spectrometer\u000a    \u000a   Thermo Fisher Scientific introduces the Q Exactive high-performance\u000a      benchtop\u000a      quadrupole-Orbitrap LS-MS\/MS. It is the first commercially available\u000a      instrument to bring\u000a      together quadrupole precursor selection and high-resolution accurate mass\u000a      (HR\/AM)\u000a      OrbitrapTM mass analysis to deliver high confidence\u000a      quantitative and qualitative\u000a      (quan\/qual) workflows. With innovate HR\/AM QuanfirmationTM\u000a      capability, the Q\u000a      ExactiveTM mass spectrometer makes it possible to identify,\u000a      quantify and confirm more\u000a      trace-level metabolites, contaminants, peptides and proteins in complex\u000a      mixtures in one\u000a      analytical run. Unlike other technologies, high confidence results are\u000a      obtained without\u000a      sacrificing MS\/MS sensitivity, mass resolution or quantitative\u000a      reproducibility.\u000a    Features\u000a    \u000a      Resolving power up to 140.000\u000a      Maximum scan speed 12 Hz\u000a      Intra &#8212; scan dynamic range &gt; 5000:1\u000a      Quadrupole mass filter\u000a      Spectral multiplexing for enhanced duty cycle\u000a      S-Lens ion source for increased sensitivity\u000a    \u000a    Mathematical modelling has had a significant impact on the design of the\u000a      OrbitrapTM device.\u000a    The results of reference [3.5] on Hamiltonian perturbation theory\u000a      of the motion of charged particles\u000a      had been used since 2011 by the Life Science Mass Spectrometry,\u000a      Chromatography and Mass\u000a      Spectrometry Division (CMD) of Thermo Fisher Scientific (Bremen) GmbH, for\u000a      OrbitrapTM mass\u000a      spectrometer performance modelling. The mathematical techniques were\u000a      implemented in the\u000a      MASIM-3D software package in 2012. The averaged motion equations describe\u000a      the resonant\u000a      interactions of many ions of the same or close masses. The calculation\u000a      time decrease gained from\u000a      averaging ensures the modelling of a reasonably large number of ion\u000a      macro-particles (up to ~\u000a      2x103) to reveal the sophisticated space charge effects of\u000a      self-bunching and coalescence in an\u000a      OrbitrapTM mass spectrometer. The coalescence is a\u000a      synchronization phenomenon when ions of\u000a      close masses move with the same frequency. Thus they create one peak on\u000a      the observed Fourier\u000a      spectrum and cannot be identified\/distinguished using this spectrum.\u000a      Numerical analysis of ionic\u000a      motion close to the threshold of synchronisation allows reliable\u000a      estimation of the mass resolving\u000a      power limitation in an OrbitrapTM mass spectrometer. The\u000a      calculated dependence of the\u000a      synchronisation threshold as a function of intentionally introduced small\u000a      static perturbations allows\u000a      the coalescence effect to be controlled.\u000a    The modelling of space-charge effects in the OrbitrapTM mass\u000a      spectrometer by direct tracing of\u000a      multiple interacting ions is an extremely computationally demanding and\u000a      time-consuming task.\u000a      Obtaining the required accuracy is a major challenge. However, using the\u000a      averaged Hamiltonian\u000a      equations derived in [3.5] allows specialists at Thermo Fisher\u000a      Scientific to model the space-charge\u000a      effects in reasonable computer time with appropriate accuracy. This allows\u000a      them to try many\u000a      variations of the parameters in order to determine the necessary values to\u000a      optimise the OrbitrapTM\u000a      mass spectrometer's performance.\u000a    The beneficiaries of the research are Thermo Fisher Scientific and\u000a      indirectly their customers in\u000a      healthcare research and industry. OrbitrapTM mass spectrometer\u000a      is used in many hospitals and\u000a      laboratories in the UK and worldwide, in particular for drug monitoring\u000a      and food safety analysis.\u000a    Thermo Fisher Scientific is a large precision healthcare equipment global\u000a      company with offices\u000a      and operations in most countries around the world, and 2013 revenue\u000a      guidance between $12.83\u000a      billion and $12.95 billion.\u000a    ","ImpactSummary":"\u000a    This case study describes the impact of research at Loughborough\u000a      University from 2009-2012 into\u000a      the mathematical modelling of the dynamics of ions using perturbation\u000a      theory of Hamiltonian\u000a      systems of equations. Outcomes from this research have been incorporated\u000a      into software used for\u000a      the performance modelling of a series of high-precision Fourier Transform\u000a      Mass Spectrometers\u000a      manufactured by Thermo Fisher Scientific GbmH and branded as OrbitrapTM\u000a        with an average price\u000a      $0.5 million. The derived methodology reduces the time of numerical\u000a      modelling of the behaviour of\u000a      charged particles in an OrbitrapTM instrument by a\u000a      factor of 100 to 1000. This reduction is of\u000a      significant benefit to the Life Science Mass Spectrometry, Scientific\u000a      Instrumentation Division of\u000a      Thermo Fisher Scientific and indirectly the users of the instrument.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Loughborough University\u000a    ","Institutions":[{"AlternativeName":"Loughborough University","InstitutionName":"Loughborough University","PeerGroup":"B","Region":"East Midlands","UKPRN":10004113}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2944388","Name":"Bremen"}],"References":"\u000a    \u000a3.1. Neishtadt A, Artemyev A, Zelenyi L, Vainchtein D, (2009),\u000a      Surfatron acceleration in\u000a      electromagnetic waves with low phase velocity, JETP Letters,\u000a      89(9), 441-447, DOI:\u000a      10.1134\/S0021364009090045\u000a    \u000a\u000a3.2. Artemyev A, Neishtadt A, Zelenyi L, Vainchtein D, (2010),\u000a        Adiabatic description of\u000a        capture into resonance and surfatron acceleration of charged particles\u000a        by\u000a        electromagnetic waves, Chaos, 20(4), 043128, DOI:\u000a        10.1063\/1.3518360\u000a    \u000a\u000a3.3. Artemyev A, Vainchtein D, Neishtadt A, Zelenyi L (2011),\u000a      Resonant acceleration of charged\u000a      particles in the presence of random fluctuations, Physical Review E,\u000a      84(4), 046213, DOI:\u000a      10.1103\/PhysRevE.84.046213\u000a    \u000a\u000a3.4. Vasiliev A, Neishtadt AI, Artemyev A, (2011), Nonlinear dynamics\u000a        of charged particles\u000a        in an oblique electromagnetic wave, Physics Letters A,\u000a        375(34), 3075-3079, DOI:\u000a        10.1016\/j.physleta.2011.06.055\u000a    \u000a\u000a3.5. Grinfeld D, Makarov A, Skoblin M, Monastyrskiy M, Denisov\u000a        E, Neishtadt A, (2012),\u000a        Perturbation theory and space-charge ion dynamics in Orbitrap mass\u000a        analyser,\u000a        Proceedings of 13th Seminar\u000a          \"Recent Trends in Charged Particle Optics and Surface\u000a        Physics Instrumentation\", Brno, 2012, ISBN\u000a        978-80-87441-07-7, 21-24. (Available from:\u000a        http:\/\/www.trends.isibrno.cz\/)\u000a    \u000aThe quality of this research is recognised internationally in terms of\u000a      originality, significance and\u000a      rigour. Papers 3.1 - 3.4 are published in the respectable academic\u000a      journals, paper 3.5 was\u000a      presented at one of the main international conferences in this particular\u000a      area.\u000a    ","ResearchSubjectAreas":[{"Level1":"3","Level2":"7","Subject":"Theoretical and Computational Chemistry"},{"Level1":"2","Level2":"2","Subject":"Atomic, Molecular, Nuclear, Particle and Plasma Physics"},{"Level1":"3","Level2":"6","Subject":"Physical Chemistry (incl. Structural)"}],"Sources":"\u000a    The following sources of corroboration can be made available at request:\u000a    5.1. Letter from: Director of Global Research for Life Science\u000a      Mass Spectrometry,\u000a      Thermo Fisher Scientific (Bremen) GmbH\u000a    Advanced Mass Spectrometry\u000a    Bremen\u000a    Germany\u000a    ","Title":"\u000a    Improved modelling of ion dynamics in the Thermo Scientific OrbitrapTM\u000a      mass analyser using Hamiltonian perturbation theory\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Perturbation theory of Hamiltonian systems is a well-developed\u000a      mathematical subject with\u000a      immense applications. One classical domain of application of this theory\u000a      is the description of the\u000a      dynamics of charged particles in electromagnetic fields. Professor Anatoly\u000a      Neishtadt, an employee\u000a      of Loughborough University from March 2007, has expertise both in the\u000a      mathematical aspects of\u000a      Hamiltonian perturbation theory and in its applications. References 3.1-3.5\u000a      describe research into\u000a      the motion of charged particles in different electromagnetic field\u000a      configurations using perturbation\u000a      theory of Hamiltonian systems that was conducted at Loughborough\u000a      University in 2009-2012.\u000a    In references [3.1-3.4] the dynamics of a charged particle under\u000a      the action of an electromagnetic\u000a      wave in plasma with a background uniform magnetic field is studied. The\u000a      possibility (in the\u000a      framework of the considered model) of unlimited acceleration of charged\u000a      particles due to capture\u000a      into resonance with such a wave is established for the first time. The\u000a      methodology of [3.1-3.4] was\u000a      used in (the most significant for this case study) paper [3.5] for\u000a      the mathematical modelling of ion\u000a      dynamics in the electrostatic field of the OrbitrapTM mass\u000a      spectrometer. This paper is the result of a\u000a      collaboration with a group of physicists and applied mathematicians from\u000a      Thermo Fisher Scientific\u000a      GbmH (D. Grinfeld, A. Makarov, E. Denisov) and A.M. Prokhorov General\u000a      Physics Institute,\u000a      Russian Academy of Sciences (M. Monastyrskiy, M. Skoblin). The authors\u000a      have suggested the use\u000a      of Hamiltonian perturbation theory for the description of the motion of\u000a      charged particles in Fourier\u000a      Transform mass spectrometers manufactured by Thermo Scientific GbmH and\u000a      branded as\u000a      OrbitrapTM.\u000a    OrbitrapTM is a trap for ions. In this trap ions cycle around\u000a      the axis of the trap and move back and\u000a      forth along this axis. The use of OrbitrapTM in mass\u000a      spectrometry is based on measuring the ions'\u000a      axial oscillation frequency. Peaks of the Fourier spectrum of the\u000a      registered electric signal\u000a      correspond to assortments of ions with different mass\/charge ratios. In an\u000a      ideal OrbitrapTM, axial\u000a      motion of an ion is harmonic with the frequency independent of the ion's\u000a      energy and inversely\u000a      proportional to the square root of the mass-to-charge ratio. This allows\u000a      the identification of different\u000a      sorts of ions. This fundamental property of an ideal ion trap is corrupted\u000a      by perturbations arising\u000a      from the presence of the ion injection aperture, inaccuracies of electrode\u000a      manufacture and\u000a      Coulomb interaction between the ions (space-charge effects). Deviations of\u000a      the real field from the\u000a      ideal one can be treated as small perturbations. The dynamics of ions in\u000a      an OrbitrapTM in the\u000a      presence of perturbations may be described by a Hamiltonian system of\u000a      ordinary differential\u000a      equations with slow and fast variables. In reference [3.5] the\u000a      averaged Hamiltonian motion\u000a      equations are derived through averaging over fast variables. The obtained\u000a      equations contain only\u000a      slowly changing variables. The use of these derived equations\u000a      significantly reduces the time of\u000a      numerical modelling of the motion of the ions in the OrbitrapTM\u000a      by a factor of between 100 to 1000,\u000a      with the simulation accuracy exceeding that of direct trajectory tracing.\u000a    "},{"CaseStudyId":"37384","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council","Natural Environment Research Council"],"ImpactDetails":"\u000a    Models of ocean circulation, in conjunction with numerical weather\u000a      prediction models, are essential\u000a      in forecasting over long time periods, from one week to many decades,\u000a      because ocean transport\u000a      accounts for much of the energy that drives weather and climate systems.\u000a      The initial beneficiaries\u000a      of the research are forecasters using oceanic models, since the correction\u000a      reduces or eliminates\u000a      spurious overturning circulations and other imbalances caused in previous\u000a      data assimilation\u000a      schemes by systematic model errors.\u000a    Between 2000 and 2004, the correction for the pressure gradient term\u000a      developed by Nichols and\u000a      Martin, together with Bell, was successfully implemented into the Met\u000a      Office's Forecast Ocean\u000a      Assimilation Model (FOAM). FOAM forms its operational ocean forecasting\u000a      system. Significant\u000a      improvements in the accuracy of the ocean forecasts were achieved as a\u000a      result of incorporating\u000a      the error correction process into FOAM. Results showing the impact of the\u000a      pressure correction on\u000a      the ocean assimilation are shown in Bell et al (2004). At the end of 2008\u000a      the FOAM system was\u000a      transitioned to use a different core component for its ocean model. This\u000a      new component is NEMO\u000a      (Nucleus for European Modelling of the Oceans - http:\/\/www.nemo-ocean.eu\/),\u000a      a community ocean\u000a      modelling system for oceanographic research, operational oceanography,\u000a      seasonal forecasting\u000a      and climate studies. The significant impact of Reading's pressure bias\u000a      correction technique on the\u000a      accuracy of forecasting continued to be recognised and the correction\u000a      technique was again\u000a      implemented into the FOAM data assimilation system using NEMO. This system\u000a      is used for\u000a      producing short-range forecasts of the ocean and sea-ice state (out to 7\u000a      days), and is now also\u000a      used directly to initialise the ocean component of seasonal forecasts.\u000a    In addition to the Met Office implementation, the correction technique\u000a      was also adopted by\u000a      ECMWF and incorporated into their ocean assimilation system, resulting in\u000a      improved predictions of\u000a      ocean circulation. ECMWF also built on the original correction method of\u000a      Reading, extending the\u000a      scheme to allow for temporal variations in the bias error. Between 2008\u000a      and 2009, the ECMWF\u000a      also transitioned to NEMO as the ocean modelling element of its\u000a      forecasting system and the\u000a      pressure bias correction technique was again found to be an important\u000a      component needed for\u000a      assimilation with the new ocean model.\u000a    In practice ocean and weather forecasting assimilation systems have been\u000a      used separately to\u000a      provide input data to each other, but recently data assimilation systems\u000a      for coupled oceanic and\u000a      atmospheric models have been under development at ECMWF and the Met\u000a      Office. Although it\u000a      was expected that for coupled models, the pressure correction technique\u000a      might not be required, it\u000a      was found that the bias error correction scheme is still needed to avoid\u000a      spurious ocean dynamics\u000a      and maintain balances in the coupled systems.\u000a    Forecasting systems, such as those relying on the correction schemes\u000a      developed by the University\u000a      of Reading, require a vast quantity of input data collected from\u000a      satellites, ocean buoys, aircraft and\u000a      shipping, radiosondes, and radar, as well as ground stations. These data\u000a      are assimilated into\u000a      complex multi-scale models of the ocean and atmosphere. Improvements to\u000a      data assimilation\u000a      techniques, such as those developed at Reading, enable better use of this\u000a      expensively acquired\u000a      data to give more accurate weather and climate predictions. Good forecasts\u000a      enable good planning\u000a      and the research on data assimilation at Reading continues to bring\u000a      significant benefits to the\u000a      whole community.\u000a    Accurate seasonal forecasts, extending for a decade or more, are\u000a      particularly important for\u000a      understanding the effects of climate change and in developing strategies\u000a      for living with changes in\u000a      our environment as well as for mitigating hazardous conditions that may\u000a      arise, such as flooding,\u000a      drought, intense rainfall, heavy snow and ice, or excessive temperatures.\u000a      Improvement in the\u000a      accuracy of ocean, weather and climate forecasting has impacts on\u000a      economic, commercial and\u000a      organisational elements of society as well as on the environment. The\u000a      development from the\u000a      University Reading, and the improvement in accuracy it has allowed, has\u000a      made an important\u000a      contribution to such advances.\u000a    ","ImpactSummary":"\u000a    Ocean circulation accounts for much of the energy that drives weather and\u000a      climate systems; errors\u000a      in the representation of the ocean circulation in computational models\u000a      affect the validity of\u000a      forecasts of the dynamics of the ocean and atmosphere on daily, seasonal\u000a      and decadal time\u000a      scales. Research undertaken by the University of Reading investigated\u000a      systematic model errors\u000a      that resulted from data assimilation schemes embedded in the key processes\u000a      used to predict\u000a      ocean circulation. The researchers developed a new bias correction\u000a      technique for use in ocean\u000a      data assimilation that alleviates these errors. This has led to\u000a      significant improvements in the\u000a      accuracy of the forecasts of ocean dynamics. The technique has been\u000a      implemented by the Met\u000a      Office and by the European Centre for Medium Range Weather Forecasting\u000a      (ECMWF) in their\u000a      forecasting systems, resulting in major improvements to the prediction of\u000a      the weather and climate\u000a      from oceanic and atmospheric models. The assimilation technique is also\u000a      leading to better use of\u000a      expensively acquired satellite and in-situ data and improving ocean and\u000a      atmosphere forecasts\u000a      used by shipping and civil aviation, energy providers, insurance\u000a      companies, the agriculture and\u000a      fishing communities, food suppliers and the general public. The impact of\u000a      the correction procedure\u000a      is also important for anticipating and mitigating hazardous weather\u000a      conditions and the effects of\u000a      long-term climate change.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University of Reading\u000a    ","Institutions":[{"AlternativeName":"Reading (University of)","InstitutionName":"University of Reading","PeerGroup":"B","Region":"South East","UKPRN":10007802}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000aEarly paper: (20 citations, 48 Google Scholar) Griffith &amp;\u000a      Nichols, Adjoint methods in data\u000a      assimilation for estimating model error, Flow, Turbulence and Combustion,\u000a      65, (2000),\u000a      469-488; DOI:10.1023\/A:1011454109203\u000a    \u000a\u000aMain research paper: (43 citations, 81 Google Scholar) Bell,\u000a      Martin &amp; Nichols, Assimilation of data\u000a      into an ocean model with systematic errors near the equator, Quarterly J.\u000a      of the Royal Met Soc,\u000a      130, (2004), 873-893; DOI: 10.1256\/qj.02.109\u000a    \u000a\u000aPapers of Griffith &amp; Nichols (2000) and Bell, Martin &amp; Nichols\u000a        (2004) cited in: Dee, Bias and Data\u000a      Assimilation, Quarterly J. of the Royal Met Soc, 131, (2005), 3323-3343.\u000a      (122 citations)\u000a    \u000a\u000aUse in ECMWF system: (19 citations, 28 Google Scholar) Balmaseda,\u000a      Dee, Vidard &amp; Anderson, A\u000a      multivariate treatment of bias for sequential data assimilation:\u000a      Application to the tropical oceans,\u000a      Quarterly J. of the Royal Met Soc. 133: (2007), 167-179; DOI:\u000a      10.1002\/qj.12\u000a    \u000a\u000aUse in Met Office system: (9 citations, 89 Google Scholar) Martin,\u000a      Hines &amp; Bell, Data assimilation\u000a      in the FOAM operational short-range ocean forecasting system: a\u000a      description of the scheme and\u000a      its impact, Quarterly J of the Royal Met Soc, 133, (2007), 981-995; DOI:\u000a      10.1002\/qj.74\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"4","Level2":"5","Subject":"Oceanography"},{"Level1":"4","Level2":"1","Subject":"Atmospheric Sciences"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    Need for use with new NEMO model in Met Office system: Lea,\u000a      Drecourt, Haines &amp; Martin,\u000a      Ocean altimeter assimilation with observational- and model-bias\u000a      correction, Quarterly J of the\u000a      Royal Met Soc, 134, (2008), 1761-1774; DOI: 10.1002\/qj.320.\u000a    Main paper stating the implementation of bias correction scheme in new\u000a        Met Office-NEMO system:\u000a      Storkey, Blockley, Furner, Guiavarc'h, Lea, Martin, Barciela, Hines, Hyder\u000a      &amp; Siddorn, Forecasting\u000a      the ocean state using NEMO: The new FOAM system, Journal of Operational\u000a      Oceanography, 3,\u000a      (2010), 3-15.\u000a    ECMWF Newsletter article stating that a bias correction scheme of the\u000a        type used in their previous\u000a        system needs to be put into the new DA scheme NEMOVAR: Mogenson,\u000a      Balmaseda, Weaver,\u000a      Martin &amp; Vidard, NEMOVAR: A variational data assimilation system for\u000a      the NEMO ocean model,\u000a      ECMWF Newsletter No. 120 - Summer 2009, (2009) pp17-21.\u000a      http:\/\/www.ecmwf.int\/publications\/newsletters\/pdf\/120.pdf\u000a    Pages 2, 18 and 34 of the following notes describe the use of the bias\u000a        correction scheme:\u000a      Mogensen, Data assimilation in the ocean, ECMWF Training Course slides\u000a      (2010).\u000a      http:\/\/bit.ly\/1faaI15\u000a    Page 17 of the following ECMWF publication refers to the incorporation\u000a        of the bias correction\u000a        based on the original work: Mogensen, Balmaseda &amp; Weaver, The\u000a      NEMOVAR ocean data\u000a      assimilation system as implemented in the ECMWF ocean analysis for System\u000a      4, ECMWF\u000a      Technical Memorandum 668, (2012). http:\/\/bit.ly\/17Wixks\u000a    Foam system description: http:\/\/www.ncof.co.uk\/FOAM-System-Description.html,\u000a      (2011).\u000a    ","Title":"\u000a    Ocean and climate forecasting improved by developments in data\u000a        assimilation\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Data assimilation techniques are used to improve predictions from\u000a      computational models. A\u000a      numerical model can never completely describe the complex physical\u000a      processes underlying the\u000a      behaviour of a real world dynamical system. Data assimilation combines\u000a      current estimates of state\u000a      variables such as temperature, pressure, humidity, and wind speed from the\u000a      model with physical\u000a      observations of the ocean and atmosphere in order to improve forecasts and\u000a      reduce uncertainty in\u000a      the forecast accuracy. Mathematically, the assimilation problem is an\u000a      ill-posed inverse problem,\u000a      matching model data to observations, that requires regularization and is\u000a      solved using cycled\u000a      variational methods or sequential filter methods. In addition to improving\u000a      forecasts from the model,\u000a      data assimilation can also be used to identify systematic bias errors\u000a      within the model, highlighting\u000a      where the model forecasts are consistently incorrect in relation to\u000a      physical observations.\u000a      Researchers at Reading have demonstrated that where systematic errors\u000a      exist in a model, these\u000a      can be overcome by using a correction term within the data assimilation\u000a      scheme and have\u000a      established a sound mathematical basis for this approach. As a result of\u000a      the new assimilation\u000a      scheme, the final model outputs are corrected for the inherent model\u000a      errors.\u000a    Data assimilation schemes in the early 1990s assumed that models were\u000a      perfect and ignored\u000a      systematic errors in the model. Between 1993 and 1997, A K Griffith, an\u000a      EPSRC PhD student&#8224; at\u000a      Reading, holding a CASE Award with the Met Office, and Prof N K Nichols at\u000a      Reading,\u000a      investigated the significant effects of systematic errors on generic data\u000a      assimilation schemes using\u000a      simplified models of atmospheric motion. They developed methods to correct\u000a      for these errors\u000a      within the assimilation process and provided a strong mathematical\u000a      foundation for these methods\u000a      based on control theory. These results were presented at the Newton\u000a      Institute meeting on the\u000a      Mathematics of Atmosphere and Ocean Dynamics (http:\/\/bit.ly\/18kXE19).\u000a      After the meeting, Dr M J\u000a      Bell from the Met Office (at the time manager of the Met Office Forecast\u000a      Ocean Assimilation Model\u000a      team, now Head of the National Centre for Ocean Forecasting) proposed that\u000a      Reading should\u000a      extend the method to oceanic models. From 1997 - 2000, in conjunction with\u000a      the Met Office, M J\u000a      Martin, a NERC CASE student&#8225; at Reading, and Prof Nichols, also at\u000a      Reading, investigated the\u000a      extension and application of the new correction method to estimating model\u000a      bias errors in oceanic\u000a      systems.\u000a    Under previous data assimilation schemes, spurious ocean circulation\u000a      occurred when thermal and\u000a      wind data with systematic errors near the equator were assimilated into\u000a      oceanic models. In the\u000a      work carried out during the second studentship, Nichols and Martin,\u000a      together with Bell, acting as\u000a      Martin's industrial supervisor, developed a method for correcting the bias\u000a      error in the pressure\u000a      gradient of existing oceanic models. The correction of the bias error,\u000a      using the model and\u000a      observation differences, improves the model dynamics by reducing or\u000a      eliminating spurious deep\u000a      ocean overturning circulations and restoring temperature and salinity\u000a      balances in the ocean\u000a      system. The technique relies on state augmentation, in which the data\u000a      assimilation scheme is used\u000a      to find both the ocean state variables and the model bias errors. The bias\u000a      correction scheme, as\u000a      well as correcting temperature and density imbalances leading to the\u000a      spurious circulation, also\u000a      improves the estimated horizontal velocities in the forecast, particularly\u000a      the strength and\u000a      positioning of the equatorial undercurrent near the ocean surface in the\u000a      tropics. This is especially\u000a      important for seasonal forecasting, which is dependent on the correct\u000a      simulation of the El Ni&#241;o\u000a      Southern Oscillation cycle, the cause of extreme weather (floods and\u000a      droughts) in many regions of\u000a      the world. The bias correction used in the ocean forecast model, coupled\u000a      with the forecast model\u000a      of the atmosphere, then ensures that the dynamical adjustment of the\u000a      system to the assimilated\u000a      data does not interfere with forecast skill.\u000a    &#8224; A K Griffith - Data assimilation for numerical weather prediction using\u000a      control theory, EPSRC\u000a      CASE Studentship (1993 - 1997) - University of Reading PhD, (1997) http:\/\/bit.ly\/1ahHeb7\u000a    &#8225; M J Martin - Data assimilation in ocean circulation models with\u000a      systematic errors, NERC CASE\u000a      Studentship (1997 - 2000) - University of Reading PhD, (2000) http:\/\/bit.ly\/1ahHWoX\u000a    "},{"CaseStudyId":"37393","Continent":[{"GeoNamesId":"6255146","Name":"Africa"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"1605651","Name":"Thailand"},{"GeoNamesId":"149590","Name":"Tanzania"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Informing international policy\u000d\u000a      The main impact of the research has been to provide the substantive\u000d\u000a      quantitative data that has informed international policy decisions both on\u000d\u000a      the regulation of trade in elephant products and on law enforcement\u000d\u000a      regarding the illicit ivory trade and the illegal killing of elephants.\u000d\u000a      Research findings (references 3.1 and 3.3) formed the main evidence in\u000d\u000a      mandated reports by MIKE (reference 3.2) and ETIS (reference 3.4) to the\u000d\u000a      last two CoPs respectively. These reports provide the evidence framing\u000d\u000a      much of the discussion between non-governmental organisations, governments\u000d\u000a      and pressure groups. For example, at the 15th CoP in March\u000d\u000a      2010, partly as a result of the MIKE report, the decision was made not to\u000d\u000a      allow regulated sales of ivory from Tanzania and Zambia.\u000d\u000a    At the 16th CoP in Bangkok in March 2013, there was \"unprecedented\u000a        uptake of the ETIS results... and motivation to seriously tackle\u000d\u000a        outstanding problems for the benefit of elephant conservation\" (as\u000d\u000a      reported in Pachyderm magazine). The ETIS report identified three\u000d\u000a      groups of countries most heavily implicated in the trade, including\u000d\u000a      Thailand, the host country. At the opening ceremony the Prime Minister of\u000d\u000a      Thailand announced that her country would be pursuing \"the goal of\u000d\u000a        putting an end to ivory trade and to be in line with international norms\".\u000a      CITES adopted a separate decision for each of the three groups of\u000d\u000a      countries identified by ETIS, impacting over twenty countries from Africa,\u000d\u000a      Asia and the Middle East. These decisions include the creation of a CITES\u000d\u000a      Ivory Enforcement Task Force to review law enforcement strategies for\u000d\u000a      combating illegal trade in eight countries in partnership with\u000d\u000a      international organisations such as the World Bank, the World Customs\u000d\u000a      Organization, the United Nations Office on Drugs and Crime and INTERPOL.\u000d\u000a\u0009  Raising\u000a        public awareness\u000d\u000a    The research outputs, in particular the bias-adjusted indicators, have\u000d\u000a      been used by environmental organisations to raise awareness of the\u000d\u000a      increasing threat to elephants from the illegal ivory trade. For example,\u000d\u000a      a major report Elephants in the Dust &#8212; The African Elephant Crisis\u000d\u000a      by United Nations Environment Programme, CITES, International Union for\u000d\u000a      Conservation of Nature (a global environmental organisation) and TRAFFIC,\u000d\u000a      drawing together all the recent research on elephants, was released at a\u000d\u000a      press conference at the 16th CoP in March 2013. The statistical\u000d\u000a      contributions of this research form much of the evidential basis both for\u000d\u000a      MIKE and ETIS. The headline result of the ETIS analysis was the primary\u000d\u000a      lead in a plethora of news stories generated by the international media.\u000d\u000a      The PIKE indicator is also referenced in the World Wide Fund for Nature\u000d\u000a      Crime Scorecard produced in 2012.\u000d\u000a    Improving monitoring systems and informing best practice\u000d\u000a      Both monitoring systems have been refined and improved as a result of the\u000d\u000a      research. For example, in developing the analytical framework for ETIS it\u000d\u000a      became necessary to restructure the ETIS database to ensure that the data\u000d\u000a      being collected are of the highest quality and the most relevant for the\u000d\u000a      required analytical purposes. The fields in the database were therefore\u000d\u000a      revised and data collection methodologies improved. In particular, an\u000d\u000a      online, government-restricted mechanism enabling countries to directly\u000d\u000a      enter and access data on illegal ivory seizures data is expected to be\u000d\u000a      launched by the end of 2013. This is a significant development that will\u000d\u000a      impact all countries that report to CITES (typically about 80 countries\u000d\u000a      from Europe, America, Africa, Asia and Australasia report seizures to\u000d\u000a      ETIS).\u000d\u000a    More generally, the long-term involvement of Mr Burn and Dr Underwood in\u000d\u000a      guiding the development of MIKE and ETIS since their inception has led to\u000d\u000a      a strong statistical underpinning to the two monitoring systems and has\u000d\u000a      helped shape their overall direction and identify their limitations. This\u000d\u000a      influence has, in part, been made via the MIKE-ETIS Technical Advisory\u000d\u000a      Groups (a group of experts working in elephant conservation and the\u000d\u000a      illegal ivory trade), where the research methodologies and results have\u000d\u000a      been presented and discussed.\u000d\u000a    During the period of the Darwin Initiative project, research\u000d\u000a      methodologies, results and the new database were presented in 2012 to the\u000d\u000a      United Nations Environment Programme's fourth African Elephant Meeting\u000d\u000a      held in Nairobi, to the 62nd CITES Standing Committee and to a\u000d\u000a      training event for African CITES Parties involved in reporting ETIS data\u000d\u000a      to CITES. Mr Burn and Dr Underwood also initiated a workshop on the\u000d\u000a      drivers of the illegal ivory trade, the results of which have informed\u000d\u000a      UNEP's Elephants in the Dust report.\u000d\u000a    In a press release from TRAFFIC in June 2012, Tom Milliken, who manages\u000d\u000a      ETIS, stated: \"Since 1997, our long-term collaboration with Bob Burn\u000d\u000a        and Dr Fiona Underwood from the University of Reading has progressively\u000d\u000a        scaled up the science behind ETIS and made it a `best practice',\u000d\u000a        state-of-the-art monitoring tool for the global conservation community\".\u000a      During the 62nd CITES Standing Committee, the Secretary General\u000d\u000a      of CITES also publicly recognised Mr Burn's contribution to the scientific\u000d\u000a      underpinning of MIKE and ETIS.\u000d\u000a    MIKE and ETIS are unique global databases. Reading statisticians are the\u000d\u000a      only researchers who have been given access to the raw ETIS data and,\u000d\u000a      until 2010 (when Mr Burn stood down from the MIKE Technical Advisory\u000d\u000a      Group), the only statisticians who had been given access to the MIKE data.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The research at the University of Reading has developed statistical\u000d\u000a      methods and information systems for two global monitoring systems for\u000d\u000a      elephants: MIKE (Monitoring the Illegal Killing of Elephants) and ETIS\u000d\u000a      (Elephant Trade Information System). The systems provide quantitative\u000d\u000a      evidence, via bias-adjusted indicators, on global and regional trends in\u000d\u000a      the illegal killing of elephants and the illicit ivory trade. This\u000d\u000a      evidence forms the substance of reports discussed at the Convention for\u000d\u000a      International Trade in Endangered Species of Wild Fauna and Flora (CITES).\u000d\u000a      Based on this information, CITES has adopted decisions to introduce\u000d\u000a      interventions targeting over 20 countries in Africa, Asia and the Middle\u000d\u000a      East aimed at curbing the illegal ivory trade. As well as providing the\u000d\u000a      underpinning data that has informed international policy on illicit\u000d\u000a      trading of this threatened species, the evidence has also helped raise\u000d\u000a      public awareness of the threats to elephants as well as improving\u000d\u000a      monitoring systems and increasing their reach.\u000d\u000a    ","ImpactType":"Environmental","Institution":"\u000d\u000a    University of Reading\u000d\u000a    ","Institutions":[{"AlternativeName":"Reading (University of)","InstitutionName":"University of Reading","PeerGroup":"B","Region":"South East","UKPRN":10007802}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"184745","Name":"Nairobi"},{"GeoNamesId":"1609350","Name":"Bangkok"}],"References":"\u000d\u000a    Outputs:\u000d\u000a    Publications have been internally reviewed and assessed as of at least 2*\u000d\u000a      quality. Outputs marked as * are suggested as those to assessed quality of\u000d\u000a      research:\u000d\u000a    \u000a1. *Burn, R.W., Underwood FM, Blanc J. (2011) Global trends and factors\u000d\u000a      associated with the illegal killing of elephants: a hierarchical Bayesian\u000d\u000a      analysis of carcass encounter data. PLoS ONE 6(9): e24165. doi:\u000a        10.1371\/journal.pone.0024165\u000d\u000a    \u000a\u000a2. *Burn, R.W. et al. (2010) Trends and factors associated with\u000d\u000a      the illegal killing of elephants. CoP15Inf.41-p.1. http:\/\/www.cites.org\/common\/cop\/15\/inf\/E15i-41.pdf.\u000d\u000a    \u000a\u000a3. Underwood, F.M., Burn, R.W. (2013) Dissecting the illegal ivory trade:\u000d\u000a      an analysis of ivory seizures data. PLOS One 8(10);\u000d\u000a      doi:10.1371\/journal.pone.0076539\u000d\u000a    \u000a\u000a4. Milliken, T., Burn RW, Underwood FM, Sangalakula L (2012). The\u000d\u000a      Elephant Trade Information System (ETIS) and the illicit trade in ivory. A\u000d\u000a      report to the 16th Conference of the Parties to CITES Proceedings of the\u000d\u000a      Conference of the Parties to CITES (UNEP), Bangkok: 16, Doc 53.2.2. http:\/\/www.cites.org\/eng\/cop\/16\/doc\/E-CoP16-53-02-02.pdf\u000d\u000a    \u000aResearch Grant:\u000d\u000a    Who: F.M. Underwood R.W. Burn (Visiting Research Fellow),\u000d\u000a      University of Reading (lead organisation), T. Milliken,TRAFFIC\u000d\u000a      East\/Southern Africa (project partner).\u000d\u000a      Grant title: Enhancing the Elephant Trade Information System to\u000d\u000a      Guide CITES Policy.\u000d\u000a      Project Number: 17020. Sponsor: Darwin Initiative, DEFRA,\u000d\u000a      UK Government.\u000d\u000a      Dates: September 2009 &#8212; August 2012. Amount: &#163;239,399.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    Informing international policy\u000d\u000a    \u000d\u000a      CITES decisions taken at the 16th CoP (http:\/\/www.cites.org\/eng\/dec\/valid16\/E16-Dec.pdf)\u000d\u000a        Decisions 16.78-16.80 are all derived from the ETIS analysis, and the\u000d\u000a        latter two are specifically referenced in that way. Decision 16.81\u000d\u000a        mandates even broader linkages with the United Nations Office on Drugs\u000d\u000a        and Crime to tackle illegal ivory trade issues.\u000d\u000a      Milliken T (2013) Progress in updating the Elephant Trade Information\u000d\u000a        System. Pachyderm 53 111-117 describing the outcomes of the 16th\u000d\u000a        CoP with regard to elephants\u000d\u000a        http:\/\/www.pachydermjournal.org\/index.php\/pachy\/article\/view\/328\/226\u000a\u000d\u000a      CITES decisions at 15th CoP regarding Tanzania: http:\/\/www.cites.org\/eng\/cop\/15\/doc\/E15-68A06a%29.pdf\u000a\u000d\u000a    \u000d\u000a    Raising public awareness\u000d\u000a    \u000d\u000a      UNEP, CITES, IUCN, TRAFFIC (2013) Elephants in the Dust &#8212; The\u000d\u000a          African Elephant Crisis. A rapid response assessment. United\u000d\u000a        Nations Environment Programme, GRID-Arendal\u000d\u000a      WWF Wildlife Crime Scorecard Report (2012):\u000d\u000a        http:\/\/www.wwf.org.uk\/wwf_articles.cfm?unewsid=6135\u000a\u000d\u000a      `Stop ivory poaching or face sanctions, nations warned at Cites' (Guardian,\u000d\u000a        6 March 2013):\u000d\u000a        http:\/\/www.guardian.co.uk\/environment\/2013\/mar\/06\/ivory-poaching-sanctions-cites\u000a\u000d\u000a    \u000d\u000a    Improving monitoring systems\u000d\u000a    \u000d\u000a      Minutes of Technical Advisory Group meetings &#8212; in particular the\u000d\u000a        minutes of the 8th meeting stressing the need to ensure that\u000d\u000a        results of analysis are communicated in non-technical language to a\u000d\u000a        broad audience, and the minutes of the11th meeting which\u000d\u000a        demonstrates the continuous use of PIKE. http:\/\/www.cites.org\/eng\/prog\/mike\/mike_etis_tag.php\u000a\u000d\u000a      Press release regarding initial release of new ETIS database\u000d\u000a        http:\/\/www.traffic.org\/home\/2012\/7\/26\/statisticians-help-fight-against-increasing-illegal-ivory-tr.html\u000a\u000d\u000a      Minutes of 4th African Elephant meeting where new ETIS\u000d\u000a        database presented\u000d\u000a        http:\/\/www.cites.org\/eng\/prog\/mike\/reg_meet\/aem4\/AEM4_summary_record_EN.pdf\u000a\u000d\u000a    \u000d\u000a    Individuals who could corroborate the impact (contact details provided\u000d\u000a        separately)\u000d\u000a    \u000d\u000a      Secretary General of CITES: importance of scientific evidence base to\u000d\u000a        inform discussions in CoP as well as ETIS and MIKE roles within this.\u000d\u000a      Executive Director of TRAFFIC International: role of ETIS report on\u000d\u000a        CoP\u000d\u000a      Director of ETIS \/ Elephant and Rhino Programme Co-ordinator, TRAFFIC:\u000d\u000a        role of statisticians in the development of ETIS analytical framework\u000d\u000a        and its database, contributing to best practice and improving the\u000d\u000a        monitoring systems.\u000d\u000a      Acting Coordinator, CITES MIKE: role of statistical analysis in\u000d\u000a        developing PIKE to ensure scientific basis of MIKE\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Informing international decision making on the protection of elephants\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Background\u000d\u000a      CITES, a United Nations convention administered by the United Nations\u000d\u000a      Environment Program (UNEP), is the global regulatory body for\u000d\u000a      international wildlife trade. In 1989 CITES banned all trade in elephant\u000d\u000a      products, but in 1997, at the 10th meeting of the CITES\u000d\u000a      Conference of the Parties (CoP) agreed to a tightly regulated one-off sale\u000d\u000a      of ivory from three Southern African countries to Japan. This was a\u000d\u000a      controversial decision but it led the CITES Parties to mandate two global\u000d\u000a      monitoring systems for elephants, MIKE and ETIS. Part of the remit of\u000d\u000a      these systems was to provide evidence on levels and trends in the illegal\u000d\u000a      killing of elephants (MIKE) and the illegal trade in ivory (ETIS) and, as\u000d\u000a      far as possible, relate these results to decisions taken about protecting\u000d\u000a      elephants under the Convention. Both programmes are required to provide\u000d\u000a      comprehensive analyses to meetings of the Conference of the Parties (CoPs)\u000d\u000a      every two to three years and update reports to the annual Standing\u000d\u000a      Committee meetings to support decision-making for elephants under the\u000d\u000a      Convention.\u000d\u000a    The MIKE programme was designed as a site-based initiative to collect\u000d\u000a      elephant mortality data, especially the number of illegally killed\u000d\u000a      elephants, at selected sites throughout elephant range in Africa and Asia.\u000d\u000a      The focus of ETIS was on seizures of illegal ivory trade made by law\u000d\u000a      enforcement agencies across the world. However, when these programmes were\u000d\u000a      first set up, the statistical methods required to analyse the data were\u000d\u000a      not in place.\u000d\u000a    Involvement of the University of Reading in MIKE and ETIS\u000d\u000a      ETIS was to be based on an earlier system called BIDS (Bad Ivory Database\u000d\u000a      System) that was run by TRAFFIC International, the wildlife trade\u000d\u000a      monitoring network. In 1997 Mr Robert Burn (then of the Statistical\u000d\u000a      Services Centre, University of Reading), was contracted to carry out a\u000d\u000a      review of BIDS and advise how it could be adapted to meet CITES\u000d\u000a      requirements. He presented this review at a MIKE\/ETIS technical design\u000d\u000a      workshop in 1997 and identified the major analytical challenges.\u000d\u000a      Subsequently he was invited to sit on the Technical Advisory Groups for\u000d\u000a      both MIKE and ETIS and to carry out a number of consultancies for both\u000d\u000a      monitoring systems, including: the design of the site selection protocol\u000d\u000a      for MIKE; baseline analysis of the MIKE data for the 12 to 14th\u000d\u000a      CoPs (between 2002 and 2007); developing the ETIS database; and carrying\u000d\u000a      out comprehensive analyses of the ETIS data for the 12th to 15th\u000d\u000a      CoPs (between 2002 and 2010). The methodology of these early analyses were\u000d\u000a      largely ad-hoc and carried out to meet very short deadlines. Even so, the\u000d\u000a      very first analysis of the ETIS data by Mr Burn, presented to the 12th\u000d\u000a      CoP in 2002, indicated the importance of China in the illegal ivory trade\u000d\u000a      at a time when the focus was on Japan. Partly as a result of these changes\u000d\u000a      CITES allowed China to participate in the tightly controlled sale of ivory\u000d\u000a      in 2008.In 2004, Dr Fiona Underwood joined the Statistical Services Centre\u000d\u000a      and participated in some of the above work. She had previously been\u000d\u000a      involved, together with Mr Burn, in designing the MIKE data analysis\u000d\u000a      strategy published in 2003.\u000d\u000a    In 2008, Mr Burn and Dr Underwood (by then a lecturer at the University\u000d\u000a      of Reading) were contracted by MIKE to develop methods for an in-depth\u000d\u000a      analysis of the MIKE data. This effort resulted in a research paper with\u000d\u000a      the MIKE data analyst (reference 3.1) that described the methodology and\u000d\u000a      applied it to MIKE data on elephant mortality from 2002 to 2009. In 2009,\u000d\u000a      Dr Underwood and Mr Burn (by then a visiting Research Fellow at the\u000d\u000a      University of Reading), with TRAFFIC sought funding for a 3-year research\u000d\u000a      project aimed at ensuring the long-term sustainability of ETIS through the\u000d\u000a      further development of the analytical framework, new database software to\u000d\u000a      support the system, formalised standard operating procedures and training\u000d\u000a      materials to enhance global participation of the CITES Parties. One of the\u000d\u000a      technical outcomes of the project was a scientific paper (reference 3.3)\u000d\u000a      describing the new analytical methods and the results from analysing ETIS\u000d\u000a      data from 1996 through 2011. A comprehensive report was issued to the 16th\u000d\u000a      CITES CoP as a formal agenda item (reference 3.4), showing illegal ivory\u000d\u000a      trade trends, identifying key countries in the illicit trade and\u000d\u000a      describing trade routes and dynamics driving the trade. This report led to\u000d\u000a      a series of actions directed at specific countries to curtail the illegal\u000d\u000a      trade in elephant ivory.\u000d\u000a    Statistical research\u000d\u000a      The key statistical issue for both MIKE and ETIS is that they aim to\u000d\u000a      monitor and report on covert and illegal processes, which cannot be\u000d\u000a      monitored using standard statistical methods. The monitoring is carried\u000d\u000a      out by law enforcement agencies that intervene in, rather than passively\u000d\u000a      observe, the process being monitored. Furthermore, law enforcement\u000d\u000a      agencies differ in their ability to intervene and to report their\u000d\u000a      interventions to the relevant monitoring system. This variability is\u000d\u000a      difficult to measure but must be accounted for so that estimates of\u000d\u000a      illegal activity can be compared between countries and over time. There\u000d\u000a      had been very little statistical research into these problems prior to\u000d\u000a      this work. A further aim of the research was to produce simple indicators\u000d\u000a      that can be used to inform policy makers about trends.\u000d\u000a    For both monitoring systems, proxy variables were identified or developed\u000d\u000a      that could account for differences in detecting and reporting illegal\u000d\u000a      activity over time and between countries or sites. These included\u000d\u000a      background variables relating to corruption and development, and variables\u000d\u000a      specific to MIKE and ETIS.\u000d\u000a    For MIKE, the indicator PIKE, the Proportion of Illegally Killed\u000d\u000a      Elephants (of all elephant carcasses found on patrol) was developed.\u000d\u000a      Bayesian hierarchical models were used to capture the structure of the\u000d\u000a      data, trends in PIKE were described and the relative role of\u000d\u000a      poaching in individual sites and countries was estimated.\u000d\u000a\u0009  For ETIS, a new\u000d\u000a      modelling framework was developed that extended the ideas in MIKE by\u000d\u000a      modelling the underlying process by which data enter ETIS using Bayesian\u000d\u000a      hierarchical latent variable models. Proxies that describe differences in\u000d\u000a      the latent variables (reporting and seizure rates) between countries and\u000d\u000a      over time were identified and tested. A multivariate negative binomial\u000d\u000a      model enabled different dynamics to be considered for trade in raw and\u000d\u000a      worked ivory in three weight classes. Smoothed bias-adjusted relative\u000d\u000a      indicators of the number of transactions by country, year and ivory class\u000d\u000a      were produced and aggregated to give a global Transactions Index and\u000d\u000a      Weights Index. The framework was applied to over 11,000 records of illegal\u000d\u000a      ivory seizures from 1996 to 2011 from 68 countries. The headline result is\u000d\u000a      that globally illegal ivory trade activity in 2011 has more than doubled\u000d\u000a      since 2007, and tripled since 1998.\u000d\u000a    "},{"CaseStudyId":"37404","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000a    The research was used in collaboration with digital media agency Bloom\u000a      Media Ltd. to develop Whisper, a new data analytics tool capable of\u000a      analysing social network data in real time as a framework for return on\u000a      investment (RoI) measurement.\u000a    Applying the research\u000a      In 2011Bloom were looking to speed up the market research cycle and\u000a      measure return on investment (RoI) for their clients, something not then\u000a      possible for social media campaigns. Bloom's Head of Data Insight, Mr\u000a      Peter Laflin, approached Prof Grindrod to discuss a collaborative approach\u000a      for commercial development of the research [6] (which he had read about in\u000a      [3], Section 3).\u000a    Initially, the researchers assisted Bloom to secure funding through the\u000a      Technology Strategy Board to develop a proof of concept that\u000a      communicability could be used to effectively measure social media\u000a      marketing campaigns. Ultimately this led to the creation of Whisper [1],\u000a      described as a world class platform for analysing Twitter and other social\u000a      media data feeds and which incorporates algorithms generated by the\u000a      research within its engine.\u000a    Whisper is marketed by Bloom [2] as an entirely new planning tool that\u000a      allows their clients to monitor the opinions, stories, emotions and\u000a      affinities of social media communities discussing topics that resonate\u000a      with their clients' products or values. This is achieved using algorithms\u000a      developed by the researchers that enable visualisation of the structures\u000a      of these communities as they adapt in real time. This enables Bloom to\u000a      assess huge data sets and identify the key influencers within these\u000a      communities and allows Bloom's clients to engage efficiently with these\u000a      communities in real time, giving them insight into their customers' brand\u000a      affinity, mood, device use and location.\u000a    In a letter to Prof Grindrod [5], Mr Laflin describes the key role of the\u000a      research: \"Whisper is the world's first data analytics tool that can\u000a      accurately measure the impact and RoI from social media. At the heart of\u000a      Whisper is a specific implementation of your work and the measure of\u000a      `influence' is a proxy for your communicability ideas\". Mr Laflin\u000a      highlighted that by the end of 2013, Bloom will have invested over &#163;200k\u000a      in this work, and intend to continue investment and development in order\u000a      to stay \"at the cutting edge of the scientific measurement of marketing\".\u000a    Impacts within Bloom\u000a      The unique insight provided by Whisper has enabled Bloom to alter their\u000a      brand direction. As a result of exhibiting the technology, they have\u000a      developed substantial new business opportunities, including significant\u000a      growth and a range of new clients such as Anglian Home Improvements, ADT\u000a      and LA Fitness[9]. Income has doubled to &#163;2.4m and staff numbers have\u000a      doubled to 60 during the financial year 2012-2013[8]. Additionally, Bloom\u000a      now employs highly skilled mathematics graduates and postgraduates to\u000a      develop and grow this increasingly important area. The company have also\u000a      developed additional market opportunities by providing analytics services\u000a      to other marketing agencies for use under their own names.\u000a    This work has significantly raised the profile of the company within the\u000a      industry. For example, they have been nominated for the 2013 DADI awards\u000a      and the Innovation category of the 2012 Some Comms Awards for their use of\u000a      Whisper with Anglian Home Improvements [9].\u000a    Impacts on Bloom's clients\u000a      A key aim for Bloom was to provide their clients with clear evidence of\u000a      RoI for social marketing activities. The tool and framework have enabled\u000a      Bloom to identify the true influencers within a social network, and then\u000a      use this information to plan viral content for their clients. This has\u000a      enabled Bloom's clients to gain greater value from their marketing\u000a      activities and an improved understanding of their customers, leading to\u000a      better targeting and changes in their digital marketing approach.[11]\u000a    Raising public awareness\u000a      Graphics produced by Whisper, which show the evolving networks in a clear\u000a      and simple format and demonstrate how network members interact and the\u000a      relative influence of individuals, have contributed to the public\u000a      understanding of social network interactions. An example was on show at\u000a      the Royal Society 2013 Summer Science Exhibition and was runner up in the\u000a      Infographics category of the Exhibition's image competition [7]. The\u000a      exhibition attracted over 10,000 members of the public including 2,000\u000a      school students[10], with many more reached through coverage on TV, in the\u000a      media and online.\u000a    Wider impacts\u000a      The benefits of the research within social media were immediately\u000a      recognised, as the methods work efficiently and for large matrices.\u000a      However, applications for other sectors are also being developed by the\u000a      researchers, and early work with the defence sector has led to a 2011 TSB\u000a      grant to develop a proof of concept.\u000a    ","ImpactSummary":"\u000a    Researchers in the Centre for the Mathematics of Human Behaviour at the\u000a      University of Reading have developed a novel approach for the real-time\u000a      monitoring of evolving social networks. These networks, in which\u000a      connections between individuals change over time, are an important\u000a      opportunity for online advertising. The research has been used in\u000a      collaboration with Bloom Media Ltd to develop a new tool that gives their\u000a      clients a better understanding of the impacts of social media campaigns.\u000a      As a result Bloom are leading the field in this area, allowing them to\u000a      attract major new clients and leading to significant growth of the\u000a      business. The company now directly employs highly skilled mathematics\u000a      graduates specifically to work in this area.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Reading\u000a    ","Institutions":[{"AlternativeName":"Reading (University of)","InstitutionName":"University of Reading","PeerGroup":"B","Region":"South East","UKPRN":10007802}],"Panel":"B         ","PlaceName":[],"References":"\u000a    All references are peer-reviewed. [1],[2] and [6] are published in highly\u000a      regarded international journals, [2] has since been cited in several\u000a      reviews and research articles on temporal networks. [6] appeared in SIAM\u000a      review with 5-year impact factor of 8. [3] is in SIAM news, which\u000a      addresses a wide audience and is a news-journal for world-wide applied\u000a      mathematics community.\u000a    \u000a[1] Grindrod, P., &amp; Higham, D. (2010). Dynamical models, inverse\u000a      problems and propagation. Proceedings of the Royal Society, Series A,\u000a      vol. 466 no. 2115, 753-770. doi:10.1098\/rspa.2009.0456\u000a      Citations on Web of Science: 12 as of 18 Sep 2013\u000a    \u000a\u000a[2] Grindrod, P., Parsons, M. C., Higham, D. &amp; Estrada, E. (2011).\u000a      Communicability across evolving networks. Physical Review E, 83,\u000a      Issue 4 pp 046120. doi: 10.1103\/PhysRevE.83.046120\u000a      Citations on Google Scholar: 41 as of 18 Sep 2013\u000a    \u000a\u000a[3] Desmond J. Higham, Peter Grindrod, and Ernesto Estrada (2011) People\u000a      who read this article also read... Part I and II(SIAM) Newsletter article,\u000a        SIAM News, Volume 44 , Numbers 1 and 2\u000a    \u000a\u000a[4] Grindrod, P., &amp; Higham, D. (2011). Models for evolving networks:\u000a      with applications in telecommunication and online activities. IMA Journal\u000a      of Management Mathematics doi: 10.1093\/imaman\/dpr001\u000a    \u000a\u000a[5] Vukadinovic-Greetham, D., Stoyanov, Z., &amp; Grindrod, P. (2013).\u000a      Centrality and spectral radius in dynamic networks. Computing and\u000a        Combinatorics, Lecture Notes in Computer Science. 7936, pp. 791-800.\u000a      Hangzhou, China: Springer. DOI 10.1007\/978-3-642-38768-5_72\u000a    \u000a\u000a[6] Grindrod, P., &amp; Higham, D. (2013). A matrix iteration for\u000a      summarizing dynamic networks. SIAM review, 55, 118-128. http:\/\/dx.doi.org\/10.1137\/110855715\u000a      1 citation on 18 Sep 2013\u000a    \u000aRelevant grant funding:\u000a    EPSRC grant EP\/G065802\/1 (as part of `The Horizon Hub' led by the\u000a      University of Nottingham)\u000a      PI (Reading): Peter Grindrod\u000a      Value to Reading &#163;886,658. Period: 01\/10\/2009 to 31\/05\/2015.\u000a    EPSRC MOLTEN EP\/I016031\/1\u000a      PIs: Peter Grindrod and Desmond Higham (University of Strathclyde)\u000a      Title: Mathematics Of Large Technological Evolving Networks.\u000a      Value: &#163;171,474. Period: 01\/03\/2011 to 28\/02\/2013\u000a    TSB #710104 (Awarded to Bloom Media)\u000a      PI: Peter Grindrod\u000a      Title: Digital Business Analytics for decision makers\u000a      Value: &#163;94,000 Period: 01\/12\/2011 to 30\/11\/2012\u000a    Centre for Defence Enterprise (a part of Defence Science and Technology)\u000a      DSTLX-1000059966\u000a      (Awarded to CountingLab Ltd - spin out company of the University of\u000a      Reading)\u000a      PI: Peter Grindrod\u000a      Title: Applying New Thinking To Counter-Terrorism: Communicability across\u000a      an Evolving Social\u000a      Network - Determining the biggest influencers, risers and fallers\u000a      Value: &#163;40,500 + VAT Period: 01\/09\/2011 to 12\/03\/2012\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"8","Level2":"6","Subject":"Information Systems"}],"Sources":"\u000a    \u000a      Whisper: http:\/\/www.bloomagency.co.uk\/whisper\/\u000a\u000a      The collaboration between the researchers and Bloom Media was\u000a        highlighted in an online article: http:\/\/www.thedrum.co.uk\/news\/2012\/03\/27\/centre-mathematics-human-behaviour-university-reading-appoints-bloom-develop-next\u000a\u000a      The impact of the collaboration with Bloom has been highlighted on\u000a        their website: http:\/\/www.bloomagency.co.uk\/measuring-influence-at-digital-futures-2012\/\u000a\u000a      The collaborative work has also been presented as a conference paper\u000a        at SocInfo2012, 5-7 Dec 2012, Lausanne. Laflin P, et al. Dynamic\u000a        targeting in an online social medium.\u000a      Letter from Head of Data Insight, Bloom Agency - available upon\u000a        request.\u000a      http:\/\/www.bloomagency.co.uk\/whisper\/history\/\u000a      Royal Society Picturing Science 2013 image competition (http:\/\/royalsociety.org\/grants\/picturing-science\/).\u000a        Runner-up: Infographics category. Image title \"Twitter activity: a\u000a        snapshot of tweeter-follower interactions as a conversation grows\" (http:\/\/www.bloomagency.co.uk\/from-data-to-art-bloom-at-the-royal-society\/;\u000a        image at http:\/\/www.flickr.com\/photos\/royalsociety\/8691325239\/in\/set-72157633103539082\/lightbox\/).\u000aCompetition\u000a        to select outstanding images of science provided by Research Fellows.\u000a        The judging criteria are visual impact of the image; public appeal of\u000a        the image; and scientific story behind the image.\u000a      http:\/\/www.thedrum.com\/news\/2013\/04\/02\/digital-agency-bloom-sees-income-grow-24m-it-doubles-staff-numbers\u000a      Press releases from Bloom http:\/\/www.bloomagency.co.uk\/adt-helps-keep-home-safe-while-youre-away\/\u000a        http:\/\/www.bloomagency.co.uk\/anglian-team-up-with-bloom-to-offer-one-lucky-winner-a-different-type-of-glass\/ http:\/\/www.bloomagency.co.uk\/time-waits-for-no-man-as-new-new-la-fitness-begins-roll-out\/\u000a\u000a      Figures found at the top of this page http:\/\/royalsociety.org\/summer-science\/proposals\/\u000a\u000a      Press release from Very - http:\/\/www.bloomagency.co.uk\/very-and-bloom-team-up-to-push-second-screening-boundaries\/\u000a\u000a    \u000a    ","Title":"\u000a    Applying the mathematics of evolving networks for more effective social\u000a      media marketing\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Research background\u000a      Digital communications between individuals have resulted in large,\u000a      multi-layered social networks that evolve from moment to moment. Although\u000a      methods exist for analysing and modelling static networks, recent trends\u000a      in communications, transport and energy have highlighted the need for\u000a      methods that are appropriate for dynamic, evolving networks. Therefore,\u000a      funded by an EPSRC Digital Economy programme through the Horizon hub,\u000a      Professor Peter Grindrod collaborated with Prof Desmond Higham (University\u000a      of Strathclyde) to develop a simple mathematical model of evolving\u000a      networks [1], defined here as a fixed set of vertices (e.g. a group of\u000a      people) with edges (e.g. peer-to-peer communication) that appear and\u000a      disappear over time. The approach uses a temporal series of snapshot\u000a      graphs instead of collapsing different time points on to one (denser)\u000a      graph, thereby retaining both information about time-flow and the benefits\u000a      of more sparse graphs.\u000a    Main contribution\u000a      The core of this work involved exploring centrality measures in evolving\u000a      networks. Centrality is the relative importance of an individual (a\u000a      vertex) and determines its involvement in a network. Prior to this work,\u000a      centrality measures had been developed for static networks, but had not\u000a      been successfully translated to evolving networks. The full technical\u000a      details are given in [2] although the discussions leading to the impact\u000a      were prompted by a SIAM news article [3], which was written for a wider\u000a      audience.\u000a    Katz centrality computes the relative influence of a node within a static\u000a      network by measuring its number of immediate neighbours and other nodes\u000a      that connect to it through those immediate neighbours. An attenuation\u000a      factor lessens the influence of longer pathways. The researchers built an\u000a      extension of Katz centrality for evolving networks that gives\u000a      communicability across time steps. When peer-to-peer communications can be\u000a      observed, this allows the identification of individual strong broadcasters\u000a      and listeners [4]. It can also be scaled up to very large data sets as it\u000a      involves handling large but sparse matrices.\u000a    Fast algorithmic computations of centrality ranking can be achieved using\u000a      sparsity and series approximation. Although for some real applications the\u000a      choice of the attenuation factor can be challenging, the researchers\u000a      showed that this can be solved simply by normalisation and probabilistic\u000a      interpretation of the attenuation factor as a proxy for radius of\u000a      centrality [5].\u000a    Further developments\u000a      The researchers are now working on a continuous version in which edges are\u000a      created and destroyed continuously rather than being simply snapshots in\u000a      time. Additionally, `memory' is being incorporated into the centrality\u000a      ranking, so that recently appearing edges are given more weight than older\u000a      ones [6].\u000a    Key researchers\u000a      Peter Grindrod (Professor, 2008-Sept 2103)\u000a      Mark Parsons (PhD student 2010-July 2013; PDRA July-Oct 2013)\u000a      Danica Greetham (PDRA Nov 2010-Oct 2011; Lecturer Oct 2011-present)\u000a    "},{"CaseStudyId":"37454","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000a    The impact of the research has taken place through three routes: i) the\u000a      uptake of the methodology in the implementation of clinical trials by two\u000a      pharmaceutical companies, ii) citation of the research in the regulatory\u000a      guidance on adaptive clinical trial design, and iii) increased awareness\u000a      by clinicians and other medical professionals of the benefit of the\u000a      adaptive design methodology in their patient groups.\u000a    i) Uptake of the research by pharmaceutical companies\u000a    The underpinning research was initially disseminated through conference\u000a      presentations attended by industry professionals. The full publication of\u000a      the research in the statistical literature in 2003 and 2005, led to two\u000a      companies independently approaching Drs Stallard and Todd to discuss the\u000a      research who each subsequently adopted the phase II\/III adaptive design\u000a      approach proposed by the Reading team in their drug development\u000a      programmes, as outlined below.\u000a    AstraZeneca:\u000a    Based on the Reading team's approach, AstraZeneca designed a phase II\/III\u000a      multi-national pivotal trial ([1,2] in Section 5), HORIZON III, for\u000a      cediranib (Recentin). The impact of the team's work in this setting was\u000a      illustrated in a press release ([3] in Section 5) to the investment\u000a      community in February 2008, which contained a quote by John Patterson,\u000a      AstraZeneca's Executive Director for Development, who said: \"Due to the\u000a        Phase II\/III trial design, HORIZON III is able to move directly into\u000a        Phase III utilizing all the Phase II data and this saves valuable time\u000a        in assessing the potential benefit of RECENTIN in the first line\u000a        metastatic colorectal cancer setting\". By adopting this new\u000a      methodology there is clear utility for pharmaceutical companies in terms\u000a      of greater efficiency in clinical trials via potential for reducing the\u000a      numbers of patients entered into trials with significant ethical benefits.\u000a      The AstraZeneca development programme continues today through further\u000a      clinical trials, although cediranib is not yet available in the UK.\u000a    The study by AstraZeneca was, to our knowledge, one of the first seamless\u000a      phase II\/III trials to be conducted by any pharmaceutical company. To date\u000a      the total number of seamless phase II\/III studies remains small whilst\u000a      companies assess the benefits of these new approaches. The results of the\u000a      completed trial have been presented by AstraZeneca at international\u000a      medical conferences ([4] in Section 5) and in 2012, a presentation was\u000a      given at the PSI (Statisticians in the Pharmaceutical Industry) annual\u000a      conference entitled \"Adaptive Trial Designs: Lessons Learned in Oncology\u000a      in AstraZeneca\" ([5] in Section 5). Through this outreach the concept of\u000a      seamless adaptive designs as a plausible approach to clinical trial study\u000a      design has been highlighted to the medical community and to industry.\u000a      AstraZeneca have indicated that they would consider the adaptive design\u000a      approach in future trials and have developed their own code for\u000a      implementation, based on the methodological concepts described in\u000a      Reading's underpinning research.\u000a    Avexa:\u000a    Avexa, an Australian company, was the second pharmaceutical company to\u000a      implement a trial design based on the underpinning research in the\u000a      development of apricitabine, a treatment for drug-resistant HIV ([6] in\u000a      Section 5). The study, AVX301, began recruiting in February 2008 and\u000a      closed recruitment in January 2010. It was stopped for reasons not\u000a      associated with the trial design. Avexa press releases discuss the\u000a      progress of the study, with positive reference to the adaptive trial\u000a      approach ([7] in Section 5).\u000a    ii) Use in regulatory guidance on clinical trial design\u000a    Novel adaptive designs and the associated analytical frameworks, such as\u000a      those developed by the team at Reading, are having an increasing impact on\u000a      current thinking in clinical trial design within the pharmaceutical\u000a      industry. Indeed, two key regulatory authorities have recently produced\u000a      guidance documents on adaptive designs (US Food and Drug Administration\u000a      (FDA), 2010; European Medical Agency, 2007); both organisations anticipate\u000a      that more clinical trials will be designed using this framework and the\u000a      FDA cite the Reading work ([4, 5] in Section 3) in their guidance document\u000a      ([8] in Section 5). Whilst it is incorrect to state that the research\u000a      undertaken at Reading was the sole catalyst for driving such change in\u000a      clinical trial approach, the citation of our work in global guidelines\u000a      such as these provides indication of our contribution.\u000a    iii) Increased awareness by clinicians and other medical professionals\u000a        of the benefit of adaptive design methodology in their patient groups.\u000a    The Reading team developed a Continuing Professional Development course\u000a      entitled \"Phase II\/III Clinical Trials\". This was delivered at Reading in\u000a      November 2006 to disseminate adaptive trial methodology in general and the\u000a      research undertaken at Reading in particular. Dr Jeremy Chataway (St\u000a      Mary's Hospital), a Multiple Sclerosis (MS) specialist participated in the\u000a      course. Several years later, Dr Chataway contacted Dr Todd about the\u000a      possibility of initiating work on adaptive designs in this therapeutic\u000a      area. A colleague of Dr Chataway's, Dr Richard Nicholas was tasked with\u000a      the asking the same question of the team at Warwick University (where\u000a      Professor Stallard is now working). The joint Warwick-Reading team was\u000a      commissioned by the Multiple Sclerosis Society to conduct further work on\u000a      adaptive design methodology in the specific setting of Secondary\u000a      Progressive MS. The Society funded two projects to develop a bespoke\u000a      adaptive trial design that could be used for a study of MS. The results of\u000a      this work have been published and presented in several conferences in 2009\u000a      ([9] in Section 5). Clinicians in the MS field are, therefore, being made\u000a      more aware of adaptive designs and their advantages.\u000a    ","ImpactSummary":"\u000a    Clinical trials are costly to the pharmaceutical industry and public\u000a      funding bodies, require major commitment from volunteer patients and take\u000a      significant time to lead to patient benefit. Adaptive designs are one\u000a      approach which seeks to improve the efficiency of such studies.\u000a      Statistical research at Reading has led to novel methodology for the\u000a      design and analysis of clinical drug trials within the framework of\u000a      adaptive designs which has the potential to reduce the time taken for\u000a      effective drugs to reach the market and thus benefit specific patient\u000a      groups. To date the research has had impact in three major ways: i) it has\u000a      been adopted by pharmaceutical companies as a means of improving the\u000a      efficiency of their clinical trials, ii) the research has been cited in\u000a      the regulatory guidance on adaptive clinical trial design, and iii) it has\u000a      increased awareness by clinicians and other medical professionals of the\u000a      potential benefit of the adaptive design methodology to their patient\u000a      groups. Hence, the research has influenced industry, regulatory and health\u000a      professionals with potential significant economic benefit and improved\u000a      outcome for patients.\u000a    ","ImpactType":"Political","Institution":"\u000a    University of Reading\u000a    ","Institutions":[{"AlternativeName":"Reading (University of)","InstitutionName":"University of Reading","PeerGroup":"B","Region":"South East","UKPRN":10007802}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a1. Stallard N, Todd S (2003) Sequential designs for phase III clinical\u000a      trials incorporating treatment selection. Stat Med 22,\u000a      689-703. DOI:10.1002\/sim.1362\u000a    \u000a\u000a2. Todd S (2003) An adaptive approach to implementing bivariate group\u000a      sequential clinical trial designs. J Biopharm Stat 13, 605-619.\u000a      DOI:10.1081\/BIP-120024197\u000a    \u000a\u000a3. Todd S, Stallard N (2005) A new clinical trial design combining phases\u000a      2 and 3: Sequential designs with treatment selection and a change of\u000a      endpoint. Drug Inf J 39, 109-118. ISSN 0092-8615\/2005\u000a    \u000a\u000a4. Kelly, P.J., Stallard, N. and Todd, S. (2005). An adaptive\u000a      group-sequential design for phase II\/III clinical trials that select a\u000a      single treatment from several. J. Biopharmaceutical Stat 15,\u000a      641-658. DOI: 10.1081\/BIP-200062857\u000a    \u000a\u000a5. Stallard N, Todd S (2005) Point estimates and confidence regions for\u000a      sequential trials involving selection. J Stat Plan Inference 135,\u000a      402-419. DOI: 10.1016\/j.jspi.2004.05.006\u000a    \u000aThese five publications have received a total of 138 citations, including\u000a      68 for [1] and 26 for [3].\u000a    Grants\u000a    \"Combined phase II\/III clinical trial designs\". Four grants totalling &#163;104,400\u000a      from Novartis Pharma (with Dr N. Stallard and Professor J. Whitehead, then\u000a      MPS Research Unit). January 2000 &#8212; December 2004.\u000a    \"Comparison of adaptive designs with group sequential designs when\u000a      treatment selection and evaluation are required\". &#163;20,000 from\u000a      Novartis Pharma (with Dr N. Stallard, then MPS Research Unit). September\u000a        2002 &#8212; February 2003.\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"12","Subject":"Oncology and Carcinogenesis"},{"Level1":"11","Level2":"3","Subject":"Clinical Sciences"},{"Level1":"11","Level2":"15","Subject":"Pharmacology and Pharmaceutical Sciences"}],"Sources":"\u000a    Sources to corroborate the AstraZeneca implementation of the phase II\/III\u000a      design:\u000a    1. The study is registered (mentioning the phase II\/III design) at:\u000a      www.clinicaltrials.gov\/ct2\/show\/NCT00384176?term=HORIZON+AstraZeneca&amp;rank=3\u000a    2. A publication of the trial design is available at:\u000a      jco.ascopubs.org\/content\/early\/2012\/09\/10\/JCO.2012.42.5355\/suppl\/DC1\u000a      The protocol (linked at the end of the abstract) cites the research ([3]\u000a      in Section 3) on p31.\u000a    3. Press release: www.astrazeneca.com\/Media\/Press-releases\/Article\/20080227--AstraZeneca-\u000a        Provides-Update-On-RECENTIN-Clinical-Deve\u000a    4. Medical conference presentations on the HORIZON study: Oral\u000a      presentation: Schmoll H, et al. \"mFOLFOX6 + cediranib vs mFOLFOX6\u000a      + bevacizumab in previously untreated metastatic colorectal cancer (MCRC):\u000a      a randomized, double-blind, phase II\/III study (HORIZON III). Annals\u000a        of Oncology 21 (Supplement 8): viii189-viii224, 2010. Abstract 580O.\u000a      bit.ly\/1ae6QZA.\u000a      Poster: Wilson D, et al. Application of adaptive study designs:\u000a      Phase II and III results from the cediranib (CED) horizon (HZ) II and III\u000a      studies. 2011 ASCO Meeting. meetinglibrary.asco.org\/content\/83624-102\u000a    5. Statistical conference presentation on the HORIZON study: Presentation\u000a      slides available for the presentation on \"Adaptive Trial Designs: Lessons\u000a      Learned in Oncology in AstraZeneca\" Sources to corroborate the Avexa\u000a      implementation of the phase II\/III design:\u000a    6. The study is registered (mentioning the phase 2b\/3 design) at:\u000a      http:\/\/www.clinicaltrials.gov\/ct2\/show\/NCT00612898?term=AVX301&amp;rank=2\u000a    7. Press releases related to the study on aprictiabine are on the\u000a      company's website under the tab `News': http:\/\/www.avexa.com.au\/\u000a      Releases on Mar 24 2009 and Jun 4 2009, amongst others, discuss study\u000a      AVX301\u000a    Source to corroborate the regulatory documentation:\u000a    8. FDA Guidance: http:\/\/www.fda.gov\/downloads\/Drugs\/...\/Guidances\/ucm201790.pdf\u000a      Lines 1928-1930 and 1996-1997give references 4 and 5 of Section 3,\u000a      respectively. Sources to corroborate the increased awareness by clinicians\u000a      and other medical professionals\u000a    9. Conference abstracts:\u000a      \"Developing a new trial design in secondary progressive multiple\u000a      sclerosis: A seamless adaptive approach.\" Presented at: ABN Joint Annual\u000a      Meeting\/Spanish Society of Neurology, Liverpool, UK, Jun 22-26 2009.\u000a      DOI:10.1136\/jnnp.2009.195214r.\u000a      \"Adaptive seamless trial designs in neurology: a case study in secondary\u000a      progressive multiple sclerosis.\" Presented at the 19th World Congress of\u000a      Neurology, Bangkok, Thailand, Oct 24-30 2009.\u000a      DOI:10.1016\/S0022-510X(09)70456-3.\u000a      \"Adaptive clinical trials incorporating treatment selection and\u000a      evaluation: methodology and application in progressive multiple\u000a      sclerosis.\" Presented at the 25th Congress of the European Committee for\u000a      Treatment and Research in Multiple Sclerosis, Dusseldorf, Germany, Sep\u000a      09-12 2009. \u000a    ","Title":"\u000a    Development of novel adaptive designs to improve efficiency in\u000a        clinical trials\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Traditionally the introduction of a new drug onto the market involves the\u000a      implementation of a series of clinical trials, progressing through Phases\u000a      I, II (sometimes split into IIa and IIb), III trials, with the studies\u000a      increasing in size, duration, and thereby costs and complexity, in order\u000a      to unequivocally demonstrate the efficacy and safety of a drug or drug\u000a      combinations, in defined patient groups. Phase I trials are often\u000a      first-in-man studies to consider safety, phase II trials are looking for\u000a      evidence of efficacy (IIa trials sometimes called proof-of-concept\u000a      studies, with IIb trials considering the question of finding an\u000a      appropriate dose or treatment format) and phase III trials are the\u000a      definitive comparison of, usually, a single experimental treatment with a\u000a      control, either the current standard or placebo. These trials take many\u000a      years to complete with most phase III trials for cancer treatment, for\u000a      example, taking up to 8-10 years to reach conclusion.\u000a    Conventionally the later phases of the drug development process (IIb and\u000a      III) are designed as what are known as `fixed sample size trials'. In such\u000a      designs the number of patients to be recruited to the particular trial is\u000a      calculated in advance of the study and data are collected on all of the\u000a      patients before any analyses are carried out. This statistical approach\u000a      was originally developed in the context of agricultural trials where all\u000a      measurements naturally occur simultaneously at harvest. In the medical\u000a      context, all observations are not available simultaneously as patients are\u000a      recruited to trials sequentially over a period of months, if not years.\u000a      This difference in the method of data accrual led to a different approach\u000a      being proposed for use in some clinical settings &#8212; for example where\u000a      patient recruitment is slow and over a long duration, combined with\u000a      rapidly observable measurements &#8212; that of `sequential trials' \/ `group\u000a      sequential trials'. In such a trial, data from patients are analysed at\u000a      one or more interim points in the trial as they accumulate, in order to\u000a      determine whether there is already sufficient evidence to draw valid\u000a      conclusions about the efficacy and safety of the treatment under study.\u000a      Research into this methodology peaked around the mid-to late-1990's.\u000a      Meanwhile, interest was growing in the possibility that clinical trials\u000a      could be designed with other adaptive features (not just stopping\u000a      for efficacy \/ futility), such as changing the patient population under\u000a      study, changing the primary endpoint, changing the treatment regimens\u000a      being tested. Such adaptations have significant potential to reduce the\u000a      time taken for definitive results to be obtained from clinical trials and\u000a      are therefore of great interest to the pharmaceutical industry in reducing\u000a      the cost of drug development and to medical professionals in accelerating\u000a      the availability of new treatment regimens for their patients. The\u000a      over-arching term now commonly used for trials where adaptations are\u000a      planned as part of their conduct is `adaptive designs'.\u000a    Partially funded by grants for methodological research from the\u000a      pharmaceutical company, Novartis, researchers at the University of Reading\u000a      were one of the first groups to begin work on developing methodology in\u000a      this field. The setting envisaged was where it is desirable to take more\u000a      than one experimental treatment into phase III, along with the control\u000a      treatment and then to make a selection, at a first interim analysis, of\u000a      the most promising of these treatments to continue in the trial along with\u000a      the comparator (treatment selection). This approach might be appropriate\u000a      when a stand alone phase IIb trial has not been conducted perhaps because\u000a      there are only a small number of candidate doses or treatments of\u000a      interest, or where the phase IIb study has been conducted but a single\u000a      experimental treatment was not identified. Taking the existing `group\u000a      sequential framework', we introduced a treatment selection element into\u000a      the design ([1] in Section 3). The methodological development was further\u000a      progressed: i) to allow the treatment selection to be made using a\u000a      different, (usually available earlier), endpoint to that which is the\u000a      primary interest ([3] in Section 3), ii) with associated consideration of\u000a      correlation ([2] in Section 3) and then, iii) to a consideration of how\u000a      further flexibility could be introduced by allowing the treatment\u000a      selection to happen over a number of successive stages, not just at the\u000a      first interim stage ([4] in Section 3). Once such designs had been\u000a      developed it was necessary to determine a new analysis framework since\u000a      traditional methods of statistical analysis are no longer appropriate.\u000a      Methodology for analysis was also developed ([5] in Section 3). Because\u000a      these new designs combine the dose-finding \/ treatment selection element\u000a      of the traditional phase II (in particular IIb) trial, along with the\u000a      definitive confirmatory analysis of phase III, they have become known by\u000a      several terms including seamless phase II\/III (or sometimes IIb\/III)\u000a      clinical trials, adaptive seamless designs, adaptive group sequential\u000a      designs.\u000a    The research was conducted by Dr Nigel Stallard and Dr Susan Todd (both\u000a      Senior Research Fellows in the Medical and Pharmaceutical Statistics\u000a      Research Unit at that time). Dr Todd (now Professor Todd) is still at the\u000a      University of Reading, whereas Dr Stallard (now Professor Stallard) moved\u000a      to the University of Warwick in 2005. The work described in [4] in Section\u000a      3 included a contribution by Dr Patrick Kelly who was also employed at the\u000a      University of Reading as a Research Fellow at that time.\u000a    "},{"CaseStudyId":"38830","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"3175395","Name":"Italy"},{"GeoNamesId":"6251999","Name":"Canada"}],"Funders":["Medical Research Council","Science and Technology Facilities Council"],"ImpactDetails":"\u000d\u000a    The underpinning research elucidated physical mechanisms by which\u000d\u000a      viscosity radically alters the\u000d\u000a      flagellar waveform, internal energy transport requirements, and resulting\u000d\u000a      cell trajectory. This means\u000d\u000a      that experimental work in the highly active field of sperm chemotaxis\u000d\u000a      (chemical guidance to the\u000d\u000a      egg) and also metabolic studies can only be interpreted accurately if\u000d\u000a      physiological viscosity fluids\u000d\u000a      are used. The research also provided mathematically-based computer codes\u000d\u000a      with which flow fields\u000d\u000a      and cell trajectories resulting from observed cell shapes, waveforms and\u000d\u000a      surface interactions can\u000d\u000a      be computed and visualised, the results of which have challenged\u000d\u000a      intuitions regarding the fluid\u000d\u000a      mechanics of motility based on inaccurate analogies in macroscale flow.\u000d\u000a      Taken together, these\u000d\u000a      findings have had a distinct and material impact on practices in clinical\u000d\u000a      science. This impact has\u000d\u000a      occurred in a short time-frame, following the first publications of the\u000d\u000a      research in 2009 (relative to\u000d\u000a      typical lab-to-bedside timescales of the order of 10-20 years). In more\u000d\u000a      detail:\u000d\u000a    4.1 Clinical perspectives and training\u000d\u000a    Despite the relatively short time-frame since 2009, the work is impacting\u000d\u000a      the medical science\u000d\u000a      perspectives of world-leading laboratories by changing the focus of their\u000d\u000a      work. Prof. David\u000d\u000a      Clapham, Howard Hughes Medical Institute and Harvard University, a\u000d\u000a      pre-eminent figure in\u000d\u000a      biomedicine, states: `Spermatozoa are short-lived mechanically driven\u000d\u000a        cells and their biology is\u000d\u000a        dominated by low Reynolds number fluid mechanics. Biologists without the\u000d\u000a        training of Dr. Smith\u000d\u000a        naturally have poor insight and understanding of mechanical forces on\u000d\u000a        the microscopic scale...\u000d\u000a        Without such collaborations across fields of expertise, the field will\u000d\u000a        stall, and worse, go in the\u000d\u000a        wrong direction due to misinterpretation of data... [Kirkman-Brown and\u000d\u000a        Smith] have made\u000d\u000a        remarkable inroads in areas of sperm research. I refer to your work\u000d\u000a        often to understand the\u000d\u000a        problems we are approaching in sperm biology. In particular, [Smith]\u000d\u000a        helped us a great deal in\u000d\u000a        understanding microscopic mechanics for our 2013 paper in Current\u000d\u000a        Biology (Miki K and Clapham\u000d\u000a        DE. 2013. Rheotaxis Guides Mammalian Sperm)...' [S8] The latter\u000d\u000a      study exemplifies fluid dynamics\u000d\u000a      of cell motility, a new addition to a group specialising in cell\u000d\u000a      electrophysiology.\u000d\u000a    The influence on clinicians' perspectives is also evidenced from a paper\u000d\u000a      by the clinical group led by\u000d\u000a      Prof. Chris Barratt (Ninewells Hospital and Medical School, Dundee) which\u000d\u000a      give the following\u000d\u000a      corroboration `...experiments modelling and examining sperm behaviour\u000d\u000a        in these physiologically\u000d\u000a        relevant environments are essential for obtaining an accurate analysis,\u000d\u000a        as recently demonstrated\u000d\u000a        by Smith and colleagues' &#8212; Barratt, Kay &amp; Oxenham, J. Biol.,\u000d\u000a      8:63, 2009.\u000d\u000a    The clinical science journal Molecular Human Reproduction, a\u000d\u000a      publication which generally focuses\u000d\u000a      on biochemical and molecular processes, summarised the concepts described\u000d\u000a      above, with a\u000d\u000a      mathematical modelling image from Smith and Kirkman-Brown being chosen to\u000d\u000a      illustrate the\u000d\u000a      journal front cover in a special issue on sperm motility published in\u000d\u000a      August 2011.\u000d\u000a      Members of the ChRS team have disseminated these findings at meetings for\u000d\u000a      clinical scientists\u000d\u000a      internationally, including the following invited oral presentations,\u000d\u000a      growing from more specialist\u000d\u000a      meetings in 2009 to major international conferences in 2011 and 2013.\u000d\u000a    \u000d\u000a      2009: International Symposium of Advanced Research Progress in\u000d\u000a        Reproductive Medicine,\u000d\u000a        Changzhou Women and Children's Health Hospital, China [S1], Maternal\u000d\u000a        Communication\u000d\u000a        with Gametes and Embryo COST Meeting, Alghero, Sardinia, Italy (convened\u000d\u000a        by Dr Alireza\u000d\u000a        Fazeli, a leading figure in British Andrology) [S10].\u000d\u000a      2011: Gordon Research Conference on Fertilization and Activation of\u000d\u000a        Development, NH,\u000d\u000a        USA (an internationally-leading meeting for specialists in the field)\u000d\u000a        [S2], 1st International\u000d\u000a        CAESAR Conference on Sperm Signaling and Motility, Bonn, Germany\u000d\u000a        (including leaders\u000d\u000a        such as Profs. U.B. Kaupp, D. Clapham and M. Eisenbach).\u000d\u000a      2013: Society for Reproduction and Fertility Annual Conference,\u000d\u000a        Cambridge, UK, Society\u000d\u000a        for the Study of Reproduction Annual Meeting, Montreal, Canada.\u000d\u000a    \u000d\u000a    The work has further impacted the training of clinical and research\u000d\u000a      scientists through Dr Kirkman-Brown's\u000d\u000a      teaching, for example at Frontiers in Reproduction, the\u000d\u000a      internationally-renowned six week\u000d\u000a      residential course at the Marine Biology Laboratory in Woods Hole,\u000d\u000a      Massachusetts. Each year this\u000d\u000a      course trains 20 future leaders in reproduction; without the underpinning\u000d\u000a      research, this strand of\u000d\u000a      thinking would not have been part of the curriculum.\u000d\u000a    Kirkman-Brown continues to bring the awareness of these ideas to his work\u000d\u000a      as British Andrology\u000d\u000a      Society Secretary, and UK host for the Basic Semen Analysis course\u000d\u000a      affiliated to the European\u000d\u000a      Society of Human Reproduction and Embryology.\u000d\u000a    Finally, studies at BWH are underway applying the model to test motility\u000d\u000a      drugs. A manuscript on the\u000d\u000a      energetic effects of the drug 4-aminopyridine is in preparation. The\u000d\u000a      mathematical approach\u000d\u000a      developed allows calculation of energy expenditure and transport in the\u000d\u000a      flagellum, quantifying how\u000d\u000a      the metabolism of the cell is modified under drug stimulation.\u000d\u000a    4.2 Grant funding for the ChRS clinical research laboratory at\u000d\u000a      Birmingham Women's Hospital\u000d\u000a    The research has brought two major grants to the ChRS labs at Birmingham\u000d\u000a      Women's Hospital\u000d\u000a      (MRC &#163;379K and STFC &#163;176K), allowing the laboratory to be equipped with\u000d\u000a      imaging equipment\u000d\u000a      for sperm motility research, including a high speed camera and an optical\u000d\u000a      splitting device\u000d\u000a      (designed by Dr Kirkman-Brown in collaboration with SME CAIRN Research\u000d\u000a      Ltd; for the work see\u000d\u000a      4.4 ).\u000d\u000a    The underpinning research motivated a sub-project as part of a much\u000d\u000a      larger successful bid\u000d\u000a      `Experimental Medicine Network of Excellence' to Advantage West Midlands\u000d\u000a      (the former regional\u000d\u000a      development agency). The Reproductive Medicine project brings\u000d\u000a      approximately &#163;278K of\u000d\u000a      infrastructure to ChRS. The new equipment (`BAMBI') allows high-throughput\u000d\u000a      screening of\u000d\u000a      samples, tracking and imaging of live cells, and precise spatial and\u000d\u000a      temporal control of photo-releasable\u000d\u000a      hormones and pharmacological agents [S9].\u000d\u000a    Combined with the fluid dynamic knowledge of the effect of viscosity on\u000d\u000a      motility and chemical\u000d\u000a      dispersion, these capabilities mean that Birmingham Women's Hospital now\u000d\u000a      has a system\u000d\u000a      unrivalled in the UK through which motility dysfunction and new therapies\u000d\u000a      can be investigated.\u000d\u000a      BAMBI is currently being used to trial a new potential motility drug\u000d\u000a      `Omega'.\u000d\u000a    4.3 Public awareness and behaviour\u000d\u000a    Findings have fed into ChRS TV work, including an episode of the 2008 BBC\u000d\u000a      TV Series Don't Die\u000d\u000a        Young on the Male Reproductive Organs [S4], designed to raise\u000d\u000a      awareness of the challenge faced\u000d\u000a      by sperm, and consequent need for men to be aware of the effect of poor\u000d\u000a      lifestyle and prevalence\u000d\u000a      of subfertility. These ideas also contributed to an\u000d\u000a      internationally-screened 2009 TV programme The\u000d\u000a        Great Sperm Race (Channel 4 and Discovery Channel) [S3]. Finally,\u000d\u000a      work from refs. R4, R5 have\u000d\u000a      received press attention internationally [for example, S5]. Dr Smith has\u000d\u000a      drawn on this research in\u000d\u000a      public engagement activities he has led including a `Meet the Scientist'\u000d\u000a      presentation at Birmingham\u000d\u000a      thinktank (February 2010) and talks for local school children\u000d\u000a      interested in STEM subjects, including\u000d\u000a      the 2010 British Science Festival.\u000d\u000a    4.4 Commercial impact\u000d\u000a    The imaging device, referred to in 4.2, and designed by Dr Kirkman-Brown\u000d\u000a      in conjunction with\u000d\u000a      CAIRN Research Ltd, is now marketed by the company as the Optosplit III\u000d\u000a      Image Splitter, a 3-way\u000d\u000a      image splitter as a device for dividing an image into either one, two or\u000d\u000a      three separate, spatially\u000d\u000a      equivalent components which can be displayed side by side on a single\u000d\u000a      camera chip. This device\u000d\u000a      has generated around &#163;50K of sales for CAIRN, in addition to prompting\u000d\u000a      further independent\u000d\u000a      developments by the company, including a 4-way image splitter. [S7]\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Deficiencies in sperm motility, the ability of cells to migrate actively\u000d\u000a      through the female reproductive\u000d\u000a      tract, are implicated in around half of all cases of subfertility. The\u000d\u000a      biochemical regulation of motility\u000d\u000a      is a subject of considerable interest in clinical science due to its\u000d\u000a      potential for improvements in\u000d\u000a      diagnosis and treatment of subfertility, however the accompanying physical\u000d\u000a      aspects of motility\u000d\u000a      have hitherto received less attention. In 2005, mathematicians Dr David\u000d\u000a      Smith and Prof. John\u000d\u000a      Blake began working closely with Dr Jackson Kirkman-Brown, Science Lead\u000d\u000a      for Birmingham\u000d\u000a      Women's Fertility Centre, as core investigators in the Centre for Human\u000d\u000a      Reproductive Science\u000d\u000a      (ChRS), a clinical research and development network centred at Birmingham\u000d\u000a      Women's Hospital\u000d\u000a      (BWH). The resulting mathematical models of sperm motility have been\u000d\u000a      impacting clinical science\u000d\u000a      in three ways. (1) Changing the awareness of practising and trainee\u000d\u000a      clinical scientists, both\u000d\u000a      nationally and internationally. (2) Assisting the capture of\u000d\u000a      infrastructure funding for translational\u000d\u000a      research by opening new scientific avenues. (3) Increasing public\u000d\u000a      awareness through outreach, TV\u000d\u000a      and the press, in turn encouraging changes to lifestyle. (4) Commercial\u000d\u000a      impact through the\u000d\u000a      marketing of a novel image-splitting device developed as part of this\u000d\u000a      programme.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    University of Birmingham\u000d\u000a    ","Institutions":[{"AlternativeName":"Birmingham (University of)","InstitutionName":"University of Birmingham","PeerGroup":"A","Region":"West Midlands","UKPRN":10006840}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2523228","Name":"Sardegna"},{"GeoNamesId":"6254926","Name":"Massachusetts"},{"GeoNamesId":"6077243","Name":"Montréal"},{"GeoNamesId":"1886760","Name":"Suzhou"}],"References":"\u000d\u000a    * Denotes sponsor.\u000d\u000a    \u000a[R1] Denissenko, Kantsler, Smith &amp; Kirkman-Brown* (2012) Human sperm\u000d\u000a      cells swimming in\u000d\u000a      microchannels. Proc. Natl. Acad. Sci. USA 109, 8007-8010.\u000d\u000a      doi:10.1073\/pnas.1202934109\u000d\u000a    \u000a\u000a[R2] J. Kirkman-Brown* and D. J. Smith (2011) Sperm motility: is\u000d\u000a      viscosity fundamental to\u000d\u000a      progress? Journal publication: Mol. Hum. Reprod. 17, 539-544. doi:\u000d\u000a      10.1093\/molehr\/gar043\u000d\u000a    \u000a\u000a[R3] E. A. Gaffney, H. Gad&#234;lha, D. J. Smith, J. R. Blake and J.\u000d\u000a      Kirkman-Brown* (2011) Mammalian\u000d\u000a      sperm motility: observation and theory. Annu. Rev. Fluid Mech. 43,\u000d\u000a      501-528. doi: 10.1146\/annurev-fluid-121108-145442\u000d\u000a    \u000a\u000a[R4] H. Gad&#234;lha, E. A. Gaffney, D. J. Smith and J. C. Kirkman-Brown*\u000d\u000a      (2010) Non-linear instability\u000d\u000a      in flagellar dynamics: A novel modulation mechanism in sperm migration?\u000d\u000a      2010. J. R. Soc.\u000d\u000a      Interface, 7, 1689-1697. doi: 10.1098\/rsif.2010.0136\u000d\u000a    \u000a\u000a[R5] D. J. Smith, E. A. Gaffney, J. R. Blake and J. C. Kirkman-Brown*\u000d\u000a      (2009) Human sperm\u000d\u000a      accumulation near surfaces: a simulation study. J. Fluid Mech. 621,\u000d\u000a      289-320.\u000d\u000a      doi:10.1017\/S0022112008004953\u000d\u000a    \u000a\u000a[R6] D. J. Smith, E. A. Gaffney, H. Gad&#234;lha, N. Kapur, J. Kirkman-Brown*\u000d\u000a      (2009) Bend propagation\u000d\u000a      in the flagella of migrating human sperm, and its modulation by viscosity.\u000d\u000a      Cell Motil. Cytoskel. 66,\u000d\u000a      220-236. doi:10.1002\/cm.20345\u000d\u000a    \u000aReferences 3, 4 and 5 best indicate the quality of the underpinning\u000d\u000a      research\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"6","Level2":"1","Subject":"Biochemistry and Cell Biology"},{"Level1":"11","Level2":"9","Subject":"Neurosciences"},{"Level1":"11","Level2":"14","Subject":"Paediatrics and Reproductive Medicine"}],"Sources":"\u000d\u000a    [S1] International Symposium of Advanced Research Progress in\u000d\u000a      Reproductive Medicine,\u000d\u000a      Changzhou Women and Children's Health Hospital, 18th-21st\u000d\u000a      Sept. 2009, Pp. 61-66.\u000d\u000a    [S2] Gordon Research Conference on Fertilization and Activation of\u000d\u000a      Development 2011.\u000d\u000a      http:\/\/www.grc.org\/programs.aspx?year=2011&amp;program=fert\u000d\u000a    [S3] http:\/\/www.channel4.com\/programmes\/the-great-sperm-race\u000d\u000a    [S4] http:\/\/www.bbc.co.uk\/programmes\/b00cv2vd\u000d\u000a    [S5] http:\/\/www.abc.net.au\/science\/articles\/2010\/07\/30\/2969046.htm\u000d\u000a    [S6] Birmingham Programme &#8212; British Science Festival 2010 (p. 3)\u000d\u000a    [S7] Corroborating letter, Managing Director, Cairn Research Ltd,\u000d\u000a      21\/11\/2013\u000d\u000a    [S8] Corroborating letter, Howard Hughes Medical Institute dated\u000d\u000a      16\/5\/2013.\u000d\u000a    [S9] Birmingham Advanced Microscopy for Biomedical Imaging, Specification\u000d\u000a      and Requirements\u000d\u000a      document.\u000d\u000a    [S10] Maternal Communication with Gametes and Embryo (Proceedings of the\u000d\u000a      2nd Meeting of\u000d\u000a      GEMINI) 1st-3rd Oct. 2009, p.30. ISBN\u000d\u000a      978-0-9563694-0-6, GEMINI COST ACTION FA0702.\u000d\u000a    ","Title":"\u000d\u000a    Bringing awareness of viscous fluid mechanics to clinical reproductive\u000d\u000a        medicine\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    We only partially understand the fundamental process in human\u000d\u000a      reproduction by which a single\u000d\u000a      sperm, from an initial population of 106-108,\u000d\u000a      migrates through the high viscosity fluids lining the\u000d\u000a      closely-opposed and convoluted internal architecture of the female\u000d\u000a      reproductive tract and\u000d\u000a      eventually fertilises an egg. Sperm are propelled through biological\u000d\u000a      fluids by beating a single\u000d\u000a      flagellum, a physical process; however the vast majority of clinical\u000d\u000a      research effort focuses on\u000d\u000a      molecular biology and biochemistry, physical aspects being relatively\u000d\u000a      neglected by comparison.\u000d\u000a    Around one in six couples in the UK fail to conceive after 12 months and\u000d\u000a      Assisted Reproduction\u000d\u000a      Therapy has become routine. The number of IVF cycles performed annually is\u000d\u000a      increasing steadily,\u000d\u000a      reaching nearly 60000\/year in 2010, every cycle incurring both financial\u000d\u000a      cost and a physical and\u000d\u000a      emotional toll; success rates however remain below 50%. Because physical\u000d\u000a      aspects are relatively\u000d\u000a      neglected, clinical diagnostics, research and development are usually\u000d\u000a      restricted to visual\u000d\u000a      assessments of sperm count, morphology and motility in laboratory media.\u000d\u000a      These fluids used in\u000d\u000a      laboratory assays are very different from secretions such as cervical\u000d\u000a      mucus, a limitation also\u000d\u000a      present in most clinical research on the biochemical regulation of\u000d\u000a      motility.\u000d\u000a    The research is a multidisciplinary effort to elucidate the physical\u000d\u000a      principles underlying sperm\u000d\u000a      motility, through integrating mathematical models with the physiological\u000d\u000a      features identified to be\u000d\u000a      potentially important by clinical scientists. These include the shape of\u000d\u000a      the DNA-carrying `head'\u000d\u000a      (morphology) and physiological fluid properties such as viscosity. A\u000d\u000a      distinctive feature is the two-way\u000d\u000a      exchange of knowledge, both the integration of clinical data and questions\u000d\u000a      into mathematical\u000d\u000a      models, and the communication of the results of models, which are often\u000d\u000a      counterintuitive, to clinical\u000d\u000a      scientists. The collaboration between Mathematics and ChRS was initiated\u000d\u000a      by Professor John\u000d\u000a      Blake (School of Mathematics) in 2004; the work was subsequently conducted\u000d\u000a      by Dr David Smith\u000d\u000a      (as MRC Fellow from 2006-2009, then Lecturer).\u000d\u000a    2.1 Mathematical modelling of the fluid dynamics of sperm motility\u000d\u000a    Sperm motility is a very low Reynolds number fluid dynamic process,\u000d\u000a      involving static boundary\u000d\u000a      conditions associated with bounding surfaces, and dynamic boundary\u000d\u000a      conditions associated with\u000d\u000a      the moving flagellum. The underpinning research involved the investigation\u000d\u000a      of the disturbance flow\u000d\u000a      field and energetic requirements, taking into account aspects identified\u000d\u000a      by clinical scientists\u000d\u000a      including the effect of the non-spherical head `morphology' (shape), and\u000d\u000a      the confined internal\u000d\u000a      geometry of the female tract. The model [R5] allows efficient and accurate\u000d\u000a      computation of the\u000d\u000a      viscous-dominated flow and long timescale cell trajectories with different\u000d\u000a      head shapes and\u000d\u000a      waveforms near surfaces, opening a further area for bioengineering [R1].\u000d\u000a    2.2 Nonlinear instability of flagellar movement at high viscosity\u000d\u000a    The next stage involved examining how the elastic structure of the\u000d\u000a      internally-actuated flagellum\u000d\u000a      couples with the surrounding viscous fluid, to alter the waveform and\u000d\u000a      hence trajectory [R4]. The\u000d\u000a      geometrically nonlinear model showed how viscosity alters the flagellar\u000d\u000a      wavelength and cell\u000d\u000a      trajectory through viscous-elastic interaction. The inclusion of\u000d\u000a      nonlinearity revealed a symmetry-breaking\u000d\u000a      bifurcation at high viscosity, demonstrating that experimentally-observed\u000d\u000a      circling motion of\u000d\u000a      cells is not necessarily due to biochemical signalling. Moreover the model\u000d\u000a      showed how defective\u000d\u000a      cell head shapes can radically alter cell trajectory through nonlinear\u000d\u000a      interaction, an effect that\u000d\u000a      would be impossible to predict from heuristic reasoning or intuition. This\u000d\u000a      study was conducted\u000d\u000a      jointly between the Universities of Birmingham and Oxford; a principal\u000d\u000a      part of the work was\u000d\u000a      conducted while the lead author (H. Gad&#234;lha) conducted a research visit in\u000d\u000a      Birmingham, jointly\u000d\u000a      supervised by Smith and Kirkman-Brown.\u000d\u000a    2.3 Analysis of local mechanical activity in the flagellum in\u000d\u000a        physiological viscosity fluid\u000d\u000a    The energetics of motility are of great interest, for example as a cause\u000d\u000a      of subfertility, and as a\u000d\u000a      potential contraceptive target. Existing dogma based on low viscosity\u000d\u000a      observations held that\u000d\u000a      energy need not be transported significantly along the flagellum. Refs.\u000d\u000a      [R6, R3] describe the\u000d\u000a      capture from imaging data of the movement of the flagellum in\u000d\u000a      physiological viscosity fluid, and\u000d\u000a      spatially-localised estimates of required activity (Smith's contributions\u000d\u000a      to this work included these\u000d\u000a      calculations). The above findings have been reported and commented on in\u000d\u000a      clinical science\u000d\u000a      publications, conferences and training courses, results being conveyed\u000d\u000a      through images, videos\u000d\u000a      and metaphors appropriate to the audience.\u000d\u000a    "},{"CaseStudyId":"38831","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000a    This research has generated economic impact by helping improve the\u000a      computational toolkit central\u000a      to the application of CFD in aircraft design by engineers at The Boeing\u000a      Company. These\u000a      improvements are bringing financial savings for the company through\u000a      reducing the amount of\u000a      skilled engineering time wasted in generating unproductive results. The\u000a      productivity is increased\u000a      through the further improvement and validation of the computational code\u000a      used in the design\u000a      process. Boeing has confirmed the existence of substantial savings to the\u000a      company as a result but\u000a      is unable to provide financial data for commercial reasons.\u000a    CFD in aircraft design: The overall significance of CFD in the\u000a      aircraft design process is now well-established.\u000a\u0009  Johnson and colleagues at Boeing have said that the\u000a      application of CFD has\u000a      \"revolutionised the process of aerodynamic design\", joining the wind\u000a      tunnel and flight test as\u000a      primary tools, and described the resulting financial savings to their\u000a      company as \"tens of millions of\u000a      dollars\" over a twenty year period [see page 4, source 1]. CFD also\u000a      provided added-value by\u000a      achieving design solutions that would otherwise be unachievable, as well\u000a      as shortening the design\u000a      development process by reducing or eliminating the need to build\u000a      successive prototypes.\u000a    Project engineers at Boeing (and elsewhere) use commercial codes to\u000a      undertake CFD analyses.\u000a      These codes take many years to design and validate, are applied to \"live\"\u000a      tasks where appropriate\u000a      during their development phase and are then released allowing decades of\u000a      use across Boeing and\u000a      a wider aerodynamics community. For instance, development work on Boeing's\u000a      current\u000a      \"workhorse\" code, TRANAIR, began in 1984 with useful results published in\u000a      1989 and on-going\u000a      development in the 1990s. These codes are used extensively; Trainair was\u000a      run more than 70,000\u000a      times between 1989 and 2004, with about 90 users in Boeing. The code was\u000a      heavily applied in the\u000a      design of aircraft such as the Boeing 777 [page 4, source 1], one of the\u000a      company's best-selling\u000a      products with more than 1,000 built to date.\u000a    Contribution to Boeing's new code: Boeing began the process of\u000a      developing their next-generation\u000a\u0009  computational code (BCFD) in 1998; as with previous codes this\u000a      is already in use\u000a      where appropriate within the Company, with formal release of the code and\u000a      publication expected to\u000a      follow in the next five years. The ultimate purpose of the new code is to\u000a      allow the generation of\u000a      aerodynamic data for various flow regimes about realistic complex\u000a      geometries for complex\u000a      geometries in a timely and affordable manner. However, the complex nature\u000a      of the flows and\u000a      geometries involved places substantially increased demands on the solution\u000a      methodology and\u000a      resources required for the design of any reliable and accurate CFD code\u000a      aimed for handling\u000a      complex flow.\u000a    Currently most simulations carried out at Boeing involve\u000a      Reynolds-Averaged Navier-Stokes\u000a      (RANS) codes. While current RANS turbulence models have been successful\u000a      for analysing\u000a      attached, transonic flows, whether or not these same models are applicable\u000a      to complex flows with\u000a      smooth surface separation is an open question. A prerequisite for\u000a      answering this question is\u000a      absolute confidence that the CFD codes employed reliably solve the\u000a      continuous equations\u000a      involved. Hence, the Boeing CFD team wanted to investigate the solver\u000a      issues in detail to make\u000a      sure that a correct decision about the code design would be made. It was\u000a      clear that a detailed\u000a      investigation of a solution reconstruction procedure on unstructured\u000a      viscous grids was required. As\u000a      for many discretization schemes solution reconstruction was an essential\u000a      part of the scheme.\u000a      Based on her earlier work as a research consultant for The Boeing Company,\u000a      Dr Petrovskaya was\u000a      asked by the CFD research team at Boeing to investigate the reconstruction\u000a      problem in depth.\u000a      The research carried out by Dr Petrovskaya has had impact in the following\u000a      ways:\u000a    1. It was demonstrated that, in two and three dimensions, near singular\u000a      grid node locations can\u000a      cause severe problems. This is especially true for unstructured viscous\u000a      grids with high aspect ratio\u000a      cells and wide disparities in cell sizes and shapes, as well as for\u000a      under-resolved curved\u000a      boundaries. Hence based on the research by Dr Petrovskaya, the Boeing CFD\u000a      team identified the\u000a      solution reconstruction procedure on unstructured grids as a critical task\u000a      associated with the\u000a      design of a solver for computational toolkits in modern CFD (see sources 2\u000a      &amp; 3 on page 4).\u000a    2. Cases have been documented where a higher order least-squares\u000a      algorithm yielded\u000a      reconstructed values two orders of magnitude larger than any values being\u000a      interpolated. For grids\u000a      with 30-300 million nodes it is unlikely that anomalous reconstructions\u000a      would not arise and a\u000a      disastrous reconstruction can feed on itself yielding worse and worse\u000a      grids. Those cases helped\u000a      CFD researchers at Boeing to admit that higher order solution\u000a      reconstruction can be dangerous on\u000a      unstructured viscous grids unless the solution latent features are\u000a      resolved (sources 1 &amp; 3, page 4).\u000a      That in turn made the impact on the choice of a baseline discretization\u000a      scheme used in the Boeing\u000a      solver. In particular this issue has been discussed at the MTCA'09\u000a      workshop held in September\u000a      2009 in University of Birmingham (source 4).\u000a    3. The research on numerically distant points in a least-squares\u000a      procedure carried out by Dr\u000a      Petrovskaya revealed true nature of a large reconstruction error that\u000a      appears on coarse\u000a      unstructured grids. Hence Boeing researches admitted that a least-squares\u000a      reconstruction\u000a      procedure should be taken into account when a grid refinement algorithm is\u000a      considered. The low\u000a      accuracy of reconstruction may affect a solution on the initial grid and\u000a      this issue must be taken into\u000a      account in as well when a solution grid adaptation algorithm is designed\u000a      (source 2).\u000a    Boeing's confirmation of the impact: The leaders of Boeing's CFD\u000a      team have written jointly to\u000a      the University corroborating the impact of Dr Petrovskaya's research in\u000a      helping the company tackle\u000a      important unsolved problems in 2008 that were limiting progress in\u000a      advancing the applicability of\u000a      CFD to its product lines. They had turned to Dr Petroskaya to address\u000a      these problems because of\u000a      the quality of her extensive research in the field and said that \"The\u000a      algorithmic problems associated\u000a      with providing engineers with reliable codes to analyse such flows are\u000a      unbelievably difficult. Most\u000a      CFD researchers have given up and moved on to lower hanging fruit\" and\u000a      that Dr Petrovskaya was\u000a      able to track down the source of the difficulties Boeing faced with their\u000a      existing methods and\u000a      provided solutions \"that pointed us in the right direction\" (source 5).\u000a    As a result of this input in 2008, Boeing's subsequent and current codes\u000a      have been improved and\u000a      these benefits are being extended to cover further aspects of aircraft\u000a      design. The current CFD\u000a      toolkit (in-house computational code BCFD) has already been used in the\u000a      design and aerodynamic\u000a      optimization of the latest Boeing product &#8212; Boeing 787. For instance, wing\u000a      body fairing and winglets\u000a      optimization for the Boeing 787 has been done by means of CFD only.\u000a      Implementation of CFD in the\u000a      design of their new aircraft allowed Boeing to reduce the testing time in\u000a      the wind tunnel for the 787\u000a      aircraft by 30% in comparison with testing carried out for Boeing 777. The\u000a      company has confirmed\u000a      that \"The resultant improvements in our products as well as the gains in\u000a      engineering productivity\u000a      are substantial although quantification is again closely held.\" (source 5)\u000a    ","ImpactSummary":"\u000a    This case study demonstrates the benefits achieved when the mathematical\u000a      and computational\u000a      aspects of a computational fluid dynamics (CFD) problem were brought\u000a      together to work on real-world\u000a\u0009  aerodynamic applications. While earlier insight on the solution\u000a      reconstruction problem was\u000a      purely based on empirical intuition, research in the School of Mathematics\u000a      at the University of\u000a      Birmingham by Dr Natalia Petrovskaya has resulted in the development of\u000a      the necessary synthetic\u000a      judgement in which the importance of accurate reconstruction on\u000a      unstructured grids has been fully\u000a      recognised by the CFD researchers at the Boeing Company. Boeing has\u000a      confirmed that the\u000a      research has led to substantial resultant improvements in their products\u000a      as well as gains in\u000a      engineering productivity. For instance, wing body fairing and winglets\u000a      optimization for the Boeing 787\u000a      has been done by means of CFD only. Implementation of CFD in the design of\u000a      their new aircraft\u000a      allowed Boeing to reduce the testing time in the wind tunnel for the 787\u000a      aircraft by 30% in\u000a      comparison with testing carried out for Boeing 777. Efficient use of CFD\u000a      in the design of new\u000a      aircrafts has helped the Boeing Company to further strengthen their core\u000a      operations, improve their\u000a      execution and competitiveness and leverage their international advantage.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Birmingham\u000a    ","Institutions":[{"AlternativeName":"Birmingham (University of)","InstitutionName":"University of Birmingham","PeerGroup":"A","Region":"West Midlands","UKPRN":10006840}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Research outputs in peer-reviewed journals:\u000a    \u000a1) N.B.Petrovskaya. Discontinuous Weighted Least-Squares Approximation on\u000a      Irregular Grids.\u000a      CMES: Computer Modeling in Engineering &amp; Sciences, 2008, vol.32(2),\u000a      pp.69-84, doi:\u000a      10.3970\/cmes.2008.032.069\u000a    \u000a\u000a2) N.B.Petrovskaya. The Accuracy of Least-Squares Approximation on Highly\u000a      Stretched\u000a      Meshes. Int. J. Comput. Methods, 2008, vol.5(3), pp.449 - 462, doi:\u000a      10.1142\/S0219876208001558\u000a    \u000a\u000a3) N.B.Petrovskaya. Quadratic Least-Squares Solution Reconstruction in a\u000a      Boundary Layer\u000a      Region. Commun. Numer.Meth. Engng., 2010, vol.26 (12), pp.1721-1735, doi:\u000a      10.1002\/cnm.1259.\u000a    \u000a\u000a4) N.B.Petrovskaya. Data Dependent Weights in Discontinuous Weighted\u000a      Least-Squares\u000a      Approximation with Anisotropic Support. Calcolo, 2011, vol.48(1),\u000a      pp.127-143, doi:\u000a      10.1007\/s10092-010-0032-7\u000a    \u000a\u000a5) V. Wolkov 1, Ch. Hirsch, N.B.Petrovskaya. Application of a\u000a      Higher Order Discontinuous\u000a      Galerkin Method in Computational Aerodynamics. Mathematical Modeling of\u000a      Natural\u000a      Phenomena, 2011, vol.6(3) (invited issue on computational aerodynamics),\u000a      pp.237-263,\u000a      doi:10.1051\/mmnp\/20116310\u000a    \u000aPapers 1, 2 and 3 best indicate quality of research\u000a    (1 ) Author in employment with the sponsoring company\u000a    Research grants:\u000a      Dr Petrovskaya's research has been supported by the consultancy agreement\u000a      66-ZB-B001-10A-533\u000a\u0009  between The Boeing Company and University of Birmingham, UK (1\/1\/07-31\/3\/07).\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    \u000a      Forrester T. Johnson, Edward N. Tinoco, N. Jong Y, Thirty years of\u000a          development and\u000a          application of CFD at Boeing Commercial Airplanes, Seattle, J.\u000a        Computers &amp; Fluids 34 (2005)\u000a        1115-1151, doi:10.1016\/j.compfluid.2004.06.005\u000a      F. T. Johnson, D. S. Kamenetskiy, R. G. Melvin, V. Venkatakrishnan, L.\u000a        B. Wigton, D. P.\u000a        Young, S. R. Allmaras, J. E. Bussoletti and C. L. Hilmes. Observations\u000a          Regarding Algorithms\u000a          Required for Robust CFD Codes. Mathematical Modeling of Natural\u000a        Phenomena, 2011, vol.6(3)\u000a        (invited issue on computational aerodynamics), pp.2-27,\u000a        doi:10.1051\/mmnp\/20116301\u000a      S. R. Allmaras,\u000a        \u000a          J.E. Bussoletti, C.\u000a          L. Hilmes, F.\u000a          T. Johnson, \u000a          R.G. Melvin, E.N.\u000a          Tinoco, V.Venkatakrishnan,\u000a        L.\u000a          B. Wigton and D.\u000a          P. Young. Algorithm Issues and Challenges Associated with\u000a          the Development of Robust CFD Codes. Variational\u000a          Analysis and Aerospace Engineering, 2009,\u000a        vol.33, pp.1-19, doi: 10.1007\/978-0-387-95857-6_1\u000a      F.T.Johnson. Algorithm Issues Associated with Extending CFD\u000a          Applicability to the Full Flight\u000a          Envelope. invited lecture at the MTCA'09 workshop, 16 September\u000a        2009 (the presentation is\u000a        available from forrester.johnson@boeing.com by request)\u000a      Corroborating statement provided jointly by Technical Fellow, The\u000a        Boeing Company and Senior\u000a        Technical Fellow (now retired) and currently contractor to The Boeing\u000a        Company, 6th Oct 2012.\u000a    \u000a    ","Title":"\u000a    Algorithms of accurate solution reconstruction on unstructured grids\u000a        in\u000a        computational aerodynamics: impact on aircraft design at the Boeing\u000a        Company\u000a    ","UKLocation":[{"GeoNamesId":"2655603","Name":"Birmingham"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Dr Natalia Petrovskaya is a lecturer in the School of Mathematics at the\u000a      University of Birmingham.\u000a      The underpinning research is Dr Petrovskaya's body of work on a novel and\u000a      efficient solution\u000a      reconstruction procedure from discrete data considered on unstructured\u000a      grids with arbitrary\u000a      geometry.\u000a    A least-squares (LS) method is one of the most well-known approaches in\u000a      solving the problem of\u000a      finding the best polynomial approximation to the input data. While general\u000a      accuracy estimates of\u000a      the LS method are based on the assumption that all observations made to\u000a      obtain LS data should\u000a      provide equally precise information, data used in many practical\u000a      applications is of varying quality in\u000a      terms of the uncertainty of the measurement. Thus a common approach is to\u000a      use weighted least-squares\u000a\u0009  approximation to improve the accuracy of LS approximation.\u000a      Discontinuous weighted\u000a      least-squares (DWLS) approximation is a modification of a weighted LS\u000a      method that is heavily\u000a      used in computational aerodynamics. The method approximates a given\u000a      function at each point\u000a      belonging to a set of points selected over a computational grid.\u000a    One basic feature of DWLS reconstruction that stems from the nature of\u000a      computational problems\u000a      where the method is exploited is that a reconstruction stencil may present\u000a      a highly irregular\u000a      geometry. The DWLS reconstruction on irregular meshes appeared to be a\u000a      challenging and difficult\u000a      problem, as the method can lose accuracy to an unacceptable limit. Earlier\u000a      insight into the\u000a      problem, made by researchers at Boeing and NASA, attributed poor accuracy\u000a      of the method on\u000a      irregular grids to the impact of distant points on the results of DWLS\u000a      reconstruction. However, it\u000a      turned out that inverse distance weighting of stencil points was not\u000a      efficient in practical\u000a      aerodynamic computations, and further insight into the problem was\u000a      required. The study of this\u000a      problem made by Dr Petrovskaya in 2007-08 revealed that, while the inverse\u000a      distance weight\u000a      function has been well investigated for points that are remote in the\u000a      physical space, another class\u000a      of distant points (numerically distant points) may appear in the\u000a      reconstruction stencil on coarse\u000a      grids.\u000a    The crucial and significant research finding was to demonstrate that the\u000a      numerically distant points\u000a      adversely affect the accuracy of the reconstruction but they cannot be\u000a      eliminated from the stencil\u000a      by inverse distance weighting. As a result of the research carried out by\u000a      Dr Petrovskaya it became\u000a      clear that the numerically distant points have to be weighted in the data\u000a      space in order to remove\u000a      them from the reconstruction stencil. Dr Petrovskaya resolved this issue\u000a      by suggesting a new\u000a      approach that allows the measurement of distance between points in the\u000a      data space. Based on\u000a      this fundamental concept, a novel reconstruction algorithm has been\u000a      designed and a\u000a      computational code has been written for a reconstruction procedure on\u000a      unstructured grids with\u000a      arbitrary geometry of grid cells. As a result of Dr Petrovskaya' research\u000a      the importance of the\u000a      reconstruction problem has been fully acknowledged by the Boeing CFD team\u000a      and that issue was\u000a      taken into account and implemented while designing a new computational\u000a      toolkit.\u000a    "},{"CaseStudyId":"38832","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000d\u000a    It is standard practice in the oil and gas industry to use reservoir\u000d\u000a      simulators based on numerical methods such as the finite difference or\u000d\u000a      finite element techniques. This kind of approach has been shown to be\u000d\u000a      enormously successful over the years in modelling a wide variety of\u000d\u000a      physical processes in the reservoir e.g., faults, rock layering effects,\u000d\u000a      complex fluid phase behaviour, etc.\u000d\u000a    While reservoir simulators of this type will continue to play a crucial\u000d\u000a      role in the industry, it is well known that to use them takes considerable\u000d\u000a      expertise and time. Because of the numerical nature of the modelling\u000d\u000a      process, gridding, time stepping, stiffness and convergence issues require\u000d\u000a      care and attention. Extremely long execution times are often necessary for\u000d\u000a      certain types of problems, e.g., hydraulically fractured wells, and to\u000d\u000a      maintain numerical stability.\u000d\u000a    Analytical techniques, for the reason outlined above, can therefore play\u000d\u000a      a valuable role in the industry. Such techniques, although they may have\u000d\u000a      some simplifying assumptions, allow a reservoir or production engineer to\u000d\u000a      perform a quick and reliable study of their reservoir in order to obtain a\u000d\u000a      broad understanding of the dynamical processes and make approximate\u000d\u000a      costing forecasts. Moreover, analytical solutions are extremely fast to\u000d\u000a      compute and provide none of the stability, time stepping and convergence\u000d\u000a      issues seen with a numerically based simulator. Also, a necessary step in\u000d\u000a      many reservoir studies involves the history matching of observed data by\u000d\u000a      optimizing model parameters. The history matched model is then used for\u000d\u000a      performance prediction. Given the speed and reliability of analytical\u000d\u000a      results, there is a clear opportunity to exploit their use in history\u000d\u000a      matching studies.\u000d\u000a    There has been much work in the literature regarding analytical\u000d\u000a      approaches, particularly for well testing applications, but also from a\u000d\u000a      full field reservoir standpoint, where multiple wells and reservoir\u000d\u000a      boundaries must be accounted for to forecast production over the required\u000d\u000a      timescales. Algorithms for full field simulation problems based on\u000d\u000a      analytical approaches have been presented in the literature for porous\u000d\u000a      media with homogeneous and isotropic permeability in a variety of sources.\u000d\u000a      A more complex problem involves the application of analytical approaches\u000d\u000a      to full field scenarios where the reservoir has inhomogeneous and\u000d\u000a      anisotropic permeability and variable geometry.\u000d\u000a    The programme of research reported in this case study has been developed\u000d\u000a      in collaboration with, and supported by, Schlumberger Technology Centre,\u000d\u000a      Abingdon, UK. It has involved the novel development, via matched\u000d\u000a      asymptotic expansions, of a predictive analytical theory for describing\u000d\u000a      the flow of oil in a thin anisotropic, inhomogeneous porous layer, with\u000d\u000a      injection and extraction via line sources and sinks. This provides a\u000d\u000a      tractable model for oil extraction from spatially extended reservoirs. The\u000d\u000a      objective has been to provide a computationally rapid tool to enable\u000d\u000a      multiple realisations of extraction strategies to be assessed and compared\u000d\u000a      rapidly, with the goal of optimizing the oil well locations in a new\u000d\u000a      reservoir. The user friendly computational tool that has been provided via\u000d\u000a      the matched asymptotic theory developed in this case study has more than\u000d\u000a      achieved this goal. Pre-existing, fully numerical models require the order\u000d\u000a      of days to obtain a single realisation, and are also sensitive to\u000d\u000a      parameter changes, and therefore model modification. However, the\u000d\u000a      computations associated with the theory developed here provide an accurate\u000d\u000a      realisation in less than two minutes of computational time on MATLAB, and\u000d\u000a      the computations are stable and robust to parameter changes. [text removed\u000d\u000a      for Publication] (Group Leader, Schlumberger Technology Centre, Abingdon)\u000d\u000a      has commented `This new approach now makes strategy assessment for new\u000d\u000a        oil reservoirs, via multiple computational realisations, followed by\u000d\u000a        optimization, a significantly viable and attractive approach. Ideas from\u000d\u000a        this approach have been implemented by Schlumberger in their recent\u000d\u000a        efforts to build a semi-analytical Gridless Reservoir Estimation and\u000d\u000a        Analysis Tool (GREAT)' . Moreover, Schlumberger report that there is\u000d\u000a      further good potential to employ this method in parallel with pre-\u000d\u000a      existing numerical simulators at Schlumberger, to speed up performance,\u000d\u000a      and therefore to considerably extend their viability as optimisation\u000d\u000a      tools. The project has been supported by four one-day workshops in May\u000d\u000a      2006, June 2007, May 2008 and September 2009, involving the Computational\u000d\u000a      Oil Recovery Research Group at Schlumberger Technology Centre, Abingdon.\u000d\u000a      The workshops focussed on dissemination and discussion of the developing\u000d\u000a      theory, its application and implementation, and computational methodology\u000d\u000a      and efficiency. In addition D J Needham presented two research seminars at\u000d\u000a      Schlumberger Technology Centre, Abingdon, relating to the description of\u000d\u000a      the theory being developed and aspects of implementation of the theory. In\u000d\u000a      a broader context, the research project outlined in this case study has\u000d\u000a      significant implications for the worldwide petrochemical industry through\u000d\u000a      the commercial services provided by Schlumberger to this industry,\u000d\u000a      particularly in respect of current resource management and the necessity\u000d\u000a      to optimise extraction strategies as efficiently as possible in the face\u000d\u000a      of declining reserves. Associated with this activity has been the\u000d\u000a      production of three detailed reports for Schlumberger:\u000d\u000a    `The unsteady flow of a weakly compressible fluid in a thin porous layer\u000d\u000a      I: Two-Dimensional Theory'. (D J Needham, S Langdon) Contract Report to\u000d\u000a      Schlumberger Technology Centre, Abingdon\u000d\u000a    `The unsteady flow of a weakly compressible fluid in a thin porous layer\u000d\u000a      II: Three-Dimensional Theory'. (D J Needham, S Langdon) Contract Report to\u000d\u000a      Schlumberger Technology Centre, Abingdon\u000d\u000a    `The unsteady flow of a weakly compressible fluid in a thin porous layer\u000d\u000a      III: Three-Dimensional Computations'. (S Langdon, D J Needham, B A\u000d\u000a      Samson*, J P Gilchrist*) Contract Report to Schlumberger Technology\u000d\u000a      Centre, Abingdon\u000d\u000a    The work has also led to three significant publications in high quality\u000d\u000a      Applied Mathematics journals.\u000d\u000a    (* Author employed at Schlumberger Technology Centre, Abingdon )\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This case study reports the development of a new approach to solving full\u000d\u000a      field reservoir problems with inhomogeneous and anisotropic permeability\u000d\u000a      and variable reservoir This comprehensive body of work arose from\u000d\u000a      discussions between scientists at the Schlumberger Technology Centre,\u000d\u000a      Abingdon, and the internationally recognised Nonlinear Waves group in the\u000d\u000a      School of Mathematics, and has been supported under two contracts with\u000d\u000a      Schlumberger Oilfield UK PLC through their Technology Centre in Abingdon\u000d\u000a      UK. The work has provided Schlumberger with a fast, robust and efficient\u000d\u000a      tool for the rapid assessment of optimisation problems relating to oil\u000d\u000a      well location sites in new oil reservoirs, and has been implemented in\u000d\u000a      their recently developed GREAT facility for reservoir estimation and\u000d\u000a      analysis. Schlumberger PLC is an international company which plays a\u000d\u000a      premier role in supplying the petrochemical industry with services such as\u000d\u000a      seismic acquisition and processing, well testing and directional drilling,\u000d\u000a      flow assurance and extraction strategy. The work described in this case\u000d\u000a      study took place from 2007 to 2011, and involved D J Needham (University\u000d\u000a      of Birmingham) and S Langdon (University of Reading).\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Birmingham\u000d\u000a    ","Institutions":[{"AlternativeName":"Birmingham (University of)","InstitutionName":"University of Birmingham","PeerGroup":"A","Region":"West Midlands","UKPRN":10006840}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a`The unsteady flow of a weakly compressible fluid in a thin porous layer\u000d\u000a      I: Two-Dimensional Theory', SIAM Jl Appl Math, 2009, 64, 4, 1084-1109. (D\u000d\u000a      J Needham, S Langdon, G S Busswell*, J P Gilchrist*)\u000d\u000a    \u000a\u000a`The unsteady flow of a weakly compressible fluid in a thin porous layer\u000d\u000a      II: Three-Dimensional Theory'. Q Jl Mech Appl Math, 2013, 66, 1, 97-122.\u000d\u000a      (D J Needham, S Langdon, B A Samson*, J P Gilchrist*)\u000d\u000a    \u000a\u000a`The unsteady flow of a weakly compressible fluid in a thin porous layer\u000d\u000a      III: Three-Dimensional Computations'. Q Jl Mech Appl Math, 2013, 66, 1,\u000d\u000a      123-155. (S Langdon, D J Needham, B A Samson*, J P Gilchrist*)\u000d\u000a    \u000a* Author employed at Schlumberger Technology Centre, Abingdon\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000d\u000a    Corroboration of the impact described in this case study can be obtained\u000d\u000a      from Group Leader, Schlumberger Technology Centre, Schlumberger Oilfield\u000d\u000a      UK PLC.\u000d\u000a    ","Title":"\u000d\u000a    Efficient Development and Assessment of Extraction Strategies in the\u000d\u000a        Petroleum Industry\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2657780","Name":"Abingdon"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Here, we describe the two-dimensional problem. The full three-dimensional\u000d\u000a      problem has been dealt with in subsequent work. We introduce the parameter\u000d\u000a      E = h\/l and consider asymptotic solutions to the equations of\u000d\u000a      motion of the fluid in increasing powers of E, with 0&lt;E&lt;&lt; 1. In\u000d\u000a      the vicinity of a well (the inner region) the pressure field is\u000d\u000a      two-dimensional, but away from the wells (the outer region) the\u000d\u000a      pressure field is only one-dimensional. This immediately leads to a\u000d\u000a      reduction in complexity. Here, however, rather than solving the full\u000d\u000a      equations of motion numerically in the inner and outer regions, we\u000d\u000a      construct two-term expansions in both the inner and outer regions. The\u000d\u000a      expansions in the inner and outer regions can then be matched, via the Van\u000d\u000a      Dyke asymptotic matching principle, enabling us to derive amenable\u000d\u000a      analytical expressions for all significant process quantities.\u000d\u000a    We begin by deriving the equations of motion in the porous medium.\u000d\u000a      Conservation of mass and momentum lead to a strongly parabolic linear\u000d\u000a      initial boundary value problem for the dynamic fluid pressure (from which\u000d\u000a      the fluid velocity field can be deduced), with Neumann boundary\u000d\u000a      conditions, under the assumption that the walls are impenetrable to the\u000d\u000a      fluid in the porous medium. This initial boundary value problem has a\u000d\u000a      unique solution, but its direct computation would be expensive, primarily\u000d\u000a      due to stiffness when 0 &lt; E &lt;&lt; 1. We thus consider the\u000d\u000a      associated steady state problem [SSP], a linear strongly elliptic Neumann\u000d\u000a      problem, which also has a unique solution (up to a constant) under the\u000d\u000a      further constraint that the sum of the total volume fluxes at the wells\u000d\u000a      (the line sources and sinks) is zero. Solution of the steady state problem\u000d\u000a      is then considered. Subtracting the solution of the steady state problem\u000d\u000a      from the solution of the initial value problem leads to a strongly\u000d\u000a      parabolic homogenous problem with no discontinuities across the sources\u000d\u000a      and sinks. The solution of this problem leads to a regular self-adjoint\u000d\u000a      eigenvalue problem [EVP] whose solution is considered.\u000d\u000a    Rather than solving [SSP] and [EVP] directly, the solution to each\u000d\u000a      problem is considered in the asymptotic limit E &#8594; 0, via the\u000d\u000a      method of matched asymptotic expansions. For the two-dimensional problem\u000d\u000a      these asymptotic solutions can be constructed analytically. To solve\u000d\u000a      [SSP], we proceed first with the situation when the wells are well spaced\u000d\u000a      and are away from the reservoir boundaries, after which the case of wells\u000d\u000a      close to a boundary, or close together, is considered. The asymptotic\u000d\u000a      solution can be constructed directly in the outer region, up to\u000d\u000a    O(E2). In the inner region, determination of the\u000d\u000a      leading order terms reduces to the solution of a strongly elliptic problem\u000d\u000a      whose solution can be written analytically in terms of the eigenvalues and\u000d\u000a      corresponding eigenfunctions of a regular Sturm-Liouville eigenvalue\u000d\u000a      problem. The asymptotic solution of [EVP] also reduces to a regular\u000d\u000a      Sturm-Liouville eigenvalue problem identical in structure to that\u000d\u000a      discussed earlier and a consideration of this allows us to demonstrate\u000d\u000a      that the solution to the full initial boundary value problem approaches\u000d\u000a      the solution to the steady state problem through terms exponentially small\u000d\u000a      with respect to time t as t &#8594; &#8734;. With Dz being the\u000d\u000a      permeability scale in the vertical direction and Dx being the\u000d\u000a      permeability scale in the horizontal direction, the further generalisation\u000d\u000a      that Dz = o(Dx) rather than O (Dx) is\u000d\u000a      considered, where it is shown that the structure of the solution is\u000d\u000a      identical to that found for the case that Dz = O(Dx),\u000d\u000a      after a suitable redefinition of the parameter E. The constraint\u000d\u000a      on the sum of the total volume fluxes at the wells being zero is removed\u000d\u000a      at a later stage.\u000d\u000a    The development of the two-dimensional theory into a three-dimensional\u000d\u000a      theory is straightforward and has been completed in detail. Finally a fast\u000d\u000a      and very efficient numerical implementation has been developed and\u000d\u000a      delivered as a tool to Schlumberger. Aspects of this tool have been\u000d\u000a      incorporated into the GREAT facility recently developed by Schlumberger.\u000d\u000a    "},{"CaseStudyId":"38833","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"1861060","Name":"Japan"},{"GeoNamesId":"6251999","Name":"Canada"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":[],"ImpactDetails":"\u000a    This research has led to the development of new optimization routines\u000a      which have been commercialised as a product by the Numerical Algorithms\u000a      Group (NAG). This product is based on the PENNON software code developed\u000a      by Kocvara and Stingl. The Chief Technical Officer at NAG has confirmed\u000a      that they expect to include it under licence in Mark 24 of the NAG C\u000a      Library, available in February 2014. [source 1] These novel routines for\u000a      nonlinear optimization will help NAG attract new customers, as well as\u000a      bringing further benefits to industrial and commercial end users.\u000a      Inclusion in the NAG Library will mean that this product is actively\u000a      marketed to the company's worldwide client base which includes many major\u000a      corporations in the finance sector and engineering industries. The\u000a      company's annual financial statement demonstrates their international\u000a      reach with 44% of their &#163;8.2m turnover in 2012\/2013 coming from outside of\u000a      the UK, including 17% from the USA and Canada, 8% from Japan and 19% from\u000a      Europe and the rest of the world. [source 2]\u000a    Market requirements\u000a    Semidefinite optimization is a relatively new field of optimization which\u000a      is of growing interest for several reasons. Many practical problems in\u000a      operations research and combinatorial optimization can be modelled or\u000a      approximated as semidefinite optimization problems. In automatic control\u000a      theory, SDP's are used in the context of linear matrix inequalities. SDP\u000a      is a special case of cone programming. The quickly growing number of\u000a      applications in many research and industrial areas include robust\u000a      optimization, control theory, relaxations and approximations to\u000a      combinatorial optimization problems, optimization of mechanical structures\u000a      in automotive and aerospace engineering, chemical engineering, image\u000a      recognition, machine learning, financial engineering, and many others.\u000a      Many of these problems are intrinsically nonlinear. However, the software\u000a      currently available is only capable of solving problems with linear\u000a      constraints. PENNON is the only software that can solve nonlinear\u000a      problems.\u000a    NAG and PENNON\u000a    NAG is a well-established not-for-profit company with international reach\u000a      that provide high quality methods for the solution of mathematical and\u000a      statistical problems. Its products are widely used by major companies,\u000a      universities, supercomputing sites and numerous independent software\u000a      vendors in many parts of the world. The NAG Library is the oldest and\u000a      best-known product of NAG, and is used by developers to add mathematical\u000a      and statistical functionality to their applications, or to solve\u000a      complicated mathematical problems. [source 3]\u000a    NAG closely collaborates with Birmingham's Optimization Group led by\u000a      Kocvara. One of the outcomes of the collaboration will be a set of new\u000a      routines in the NAG Libraries based on the Pennon optimization package.\u000a      These routines will further include new algorithms for conic quadratic\u000a      optimization that will be developed under the supervision of Michal\u000a      Kocvara with the financial support by NAG. [source 4]\u000a    In NAG, work is underway on routines for linear and nonlinear\u000a      semidefinite programming. This will include routines for dense and sparse\u000a      scenarios, and can be applied to many problems in operations research and\u000a      combinatorial optimization. In the next phase, the newly developed\u000a      optimization library will be extended by routines for nonlinear\u000a      second-order conic optimization\u000a    NAG turned to PENNON for this because of the increasing demand for\u000a      reliable software for semidefinite and conic quadratic optimization and,\u000a      based on the popularity and generality of PENNON, they decided to base the\u000a      new implementation solely on this code. This allowed them to bring a\u000a      product to market more quickly than would otherwise have been possible.\u000a    A further aspect of the impact achieved is the investment in further\u000a      development made by NAG. This is through funding by NAG of a postdoctoral\u000a      research fellow at Birmingham supervised by Michal Kocvara for two year\u000a      who will develop new algorithms and software for this class of problems.\u000a      NAG has agreed to pay the salary costs for the research fellow over the\u000a      two years (&#163;73,000) for the two year period. [source 5]\u000a    ","ImpactSummary":"\u000a    New optimization routines have been commercialised as a product by the\u000a      Numerical Algorithms Group (NAG). These routines are based on research in\u000a      the School of Mathematics at the University of Birmingham. NAG has\u000a      confirmed that their expectation is that they will release this new\u000a      product, under licence, in Mark 24 of the NAG C Library, to be\u000a      made available in February 2014. The product is based on the\u000a      PENNON software code developed by Michal Kocvara (Birmingham) and Michael\u000a      Stingl (Erlangen). NAG are an international benchmark provider of\u000a      numerical algorithms and software in mathematics, and as optimization\u000a      becomes ubiquitous, the novel routines for nonlinear optimization will\u000a      help NAG attract new customers and bring further benefits to industrial\u000a      and commercial end users. Inclusion in the NAG Library will mean that this\u000a      product is actively marketed to the company's worldwide client base which\u000a      includes many major corporations in the finance sector and engineering\u000a      industries (44% of NAG's &#163;8.2m turnover in 2012\/2013 was outside of the\u000a      UK).\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Birmingham\u000a    ","Institutions":[{"AlternativeName":"Birmingham (University of)","InstitutionName":"University of Birmingham","PeerGroup":"A","Region":"West Midlands","UKPRN":10006840}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a&#8226; M. Kocvara and M. Stingl. PENNON: Software for Linear and Nonlinear\u000a      Matrix Inequalities. In: Handbook on Semidefinite, Conic and Polynomial\u000a        Optimization, Anjos, Miguel F.; Lasserre, Jean B. (Eds.), Springer,\u000a      2012, pp. 755-794, ISBN 978-1-4614-0768-3 [available from the University]\u000a    \u000a\u000a&#8226; (*) J. Fiala, M. Kocvara and M. Stingl: Introducing PENLAB, a Matlab\u000a      code for nonlinear conic optimization, presentation at the 21st\u000a      International Symposium on Mathematical Programming, Berlin, August 19-24,\u000a      2012. [available from the University]\u000a    \u000a\u000a&#8226; (*) J. Fiala, M. Kocvara and M. Stingl: PENLAB: A MATLAB solver for\u000a      nonlinear semidefinite optimization. Preprint of the Newton Institute for\u000a      Mathematical Sciences No. NI13056-POP, Cambridge, 2013. http:\/\/www.newton.ac.uk\/preprints\/NI13056.pdf\u000a      (Submitted to Mathematical Programming Computation.)\u000a    \u000a(*) Authors in employment with sponsoring partner\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000a    \u000a      Corroboration from Chief Technical Officer, NAG, email dated 9th\u000a        October 2013\u000a      source: Financial Statements The Numerical Algorithms Group Limited\u000a        for year ended 31\/3\/13. Company no: 1249803 (downloaded from Companies\u000a        House)\u000a      www.nag.co.uk\u000a      www.nag.co.uk\/collaboration-university-birmingham\u000a      Corroboration from Development Executive, Development and Alumni\u000a        Office, University of Birmingham, email dated 8\/10\/2012\u000a    \u000a    ","Title":"\u000a    Commercialisation of Conic Optimization Routines in NAG Library\u000a    ","UKLocation":[{"GeoNamesId":"2655603","Name":"Birmingham"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research for this case study has been conducted by\u000a      Michal Kocvara (Professor of Mathematical Optimisation at Birmingham since\u000a      January 2007) working with Michael Stingl (Erlangen).\u000a    Kocvara and Stingl's research programme has focused on the development of\u000a      algorithms and software for nonlinear semidefinite and conic quadratic\u000a      optimization. Over the last decade they have been developing a new\u000a      algorithm for the solution of problems in linear and nonlinear\u000a      semidefinite optimization, which was the first of its kind.\u000a    Based on this algorithm, Kocvara and Stingl have further developed a\u000a      software package, PENNON (PENalty method for NONlinear and semidefinite\u000a      programming). Currently, this is the only software available for nonlinear\u000a      semidefinite optimization.\u000a    The particular aspects of this research programme carried out by Prof\u000a      Kocvara following his appointment to Birmingham include the development of\u000a      the general nonlinear algorithm and code PENNON. Recently, Kocvara and\u000a      Fiala (NAG) have developed and released a free open source version of the\u000a      code named PENLAB.PENNON is based on a generalized augmented Lagrangian\u000a      method. Its uniqueness lies in a special penalty\/barrier function which\u000a      allows it to handle generic matrix inequalities side by side with\u000a      nonlinear constraints. It can be used to solve problems such as linear\u000a      semidefinite programming problems (SDP) or formulations with bilinear\u000a      matrix inequalities (BMI) as well as fully nonlinear semidefinite\u000a      programming problems (NLP-SDP).\u000a    Results of independent benchmarks, in the context of problems with linear\u000a      constraints, demonstrate that the program is competitive to, and often\u000a      better than, other programs developed by leading world researchers (see http:\/\/plato.la.asu.edu\/bench.html,\u000a      Semidefinite Programming and Nonlinear Programming). Moreover, in the\u000a      context of problems with nonlinear constraints, PENNON is the only\u000a      existing software available.\u000a    Various versions of the code (PENNON, PENSDP, PENBMI, PENNLP) were\u000a      implemented at Argonne National Laboratories on the NEOS server that\u000a      includes collection of most effective software for mathematical\u000a      optimization (see http:\/\/neos-server.org).\u000a      More than 300 licenses of the code have been awarded worldwide.\u000a    "},{"CaseStudyId":"38834","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3144096","Name":"Norway"}],"Funders":[],"ImpactDetails":"\u000a    The projects upon which this case study is based arose out of contacts\u000a      between the Birmingham\u000a      researchers and senior research staff at the Statoil Research Centre,\u000a      Porsgrunn and were reported\u000a      to Statoil beginning in 2008, leading to subsequent benefits for the\u000a      company. Statoil AS is a fully\u000a      integrated international petrochemical company (the Norwegian government\u000a      being a 67%\u000a      shareholder) with significant operations in 36 countries worldwide. By\u000a      revenue, Statoil was ranked\u000a      as the world's 13th largest oil and gas company in 2010.\u000a    Flow Assurance is an accepted terminology in the oil industry; it defines\u000a      the technology area that\u000a      seeks to ensure uninterrupted flow from the oil\/gas reservoir to the\u000a      control platform.\u000a      Since the production from an oil\/gas reservoir almost always includes gas,\u000a      oil and water, ensuring\u000a      uninterrupted flow requires having control of multiphase transport\u000a      (oil+water+gas) in potentially\u000a      very long pipelines (140km is the current limit. However, Statoil are\u000a      working on a pipeline that is\u000a      600km long). Control of multiphase transport means: understanding pressure\u000a      drop, understanding\u000a      the presence of flow instabilities and being able to predict the amount of\u000a      liquid\/gas that a\u000a      processing unit (platform) may receive an any one time (in order to be\u000a      able to dimension the\u000a      separators etc properly).\u000a    A theoretical approach to nonlinear waves in such pipe flows forms a\u000a      fundamental part of the long-term\u000a      aim to understand details of the flow structure better so that models can\u000a      be significantly\u000a      improved. These improved models can then be used to give better\u000a      predictions of the flow\u000a      behaviour so that the whole field (pipelines + processing units) can be\u000a      dimensioned with more\u000a      precision. Success with this achieves the following: 1) preventing\u000a      over-dimensioning of systems\u000a      (which is very expensive and may render a project uneconomical), 2)\u000a      preventing unexpected bottle\u000a      necks (it is very expensive to make modifications on an existing platform\u000a      since production has to\u000a      cease). Improving flow models is very much a brick-by-brick building\u000a      process &#8212; since flow\u000a      simulators contain many different sub models (eg for slug flow, stratified\u000a      flow, dispersed flow etc)\u000a      and all these models interact in a highly nonlinear way, it is not always\u000a      evident how an\u000a      improvement in one sub model may increase the overall predictive quality\u000a      of the flow simulator.\u000a    The work reported in this case study on nonlinear waves is one of those\u000a      bricks &#8212; it enables\u000a      fundamental understanding which supports the long-term aim of developing\u000a      better and more robust\u000a      flow models. The impact of having access to such models is enormous. As\u000a      reported by [text removed\u000a      for Publication] (Leader, Multiphase Flow Research and Development Statoil\u000a      AS) `In the North Sea\u000a        context this is rapidly approaching an economic value of many billions\u000a        of NOK's'.\u000a    Multiphase flow is one main branch of the Flow Assurance tree. The other\u000a      main branch is related\u000a      to control of pipe line deposits. Deposits come in many different forms,\u000a      for example, sand, wax,\u000a      hydrates, scale and asphaltenes. Some deposits (like sand) are governed\u000a      only by the flow. Other\u000a      deposits (like hydrates and wax) are dependent on the flow as well as\u000a      thermal effects. It is\u000a      generally accepted that wax formation is one of the most practically\u000a      difficult deposits to deal with.\u000a      The reason is that there is only one reliable way to remove wax, and that\u000a      is by means of pigging\u000a      (sending scraping devices through the pipeline that mechanically remove\u000a      the wax). The problem\u000a      with wax is that it grows slowly and by the time it is noticed that wax\u000a      formation is occurring\u000a      (pressure drop in the pipeline increases), it is almost always a problem\u000a      that is too big to handle.\u000a      This means that a lot of preventative pigging occurs, simply to avoid the\u000a      situation where wax gets\u000a      an opportunity to build up. Statoil report that the situation is made more\u000a      complicated by the fact\u000a      that wax formation is not fully understood and many current models are\u000a      simply incorrect. As Statoil\u000a      report `This means that models can rarely be used to determine the\u000a        pigging frequency, again\u000a        leading to a large amount of (probably unnecessary) preventative\u000a        pigging. These pigging\u000a        operations are time consuming and expensive which can lead to a\u000a        situation where production is no\u000a        longer economical'. The work reported in this case study on the\u000a      rational model development and\u000a      analysis of wax deposition on the interior walls of heated oil pipe lines\u000a      has made significant impact\u000a      on the understanding, and hence the potential resolution, of this\u000a      significant issue.\u000a    The programme of research has involved two fundamental themes, namely:\u000a    Multiphase Pipe Line Flow: A rational mathematical model, based on\u000a      fundamental hydraulic\u000a      principles, has been developed to describe the two-layer hydraulic flow of\u000a      gas and liquid in pipe\u000a      lines. The co-current flow of gas and liquid in a circular pipe is of\u000a      tremendous importance in the oil\u000a      and nuclear industries. One significant feature which makes multiphase\u000a      gas\/liquid flow systems\u000a      special is the presence of different flow regimes. Transport of the two\u000a      phases can occur in the form\u000a      of stratified flow, slug flow or dispersed flow depending upon the\u000a      upstream inlet flow rates of each\u000a      fluid phase. Statoil report that `despite a large quantity of both\u000a      experimental and theoretical\u000a      research, focused on understanding and predicting the flow rate transition\u000a      boundaries between\u000a      these distinct flow regimes, there remain large holes in our\u000a      understanding.' In particular,\u000a      understanding and predicting the flow regime transition from uniform\u000a      two-layer flow to two-layer\u000a      slug flow is crucial to the efficient operation of gas\/liquid transporting\u000a      pipe lines in the oil and gas\u000a      industries. Such pipe lines may operate over hundreds of kilometres,\u000a      transporting gas and an\u000a      associated thin liquid layer, from the source gas field to controlled\u000a      distribution centres. The efficient\u000a      operation of the pipe line flow over such large distances is of\u000a      fundamental importance for the\u000a      effectiveness, both economically and environmentally, of gas\/oil\u000a      transport.\u000a    This project has been successful in providing a rational theoretical\u000a      framework to enable the\u000a      prediction and control of slugging behaviour in gas\/liquid pipe line\u000a      flows, and the interpretation of\u000a      experimental measurements and observational phenomena. The theory\u000a      developed now forms an\u000a      integrated part of the programme at the Statoil Research Centre, as a\u000a      source of fundamental\u000a      mechanistic understanding, to inform rational software model development\u000a      and to assist in\u000a      resolving practical issues relating to particular Statoil gas\/oil pipeline\u000a      networks in the field.\u000a    The theory has also enabled informed decisions to be made in relation to\u000a      designing future\u000a      experimental programmes relating to gas\/oil pipe line transport. On a\u000a      broader front, the theory has\u000a      resolved the fundamental issues surrounding the origin of the transition\u000a      phenomena for slug flow. It\u000a      is now broadly accepted, as a consequence of this programme of work, that\u000a      the transition\u000a      phenomena is dissipative and hydraulic in origin, rather than of\u000a      Kelvin-Helmholtz type. This\u000a      understanding has been fundamental in shifting the focus of research in\u000a      this area at Statoil, and\u000a      has had a consequent fundamental impact on advances in this area, in terms\u000a      of the design of more\u000a      economically, energetically and environmentally favourable strategies for\u000a      gas\/oil pipeline delivery.\u000a    The work has led to a detailed report for Statoil (Nonlinear waves in\u000a      two-layer hydraulic pipeline\u000a      flows' Contract Report for the Statoil Research Centre (D J Needham, J\u000a      Billingham, A C King) and\u000a      a workshop (`Gas-Liquid Pipe-Line Flows' held at the Statoil Research\u000a      Centre, Porsgrunn, Norway\u000a      in March 2008, and supported fully by Statoil. Co-Chairmen &#8212; R M S M\u000a      Schulkes*, D J Needham..\u000a      Attended by approximately 50 people, mainly from UK and Norway, including\u000a      scientists from Statoil\u000a      and University of Oslo) to discuss its implications, implementation and\u000a      future development.\u000a    This work has contributed to the development by Statoil of an active slug\u000a      control system in their\u000a      Heidrum field in the Norwegian Sea, which has produced a more stable well\u000a      stream flow into the\u000a      first stage flow separator, leading to significant economic benefits. It\u000a      is anticipated that the\u000a      success of this active slug control system will lead to its implementation\u000a      at other Statoil fields.\u000a    Pipe Line Deposit Control: A rational mathematical model, based on\u000a      fundamental thermal transfer\u000a      principles, has been developed to describe the formation and evolution of\u000a      wax layers on the interior\u000a      of pipes transporting heated oil. Up to the present, attempts at modelling\u000a      these phenomena have\u000a      had very little success, and have failed to predict even the basic\u000a      qualitative features identified by\u000a      carefully controlled experiments at Statoil Research Centre. This lack of\u000a      success, together with the\u000a      very significant implications of these phenomena to the petrochemical\u000a      industry, motivated the\u000a      establishment of this project with Statoil. There is consensus in the oil\u000a      industry that wax deposition\u000a      is governed by molecular diffusion whereby dissolved wax diffuses toward\u000a      the pipe wall. Other\u000a      deposition mechanisms have been suggested such as Brownian diffusion,\u000a      gravity settling and\u000a      shear dispersion. None of these mechanisms can be entirely discarded while\u000a      at the same time\u000a      there is still controversy about the mechanisms governing wax deposition\u000a      since the diffusion-based\u000a      model yields predictions that are qualitatively incorrect for certain\u000a      important cases.\u000a    We have proposed an entirely different approach: that wax deposition is a\u000a      phase change problem.\u000a      The crystallisation of wax is an exothermic process, meaning that wax will\u000a      only be formed there\u000a      when heat, as a result of the crystallization process, can be removed. The\u000a      growth of the wax layer\u000a      will thus be governed by the balance of heat supplied by means of\u000a      convection to the wax layer\u000a      from the oil phase and heat removed from the wax layer by means of\u000a      conduction. The problem that\u000a      is obtained is a moving boundary problem of a generalised Stefan type.\u000a    The analysis of the model has led to predictions that are now\u000a      qualitatively, and to a good degree\u000a      quantitatively, in accord with the results of the experimental programme\u000a      in place at the Statoil\u000a      Research Centre. This agreement is very encouraging and had led Statoil to\u000a      design further\u000a      experimental programmes which are now informed by the predictions of this\u000a      model. It is emerging\u000a      from this that the model established in this case study will be\u000a      developable by Statoil into a\u000a      predictive tool which will considerably reduce the significant economic\u000a      burden of the systematic\u000a      pigging operations, which are currently in use as the only effective\u000a      measure. This work has been\u000a      documented in a report to Statoil (`The development of a wax layer on the\u000a      interior wall of a circular\u000a      pipe transporting heated oil' Contract report to Statoil AS (D J Needham,\u000a      B T Johansson, T\u000a      Reeve)).\u000a    In relation to both of the above projects, D J Needham has presented four\u000a      research seminars on\u000a      this work at the Statoil Research Centre. R M S M Schulkes* presented a\u000a      research seminar on\u000a      related problems of significance for Statoil, to the School of\u000a      Mathematics, University of\u000a      Birmingham. D J Needham has given an instructional workshop on\u000a      implementation at the Statoil\u000a      Research Centre.\u000a    As an overview, [text removed for Publication] has commented: `Flow\u000a        assurance is a vital area of technology in which a\u000a        step by step approach is taken to achieve, improve and implement\u000a        technology elements. Each step\u000a        taken which contributes to our ability to extend the range of subsea\u000a        transported solutions is of\u000a        significant value. The reason is that more and more marginal reserves\u000a        can be produced. This flow\u000a        related and wax depositional work is thus an important contribution\u000a        towards revitalising and\u000a        extending oil production in the North Sea. `\u000a    ","ImpactSummary":"\u000a    This case study relates to research supported under contract by Statoil\u000a      AS, one of the world's\u000a      largest oil and gas companies, and is focused on two major issues of\u000a      significance to that industry,\u000a      namely, the mathematical modelling and analysis of (a) hydraulic two-layer\u000a      gas\/liquid flow in pipe\u000a      lines and (b) wax formation in the interior walls of pipe lines\u000a      transporting heated oil. These projects\u000a      arose out of contacts between senior research staff at the Statoil\u000a      Research Centre and the\u000a      Nonlinear Waves group in the School of Mathematics. The theoretical\u000a      research undertaken was\u000a      designed to complement the major experimental programmes developed at the\u000a      Statoil Research\u000a      Centre and was performed in collaboration with scientists there. The work\u000a      has provided Statoil with\u000a      a reliable theoretical framework to contextualise and enable comparison\u000a      with experimental results\u000a      and to inform the design of future experimental programmes. In the larger\u000a      context, the research\u000a      has played a key role in advancing the capability of Statoil to design and\u000a      implement more\u000a      economical, energy efficient, and environmentally safe strategies for\u000a      gas\/oil delivery via extended\u000a      pipeline networks. Statoil have stated that the benefit of access to\u000a      robust flow models in the North\u000a      Sea context is rapidly approaching an economic value of many billions of\u000a      Norwegian kroner.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Birmingham\u000a    ","Institutions":[{"AlternativeName":"Birmingham (University of)","InstitutionName":"University of Birmingham","PeerGroup":"A","Region":"West Midlands","UKPRN":10006840}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"3142657","Name":"Porsgrunn"},{"GeoNamesId":"3143244","Name":"Oslo"}],"References":"\u000a    \u000a`The development of slugging in two-layer hydraulic flows', IMA JL Appl\u000a      Math, 2008, 73, 1, 274-322\u000a      (D J Needham, J Billingham, R M S M Schulkes*, A C King) doi:\u000a      10.1093\/imamat\/hxm050\u000a    \u000a\u000a`The development of a wax layer on the interior wall of a circular pipe\u000a      transporting heated oil'\u000a      University of Birmingham, School of Mathematics Preprint Series 2012\/04\u000a      (to appear in Q Jl Mech\u000a      Appl Math) (D J Needham, L Amundsen*, B T Johansson, T Reeve)\u000a    \u000a* Author employed at Statoil Research Centre, Porsgrunn, Norway.\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"}],"Sources":"\u000a    Corroboration of the impact described in this case study can be obtained\u000a      from the Leader,\u000a      Multiphase Flow Research and Development, TPD RD NDS, Statoil ASA,.\u000a    ","Title":"\u000a    Mathematical Foundations of Flow Assurance Issues in the Petroleum\u000a        Industry\u000a    ","UKLocation":[{"GeoNamesId":"2655664","Name":"Billingham"},{"GeoNamesId":"2655603","Name":"Birmingham"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    In relation to the nonlinear waves project, previous theoretical models\u000a      of two-layer gas\/liquid flow in\u000a      circular pipes, driven by prescribed upstream inlet flow rates, have been\u000a      based around treating\u000a      both the gas and liquid as inviscid fluids. In relation to the inviscid\u000a      model, the uniform flow of the\u000a      two-layers can undergo linear instability at critical prescribed flow\u000a      rates. This instability is not a\u000a      long-wave instability, and is related to the classical hydrodynamic\u000a      Kelvin-Helmholtz instability at a\u000a      shear layer interface. It was originally thought that this model, with the\u000a      associated Kelvin-Helmholtz\u000a      instability, may account for the transition, in two-layer gas\/liquid pipe\u000a      flow, from uniform flow to slug\u000a      flow. However, detailed comparison with experimental work has revealed a\u000a      number of serious\u000a      flaws in this conclusion. In particular observations suggested that\u000a      dissipation\/friction are associated\u000a      with the transition, and that the development of a hydraulic model would\u000a      be essential in a rational\u000a      and predictive theory for the transition phenomena. This research\u000a      programme, from 2005-2010, led\u000a      by D J Needham (Professor of Applied Mathematics) was undertaken with\u000a      Statoil AS and began\u000a      with the development of a two-layer hydraulic theory. Simplified evolution\u000a      equations were\u000a      developed for the situation when the underlying liquid layer is shallow\u000a      compared to the overlying\u000a      gas layer, the situation of practical interest identified by Statoil for\u000a      gas\/condensate pipe lines. The\u000a      temporal stability of the uniform flow is examined via linearization of\u000a      the evolution equations, and\u000a      this leads to an equation of wave-hierarchy type with dissipation. The\u000a      temporal stability of the\u000a      uniform flow is shown to depend upon the Froude number of the liquid\u000a      layer, which is related to the\u000a      upstream volumetric flow rates in both layers. This criterion for\u000a      hydraulic in stability leads to a long-\u000a      wave maximum growth rate close to transition, and, moreover, the\u000a      transition criterion is in\u000a      qualitative, and some quantitative, agreement with experimental results.\u000a      The roll wave structures\u000a      have been identified as appearing through Hopf bifurcations, with a\u000a      nonlinear selection mechanism\u000a      associated with convective instabilities in the long wave profile `tail'.\u000a    In relation to the wax deposition problem the programme of work, led by D\u000a      J Needham, was\u000a      undertaken with Statoil AS, from 2010-2012. We have proposed an entirely\u000a      different approach\u000a      towards modelling the deposition of paraffinic wax on the inside of the\u000a      pipe wall, which has\u000a      previously been based on isothermal material diffusion mechanisms, and has\u000a      been largely\u000a      unsuccessful. Our principal observation is that the crystallisation of wax\u000a      is an exothermic process,\u000a      meaning that wax will only be formed there when heat, as a result of the\u000a      crystallization process,\u000a      can be removed. Indeed, this explains why wax forms on the inside of the\u000a      pipe wall, only when the\u000a      exterior pipe wall is subject to sufficient cooling (we observe in passing\u000a      that the diffusion model\u000a      does not offer such a simple explanation for the deposition of wax on the\u000a      pipe wall). The growth of\u000a      the wax layer will thus be governed by the balance of heat supplied by\u000a      means of convection to the\u000a      wax layer from the oil phase and heat removed from the wax layer by means\u000a      of conduction through\u000a      the pipe wall. The problem that is obtained is a moving boundary problem\u000a      of a generalised Stefan\u000a      type, which has been analysed in detail via both analytical and numerical\u000a      approaches, leading to a\u000a      reliable and predictive tool for Statoil.\u000a    The work associated with the two-layer gas\/liquid pipe line flow was\u000a      performed under contract with\u000a      Statoil AS by D J Needham, the late Professor A C King (University of\u000a      Birmingham) and J\u000a      Billingham (University of Nottingham) from 2005 to 2010. The work\u000a      associated with wax deposition\u000a      on the interior walls of heated oil pipe line flows was performed under\u000a      contract with Statoil AS by D\u000a      J Needham and B T Johansson (Senior Lecturer, University of Birmingham,\u000a      until September 2012)\u000a      from 2010 to 2012. The research has led to contract reports for Statoil,\u000a      together with two\u000a      substantial publications in high quality Applied Mathematics journals.\u000a    "},{"CaseStudyId":"40250","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3017382","Name":"France"},{"GeoNamesId":"2782113","Name":"Austria"},{"GeoNamesId":"2629691","Name":"Iceland"},{"GeoNamesId":"3144096","Name":"Norway"},{"GeoNamesId":"2658434","Name":"Switzerland"},{"GeoNamesId":"3175395","Name":"Italy"},{"GeoNamesId":"2510769","Name":"Spain"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The impacts of this research are that (i) it underpins and is essentially\u000d\u000a      embodied in modern guidelines on the design of avalanche defence barriers;\u000d\u000a      (ii) it has been used extensively by specialists across Europe who design\u000d\u000a      barriers and has played a crucial part in securing very significant\u000d\u000a      investment in infrastructure through large-scale civil engineering\u000d\u000a      projects aimed at reducing the risk of avalanche damage to settlements;\u000d\u000a      and (iii) the research findings form the basis of highly regarded training\u000d\u000a      courses for avalanche professionals.\u000d\u000a    Guidelines on design\u000d\u000a    The research findings on the interaction between obstacles and avalanches\u000d\u000a      and their implications for the design of defence barriers ([1]-[4]) have\u000d\u000a      been disseminated in a series of reports written by professional avalanche\u000d\u000a      researchers that provide practical guidance for avalanche engineers and\u000d\u000a      other users. The following two reports are particularly significant:\u000d\u000a    Background for the determination of dam height in the SATSIE dam\u000d\u000a        design guidelines (Icelandic Met Office, March 2008) [a]\u000d\u000a      The design of avalanche protection dams. Recent practical and\u000d\u000a        theoretical developments (European Commission, 2009) [b]\u000d\u000a    These publications (particularly [b], which is the more comprehensive)\u000d\u000a      are specifically targeted at engineers tasked with designing new\u000d\u000a      structures and they contain many practical guidelines. In [b], research\u000d\u000a      conducted by Hogg and H&#225;konard&#243;ttir forms the basis for Chapter 5\u000d\u000a      (`Deflecting and catching dams') and Chapter 9 (`Braking mounds').\u000d\u000a      Additionally, parts of Chapter 6 (`Special considerations for deflecting\u000d\u000a      dams') and Appendix D (`Integrated protective measures &#8212; a practical\u000d\u000a      example') feature several ideas developed by Hogg and H&#225;konard&#243;ttir. This\u000d\u000a      handbook has been reviewed in journals, whose core audience includes civil\u000d\u000a      engineers who design avalanche defences in the public and private sectors\u000d\u000a      internationally. For example, Jomelli concludes that the handbook\u000d\u000a      \"represents a useful addition to the technical literature and a good\u000d\u000a      reading opportunity for scientists, engineers and practitioners\" [c],\u000d\u000a      while Ancey observes that \"Trained engineers and practitioners will find\u000d\u000a      useful information on recent developments concerning avalanche-catching\u000d\u000a      dams\" [d].\u000d\u000a    The scientific guidelines [3,4,b] are also reported in Handbuch\u000d\u000a        Technischer Lawinenschutz (2011)[e], a thorough reference book about\u000d\u000a      snow avalanche safety and the design of snow avalanche protection\u000d\u000a      measures, including supporting structures and dams[h]. This is published\u000d\u000a      in German, edited by avalanche professionals from the Austrian Service for\u000d\u000a      Torrent and Avalanche Control [e]; an English version is planned. The book\u000d\u000a      has been well received in the engineering community. Wolfgang Fellin\u000d\u000a      writes that \"...the handbook...should become a basic reference for all\u000d\u000a      those involved in avalanche protection and alpine civil engineering\" [f].\u000d\u000a      The guidelines also feature in recently adopted Austrian technical\u000d\u000a      guidelines ONR 24806:2011-12-15: Permanent technical avalanche\u000d\u000a        protection: Design of structures, while the new methodology for\u000d\u000a      computing granular shocks [4] has been implemented in software\u000d\u000a      (`WLV-DammTool') and this tool is now used by avalanche professionals\u000d\u000a      across Europe [g].\u000d\u000a    Investment in infrastructure\u000d\u000a    The research results [1]-[4], embodied in the new guidelines [b], have\u000d\u000a      been employed at a range of locations in Iceland, thus reducing the risks\u000d\u000a      for villages and other settlements [h]. The economic cost of avalanche\u000d\u000a      damage in Iceland between 1974 and 2000 is estimated at &#163;16M, while there\u000d\u000a      were 69 fatalities. The new avalanche protection measures are intended to\u000d\u000a      avoid such losses [h]. The six projects that have been completed in\u000d\u000a      Iceland since 2008 using the research findings [1]-[4] and new guidelines\u000d\u000a      [b] are at B&#237;ldudalur (&#8364;1.9 M), &#211;lafsv&#237;k (&#8364;2.3 M), &#211;lafsfj&#246;r&#240;ur (&#8364;1.6 M),\u000d\u000a      Bolungarv&#237;k (&#8364;9.3 M), Neskaupsta&#240;ur 1 (&#8364;6.2 M) and Neskaupsta&#240;ur 2 (&#8364;10.1\u000d\u000a      M). (The figures in parentheses are the infrastructure costs of the\u000d\u000a      projects [h].) Currently there are seven projects under construction or in\u000d\u000a      the later stages of design. These are at Patreksfj&#246;r&#240;ur (two projects),\u000d\u000a      B&#237;ldudalur, &#205;safj&#246;r&#240;ur (two projects), Siglufj&#246;r&#240;ur and Eskifj&#246;r&#240;ur.\u000d\u000a      Overall, the Icelandic government is spending &#8364;5.1M per year on avalanche\u000d\u000a      defence and this level of spending will continue until at least 2020, by\u000d\u000a      which time it is envisaged that most endangered settlements will have been\u000d\u000a      protected [h]. It is assessed that the installation of these various\u000d\u000a      avalanche defence measures significantly reduces the risk of damage to\u000d\u000a      settlements in potential avalanche tracks. For instance, the chief\u000d\u000a      avalanche scientist at the Icelandic Meterological Office writes, \"We\u000d\u000a      might guestimate that the risk is in many cases reduced by an order of\u000d\u000a      magnitude....\" [h].\u000d\u000a    The research [1]-[4] and new guidelines [b] also form the basis of the\u000d\u000a      design of a number of defence structures in Norway, with the design work\u000d\u000a      carried out by avalanche engineers at the Norwegian Geophysical Institute\u000d\u000a      [i]. The projects include the dimensioning and design of a deflection dam\u000d\u000a      protecting a power station at Rolandsfjorden in the municipality of Mel&#248;y,\u000d\u000a      Nordland county, which was completed in 2011; and the dimensioning and\u000d\u000a      design of protection dams at R&#248;mmingslia, municipality of Oppdal, Oppland\u000d\u000a      county, and for Hotel Alexandria, Loen, municipality of Stryn, Sogn og\u000d\u000a      Fjordane county, and a transformer station in the municipality of\u000d\u000a      H&#248;yanger, Sogn og Fjordane county. In addition, design works are being\u000d\u000a      carried out to safeguard buildings at Kobbelv hydroelectric power station,\u000d\u000a      municipality of S&#248;rfold, Nordland county, and exposed masts of\u000d\u000a      high-voltage power line through avalanche-prone terrain in the\u000d\u000a      municipalities of Naustdal, H&#248;yanger and Sogndal, Sogn og Fjordane county.\u000d\u000a      The cost of such works is approximately &#163;2-3M, but the value of the\u000d\u000a      infrastructure they protect is dramatically higher (at least &#163;100M) [i].\u000d\u000a    The guidelines for the design of avalanche defence structures [b] have\u000d\u000a      also been adopted by alpine European countries. In France, extensive\u000d\u000a      measures are being constructed at Chamonix to defend against avalanches\u000d\u000a      originating from the Taconnaz glacier. This installation includes mounds\u000d\u000a      and deflecting and catching dams. The investment is approximately &#8364;10M and\u000d\u000a      the design, which relies on the research results of Hogg and H&#225;konard&#243;ttir\u000d\u000a      [1]-[4] , was carried out by IRSTEA (French National Research Institute of\u000d\u000a      Science and Technology for Environment and Agriculture)[j]. Researchers\u000d\u000a      from the institute also designed the deflecting dam on the Fougeret\u000d\u000a      avalanche track at Cialancier (Saint-Etienne de la Tinee) in collaboration\u000d\u000a      with RTM ('Restauration des Terrains de Montagne', France)[j], using the\u000d\u000a      research findings [1]-[4] and the new guidelines [b]. The guidelines have\u000d\u000a      also been used in Switzerland by the Institute for Snow and Avalanche\u000d\u000a      Research (SLF) and in Austria, where the Austrian Service for Torrent and\u000d\u000a      Avalanche Control are charged with the responsibility for assessing and\u000d\u000a      managing the risk from avalanches. In the latter case, they have\u000d\u000a      constructed a deflection and a catching dam at Tuiflahn (&#8364;1.5M), based on\u000d\u000a      [1]-[4] and [b], which provides protection for 34 dwellings in the most\u000d\u000a      hazardous zones and there are advanced plans for protection measures at\u000d\u000a      Dalfaz to defend 26 buildings with an estimated infrastructure cost of\u000d\u000a      &#8364;1.1M [g].\u000d\u000a    Training\u000d\u000a    Various training courses for avalanche engineers have drawn heavily on\u000d\u000a      the new approach to modelling the interactions between obstacles and\u000d\u000a      avalanches developed by Hogg and H&#225;konard&#243;ttir [1]-[4]. These include a\u000d\u000a      two-day course in 2012 for professional engineers from RTM; a half-day\u000d\u000a      course in 2009 for professional engineers from DDT ('Direction\u000d\u000a      d&#233;partementale des Territoires', Haute-Savoie, France); and contributions\u000d\u000a      to a week-long course in 2010 for engineers from the public and private\u000d\u000a      sectors in France, Italy and Spain under the framework of the European\u000d\u000a      Summer School on Avalanches (Les Deux Alpes, France) [j]. Finally, the\u000d\u000a      research conducted at Bristol has provided in depth training for\u000d\u000a      H&#225;konard&#243;ttir. She subsequently became one of the leading technical\u000d\u000a      engineers on avalanche protection schemes at VST Consulting Engineers,\u000d\u000a      Reykjavik, Iceland.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research carried out in the School of Mathematics at the University of\u000d\u000a      Bristol between 1998 and 2005 has been instrumental in the development of\u000d\u000a      structures that arrest or deflect the rapid flow of snow that\u000d\u000a      characterises avalanches in mountainous regions of the world. The research\u000d\u000a      has been embodied in a series of guidance documents for engineers on the\u000d\u000a      design of such structures and many defence dams and barriers have been\u000d\u000a      built across Europe since 2008. The guidance is now adopted as standard\u000d\u000a      practice in many of the countries that experience avalanches. Investment\u000d\u000a      in avalanche defence projects based on the design principles set out in\u000d\u000a      the guidance runs into tens of millions of pounds. The Bristol research is\u000d\u000a      also used internationally in the training of engineers who specialise in\u000d\u000a      avalanche protection schemes. Given the scale of the threat to life and\u000d\u000a      property from these potent natural hazards, the impact of the research is\u000d\u000a      considerable in terms of the societal and economic benefits derived from\u000d\u000a      the reduction of the risk posed by snow avalanches.\u000d\u000a    ","ImpactType":"Environmental","Institution":"\u000d\u000a    University of Bristol\u000d\u000a    ","Institutions":[{"AlternativeName":"Bristol (University of)","InstitutionName":"University of Bristol","PeerGroup":"A","Region":"South West","UKPRN":10007786}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"3137966","Name":"Sogn og Fjordane Fylke"},{"GeoNamesId":"3144301","Name":"Nordland Fylke"},{"GeoNamesId":"587577","Name":"Viljandi"},{"GeoNamesId":"2980291","Name":"Saint-Étienne"},{"GeoNamesId":"3143487","Name":"Oppland fylke"}],"References":"\u000d\u000a    \u000a*[1] H&#225;konard&#243;ttir, K.M., Hogg, A.J., J&#243;hannesson, T. &amp; Tomasson,\u000d\u000a      G.G. 2003 A laboratory study of the retarding effect of braking mounds. J.\u000d\u000a        Glaciology. 49. 191-200 DOI:10.3189\/172756503781830692.\u000d\u000a    \u000a\u000a*[2] H&#225;konard&#243;ttir, K.M., Hogg, A.J., Batey, J. &amp; Woods, A.W. 2003\u000d\u000a\u0009Flying\u000d\u000a        avalanches. Geophys. Res. Letters 30 DOI:\u000d\u000a      10.1029\/2003GL018172.\u000d\u000a    \u000a\u000a[3] H&#225;konard&#243;ttir, K.M., Hogg, A.J., J&#243;hannesson, T., Kern, M. &amp;\u000d\u000a      Tiefenbacher, F. 2003 Large-scale\u000aavalanche\u000a        braking mounds and catching dam experiments with snow: a study of the\u000d\u000a        airborne jet. Surveys in Geophysics 24, 543-554 DOI:\u000d\u000a      10.1023\/B:GEOP.0000006081.76154.ad.\u000d\u000a    \u000a\u000a*[4] H&#225;konard&#243;ttir, K.M. &amp; Hogg, A.J. 2005 Oblique\u000a        shocks in rapid granular flows. Phys. Fluids 17\u000d\u000a      077101 DOI: 10.1063\/1.1950688.\u000d\u000a    \u000a* references that best indicate the quality of the underpinning research.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"9","Level2":"14","Subject":"Resources Engineering and Extractive Metallurgy"},{"Level1":"9","Level2":"5","Subject":"Civil Engineering"}],"Sources":"\u000d\u000a    [a] Background for the determination of dam height in the SATSIE dam\u000d\u000a        design guidelines (Icelandic Met Office, Int. Rep. 08003, 2008.\u000d\u000a      Authors T. J&#243;hannesson, Krist&#237;n M. H&#225;konard&#243;ttir, C. B. Harbitz and U.\u000d\u000a      Domaas).\u000d\u000a      Available from http:\/\/www.vedur.is\/gogn\/snjoflod\/varnarvirki\/08003.pdf\u000d\u000a    [b] The design of avalanche protection dams. Recent practical and\u000d\u000a        theoretical developments (European Commission, Directorate-General\u000d\u000a      for Research, Publication EUR 23339, 2009, ISBN 978-92-79-08885-8, ISSN\u000d\u000a      1018-5593, DOI:10.2777\/12871. Edited by T. J&#243;hannesson, P. Gauer, P.\u000d\u000a      Issler and K. Lied. Authors: M. Barbolini, U. Domaas, T. Faug, P. Gauer,\u000d\u000a      K. M. H&#225;konard&#243;ttir, C. B. Harbitz, D. Issler, T. J&#243;hannesson, K. Lied, M.\u000d\u000a      Naaim, F. Naaim-Bouvet and L. Rammer).\u000d\u000a    [c] V. Jomelli (2009), book review of \"The design of avalanche protection\u000d\u000a      dams. Recent and theoretical developments\", Natural Hazards and Earth\u000d\u000a        Systems Sciences 9, 1133-1134.\u000d\u000a    [d] C. Ancey (2009), review of J&#243;hannesson, T, Gauer, P., Issler, D.\u000d\u000a      &amp; Lied, K. (eds) \"The design of avalanche protection dams. Recent and\u000d\u000a      theoretical developments\", J. Glaciology 55,753-754. DOI:\u000d\u000a      10.3189\/002214309789470888\u000d\u000a    [e] Handbuch Technischer Lawinenschutz. Edited by Florian\u000d\u000a      Rudolf-Miklau and Siegfried Sauermoser. Berlin, Wilhelm Ernst &amp; Sohn,\u000d\u000a      Verlag f&#252;r Architektur und technische Wissenschaften GmbH &amp; Co. 466\u000d\u000a      pp. ISBN: 978-3-433-02947-3 (see: http:\/\/www.ernst-und-sohn.de\/handbuch-technischer-lawinenschutz\u000d\u000a      and\u000d\u000a      http:\/\/www.amazon.co.uk\/Handbuch-Technischer-Lawinenschutz-Florian-Rudolf-Miklau\/dp\/3433029474).\u000d\u000a    [f] W. Fellin (2011) Book review of \"Handbuch Technischer Lawinenschutz\",\u000d\u000a      Geomechanics and Tunnelling 4, 704. DOI:\u000d\u000a      10.1002\/geot.201190055\u000d\u000a    [g] Personal communication from Head of Snow and Avalanche at Austrian\u000d\u000a      Service for Torrent and Avalanche Control.\u000d\u000a      This scientist corroborates the use of the research [1]-[4] and design\u000d\u000a        guidelines [b] in Austria, the construction and plans for construction\u000d\u000a        of avalanche protection measures at Tuiflahn and Dalfaz and the\u000d\u000a        development of the software WLV-DammTool.\u000d\u000a    [h] Personal communication from Chief Avalanche Scientist at Icelandic\u000d\u000a      Meteorological Office.\u000d\u000a      This scientist corroborates the many avalanche protection measures\u000d\u000a        deployed in Iceland, along with their infrastructure costs and risk\u000d\u000a        reduction. He confirms the continued spending and the centrality of the\u000d\u000a        research [1]-[4] in the guidelines for the design of dams, deflectors\u000d\u000a        and barriers[b,e].\u000d\u000a    [i] Personal communication from Senior Physicist, Natural Hazards,\u000d\u000a      Avalanches and Rockslides at Norwegian Geotechnical Institute.\u000d\u000a      This scientist confirms the application of the research [1]-[4],\u000d\u000a        embodied in the new guidelines [b], to avalanche defence measures in\u000d\u000a        Norway.\u000d\u000a    [j] Personal communication from Researcher at Institut\u000a        national de recherche en sciences et technologies pour l'environnement\u000d\u000a        et l'agriculture (IRSTEA).\u000d\u000a      This researcher confirms the design of the constructed avalanche\u000d\u000a        defence measures in France employed the research [1]-[4] embodied in the\u000d\u000a        new guidelines [b], and their infrastructure costs. He also corroborates\u000d\u000a        that aspects of the training courses for avalanche professionals were\u000d\u000a        based on these materials.\u000d\u000a    ","Title":"\u000d\u000a    Bristol research helps reduce the threat to people and property from snow\u000d\u000a      avalanches\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2654675","Name":"Bristol"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Background\u000d\u000a    Snow avalanches flow at speeds of up to 50 ms-1, transport\u000d\u000a      many tonnes of snow down hillsides, are extremely destructive of\u000d\u000a      infrastructure and lead to many fatalities. With the increasing\u000d\u000a      development of mountainous regions for homes, industry and tourism, there\u000d\u000a      is a pressing need to defend against snow avalanches. One key strategy is\u000d\u000a      to build structures that stop the flow or change its direction. Prior to\u000d\u000a      the publication of comprehensive guidelines on protection dams in 2009\u000d\u000a      [b], there was relatively little systematic guidance on how to design such\u000d\u000a      structures and very limited understanding of the fundamental dynamics\u000d\u000a      underlying the interaction between flow and obstacle.\u000d\u000a    The research\u000d\u000a    Motivated by the devastation and fatalities caused by snow avalanches in\u000d\u000a      Iceland during the 1990s, where several large-volume and rapidly moving\u000d\u000a      flows struck inhabited regions that were thought to be safe from\u000d\u000a      avalanches, Dr Andrew Hogg (University Lecturer, now Reader, in applied\u000d\u000a      mathematics) and his PhD student Kristin Martha H&#225;konard&#243;ttir undertook\u000d\u000a      fundamental research to analyse the interaction between rapid granular\u000d\u000a      flows and rigid stationary obstacles [1]-[4]. This research, led by Hogg,\u000d\u000a      was undertaken in the School of Mathematics, University of Bristol between\u000d\u000a      1998-2005. Specifically, three types of obstacle were analysed:\u000d\u000a      `deflecting dams' that were supposed to divert the oncoming flow away from\u000d\u000a      the regions to be protected, `catching dams' that were intended to stop\u000d\u000a      the avalanche completely and `arresting mounds' that were meant to\u000d\u000a      dissipate some of the energy of the flow and thus retard the motion.\u000d\u000a      Although such structures had been deployed prior to this research, there\u000d\u000a      was just limited understanding of how effective such measures were and no\u000d\u000a      guidelines on how to optimise their design. This research involved the\u000d\u000a      development of models and laboratory studies at various scales. This\u000d\u000a      combination is vital for research in granular materials where the\u000d\u000a      fundamental governing equations are not fully established.\u000d\u000a    The work revealed several new phenomena. First, the granular flow could\u000d\u000a      be airborne on impact and jump over the dams and mounds [2]; second, the\u000d\u000a      flow could be transformed abruptly by the deflector (via a `granular\u000d\u000a      shock') into a new flowing state [4]; and third, the flow could be\u000d\u000a      deflected by three-dimensional objects and lose a proportion of its energy\u000d\u000a      [1,3]. In each study, measurements of flow speeds and flow depths at the\u000d\u000a      obstacle provided empirical guidelines for the design of these defence\u000d\u000a      structures.\u000d\u000a    However, the aspect of the researchers' contribution that made it most\u000d\u000a      useful for the design of large-scale barriers was the development of\u000d\u000a      predictive models to characterise the flow patterns. For instance, in [4]\u000d\u000a      Hogg and H&#225;konard&#243;ttir demonstrated that `hydraulic'-like models could\u000d\u000a      predict accurately the transformation of the flowing state as a function\u000d\u000a      of the upstream, oncoming conditions. In particular, they showed how to\u000d\u000a      calculate the position of the granular shock and the depth and velocity of\u000d\u000a      the flowing material adjacent to the barrier. In [2] they showed how to\u000d\u000a      predict the trajectory of the airborne granular jet, including its launch\u000d\u000a      angle, as a function of the flow and barrier characteristics. Finally, in\u000d\u000a      [1,3] they examined the energy loss in flows deflected by\u000d\u000a      three-dimensional objects, relating it to the layout and size of the\u000d\u000a      obstacles. These models and insights were based upon fundamental\u000d\u000a      scientific principles and permitted the insights gained in the laboratory\u000d\u000a      to be applied at the natural scale.\u000d\u000a    "},{"CaseStudyId":"40251","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The introduction of dynamical systems methods, which are implemented in\u000a      the Dynamical Systems Toolbox, into the industrial practice at\u000a      Airbus provides the company with new capability for the evaluation of\u000a      aircraft ground performance and the potential of considerable savings in\u000a      time and costs (estimated at 80% of staff time compared to current\u000a      practice within the aircraft industry [c]). Airbus has fully adopted this\u000a      methodology and has developed a strategy for the continued future use of\u000a      dynamical systems methods in an internal document, Strategic Research\u000a        into Dynamical Systems Methods: A roadmap from 2009 to 2013 [a].\u000a    The software, Dynamical Systems Toolbox (DST), has transitioned\u000a      within Airbus from its Research portfolio into its Methods and Tools\u000a      portfolio. This is a mandatory stage of Airbus for institutionalisation of\u000a      new operational engineering methods. Further evidence of the integration\u000a      of these techniques into Airbus is the development of the Dynamical\u000a        Systems Toolbox interface to Airbus' own software tools was\u000a      sub-contracted and internal training sessions have been held. The new tool\u000a      is presently in use by members of the Future Project Office, the Process,\u000a      Methods &amp; Tools Department and the Landing Gear group; the latter work\u000a      on current aircraft projects, as well as the exploration of new landing\u000a      gear and aircraft concepts. It is estimated that the new tool has made\u000a      significant savings in terms of man hours during 2012[c]. Airbus UK have\u000a      described the impact of the research as approximately 80% reduction in\u000a      time and associated costs [c] for ground maneuverability analysis and\u000a      specified that the estimate of the saving is made by comparing the time\u000a      taken to conduct global assessments using bifurcation and other dynamical\u000a      systems techniques versus the time to obtain exhaustively a large number\u000a      of point solutions. This is important because it allows the efficient use\u000a      of numerical simulations for new designs. The engineer from the Modelling\u000a      &amp; Simulations Methods and Tools group writes, \"The tool provides a\u000a      method for de-risking design-decisions; the trade-off being between the\u000a      impact of a risk occurring versus the likelihood of the risk occurring\"\u000a      [b].\u000a    The research formed the basis of the enhanced capability introduced by\u000a      Airbus Process, Methods &amp; Tools via the development of fully\u000a      parameterized and validated models, the development of the Dynamical\u000a        Systems Toolbox and the demonstration of the approach via dedicated\u000a      case studies. The research has been supported and sponsored by Airbus, via\u000a      direct interaction between the industry and university based researchers.\u000a      The research has grown considerably in reaction to demand from the company\u000a      &#8212; now also including novel testing strategies and the characterization of\u000a      aircraft upset scenarios in flight. Regular research meetings (about every\u000a      six months) were conducted at University of Bristol and at Airbus at which\u000a      further dissemination could occur, together with the identification of new\u000a      projects. Additionally, there have been several subsequent PhD students\u000a      associated with this research project who have each spent extended\u000a      placements at the company. Evidence of the value of this project to Airbus\u000a      comes in part from the large number of grants and support they have\u000a      provided (&#163;415K in direct funding, plus funding in kind) [d].\u000a    There is also impact from the work in terms of staff training in research\u000a      and recruitment in Airbus. For example, a member of staff was seconded\u000a      from Airbus to perform PhD research at the University of Bristol. He is\u000a      now one of the main advocates of the use of dynamical systems methods and\u000a      is helping to lead their introduction within Airbus. In addition, a former\u000a      postdoctoral research assistant (held 2007-2010) at Bristol was hired by\u000a      Airbus and now works as Airbus Model Developer &#8212; Physical Systems at\u000a      Airbus' Design Analysis.\u000a    A further impact of this research has been the embedding of nonlinear\u000a      dynamics approaches within Airbus. It is expected that usage of the DST\u000a      will increase amongst other departments when research through currently\u000a      on-going PhD projects near completion (there are several on-going Bristol\u000a      EPSRC Mathematics CASE studentships in partnership with Airbus). There are\u000a      plans for further training courses within Airbus and improvements in the\u000a      DST interface. Airbus also plans to set up a more extensive training\u000a      course with the University of Bristol.\u000a    ","ImpactSummary":"\u000a    Evaluating the ground-based manoeuvrability of large aircraft is time\u000a      consuming and costly if explored though industry-developed complete models\u000a      of ground dynamics. Research by Krauskopf and colleagues from the\u000a      University of Bristol has shown that applying methods from dynamical\u000a      systems allow these dynamics to be investigated efficiently and with\u000a      considerable precision. This approach, and the related purpose-developed\u000a      software, Dynamical Systems Toolbox, have been adopted by Airbus.\u000a      It is now fully incorporated in the Airbus Methods and Tools\u000a      portfolio as a supported tool for the evaluation of proposed works and new\u000a      designs. The research delivers considerable savings in time and costs for\u000a      the company. Additionally, this programme of research has delivered\u000a      research training for Airbus employees and one, who studied for PhD with\u000a      Krauskopf, now leads the Airbus development and implementation of these\u000a      mathematical techniques which are being disseminated more widely within\u000a      the company. There continue to be Bristol EPSRC CASE PhD studentships in\u000a      collaboration with Airbus co-supervised by Krauskopf (7 in the assessment\u000a      period).\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Bristol\u000a    ","Institutions":[{"AlternativeName":"Bristol (University of)","InstitutionName":"University of Bristol","PeerGroup":"A","Region":"South West","UKPRN":10007786}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] Thota., P., Krauskopf, B and Lowenberg, M. 2009, Interaction\u000a        of torsion and lateral bending in aircraft nose landing gear shimmy,\u000a      Nonlinear\u000a          Dynamics 57(3) 455-467; DOI: 10.1007\/s11071-008-9455-y\u000a    \u000a\u000a*[2] Coetzee, E., Krauskopf, B. and Lowenberg, M.,2010 Application\u000a        of bifurcation methods for the prediction of low-speed aircraft ground\u000a        performance, Journal\u000a          of Aircraft (American Institute of Aeronautics and\u000a        Astronautics) 47(4) 1248-1255; DOI: 10.2514\/1.47029\u000a    \u000a\u000a*[3] Rankin, J., Krauskopf, B., Lowenberg, M. and Coetzee, E. 2010, Nonlinear\u000a        analysis of lateral loading during taxiway turns, Journal\u000a          of Guidance, Control, and Dynamics (American Institute of\u000a        Aeronautics and Astronautics) 33(6) 1708-1717;\u000a      DOI:10.2514\/1.50626\u000a    \u000a\u000a*[4] Knowles, J.A.C., Krauskopf, B. and Lowenberg, M. 2011, Numerical\u000a        continuation applied to landing gear mechanism analysis, Journal\u000a          of Aircraft (American Institute of Aeronautics and\u000a        Astronautics) 48(4) 1254-1262; DOI: 10.2514\/1.C031247\u000a    \u000a\u000a[5] Coetzee, E., Krauskopf, B. and Lowenberg, M. 2011 Analysis\u000a        of medium-speed runway exit manoeuvres, Journal\u000a          of Aircraft (American Institute of Aeronautics and\u000a        Astronautics) 48(5) 1553-1564; DOI: 10.2514\/1.C031276\u000a    \u000a\u000a[6] Rankin, J.,Desroches, M.,. Krauskopf, B. and Lowenberg, M. 2011,\u000a\u0009Canard cycles in\u000a        aircraft ground dynamics, Nonlinear\u000a          Dynamics 66(4) 681-688; DOI: 10.1007\/s11071-010-9940-y\u000a    \u000a* references that best indicate the quality of the underpinning research.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"9","Level2":"6","Subject":"Electrical and Electronic Engineering"}],"Sources":"\u000a    [a] Airbus internal report Strategic Research into Dynamical Systems\u000a        Methods: A roadmap from 2009 to 2013. Details from this report are\u000a      available from a Future Projects Engineer, Airbus Operations Ltd.\u000a      Corroborates contribution of methodology developed at the University of\u000a        Bristol to Airbus's strategy for the continued future use of dynamical\u000a        systems methods.\u000a    [b] Expert (Modelling &amp; Simulation Methods and Tools, Airbus\u000a      Operations Ltd. Can be contacted to corroborate the contribution of\u000a        the research to Airbus' decision making around risk.\u000a    [c] Future Projects Engineer, Airbus Operations Ltd.\u000a      Can be contacted to corroborate claim of 80% reduction in time and cost\u000a        for Airbus.\u000a    [d] This research activity received a total of &#163;805K of funding, of which\u000a      &#163;415K from industrial partner Airbus, as follows:\u000a    (i) PI: Krauskopf, Co-I: Lowenberg. Title: Bifurcation analysis of\u000a      nonlinear ground handling of aircraft. Sponsor: EPSRC Maths Case Award\u000a      with Airbus in the UK. Period: October 2006 - March 2010 to support PG\u000a      Rankin. Value: &#163;65K (EPSRC) plus &#163;25K (Airbus)\u000a    (ii) PI: Krauskopf, Co-I: Lowenberg. Title: Analysis of nonlinear ground\u000a      handling models. Sponsor: Airbus in the UK. Period: April 2007 - March\u000a      2010 to support PDRA Thota. Value: &#163;220K\u000a    (iii) PI: Krauskopf, Co-I: Lowenberg. Title: Modelling and nonlinear\u000a      analysis of landing gear and airframe interactions. Sponsor: Airbus in the\u000a      UK. Period: March 2007 - June 2011 to support PhD work of Coetzee (while\u000a      full-time employee of Airbus). Value: salary cost for Coetzee plus &#163;25K\u000a      (research support)\u000a    (iv) PI: Krauskopf, Co-I: Lowenberg. Title: Dynamics of statically\u000a      indeterminate landing gear systems. Sponsor: EPSRC Maths Case Award with\u000a      Airbus in the UK. Period: October 2008 - March 2012 to support PG Knowles.\u000a      Value: &#163;65K (EPSRC) plus &#163;25K (Airbus)\u000a    (v) PI: Krauskopf, Co-I: Lowenberg. Title: Dynamics of aircraft main\u000a      landing gears. Sponsor: EPSRC Maths Case Award with Airbus in the UK.\u000a      Period: October 2009 - March 2013 to support PG Howcroft. Value: &#163;65K\u000a      (EPSRC) plus &#163;30K (Airbus)\u000a    (vi) PI: Neild, Co-I: Krauskopf. Title: Dynamic substructure testing\u000a      strategies in aerospace. Sponsor: EPSRC Industrial Case Award with Airbus\u000a      in the UK. Period: August 2010 - January 2014 to support PG Terkovics.\u000a      Value: &#163;65K (EPSRC) plus &#163;30K (Airbus)\u000a    (vii) PI: Lowenberg, Co-I: Krauskopf. Title: Investigation of airliner\u000a      upset and upset recovery dynamics using bifurcation analysis. Sponsor:\u000a      EPSRC Maths Case Award with Airbus in France. Period: October 2010 - March\u000a      2014 to support PG Gill. Value: &#163;65K (EPSRC) plus &#163;30K (Airbus)\u000a    (viii) PI: Krauskopf, Co-I: Lowenberg, Neild. Title: Investigation of\u000a      coupled landing gear and fuselage vibrations. Sponsor: EPSRC Maths Case\u000a      Award with Airbus in the UK. Period: October 2011 - March 2015 to support\u000a      PG Kewley. Value: &#163;65K (EPSRC) plus &#163;30K (Airbus).\u000a    Corroborates Airbus's investment in underpinning research.\u000a    ","Title":"\u000a    Bristol's research in dynamical systems methods are adopted and made\u000a      operational within Airbus UK to develop cost-saving, high-precision\u000a      modelling platforms\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Aircraft are designed to fly and, hence, are not optimised for ground\u000a      operations in the way a car or other ground vehicles may be. Nevertheless,\u000a      a passenger aircraft needs to operate fast, reliably and safely on the\u000a      ground in order to ensure its overall commercial success. Prior to this\u000a      research, the standard methodology of aircraft manufacturers was to\u000a      conduct massive and expensive numerical simulations of industry-tested and\u000a      parametrised models for aspects of aircraft motion to evaluate the ground\u000a      performance of aircraft as part of their design, evaluation and\u000a      certification.\u000a    The underpinning research consists of the systematic mathematical\u000a      analysis of several of these models using dynamical systems techniques to\u000a      study their nonlinear dynamics and the stabilities of particular aspects\u000a      of their motion. This methodology avoids expensive brute-force numerical\u000a      simulations. The models studied represent different features of ground\u000a      manoeuvres and were developed in collaboration with Airbus. The behaviour\u000a      of these nonlinear mathematical models was evaluated using both analytical\u000a      and numerical techniques and their parameter dependence was analysed using\u000a      software, purpose-developed by Krauskopf and colleagues, the Dynamical\u000a        Systems Toolbox, which runs under MATLAB (a widely used numerical\u000a      computing environment and programming language). This enabled the\u000a      introduction of numerical continuation to follow solutions and their\u000a      changes of stability with varying parameters. Importantly, this approach\u000a      could be used within an industrial setting.\u000a    Specific projects were as follows (references from section 3 in\u000a      parentheses):\u000a    \u000a      a fully parameterized model of a mid-size aircraft was used to\u000a        evaluate aircraft ground turning and provide guidelines for the safe\u000a        operation limits and maximum load factors under different conditions.\u000a        The main motivation for this work was to evaluate the suitability of the\u000a        existing Federal Aviation Regulations for lateral loads experienced\u000a        during turning maneuvers [3].\u000a      bifurcation studies of shimmy oscillations of an aircraft nose landing\u000a        gear. A mathematical model with torsional and lateral bending modes that\u000a        are coupled through a wheel-mounted elastic tyre was developed and\u000a        studied. The bifurcation analysis was performed in terms of the forward\u000a        velocity &amp; vertical force and was used to find regions of stable\u000a        torsional and stable lateral shimmy oscillations [1].\u000a      research on ground handling studies of aircraft with more than three\u000a        sets of wheels, in particular, the Airbus A380 model. This led to the\u000a        development of the Dynamical Systems Toolbox and demonstrated\u000a        its usefulness with the evaluation of low-, medium- and high-speed\u000a        ground manoeuvers of an A380 in comparison with an A320 [2,5,6].\u000a      treatment of the dynamics of a deployment cycle of an aircraft landing\u000a        gear, showing the usefulness of dynamical systems methods with a study\u000a        of actuator placement for nose and main landing gears [4].\u000a    \u000a    The various strands of this research programme were undertaken in Bristol\u000a      from 2006 onwards and have led to a series of scientific publications in\u000a      which the underlying mathematics, and its application to aircraft\u000a      dynamics, are explained. The publications appeared in high quality applied\u000a      mathematics journals. The research was led by Krauskopf (University of\u000a      Bristol until 2011, University of Auckland from 2011), Lowenberg\u000a      (University of Bristol) and Neild (University of Bristol), together with\u000a      Bristol postgraduate students and postdoctoral research assistants Rankin,\u000a      Knowles, Thota and Desroches, and industrial contacts and supervisors at\u000a      Airbus [b,c]. There are currently three EPSRC Case students in Bristol\u000a      still co-supervised by Krauskopf.\u000a    "},{"CaseStudyId":"40252","Continent":[{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2186224","Name":"New Zealand"}],"Funders":["Wellcome Trust","Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Wavelet and multiscale methods are beginning to permeate many areas of\u000a      application. This case study deals with impact generated directly from\u000a      research carried out in Bristol, in the areas of finance, economics,\u000a      marketing and industry. The features that have made wavelets valuable in\u000a      academia (speed, efficiency, performance, sparsity, theoretical\u000a      guarantees) have also made them essential for many real-world applications\u000a      where data can be nonstationary (time-varying statistical properties) or\u000a      exhibit sharp changes (e.g. boundaries in images), and where its analysis\u000a      must be fast and reliable.\u000a    In all cases described below the impact was delivered through activities\u000a      such as (i) technical report publication (ii) presentation at\u000a      international conferences, (iii) refereed journal publication (iv) through\u000a      word of mouth transmitted by graduate students, postdocs and colleagues\u000a      and, importantly, (v) through the release of high-quality free software\u000a      such as the wavethresh [6] package, available on the Comprehensive R\u000a      Archive Network (software archive). The wavethresh package [6] is written\u000a      in the R language\/system which is the one of the major statistical\u000a      software packages and is both a statistical software environment and\u000a      programming language.\u000a    Core Inflation Measures\u000a    Obtaining key measures of critical economic time series is an extremely\u000a      important and challenging task. Merely collecting data to estimate\u000a      inflation, for example, is expensive and time-consuming. Assessing the\u000a      data and deriving estimates such as GDP or core inflation requires careful\u000a      thought and delicate statistical analysis. Such economic measures have an\u000a      enormous impact on decision- making at the central government level, in\u000a      the wider economy and also for the general public by influencing their\u000a      expectations concerning the state and future of the economy. It is,\u000a      however, immensely difficult to quantify precisely the benefits of\u000a      estimating inflation, much less the value of any specific method that\u000a      might be involved in trying to measure or control it.\u000a    Central banks make use of state-of-the-art denoising methods to improve\u000a      estimates of core inflation and other economic series. The Reserve Bank of\u000a      New Zealand (RBNZ) Discussion Paper [a] highlights that New Zealand was\u000a      the first world economy to introduce inflation targeting, dramatically\u000a      improving its inflation performance from the worst to among the middle of\u000a      the pack (Organisation for Economic Co-operation and Development\u000a      19-country average). The aim of inflation targeting is to achieve price\u000a      stability. Report [a] makes it clear that \"one of the main problems with\u000a      measuring inflation concerns the presence of short-lived shocks that\u000a      should not influence policy makers' actions.\" Report [a] further states:\u000a      \"Wavelets were specifically designed for isolating short-lived phenomena\u000a      from long term trends in a signal\". Report [a] then compares several\u000a      wavelet shrinkage techniques including the complex-valued denoising method\u000a      [4], linear wavelet shrinkage (an option in [6]) and several existing\u000a      econometric methods. Report [a] also pays careful attention to issues such\u000a      as use of nondecimated wavelets[1], boundary conditions and wavelets'\u000a      operation as a real-time tool. Report [a]'s conclusion is that \"our\u000a      wavelet measure has the performance, credibility and perspicuity needed\u000a      for it to be a suitable tool for central banks and other policy makers. We\u000a      believe that wavelets are a very promising avenue for further research\u000a      into the analysis and forecasting of economic and financial data\".\u000a      wavethresh [6] was one of the key software tools used in this work. RBNZ's\u000a      wavelet measure developed up to May 2009 in [a] was actually operationally\u000a      used by RBNZ to assist inflation targeting [b]. Hence, our research not\u000a      only influenced RBNZ to explore multiscale methods and wavelet shrinkage\u000a      methods such as those described in [4], but was used practically to feed\u000a      into a mechanism to control inflation in New Zealand, [b].\u000a    Impact Through Software\u000a    Research in [1] has become influential partly by being incorporated into\u000a      the MATLAB software package. MathWorks, the company that produces MATLAB,\u000a      implemented both the one- and two- dimensional stationary wavelet\u000a      transforms (swt and swt2) whose help pages directly\u000a      reference [1]. For reasons of customer confidentiality MathWorks are\u000a      unable to name their customers. However, MathWorks have directly confirmed\u000a      in 2012, [e], that these transforms are being used for multiscale analysis\u000a      of 1D signals in oceanographic time series and parity space analysis for\u000a      sensor validation (which uses the residual signal in analysing a\u000a      distributed network of sensors to determine whether all sensors are\u000a      functioning properly). Also, for 2D signals they are being use for medical\u000a      imaging (specifically endoscopic imaging), security inspection systems\u000a      (processing of images in homeland security applications to identify\u000a      harmful contents) and fault detection in manufacturing.\u000a    Increasing Marketing Accuracy\u000a    AC Nielsen is a large multinational market research company. Evidence of\u000a      their utilizing multiscale methods for improving accuracy in marketing\u000a      forecasts can be found in [c] and in use post-2008 [c]. The wavelet packet\u000a      transfer models introduced by [3] were used to incorporate wavelet packet\u000a      factors into a regression model used to forecast future sales of\u000a      well-defined brands of fast moving consumer goods. The wavelet-based\u000a      methods produced significantly reduced errors in future forecasting. These\u000a      improvements are economically valuable for the clients of Nielsen and\u000a      Nielsen itself, as well as reducing waste and energy costs from producing\u000a      products at the wrong times. Document [c] also demonstrates how they used\u000a      the wavelet periodogram invented in [2] to identify large variance\u000a      contributions of market response time series that were not previously\u000a      attributed to certain marketing independent variables, such as sales price\u000a      and patterns of promotional and advertising activity, and not captured by\u000a      existing algorithms.\u000a    Spatial Risk Measures and Denoising With Wavelets\u000a    [text removed for publication]\u000a    Texture Analysis in Industry\u000a    Simulating, modelling and analysing texture is an important task in many\u000a      areas including manufacturing industry. Through joint work with\u000a      researchers at Unilever texture analysis modelling and simulation methods\u000a      were developed that were specifically applied to hair product development\u000a      and new ways of analysing fabric pilling, the surface defects of textiles\u000a      caused by wear, under different conditions (e.g. how hair responds to\u000a      different hairsprays or how fabrics respond to different\u000a      detergent\/treatment regimes). The impact was delivered through an EPSRC\u000a      CASE studentship and technology transfer of research methodologies [2,5]\u000a      and software from Bristol into Unilever. The key theoretical development\u000a      was the creation of multidimensional locally stationary wavelet processes\u000a      which benefits texture modelling by (i) enabling texture features to\u000a      change their nature over the spatial domain (unlike existing stationary\u000a      techniques which force statistical constancy of textures) and (ii) the\u000a      wavelet part permitting sharp changes of texture, which is hard to do with\u000a      classical Fourier texture techniques. A key commercial advantage was that\u000a      the multiscale techniques permitted a new objective measure of texture to\u000a      discriminate between good or bad conditions. The Project Leader Advanced\u000a      Measurement and Data Modelling, Unilever R&amp;D, Port Sunlight [f] has\u000a      commented \"The work initially developed there has informed my ideas of\u000a      what that particular technology can do for us and internally rewritten\u000a      version of the code is still in use within the company\". Much of the\u000a      original code is now contained within the freeware R package LS2W which\u000a      depends on the wavethresh [6] package, both on the Comprehensive R Archive\u000a      Network repository. Our work resulted in a follow-on grant from Unilever\u000a      for &#163;20k that supported postdoctoral work on multiscale methods to analyse\u000a      body dynamics data. Overall, Unilever supported this programme of work for\u000a      five years. The research produced a trained student who subsequently\u000a      worked for Shell Research for several years and then entered academia,\u000a      carrying out further funded related research projects with Unilever and\u000a      other companies.\u000a    ","ImpactSummary":"\u000a    Wavelets and multiscale methods were introduced and rapidly became\u000a      popular in scientific academic communities, particularly mathematical\u000a      sciences, from the mid-1980s. Wavelets are important because they permit\u000a      more realistic modelling of many real-world phenomena compared to previous\u000a      techniques, as well as being fast and efficient. Bristol's research into\u000a      wavelets started in 1993, has flourished and continues today. Multiscale\u000a      methods are increasingly employed outside academia. Examples are given\u000a      here of post-2008 impact in central banking, marketing, finance, R&amp;D\u000a      in manufacturing industry and commercial software, all originating from\u000a      research at Bristol. Much of the impact has been generated from the\u000a      original research via software. This software includes freeware,\u000a      distributed via international online repositories, and major commercial\u000a      software, such as Matlab (a preeminent numerical computing environment and\u000a      programming language with over one million users worldwide).\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Bristol\u000a    ","Institutions":[{"AlternativeName":"Bristol (University of)","InstitutionName":"University of Bristol","PeerGroup":"A","Region":"South West","UKPRN":10007786}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a*[1] Nason, G.P. and Silverman, B.W. (1995) The Stationary Wavelet\u000a      Transform and some Statistical Applications. In Antoniadis, A. and\u000a      Oppenheim, G. (eds) \"Wavelets and Statistics\", Lecture Notes in\u000a        Statistics, 103, 281-300. DOI: 10.1007\/978-1-4612-2544-7_17\u000a    \u000a\u000a[2] Nason, G.P., von Sachs, R. and Kroisandt, G. (2000) Wavelet processes\u000a      and adaptive estimation of the evolutionary wavelet spectrum. J. R.\u000a        Statist. Soc. Series B, 62, 271-292. DOI:\u000a      10.1111\/1467-9868.00231\u000a    \u000a\u000a*[3] Nason, G.P. and Sapatinas, T. (2002) Wavelet packet transfer\u000a      function modelling of nonstationary time series. Statistics and\u000a        Computing, 12, 45-56. DOI: 10.1023\/A:1013168221710\u000a    \u000a\u000a*[4] Barber, S. and Nason, G.P. (2004) Real nonparametric regression\u000a      using complex wavelets. J. R. Statist. Soc. Series B, 66,\u000a      927-939. DOI: 10.1111\/j.1467-9868.2004.B5604.x\u000a    \u000a\u000a[5] Eckley, I.A., Nason, G.P. and Treloar, R. (2010). Locally stationary\u000a      wavelet fields with application to the modeling and analysis of image\u000a      texture. J. R. Statist. Soc. Series C, 59, 595-616. DOI:\u000a      10.1111\/j.1467-9876.2009.00721.x\u000a    \u000a\u000a[6] Nason, G.P. (1993, 2000, 2008) wavethresh (versions 2, 3 and 4).\u000a      Available from the Comprehensive R Archive Network (software archive).\u000a      URL: http:\/\/cran.rproject.org\/web\/packages\/wavethresh\/index.html\u000a    \u000a* references that best indicate the quality of the underpinning research.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000a    [a] Baqaee, David (2009) Using wavelets to measure core inflation: the\u000a      case of New Zealand. Reserve Bank of New Zealand Discussion Paper Series,\u000a      DP2009\/05, ISSN 1177-7567. URL: http:\/\/www.rbnz.govt.nz\/research\/discusspapers\/dp09_05.pdf\u000a      [This report corroborates how wavelets were used by RBNZ to measure\u000a        core inflation.]\u000a    [b] RBNZ, Research Manager. Can be emailed to corroborate. [This\u000a        research manager may be contacted to corroborate the use of wavelets by\u000a        Baqaee and others at RBNZ.]\u000a    [c] Personal communication from Senior Analytic Consultant, ACNielsen\u000a      Analytic Consultancy Services, Emerging Markets: , (2011) and. \"Increasing\u000a      Marketing Accuracy: Wavelet based forecasting techniques\", an \"ACNielsen\"\u000a      publicity document presented at the ESOMAR Congress Conference, London,\u000a      September 2006. (23 pages). Can be supplied on request or see URL:\u000a      http:\/\/www.esomar.org\/web\/research_papers\/Conjoint-Analysis_1426_Increasing-marketing-accuracy-\u000a        br-Wavelet-based-forecasting-techniques.php.\u000a      [The publicity document corroborates that wavelet techniques for market\u000a        forecasts based on [2,3] were used at ACNielsen and the personal\u000a        communication corroborates that they continued to be used up until at\u000a        least 2009.]\u000a    [d] [text removed for publication]\u000a    [e] Mathworks Inc, (2012) Signal Processing and Communication Product\u000a        Group have supplied information on the use of the stationary wavelet\u000a      transform and Mathwork's MATLAB help pages for swt and swt2\u000a      functions. Can be supplied on request.\u000a      [Summary: used to corroborate Impact Through Software.]\u000a    [f] Unilever R&amp;D Port Sunlight. Project Leader Advanced\u000a        Measurement and Data Modelling. Letter from Unilever to confirm the\u000a      use of wavelets in texture analysis.\u000a      [Summary: used to corroborate Texture Analysis in Industry.]\u000a    \u000a    ","Title":"\u000a    Bristol's research into multiscale methods enables more realistic\u000a      modelling of real world phenomena providing benefit to industry,\u000a      government and society.\u000a    ","UKLocation":[{"GeoNamesId":"2654675","Name":"Bristol"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research consists of a body of work carried out at the\u000a      University of Bristol by Bernard Silverman, FRS (Professor of Statistics)\u000a      from 1993-2003 and Guy Nason (initially Lecturer, now Professor of\u000a      Statistics) from 1993, supported by many graduate students and\u000a      postdoctoral researchers, and funded by a variety of sources including the\u000a      Engineering and Physical Sciences Research Council (EPSRC), the Royal\u000a      Society, Ministry of Defence, Unilever, Wellcome and the Government\u000a      Communications Headquarters (GCHQ).\u000a    Most of Bristol's early work in this area was in the development of\u000a      wavelet transforms, notably the stationary wavelet transform in 1995 [1]\u000a      and enabling software packages such as wavethresh [6], in continuous\u000a      development since 1993. These developments supported a growing research\u000a      effort in addressing problems in statistical curve estimation now known as\u000a      wavelet shrinkage. The basic goal of wavelet shrinkage is to estimate a\u000a      signal from data where the signal is contaminated by additive noise.\u000a      Wavelet shrinkage operates by (i) performing the forward wavelet\u000a      transform, (ii) shrinking or thresholding the coefficients and (iii)\u000a      applying the inverse wavelet transform. Wavelet shrinkage is especially\u000a      useful for problems where the signal has discontinuities or other\u000a      irregularities that often occur in applications (e.g. edges in images) and\u000a      there are deep mathematical reasons for this. These characteristics have\u000a      made wavelets effective in real applications such as in image compression\u000a      (for example, JPEG 2000) or fingerprint compression (the FBI fingerprint\u000a      database). The Bristol Group, led by Silverman and Nason, has made several\u000a      contributions: methods for correlated, irregularly spaced and network data\u000a      and innovations such as the stationary wavelet transform, cross-validation\u000a      for wavelets and Bayesian wavelet shrinkage methods. Below we highlight\u000a      how our methods (nondecimated wavelets and complex-valued wavelets [1,4])\u000a      were used to improve core inflation modelling by the Reserve Bank of New\u000a      Zealand and spatial risk measurement used by the Bank of America.\u000a    In time series analysis, stationary models, where the underlying\u000a      statistical properties remain invariant with time, are ubiquitous.\u000a      Unfortunately, stationary models are not appropriate for many real-world\u000a      data sets and this is becoming especially apparent with the advent of the\u000a      `big data revolution'. To address this Nason, with collaborators von Sachs\u000a      and Kroisandt in Germany, created the locally stationary wavelet models,\u000a      [2], which have the ability to adapt to changing structure and even fast\u000a      changes. Such models are capable of more realistic modelling and more\u000a      accurate forecasts and have reinforced the notion that Fourier is not\u000a      canonical for nonstationary modelling. We show below how these new\u000a      flexible models, developed by Nason together with postgraduate student\u000a      Eckley and industrialist Treloar [5], have impacted industrial texture\u000a      analysis and analysis and forecasting in marketing.\u000a    "},{"CaseStudyId":"40255","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"1605651","Name":"Thailand"},{"GeoNamesId":"2510769","Name":"Spain"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"1861060","Name":"Japan"}],"Funders":["Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000a    In the activities described below, Band, Conrey, Keating and Snaith have\u000a      used the intriguing and accessible nature of their research to encourage\u000a      audiences in several countries to engage further in mathematics and to see\u000a      the human side of mathematical investigation.\u000a    Royal Society Summer Science Exhibition: Quantum mechanics has a\u000a      counter-intuitive feel that makes it intriguing to diverse audiences. The\u000a      research on quantum graphs [4,5] was demonstrated to the public through a\u000a      hands-on exhibit in the Royal Society Summer Science Exhibition, which\u000a      took place between 5 and 10 July 2011. The Bristol team, consisting of\u000a      postgraduate and undergraduate students from the mathematics and physics\u000a      departments and led by Band, had to compete to win a place at the\u000a      exhibition.\u000a    The exhibition \"showcases the most exciting cutting-edge science and\u000a      technology research\" [c] and in 2011 attracted large and diverse\u000a      audiences, with a total of 13,812 visitors. It \"provides a unique\u000a      opportunity for members of the public to interact with scientists\" [c].\u000a      This face-to-face interaction is an effective way to demonstrate ideas and\u000a      to highlight the human side of research. The Summer Science Exhibition\u000a      achieved excellent media coverage in 2011, with news pieces appearing in\u000a      most of the national broadsheet papers including The Financial Times, The\u000a      Times, The Daily Mirror, The Sunday Express, The Daily Mail and The\u000a      Independent. The exhibition was also featured in The Washington Post\u000a      online.\u000a    The effect of the exhibition extended beyond those visitors who attended\u000a      the event. The Summer Science Exhibition website had 54,639 Visits, from\u000a      35,993 Absolute Unique Visitors resulting in 179,909 Page Views. In\u000a      particular, the page describing Bristol's \"Can you hear the shape of a\u000a      graph\" exhibit [d] has been viewed by 2,346 unique visitors. In addition,\u000a      a New Scientist blog [e] about this research and the exhibit has been\u000a      viewed by 5,300 people, and two YouTube movies of the exhibit in action\u000a      have been viewed by 2,000 people [f]. These figures indicate the number of\u000a      people reached by this activity even after the exhibition.\u000a    Public lectures: Audiences have been stimulated to explore\u000a      mathematics through the many public talks Conrey and Snaith have given\u000a      about their research, in particular [1,2,3], both to schoolchildren and in\u000a      evening lectures to general audiences. Information follows about three\u000a      representative talks for which audience feedback is available.\u000a    At a public lecture on his research at the International Centre for\u000a      Mathematical Sciences in Edinburgh in January 2013, Conrey introduced\u000a      number theory in a historical context, then brought the audience up to\u000a      date with current research. Audience feedback gives evidence of stimulated\u000a      interest [i]: \"made me want to go out and become more informed\";\u000a      \"Excellent evening! I enjoyed some of the details of the early research by\u000a      Gauss and others. It was also interesting how the topic is being\u000a      approached by current mathematicians\"; \"Very interesting subject. ... Have\u000a      started doing some modelling\/analysis on my own and will buy a book on the\u000a      subject\"; \"Very interesting. Was understandable for people below\u000a      undergraduate level (I am at school studying for standard grades). It\u000a      encouraged me to read about the subject afterwards\"; \"I intend to read up\u000a      some more on the Riemann Hypothesis.\"\u000a    At an evening lecture to the general public for the British Science\u000a      Association in 2010, Snaith's talk on her research in number theory and\u000a      random matrices received these comments [i]: \"a very complex concept was\u000a      explained very well indeed &#8212; one of the most interesting talks; it\u000a      answered some questions I was asking about!\"; \"good lecture for an\u000a      introduction to the subject &#8212; lots of questions to go and read about!\"\u000a    In June 2013, Snaith visited 21 AS-level students at Strode College,\u000a      Somerset, to speak about her research [2,3]. Her aim was to reveal the\u000a      excitement of original mathematics and how some of the keys to progress\u000a      are mathematical interactions and collaborations, as this is a side of\u000a      mathematics that is not so apparent when the subject is studied at school.\u000a      Comments from students indicate that this aim was successful [i]: \"there\u000a      is more to learn about maths than I thought\"; \"showed how mathematicians\u000a      actually work\"; \"it has changed my view on the sort of problems they\u000a      solve\". Also, seven students indicated that they would now be more\u000a      inclined to study mathematics, with comments such as: \"I didn't realise\u000a      maths went much further than Newton or Euclid\"; \"there are more areas of\u000a      mathematics to study than I thought\". These comments demonstrate that\u000a      being exposed to current mathematical research has changed students' views\u000a      on the subject and made a third of them more likely to study maths\u000a      further.\u000a    The talks, of which the above are examples, inspire audiences to engage\u000a      with maths both during and after the events, as well as to reach a clearer\u000a      understanding of what mathematicians do. Such events have often reached\u000a      large audiences: for example, Conrey's keynote address at the 2012 opening\u000a      of the \"Imaginary\" exhibition at Parque de las Ciencias [b] in Granada,\u000a      Spain, is mentioned in five blogs and the Granada newspaper. Snaith's\u000a      talks have included speaking to 900 final-year school students in the\u000a      \"mathematics in action\" programme in London, and she was chosen to give\u000a      one of the two London Mathematical Society Popular Lectures in 2009.\u000a    Science project: Impact is achieved not only through dissemination\u000a      of current research but also through enabling students to participate in\u000a      research. The following example demonstrates the profound effect on the\u000a      students involved. Conrey has co-authored a paper [j] on smooth numbers\u000a      with two high school students. Their presentation on the subject won a\u000a      Grand Prize at the Synopsis Silicon Valley Science Fair in California,\u000a      which involved 130,000 high school students. One of the students wrote\u000a      this about the experience: \"Our research for the math project was both\u000a      enthralling and rewarding. I had previously studied a number of math\u000a      concepts outside school, but this was the first time I was part of\u000a      developing and creating a new method for solving a problem. Experiencing\u000a      this new level of learning where I could discover something entirely new\u000a      was fantastic. The project has greatly increased my enthusiasm for\u000a      research and has given me ideas for other math mysteries I'd like to\u000a      investigate.\" By participating in current research the students'\u000a      perception of mathematics changed. The second student said: \"I would have\u000a      to say that the work we did definitely changed my view about mathematical\u000a      research. It was interesting seeing how mathematics could be applied in\u000a      the world past high school. I've always enjoyed math but didn't really\u000a      understand its direct applications past high school besides things like\u000a      engineering and business. I really enjoyed seeing the kind of work done by\u000a      mathematicians.\"\u000a    TV documentary: Conrey and Keating (both from the Bristol\u000a      mathematics department) and Berry (Bristol physics) were interviewed about\u000a      their research on random matrix theory and number theory for a Japanese TV\u000a      documentary (2009), \"The Cosmic Code Breakers\", subsequently dubbed into\u000a      English. This work reached an international audience (and TV viewers are\u000a      potentially a different audience from those who choose to attend public\u000a      lectures). The work of the Bristol researchers, in particular [2,3],\u000a      formed a central component of the programme. This multi-award-winning\u000a      documentary was shown at a number of film festivals in 2010 and won the\u000a      Pierre-Gilles de Gennes Prize at the Pariscience Festival for \"spreading\u000a      scientific knowledge through an original scenario\". This festival was\u000a      attended by 8,200 people, including 2,700 schoolchildren; 242 people\u000a      attended the screening. The film also won a Silver Dragon Award at the\u000a      China International Conference of Science &amp; Education Producers,\u000a      attended by over 200 professionals from the industry. In addition, it\u000a      received the Grand Prix Japan Prize from the Japan Broadcasting\u000a      Corporation &#8212; the top prize, entered in the continuing education category,\u000a      from 360 entries &#8212; and received the following praise: \"This outstanding\u000a      program holds the tension, curiosity and interest of the audience.\u000a      Interviews, brilliant graphic visuals and audio techniques come together\u000a      in a comprehensible and thrilling narrative of mystery-solving. The\u000a      dramatic story of prime numbers affected and changed the view of all of\u000a      the jury &#8212; even those afraid of math\". It was also nominated for a Banff\u000a      World Television Festival Rockie Award, and shown at the Goethe Institut\u000a      Science Film Festival, which was attended by 128,000 people in Thailand.\u000a      The programme has been syndicated by American Public Television since\u000a      January 2011.\u000a    Popular science books:The Japanese popular science book \"Primes,\u000a      zeta functions and arithmetic quantum chaos\", by Shin-ya Koyama [a],\u000a      describes research in number theory and random matrix theory. The fact\u000a      that Conrey, Keating and Snaith are three of the six mathematicians with\u000a      large photos on the cover is an indication of how prominent their work is\u000a      in this field and in the book. Their work, including [2,3], appears in it\u000a      seven times and forms a substantial part of Chapter 15. This book has sold\u000a      about 5,000 copies and at the end of 2011 was the best-selling popular\u000a      science and technology book on Amazon Japan.\u000a    Four English language popular science books, produced around 2004 but\u000a      still available and having impact, are also relevant here. In \"The Music\u000a      of the Primes\", by Marcus du Sautoy [a], which is consistently in the top\u000a      ten bestsellers on Amazon UK for History of Mathematics and Number Theory,\u000a      Bristol research is described in detail nine times and plays a significant\u000a      role in the story the book tells. Schoolchildren who visit the School of\u000a      Mathematics often quote this book as a factor that inspired them to\u000a      consider studying the subject. One reader on Amazon states (March 2010):\u000a      \"The book is packed with fascinating details about eminent mathematicians,\u000a      their eccentricities, and sometimes madness. My maths interests are mainly\u000a      in its applications, and I've tended to regard pure maths research as an\u000a      intellectual game, but this book made me want to revisit pure maths.\" [g]\u000a    In the popular book \"The Riemann Hypothesis\", by Karl Sabbagh [a], Brian\u000a      Conrey is mentioned fourteen times, Keating eight times, while the whole\u000a      Bristol team (Berry, Conrey, Keating and Snaith) accounts for about 450\u000a      lines of text.\u000a    In two further popular books on the Riemann Hypothesis, \"Prime Obsession\"\u000a      by John Derbyshire [a] (ranked 22nd in number theory and 64th\u000a      in history of mathematics by Amazon UK) and \"Stalking the Riemann\u000a      Hypothesis\" by Daniel Rockmore [a], the research of the Bristol group is\u000a      described some 14 times in each book. A reader reports on \"Prime\u000a      Obsession\" (December 2008): \"I have always felt that advanced pure\u000a      mathematics is as worthy an art as painting or sculpture, and the great\u000a      mathematicians as worthy artists as Van Gogh etc. But because of the\u000a      inaccessibility of the subject matter to the layman this great art\u000a      couldn't be widely-enough shared. With more books like Prime Obsession\u000a      this wrong will be righted.\" [h]\u000a    ","ImpactSummary":"\u000a    Thousands of exhibition visitors, public lecture-goers, readers, school\u000a      students and TV viewers have been encouraged to explore areas of number\u000a      theory and mathematical physics as a result of public engagement\u000a      initiatives in four countries by University of Bristol academics. Lay\u000a      people's encounters with the Bristol scientists have also changed their\u000a      view of mathematics, mathematicians and the nature of their work.\u000a    Audiences have been reached through the Royal Society Summer Science\u000a      Exhibition in 2011, a science fair in 2012, an award-winning Japanese TV\u000a      documentary made in 2009, popular lectures given between 2008 and 2013 and\u000a      contributions to popular science books.\u000a    Research on quantum mechanics, chaos and the Riemann Hypothesis is very\u000a      appealing to members of the general public who have an interest in popular\u000a      science. Bristol research ties these areas together. Its dissemination\u000a      through various media has captured public attention internationally and\u000a      inspired non-mathematicians to consider the mysteries addressed by\u000a      mathematical research.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of Bristol\u000a    ","Institutions":[{"AlternativeName":"Bristol (University of)","InstitutionName":"University of Bristol","PeerGroup":"A","Region":"South West","UKPRN":10007786}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] Bui, H.M., Conrey, J.B. and Young, M.P. (2011) More than 41% of the\u000a      zeros of the zeta function are on the critical line. Acta Arith.\u000a      150(1): 35-64 DOI 10.4064\/aa150-1-3.\u000a    \u000a\u000a*[2] Keating J.P. and Snaith, N.C. (2000) Random matrix theory and\u000a      $\\zeta(1\/2+it)$, Commun. Math. Phys., 214: 57-89 DOI:\u000a      10.1007\/s002200000261\u000a    \u000a\u000a*[3] Keating, J.P. and Snaith, N.C. (2000) Random matrix theory and\u000a      L-functions at s=1\/2, Commun. Math. Phys., 214: 91-110, DOI:\u000a      10.1007\/s002200000262\u000a    \u000a\u000a[4] Band, R., Sawicki, A. and Smilansky, U. (October 2010) Scattering\u000a        from isospectral quantum graphs. J. Phys. A: Math. Theor.\u000a      43(41) DOI 10.1088\/1751-8113\/43\/41\/415201.\u000a    \u000a\u000a*[5] Band, R., Berkolaiko, G., Raz, H. and Smilansky, U.\u000a      (2012) The\u000a        Number of Nodal Domains on Quantum\u000a        Graphs as a Stability Index of Graph Partitions. Commun. Math.\u000a        Phys. 311(3):815-838 DOI 10.1007\/s00220-011-1384-9.\u000a    \u000a*references that best indicate the quality of the underpinning research.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000a    [a] Prime Obsession: Bernhard Riemann and the Greatest Unsolved\u000a        Problem in Mathematics, John Derbyshire, Plume Books, reprint 2004\u000a      ISBN-13: 978-0452285255; Stalking the Riemann Hypothesis: the\u000a        Quest to find the hidden law of prime numbers, Daniel Rockmore,\u000a      published by Random House, 2005, ISBN-13: 978-0224062534; The Music of\u000a        the Primes, by Marcus du Sautoy, published by Harper Perennial, new\u000a      edition 2004, ISBN-13: 978-1841155807; The Riemann Hypothesis, by\u000a      Karl Sabbagh, published by Atlantic, reprint 2004, ISBN-13:\u000a      978-0374529352; Primes, zeta functions and arithmetic quantum chaos,\u000a      by Shin-ya Koyama (in Japanese), Tokyo: Nihonhyoronsha, 2010,\u000a      ISBN978-4-535-78553-3\u000a    [b]Sources corroborating the impact of Conrey's keynote address in\u000a        Granada, Spain in 2012.\u000a      http:\/\/depfisicayquimica.blogspot.com\/2012\/06\/brian-conrey-en-el-parque-de-las.html;\u000a      http:\/\/www.ugr.es\/~surfaces\/imaginary;\u000a      http:\/\/www.parqueciencias.com\/sala-prensa\/_detalle.html?uid=f6f4959a-b6dc-11e1-9d58-71c141a1f21c;\u000a      http:\/\/www.granada.org\/inet\/wagenda.nsf\/a6eb95bbac391668c1256e1600437e4a\/46207c09803d1dcdc1257617003608d2!OpenDocument;\u000a      http:\/\/www.rsme.es\/content\/view\/1076\/1\/;\u000a      http:\/\/canalugr.es\/prensa-y-comunicacion\/item\/58011-brian-conrey-director-del-instituto-americano-de-matem%C3%A1ticas-impartir%C3%A1-una-conferencia-en-el-parque-de-las-ciencias;\u000a      http:\/\/www.granadahoy.com\/article\/ocio\/1284518\/millon\/dolares\/para\/quien\/demuestre\/la\/hipotesis\/matematica\/riemann.html\u000a    [c] Royal Society report, \"Summer Science Exhibition and Soir&#233;es 2011:\u000a      review for exhibitors\" and http:\/\/royalsociety.org\/summer-science\/\u000a    [d] http:\/\/royalsociety.org\/summer-science\/2011\/hearing-shapes\u000a    [e] http:\/\/www.newscientist.com\/blogs\/nstv\/2011\/07\/quantum-graphs-make-haunting-music.html\u000a    [f] http:\/\/www.youtube.com\/watch?v=Yao2EmSeVyo;\u000a      http:\/\/www.youtube.com\/watch?v=3cPqY1-DaZk\u000a    [g] http:\/\/www.amazon.co.uk\/product-reviews\/1841155802\/ref=cm_cr_pr_btm_link_next_2?ie=UTF8&amp;pageNumber=2&amp;showViewpoints=0\u000a    [h] http:\/\/www.amazon.co.uk\/product-reviews\/0452285259\/ref=cm_cr_pr_viewpnt_sr_5?ie=UTF8&amp;filterBy=addFiveStar&amp;showViewpoints=0\u000a    [i] Feedback questionnaires from public lectures\u000a    [j] Conrey, J.B., Holmstrom, M.A. &amp; McLaughlin, T.L. (2013) Smooth\u000a      Neighbors. Experimental\u000a          Mathematics 22, 195-202. DOI:\u000a      10.1080\/10586458.2013.768483 \u000a    ","Title":"\u000a    Research-inspired outreach work boosts public interest in mathematics and\u000a      transforms perceptions of mathematicians\u000a    ","UKLocation":[{"GeoNamesId":"2650225","Name":"Edinburgh"},{"GeoNamesId":"2643743","Name":"London"},{"GeoNamesId":"2654675","Name":"Bristol"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"},{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Over several decades mathematicians and physicists at Bristol have\u000a      contributed significantly to the field of quantum chaos: the study of\u000a      quantum properties of systems that classically display chaos. Areas of\u000a      research that have grown out of this, and that are still active at Bristol\u000a      today, are random matrix theory (and its application to number theory) and\u000a      the study of quantum graphs. Research in these areas underpins the public\u000a      engagement projects outlined below.\u000a    One strand of research underlying this case study is in the area of\u000a      number theory. Conrey (Professor at Bristol since January 2005) is one of\u000a      the foremost analytic number theorists in the world. Importantly for this\u000a      case study, for many years he held the record for proving the most zeros\u000a      of the Riemann zeta function obey the Riemann Hypothesis [1]. He is also\u000a      involved, with Keating (Professor at Bristol since October 1995) and\u000a      Snaith (Initially Lecturer, then Reader at Bristol since October 2000;\u000a      graduate student at Bristol 1996-9), in the application of techniques from\u000a      random matrix theory to fundamental questions in number theory. In 2000\u000a      Keating and Snaith, both in the Bristol mathematical physics group,\u000a      authored two papers [2,3] which launched a new field of study. This work\u000a      opened the door to the use of random matrix theory to investigate several\u000a      long-standing and important number theoretical questions. In particular,\u000a      insight into the hundred-year-old question of the moments of the Riemann\u000a      zeta function and the subsequent related work of Conrey, Keating and\u000a      Snaith has had a very considerable impact in the number theoretic\u000a      community, as indicated by the large number of citations the main papers\u000a      received and the two London Mathematical Society prizes awarded for this\u000a      work.\u000a    The other strand of research deals with quantum graphs, another area of\u000a      research at Bristol which, like random matrix theory, grew out of the\u000a      field of quantum chaos. This work [4,5] has been led by Band (EPSRC\u000a      postdoctoral fellow at Bristol between August 2010 and July 2013:\u000a      EP\/H028803\/1 \"New approaches for isospectrality and nodal domains\"). A\u000a      quantum graph is a graph whose edges are assigned lengths and the whole\u000a      graph is equipped with a self-adjoint differential operator, by default\u000a      the Laplacian. A quantum graph can also be thought of as a network of\u000a      guitar strings tied to each other. When such a system vibrates it produces\u000a      a spectrum of sounds. The spectrum of the Laplacian describes the pitches\u000a      of those sounds. The Laplacian's eigenfunctions describe the different\u000a      vibration modes of the graph.\u000a    Band investigated the possibility of having two different graphs that\u000a      share the same spectrum (isospectral graphs). He proved a theorem that\u000a      yields a method for producing isospectral graphs and implemented this\u000a      method to produce various new examples of such graphs. He has solved open\u000a      problems in this area and his work has been recognised widely.\u000a    "},{"CaseStudyId":"40256","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2750405","Name":"Netherlands"}],"Funders":[],"ImpactDetails":"\u000a    Since 1 Jan 2008, MLwiN has been purchased by 613 non-academic individual\u000a      users and 75 organisations (67 site licenses for 50 users, and 8 for 250\u000a      users). It has also been downloaded for free by 3,846 UK academics, and\u000a      purchased by 5,518 overseas academics. The Bristol Centre for Multilevel\u000a      Modelling website is widely acknowledged as the premier resource for\u000a      research and training in multilevel modelling, with around 1,100\u000a      page-loads and 360 unique visitors per day (65% of whom are from outside\u000a      the UK). The on-line Learning Environment for Multilevel Modelling\u000a      (LEMMA), launched in April 2008 as part of the Economic and Social\u000a      Research Council National Centre for Research Methods, now has around\u000a      10,600 registered users, of which 70% are international and 14% are\u000a      non-academic. The sheer number of users demonstrates the reach and\u000a      significance of the underpinning research [1-3]. Non-academic users of\u000a      Centre software, and hence the underlying research, include the World\u000a      Health Organisation, Statistics Canada, Statistics Norway, Netherlands\u000a      Central Bureau of Statistics, the UK Departments for Education, Health,\u000a      and Work and Pensions, the Scottish Executive, the Office for National\u000a      Statistics and the Higher Education Funding Council for England. Analyses\u000a      by these bodies, and others, as well as by the academic users of the\u000a      software, have significant impact on society; we focus on three specific\u000a      areas.\u000a    A. Impact on UK and international policy and public awareness\u000a        relating to measuring educational effectiveness and school performance\u000a    Improved school evaluation policies: In the UK, Goldstein's\u000a      multilevel modelling framework (including, for example, [4-6]) has\u000a      provided the statistical toolkit which has provided evidence to inform and\u000a      influence key national policies related to school evaluation such as: the\u000a      utility of school self-evaluation, national pupil databases,\u000a      contextualised value-added (CVA) measures of school performance, and\u000a      separate value-added measures for different student groups (introduced by\u000a      the Department for Education (DfE) 2011). The research has also promoted\u000a      the use and understanding of a wider range of outcomes and measures by\u000a      DfE, the Department for Children, School and Families, the Office for\u000a      Standards in Education, Children's Services and Skills (Ofsted) and the\u000a      Learning and Skills Council [a]. MLwiN is currently used within DfE to\u000a      calculate published measures of CVA school performance, an integral part\u000a      of the Ofsted school inspection process, and also to construct the\u000a      Learning Achievement Tracker, a new tool for schools and further education\u000a      colleges to appreciate progress made by students since the end of\u000a      compulsory schooling [b]. In particular \"MLwiN allows ... complex\u000a      cross-classified multilevel structures, and these, using, in the main,\u000a      MCMC methods, were used to inform the DfE about the variations in pupil\u000a      performance associated statistically with their social background and\u000a      school attended,\" [b] which demonstrates that research items [1,2,5] are\u000a      having significant impact in this area.\u000a    Public understanding of league tables: The statistical research\u000a      carried out by [6] demonstrates the limitations of using the government's\u000a      school league tables to inform school choice. This has promoted public\u000a      understanding of the problems of league tables through widespread\u000a      communication to non-academics via popular articles and other media,\u000a      including interviews for the BBC Radio 4 programmes `Analysis' and `The\u000a      Learning Curve' and articles in the Financial Times, Daily Telegraph and\u000a      Times Educational Supplement [c]. This work demonstrates impact in terms\u000a      of both reach and significance, given that it has been incorporated into\u000a      policy documents by numerous governments and non-governmental\u000a      organisations, both overseas (including the OECD) and UK (including the\u000a      National Union of Teachers and the Institute for Government), to influence\u000a      public thinking and new policy development on educational accountability\u000a      and improvement issues [d].\u000a    Improved understanding of rural educational issues: A further\u000a      example of the importance of strong statistical research [4] with\u000a      associated software [3] for educational research resulting in societal\u000a      benefit is a recent Department for Environment, Food and Rural Affairs\u000a      report [e] in which the new sophisticated methods were used to investigate\u000a      whether higher attainment of rural school pupils was symptomatic of a\u000a      better educational environment or simply a by-product of generally higher\u000a      social position in rural areas. Missing data, and multilevel structure,\u000a      are endemic in such a study, and [3] was necessary since \"the use of\u000a      multiple imputation in this study should provide more accurate [analyses]\u000a      than a complete case analysis [throwing out records with missing data],\u000a      and should also increase the power of the analyses so that small\u000a      differences between settlement types can be more easily distinguished\"\u000a      [e].\u000a    B. Admissions to UK universities\u000a    The Schwartz Report on Fair Admission (2004) was instigated to review\u000a      \"options which English institutions providing higher education should\u000a      consider in assessing the merit of applicants for their courses.\"\u000a      Consequently, the Supporting Professionalism in Admissions programme was\u000a      set up to support higher education institutions to develop admissions\u000a      policies ensuring fair access. The programme has recently carried out a\u000a      major investigation into the use of so-called contextual data to inform\u000a      admissions decisions, publishing a 2012 report [f]. The data supporting\u000a      the review was multilevel in nature, and extensive (between 0.4 and 1.6\u000a      million individual school records). \"With such a sensitive issue, it was\u000a      important to fit a statistical model that would be robust to criticism\"\u000a      [g]. A recent version of MLwiN, which incorporates the latest MCMC module\u000a      [1] incorporating the advanced MCMC techniques developed by [2] \"was\u000a      necessary to achieve satisfactory results with such a high volume of data\u000a      and highly correlated variables of interest\" (type of school and\u000a      educational attainment being two key candidate explanatory variables for\u000a      degree performance) [g]. The report concludes that the type of school is\u000a      an important predictor of degree performance. The recommendation to HEIs\u000a      is therefore to incorporate such contextual information in their\u000a      admissions decisions, thus having an impact across the UK in terms of\u000a      accessibility of higher education.\u000a    C. Equality and diversity policy for REF2014 submissions\u000a    Analysis of RAE2008 data was carried out by the Higher Education Funding\u000a      Council for England using MLwiN. One aspect of this was a multilevel\u000a      analysis over 30,000 records with a binary response to indicate whether an\u000a      individual was included in the RAE2008 submission. \"To carry out such a\u000a      large multilevel analysis with binary response data required the use of\u000a      the recent optimised MCMC components of MLwiN\" [1], based on the research\u000a      of [2] [g]. A key finding was of selection biases against certain ethnic\u000a      groups that were not explained by controlling for other factors [h]. In\u000a      response, significantly improved rules on equality and diversity have been\u000a      introduced for REF2014 (http:\/\/www.ref.ac.uk\/equality\/),\u000a      thus having a significant impact on UK Higher Education.\u000a    ","ImpactSummary":"\u000a    Since 2008, statistical research at the University of Bristol has\u000a      significantly influenced policies, practices and tools aimed at evaluating\u000a      and promoting the quality of institutional and student learning in the\u000a      education sector in the UK and internationally. These developments have\u000a      also spread beyond the education sector and influence the inferential\u000a      methods employed across government and other sectors. The underpinning\u000a      research develops methodologies and a much-used suite of associated\u000a      software packages that allows effective inference from complicated data\u000a      structures, which are not well-modelled using traditional statistical\u000a      techniques that assume homogeneity across observational units. The ability\u000a      to analyse complicated data (such as pupil performance measures when\u000a      measured alongside school, classroom, context and community factors) has\u000a      resulted in a significant transformation of government and institutional\u000a      policies and their practices in the UK, and recommendations in\u000a      Organisation for Economic Co-operation and Development (OECD) policy\u000a      documents. These techniques for transforming complex data into useful\u000a      evidence are well-used across the UK civil service, with consequent policy\u000a      shifts in areas such as higher education admissions and the REF2014\u000a      equality and diversity criteria.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of Bristol\u000a    ","Institutions":[{"AlternativeName":"Bristol (University of)","InstitutionName":"University of Bristol","PeerGroup":"A","Region":"South West","UKPRN":10007786}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] Browne, W.J. (2012) MCMC Estimation in MLwiN, v2.26. Centre for\u000a      Multilevel Modelling, University of Bristol. http:\/\/www.bristol.ac.uk\/cmm\/software\/mlwin\/download\/2-26\/mcmc-web.pdf\u000a    \u000a\u000a*[2] Browne, W.J., Steele F., Golalizadeh, M., and Green M.J. (2009) The\u000a      use of simple reparameterizations to improve the efficiency of Markov\u000a      chain Monte Carlo estimation for multilevel models with applications to\u000a      discrete time survival models. Journal of Royal Statistical Society,\u000a      A, 172, 579-598. DOI:10.1111\/j.1467-985X.2009.00586.x\u000a    \u000a\u000a[3] Goldstein, H. (2011) REALCOM-IMPUTE, User Guide. Centre for\u000a      Multilevel Modelling, University of Bristol. http:\/\/www.bristol.ac.uk\/cmm\/software\/realcom\/imputation.pdf\u000a    \u000a\u000a[4] Goldstein, H., Kounali, D. and Robinson, A. (2008) Modelling\u000a      measurement errors and category misclassifications in multilevel models. Statistical\u000a        Modelling, 8, 243-261. DOI:10.1177\/1471082X0800800302\u000a    \u000a\u000a*[5] Leckie, G. (2009) The complexity of school and neighbourhood effects\u000a      and movements of pupils on school differences in models of educational\u000a      achievement. Journal of the Royal Statistical Society, A, 172,\u000a      537-554. DOI:10.1111\/j.1467-985X.2008.00577.x\u000a    \u000a\u000a*[6] Leckie, G. and Goldstein, H. (2009) The limitations of using school\u000a      league tables to inform school choice. Journal of the Royal\u000a        Statistical Society, A, 172, 835-851.\u000a      DOI:10.1111\/j.1467-985X.2009.00597.x\u000a    \u000a* references that best indicate the quality of the underpinning research.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"8","Level2":"6","Subject":"Information Systems"}],"Sources":"\u000a    [a] Director General, Monitoring and Assessment, UK Statistics Authority\u000a      has provided information about influence of University of Bristol research\u000a      on government and public understanding of UK policy on school evaluation.\u000a    [b] Consultant Statistician to DfE.\u000a      May be contacted to corroborate the influences of Centre for Multilevel\u000a      Modelling team's value-added studies on and use of MLwiN by DfE\u000a      statisticians and Ofsted.\u000a    [c] Sources to corroborate engagement with public.\u000a      http:\/\/news.bbc.co.uk\/1\/shared\/spl\/hi\/programmes\/analysis\/transcripts\/30_01_12.pdf\u000a      http:\/\/www.bbc.co.uk\/radio4\/factual\/learningcurve_20080616.shtml\u000a      http:\/\/www.ft.com\/cms\/s\/0\/17dfb862-7ad4-11de-8c34-00144feabdc0.html\u000a      http:\/\/www.telegraph.co.uk\/education\/secondaryeducation\/6005906\/Grammar-schools-penalised-by-new-league-tables.html\u000a      http:\/\/www.tes.co.uk\/article.aspx?storycode=6009334\u000a    [d] Wildeman (2011) \"Beware of the misleading means and measures\". In Transformation\u000a        Audit 2011, published by The Inclusive Economies Project, which is\u000a      located within the Policy and Analysis Unit of the Institute for Justice\u000a      and Reconciliation (IJR). http:\/\/transformationaudit.org\/\u000a        blog\/wp-content\/uploads\/2012\/02\/Opinion-Beware-of-the-misleading-means-and-measures.pdf\u000a      Mulgan (2010) \"Transparency Occasional Paper 1: Transparency and Public\u000a      Sector Performance\". Queensland Office of the Information Commissioner and\u000a      the Australia and New Zealand School of Government working paper, http:\/\/www.anzsog.edu.au\/media\/upload\/publication\/93_1-Mulgan-Transparency-and-Public-Sector-Performance.pdf\u000a    Masters, Rowley, Ainley and Khoo (2008) \"Reporting and comparing school\u000a      performances\". Commissioned by the Reporting and Accountability Branch,\u000a      National Education Systems Group, Commonwealth Department of Education,\u000a      Employment and Workplace Relations (DEEWR) http:\/\/apo.org.au\/research\/reporting-and-comparing-school-performances\u000a    OECD (2008) \"Measuring improvements in learning outcomes: Best practices\u000a      to assess the value-added of schools.\" Organisation for Economic\u000a      Co-operation and Development, DOI:10.1787\/9789264050259-en\u000a    Rosenkvist, M.A. (2010) \"Using student test results for accountability\u000a      and improvement.\" OECD Education Working Paper 54,\u000a      DOI:10.1787\/5km4htwzbv30-en\u000a    [e] \"Educational Attainment in Rural Areas\" A report prepared for the\u000a      Department for Environment, Food and Rural Affairs by the National Centre\u000a      for Social Research (NatCen), 31 December 2009. http:\/\/www.natcen.ac.uk\/media\/665690\/c5974000-7879-4f01-890f-c31cf9ca7489.pdf\u000a    [f] \"Fair Admissions to Higher Education: Research to describe the use of\u000a      contextual data in admissions at a sample of universities and colleges in\u000a      the UK.\" Research report by Kath Bridger, Jenny Shaw (BSV Associates Ltd)\u000a      and Joanne Moore (ARC Network) for the Supporting Professionalism in\u000a      Admissions (SPA) Programme. http:\/\/www.spa.ac.uk\/documents\/ContextualData\/Full_SPA_Contextual_data_Research_Report-Feb2012.pdf\u000a    [g] Head of Quantitative Analysis for Policy, HEFCE. May be contacted to\u000a      corroborate that the most recent versions of MLwiN were needed to carry\u000a      out the analyses.\u000a    [h] \"Selection of Staff for inclusion in RAE2008,\" http:\/\/www.hefce.ac.uk\/media\/hefce1\/pubs\/hefce\/2009\/0934\/09_34.pdf\u000a    ","Title":"\u000a    The use of multilevel statistical modelling has led to improved\u000a      evidence-based policy making in education and other sectors\u000a    ","UKLocation":[{"GeoNamesId":"2654675","Name":"Bristol"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Multilevel statistical modelling, a sophisticated methodological approach\u000a      to data analysis, has been developed at the University of Bristol since\u000a      2005. This statistical technique is crucial in analyses of complex data\u000a      sets that have non-trivial geographical and\/or temporal components, data\u000a      structures which could cause dangerous confusion if analysed using more\u000a      basic statistical techniques. The impact of these approaches has been\u000a      maximised through deployment of research such as [2] in new versions of\u000a      the hugely popular MLwiN statistical software package, along with new\u000a      software such as REALCOM-impute [3] which carries out multiple imputation\u000a      of missing data in the context of a multilevel model, MLPowSim to perform\u000a      sample size calculations for multilevel modelling, and Stat-JR, an\u000a      innovative software environment for promoting interactive statistical\u000a      modelling. Recent research has focussed on developing inference methods\u000a      for realistically complex multilevel models, and developing an\u000a      understanding of the computational methodologies required to fit complex\u000a      models to large data sets, such as those obtained from national level data\u000a      collection of, for example, pupil test scores and university applications.\u000a    Novel statistical methodology underpinning the impact:\u000a    \u000a      Introduction of re-parameterisation techniques within Markov chain\u000a        Monte Carlo (MCMC) algorithms for fitting multi-level models, allowing\u000a        the statistical analyses to become sufficiently efficient to be carried\u000a        out on significantly larger and more complex data structures [2].\u000a      Simulation-based graphical approaches to communicating statistical\u000a        uncertainty of group effects in multilevel models [6], which are\u000a        otherwise difficult to interpret and communicate.\u000a      Novel multilevel modelling formulations for complex non-hierarchical\u000a        data structures [5]. This type of data set arises, for example, when\u000a        effects of both schools and neighbourhoods are to be taken into account\u000a        in models of pupil performance.\u000a      Advanced multilevel modelling for analysis of longitudinal data with\u000a        multilevel structure. In particular when the observational units change,\u000a        but the grouping units do not, such as clinical outcomes in a hospital\u000a        in successive time periods, or pupils in a school in successive years\u000a        [6].\u000a      Multilevel statistical techniques for modelling multivariate data with\u000a        different response types at several levels, and handling correlated\u000a        measurement and misclassification errors [4].\u000a    \u000a    Novel computational implementations underpinning the impact:\u000a    \u000a      The general computational capabilities of MLwiN, and the MCMC module\u000a        in particular [1], have been updated to enable efficient inference to be\u000a        carried out with complex models and large data sets. Key parts of this\u000a        update, which have been in versions of MLwiN since 2.13 (released in\u000a        August 2009), are based on the techniques of [2].\u000a      Production of three new software packages: REALCOM-impute [3], to\u000a        allow imputation of missing data; MLPowSim, to perform sample size\u000a        calculations within the multi-level modelling framework; and Stat-JR, a\u000a        software environment for promoting interactive statistical modelling.\u000a    \u000a    The team that carried out the research described above moved to Bristol\u000a      in 2005 (Goldstein, Prof. of Social Statistics, Steele, Prof. of Social\u000a      Statistics, and Rasbash, Prof. of Computational Statistics), 2007 (Browne,\u000a      Prof. of Biostatistics) and 2009 (Leckie, Senior Lecturer in Social\u000a      Statistics).\u000a    "},{"CaseStudyId":"40257","Continent":[],"Country":[],"Funders":["Royal Society"],"ImpactDetails":"\u000a    The UK Climate Impacts Programme (UKCIP) is a government-funded\u000a      organisation which provides\u000a      climate projections for the UK, for the purposes of adaptation to climate\u000a      change. The current\u000a      report is the fifth in the sequence, denoted UKCP09 (2009) and summarised\u000a      in [c]. Users of the\u000a      previous report (UKCIP02, 2002) requested more information about\u000a      uncertainties: \"The uncertainty\u000a      aspects of [the UKCP09] were seen as instrumental in better preparing the\u000a      UK to address the\u000a      challenges of climate change\" [b]. The UK Met Office (UKMO) was asked by\u000a      the UKCIP to\u000a      implement new methods to meet this demand, which included: modelling\u000a      uncertainty, uncertainty\u000a      associated with statistical processing, and use of observations to weight\u000a      projections [d, p1].\u000a      Rougier's statistical framework [1] was \"chosen [by the UKMO] to provide\u000a      the statistical core of the\u000a      UKCP09 projections\" [a], and thus represents a key innovation in climate\u000a      change impact\u000a      assessment.\u000a    Additionally, Rougier's research on climate model emulators [2,3] was\u000a      crucial in the practical\u000a      implementation of the methods:\u000a    \"Dr Rougier's guidance was instrumental in helping [UKMO] understand how\u000a      to find\u000a      appropriate transformations of variables, build emulator regression\u000a      relationships, and\u000a      evaluate emulator performance in validation tests. His use of emulators to\u000a      identify the\u000a      individual and combined effects of key parameters on the climate\u000a      sensitivity to doubled\u000a      carbon dioxide [3] provided a clear demonstration of the importance of\u000a      ruling out unrealistic\u000a      parts of parameter space through the calculation of relative likelihood.\u000a      [UKMO] was later able\u000a      to cite this as a key justification in its estimation of relative weights\u000a      for different model\u000a      variants.\" [a]\u000a    Rougier was retained as an External Expert by the UKMO for the UKCP09\u000a      (2007-2009, &#163;10K\u000a      honorarium):\u000a    \"More generally, Dr Rougier provided expert steer and advice on the\u000a      implementation of his\u000a      Bayesian methodology as a whole. His advice also provided key insights\u000a      into the strengths,\u000a      limitations and principles behind alternative methods for the\u000a      quantification of uncertainties in\u000a      climate projections, helping [UKMO] to justify and communicate its\u000a      approach in an area\u000a      inevitably subject to a variety of potential techniques and choices.\" [a]\u000a      In delivering the UKCP09,\u000a    \"the UKMO team with Dr Rougier [have] put the UK at the leading edge of\u000a      the science and\u000a      service aspects of providing climate information for users\" [b].\u000a    The UKCP09 has been critical in helping the UK Government to meet its\u000a      obligations under the\u000a      Climate Change Act (2008). This act made the UK the first country in the\u000a      world to have a national,\u000a      legally binding, long-term framework to cut carbon emissions. The UKCP09\u000a      formed the basis for\u000a      the first UK Climate Change Risk Assessment (CCRA, 2012), and Rougier's\u000a      contribution to the\u000a      uncertainty assessment in the UKCP09 played a crucial role:\u000a    \"The assessment of future climate risks needs to take account of a wide\u000a      range of outcomes... The CCRA considered a range of potential changes in\u000a      climate, informed by the [UKCP09],\u000a      to provide an indication of these uncertainties\" [e, p11].\u000a    \"The risk assessment used UKCP09 climate projections, where possible, to\u000a      assess future\u000a      changes to sector risks. Some risks were analysed using single climate\u000a      variables, for\u000a      example temperature. Others, including flood risks, considered the\u000a      combined effects of many\u000a      climate variables and sea level rise.\" [f, piii]\u000a    Statement [a] notes that a powerful feature of Rougier's statistical\u000a      framework is its ability to handle\u000a      multiple climate variables in a consistent manner. This is crucial for\u000a      planning, where impact\u000a      typically arises from a combination of climate variables (such as\u000a      temperature and precipitation) or a\u000a      sequence of weather states (such as a drought). The CCRA has in turn\u000a      formed the basis for the\u000a      recommendations in the UK's first National Adaptation Programme (NAP,\u000a      2013). The CCRA and\u000a      the NAP were laid before Parliament (in 2012 and 2013, respectively) as\u000a      part of the Government's\u000a      obligations under the Climate Change Act.\u000a    The CCRA monetised 100 of the direct risks of climate change, suggesting\u000a      that the net costs of\u000a      climate change are of the order of tens of billions of pounds per year in\u000a      the 2050s for the 50th\u000a      percentile outcome under the Medium emissions scenario. But, as noted by\u000a      source [g], this does\u000a      not fully capture the risk for several different reasons, including that\u000a      some outcomes with non-\u000a      negligible probabilities are substantially worse. To give one example, for\u000a      tidal flooding the\u000a      UKCP09 50th percentile outcome is 550 thousand properties\u000a      affected; but the 90th percentile is 620\u000a      thousand properties, and the more extreme H++ scenario is 1.25 million\u000a      properties [g, p27]. The\u000a      wide range of possible losses in this example illustrates the importance\u000a      of uncertainty\u000a      quantification in a full assessment of risk, and the impact of the UKCP09\u000a      on the UK's risk\u000a      assessment for climate change.\u000a    The UKCP09 is being used by a wide\u000a      range of organisations to assess and\u000a      manage the impact of climate change\u000a      (about 7,000 downloads [a]). Case\u000a      studies on the UKCIP website include\u000a      Agencies and NGOs (Environment\u000a      Agency, Macaulay Institute, South West\u000a      Tourism), utilities companies (Severn\u000a      Trent Water), consultancies (JBA\u000a      consulting, Royal Haskoning, United\u000a      Sustainable Energy Agency), and County\u000a      Councils and Local Authorities. Many of\u000a      these users have incorporated the\u000a      UKCP09 uncertainty assessment into their\u000a      decision support tools.\u000a    \u000a    \u000a    \u000a    As an illustration, the figure to the right\u000a      shows an output from the Wetland Toolkit\u000a      for Climate Change created for the Environment Agency. This illustrates\u000a      the type of decision\u000a      support tool that can be developed once a probabilistic uncertainty\u000a      assessment for future weather\u000a      is provided. In this case, impact thresholds for different regions are\u000a      determined from ecological\u000a      considerations, and represented by the grey dashed lines (the current\u000a      level is indicated by the solid\u000a      blue line). A probabilistic ensemble for future weather, based on the\u000a      UKCP09, is used to assign\u000a      probabilities for the three different levels of impact, which can then be\u000a      used to screen regions in\u000a      order to identify those most at risk.\u000a    ","ImpactSummary":"\u000a    Climate change is one of the defining challenges of our time. The net\u000a      costs of climate change in\u000a      the UK could be tens of billions of pounds per year in the 2050s, and\u000a      tidal flooding alone could\u000a      affect over half a million UK properties by 2100. Dr Jonathan Rougier\u000a      worked with the UK Met\u000a      Office (UKMO) to produce the climate scenarios for the UK Climate Impacts\u000a      Programme (UKCIP)\u000a      2009 report (UKCP09). His research and advice (funded as a UKMO External\u000a      Expert) was critical\u000a      in a key innovation in the UKCP09: a comprehensive uncertainty assessment.\u000a      A Director of the\u000a      UKCIP writes \"The UKMO team with Dr Rougier [have] put the UK at the\u000a      leading edge of the\u000a      science and service aspects of providing climate information for users\"\u000a      [b].\u000a    The UKCP09 formed the basis of the UK Climate Change Risk Assessment and\u000a      the\u000a      recommendations of the UK National Adaptation Programme, which was\u000a      submitted to Parliament\u000a      as part of the Government's obligations under the Climate Change Act. The\u000a      UKCP09 has been\u000a      used for the assessment of the impact of climate change by hundreds of\u000a      organisations, including\u000a      agencies and non-governmental organisations (NGOs), utilities companies,\u000a      consultancies, and\u000a      County Councils and Local Authorities.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University of Bristol\u000a    ","Institutions":[{"AlternativeName":"Bristol (University of)","InstitutionName":"University of Bristol","PeerGroup":"A","Region":"South West","UKPRN":10007786}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a*[1] J.C. Rougier (2007), Probabilistic Inference for Future Climate\u000a      Using an Ensemble of Climate\u000a      Model Evaluations, Climatic Change, 81, 247-264.\u000a      DOI: 10.1007\/s10584-006-9156-9.\u000a    \u000a\u000a[2] J.C. Rougier and D.M.H. Sexton (2007), Inference in Ensemble\u000a      Experiments, Philosophical\u000a        Transactions of the Royal Society, Series A, 365, 2133-2143.\u000a      DOI:10.1098\/rsta.2007.2071.\u000a    \u000a\u000a*[3] J.C. Rougier, D.M.H. Sexton, J.M. Murphy, and D. Stainforth (2009),\u000a      Analysing the climate\u000a      sensitivity of the HadSM3 climate model using ensembles from different but\u000a      related experiments.\u000a      Journal of Climate, 22(13), 3540-3557.\u000a      DOI:10.1175\/2008JCLI2533.1.\u000a    \u000a\u000a[4] J.C. Rougier (2008), Efficient Emulators for Multivariate\u000a      Deterministic Functions, Journal of\u000a        Computational and Graphical Statistics, 17, 827-843.\u000a      DOI:10.1198\/106186008X384032.\u000a    \u000a\u000a[5] J.C. Rougier, S. Guillas, A. Maute, A.D. Richmond (2009), Expert\u000a      Knowledge and Multivariate\u000a      Emulation: The Thermosphere-Ionosphere Electrodynamics General Circulation\u000a      Model (TIE-GCM),\u000a      Technometrics, 51, 414-424. DOI:10.1198\/TECH.2009.07123.\u000a    \u000a\u000a*[6] M. Goldstein and J.C. Rougier (2009), Reified Bayesian Modelling and\u000a      Inference for Physical\u000a      Systems, Journal of Statistical Planning and Inference, 139(3),\u000a      1221-1239.\u000a      DOI:10.1016\/j.jspi.2008.07.019. With discussion and rejoinder.\u000a    \u000a* references that best indicate the quality of the underpinning research.\u000a    Rougier was the sole author or lead author on [1-5], and equal co-author\u000a      on [6].\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000a    [a] Hadley Centre, UK Met Office (UKMO). Statement corroborating\u000a      the UKMO's role in the\u000a      UKCP09, Rougier's role as the developer of the statistical methods, the\u000a      importance of Rougier's\u000a      research, and Rougier's collaboration with the UKMO.\u000a    [b] UK Climate Impacts Programme (UKCIP). Statement corroborating\u000a      the user demand for\u000a      statistical climate impact assessment in the UKCP09, the role of the UKMO\u000a      and Rougier, and the\u000a      breadth and depth of impact of the UKCP09.\u000a    The following documents are publicly available, and can be supplied on\u000a      demand.\u000a    [c] G.J. Jenkins et al. (2009), UK Climate Projections:\u000a        Briefing Report. Met Office Hadley Centre,\u000a      Exeter, UK. ISBN 978-1-906360-04-7.\u000a    [d] Assessing the differences &#8212; UKCIP02 &amp; UKCP09. UKCIP 2009.\u000a    [e] UK Climate Change Risk Assessment: Government Report. Defra\u000a      2012. ISBN 9780108511257.\u000a    [f] The UK Climate Change Risk Assessment 2012, Evidence Report.\u000a      Defra 2012.\u000a    [g] Scoping Study: Reviewing the Coverage of Economic Impacts in the\u000a        CCRA. Report to the\u000a      Committee on Climate Change, Adaptation Sub-Committee, Paul Watkiss\u000a      Associates, 2009.\u000a    ","Title":"\u000a    Uncertainty quantification for UK climate change legislation, and for\u000a      climate\u000a      change impact assessment\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Dr Jonathan Rougier, Reader in Statistics at the University of Bristol\u000a      (appointed Jan 2007), worked\u000a      with the UKMO over the period 2007-2009 to produce the climate scenarios\u000a      for the UKCP09.\u000a      Much of his published research from that period reflects this\u000a      collaboration.\u000a    Rougier's research concerns uncertainty assessment for complex systems &#8212;\u000a      notably\u000a      environmental systems, including climate and natural hazards. He has\u000a      developed much of the core\u000a      statistical theory in this area, and, in his on-going collaboration with\u000a      the UKMO, has been influential\u000a      in changing the way in which climate uncertainty is treated. Two aspects\u000a      of his research were\u000a      particularly important for the UKCP09 [a]:\u000a    1. A statistical framework for a comprehensive assessment of\u000a        uncertainty for complex\u000a        systems, such as climate.\u000a    The climate system manifests complicated dependencies in space and time,\u000a      as a consequence of\u000a      the underlying physical constraints of continuity and conservation. These\u000a      are best represented in a\u000a      climate simulator, a computer code which attempts to solve the underlying\u000a      physical equations. But\u000a      such codes are limited, partly due to our lack of knowledge, and partly\u000a      due to computing\u000a      constraints. Rougier's framework provides a simple representation of a\u000a      simulator's limitations, in\u000a      the form of parametric and structural uncertainty. In [1] he provided a\u000a      statistical reinterpretation of\u000a      current practice in climate science, while [6] provided a generalisation\u000a      suitable for collections and\u000a      sequences of simulators, such as the evolving simulators of the world's\u000a      major climate research\u000a      groups.\u000a    2. Emulation approaches to make the most efficient use of a limited\u000a        number of simulator\u000a        runs.\u000a    A large climate simulator runs at about one hundred model years per\u000a      calendar month. Even with\u000a      some of the largest computers in the world, climate research groups cannot\u000a      afford to do more than\u000a      a handful of runs. This makes it challenging to tune the simulator\u000a      parameters to historical\u000a      observations, and to assess uncertainty in climate projections. Rougier\u000a      has been influential in\u000a      developing efficient emulators for large simulators, notably those with\u000a      complex outputs [4], with\u000a      immediate applications in climate [5]. In his collaboration with\u000a      scientists at the UKMO, he has\u000a      advocated general approaches for computer experiments and scalar emulation\u000a      [2], and provided\u000a      new levels of detail concerning the behaviour of the UKMO climate\u000a      simulator HadCM3 [3].\u000a    "},{"CaseStudyId":"40258","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"6251999","Name":"Canada"},{"GeoNamesId":"1861060","Name":"Japan"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"2802361","Name":"Belgium"}],"Funders":["Medical Research Council"],"ImpactDetails":"\u000a    The paragraphs below describe in brief a wide range of non-academic\u000a      applications of Reversible jump MCMC, all starting or continuing after\u000a      2008. Each of these is corroborated by personal communications held on\u000a      file, and in some cases also by internal or published documents citing [1]\u000a      (and sometimes [2] or [3]).\u000a    A. Applications in Geophysical sciences\u000a    1. Geophysical source reconstruction. At Defence Research and\u000a        Development, Canada, key concepts enunciated in [1] have been used\u000a      to design an innovative Bayesian inference methodology to address the\u000a      problem of source reconstruction for the difficult case of multiple\u000a      sources when even the number of sources is unknown a priori. This effort\u000a      has had an impact within the realm of public safety and security as it\u000a      addresses a critical capability gap in current emergency and retrospective\u000a      management efforts, which involve the covert release of chemical,\u000a      biological, or radiological agents into the atmosphere.\u000a    2. Geophysical electrical resistivity. The US Geological\u000a        Survey is using methodology built on [1] to explore the space of\u000a      subsurface electrical resistivity models that are consistent with airborne\u000a      geophysical data. Data is acquired using airborne geophysical instruments\u000a      that are sensitive to the spatial distribution of electrical resistivity\u000a      below ground to depths of ~100m that, in turn, can be interpreted in terms\u000a      of geologic or hydrologic properties. \"There is real-world impact to this\u000a      work- we are using this algorithm to characterize important groundwater\u000a      aquifer systems and permafrost in various areas of the U.S.\" [a]\u000a    3. Ground flow models. The Belgian Nuclear Research Centre\u000a      (SCKCEN) has developed an MCMC simulation of a highly parameterized\u000a      groundwater flow model, based on [1], for uncertainty quantification of\u000a      subsurface transport in the context of the Belgian nuclear waste disposal\u000a      program.\u000a    4. Air pollution, greenhouse gases, remote sensing. Shell\u000a        Research uses Bayesian inference, exploiting MCMC techniques\u000a      including [1], to estimate the characteristics of sources of airborne\u000a      species (gases, particulate matter, etc.) [b]. The main value of inference\u000a      from remote sensing of airborne species to Shell, and to society in\u000a      general, is to be able to quantify contributions to greenhouse gas\u000a      emissions from specific human activities over time. The technology is also\u000a      generally useful in detecting unknown or unanticipated sources (`leaks')\u000a      of species carried on the wind, and can lead to discovery of (e.g.) new\u000a      hydrocarbon reserves. A key general ingredient of useful statistical\u000a      solutions to real-world problems is the flexibility and scalability of\u000a      Bayesian inference using MCMC, e.g. allowing characterisation of\u000a      parameters previously consigned to the `too difficult to measure or\u000a      estimate' box. Reversible jump MCMC extends this flexibility considerably\u000a      by allowing dimension-jumping.\u000a    5. Air pollution, change point models. Cox Associates\u000a      have used [1] \"in advocating (to risk analysts and regulators, in various\u000a      forums) the importance and practicality of applying better statistical\u000a      methods for causal analysis of health effects of key regulations, such as\u000a      air pollution regulations in the U.S\". They have testified on the\u000a      importance and practicality of using better methods of causal analysis in\u000a      air pollution health effects research before the Subcommittee on Energy\u000a      and Power of the House Energy and Commerce Committee of Congress on health\u000a      effects of air pollutants (2012) http:\/\/energycommerce.house.gov\/hearings\/hearingdetail.aspx?NewsID=9594.\u000a      [c]\u000a    6. Climate and land models. The Geophysical Fluid Dynamics\u000a        Laboratory (GFDL) of the US National Oceanic and Atmospheric\u000a        Administration (NOAA) uses [1] in development of land components for\u000a      climate and Earth System models. These models are needed to make climate\u000a      projections (e.g. Intergovernmental Panel on Climate Change and national\u000a      assessments) and seasonal climate predictions. This approach has been used\u000a      to estimate parameters of the phenology module which is incorporated into\u000a      a new land model. Furthermore, this new parameterization has been\u000a      incorporated into a new model of forest dynamics for forest management (a\u000a      collaborative project with the US Forest Service).\u000a    B. Applications in Ecology and the Environment\u000a    1. Phylogenetics and biodiversity. Modern molecular phylogenetic\u000a      inference is of central importance to monitoring species diversity within\u000a      changing environments. Several projects within Agriculture and\u000a        Agri-Food Canada aim to monitor and understand general features of\u000a      biodiversity, and therefore rely heavily and phylogenetic inference. Such\u000a      inference, when conducted probabilistically, rests on explicit models of\u000a      molecular evolution, and the approach in [1] helps choose the most\u000a      appropriate one, or allow phylogenies to be based on a weighted-averaging\u000a      over all possible models of a given class. Algorithms based on [1] are\u000a      implemented in several phylogenetic inference packages and continue to\u000a      stimulate applied research in their labs.\u000a    2. Phylogenetics and biodiversity. The Morton Arboretum\u000a      in Lisle, Illinois, uses [1] to characterize shifts in chromosome number\u000a      evolution as a way of understanding biodiversity shifts in sedges\u000a      Chromosome number evolves independently of genome size in a clade with\u000a      non-localized centromeres, as well as understanding macroevolutionary\u000a      shifts in decomposition rates on the tree of life. A lead scientist at\u000a      Morton Arboretum comments that \"Both of these have profound implications\u000a      for management and conservation of biodiversity as well as ecosystem\u000a      processes\".\u000a    3. Animal abundance. NOAA uses [1] to allow uncertainty\u000a      in the number of individuals when estimating the abundance of organisms in\u000a      line transect surveys with imperfect detection. It is \"currently using\u000a      this type of analysis is to estimate the number of seals in the Bering\u000a      Sea\".\u000a    4. Wildlife ecology. At the US Geological Survey Patuxent\u000a      Wildlife Research Center, [1] has been applied \"to analyses of the North\u000a      American Breeding Bird Survey, to toxicological studies, to basic\u000a      ecological work on life histories, and in demographic analyses\".\u000a    5. Ecology of salmon. The US Fish and Wildlife Service\u000a      uses [1] in comparing models aimed at assessing the effect of transporting\u000a      (exporting) water from the Sacramento &#8212; San Joaquin Rivers Delta on the\u000a      survival of juvenile salmon as the salmon were out-migrating from\u000a      freshwater to the ocean. Water is exported from the Delta for\u000a      agricultural, municipal, and personal needs and is thought to directly\u000a      affect over 25 million people in California. Coincident with the increase\u000a      in water exports over the last 50 years or so, there have been sizable\u000a      declines in the abundance of several fish species in the Sacramento and\u000a      San Joaquin river systems. Conflicts have arisen between various\u000a      stakeholders and interest groups regarding how the water is used and\u000a      divided up. There have been many lawsuits and court cases. This work was\u000a      discussed at length in the US Federal District Court (Fresno, California)\u000a      in April 2010.\u000a    6. Ecology, conservation, environment. For Land Care Research\u000a        NZ, [1] \"plays an important role on analysing data that has a\u000a      downstream effect on evolution, ecology and conservation biology\" &#8212; and\u000a      hence environment protection.\u000a    C. Agricultural applications\u000a    Quantitative Trait Loci (QTLs) in agriculture. At the national\u000a        agricultural research centre of Japan (AFFRC), \"in our ... genome\u000a      analysis of livestock and crops, some useful QTLs affecting traits of\u000a      agronomical importance were detected with the developed methods\u000a      implementing RJ-MCMC. A project to produce new cultivars of crop (tomato)\u000a      or breeds of pig with high genetic performance using the information from\u000a      the detected QTLs is now in progress\". [d]\u000a    D. Medical applications\u000a    Protein-DNA interactions and medical implications. Projects at the\u000a      UC Denver Medical School utilizing techniques based on [1] include\u000a      (i) predicting which human variations or mutations are likely to impact\u000a      protein structure and function, thereby causing human disease. This is\u000a      particularly important in rare childhood developmental and neurological\u000a      diseases; (ii) understanding the relationships among humans in order to\u000a      improve interpretation of genome-wide association studies, finding genes\u000a      that are components of quantitative disease; (iii) understanding the role\u000a      of the interaction of T-cell receptors and major histocompatibility\u000a      complex (MHC molecules) on defending disease, and also on causing\u000a      autoimmune disease when things go wrong; (iv) using these methods to\u000a      understand and predict transcription factor binding mutations, which also\u000a      can lead to disease and disease or drug interaction modifiers; (v)\u000a      understanding the biology of transposable elements, which are often\u000a      heavily implicated in novel diseases, particularly neurological disease,\u000a      and can also be useful for predicting gene regions that are likely\u000a      disease-causing mutations.\u000a    E. Social and commercial applications\u000a    Exchange reserves and Criminology in India. (i) Models quantifying\u000a      sufficiency of foreign exchange reserves: the Reserve Bank of India\u000a      uses [1] for variable selection within a quantile regression model\u000a      framework for studying adequacy of foreign exchange reserves to meet US$\u000a      demand in India under stressful market conditions. Due to the in-house\u000a      nature of these models, these are not published or shared.\u000a    (ii) Models for studying crime rates in different states of India: a\u000a      project at the Reserve Bank is attempting to determine relevance of\u000a      socio-economic variables in determining level of crimes in Indian states.\u000a      \"Understanding the relevance of various factors in determining crime rate\u000a      is very important in controlling crime rate. For instance, lack of toilet\u000a      and drinking water facilities require women in India to go away from her\u000a      house\/hutment, which increase rape rate. Thus, a positive association\u000a      between crime against women and lack of toilet\/drinking water facilities\u000a      demand public policy in developing these basic necessities. Probabilistic\u000a      models [based on [1] are] likely to throw light on such aspects of crimes\u000a      in India\".\u000a    F. Applied image analysis and computer vision\u000a    1. Computer vision &#8212; object tracking. At SORMEA, a French\u000a      company specializing in measurements and surveys, studies and modelling,\u000a      Geographic Information Systems, acoustics and product development to\u000a      improve road safety, an automatic vision-based multi-vehicle tracking\u000a      system for measuring vehicle flows on crossroads &amp; roundabouts,\u000a      yielding provenance &amp; destination statistics, has been constructed\u000a      using [1]. These statistics are requested by local communities in order to\u000a      decide whether or not to create, extend or modify crossroads &amp;\u000a      roundabouts. The system currently is commercially exploited:\u000a      http:\/\/www.sormea.fr\/fr\/r-d-innovation-anacomda\/anacomda-o-d.html\u000a    2. Imaging of geosynchronous orbits, managing space debris. The\u000a      Lawrence Livermore National Laboratory of the US Department of Energy\u000a      conducted a project to understand how conventional astronomical facilities\u000a      might aid in determining the distribution of space debris in\u000a      geosynchronous orbit. The principal application of this work is to protect\u000a      valuable space assets from collisions with debris. [1] was applied \"to\u000a      select different possible pairings of orbital tracks seen in optical\u000a      telescopes in a Monte Carlo framework\". [e]\u000a    G. Implementations of RJMCMC in Software\u000a    1. Mr Bayes: [1] is used as one of several standard techniques in\u000a      the software MrBayes [f]. The use of reversible jump MCMC was recently\u000a      expanded to integration over nucleotide substitution models, and this is\u000a      quickly becoming a standard procedure in analyses using the software. The\u000a      software is widely used across the life sciences for comparative genetics\u000a      and genomics studies, and more generally for studies in evolutionary\u000a      biology. The software has attracted more than 16,000 citations to date. It\u000a      is used widely in research but also in a number of applied contexts. One\u000a      applied context concerns the identification of strains of disease\u000a      organisms. Another focuses on phylogenetic studies (inference of\u000a      evolutionary trees). Among other things, the evolutionary trees form the\u000a      basis for classifications used in natural history museums around the world\u000a      and in a wide range of applications related to the environment.\u000a    2. WinBugs: The WinBugs system [g] is respected software for\u000a      Bayesian analysis, widely used by applied statisticians in both the\u000a      private and public sectors, and its scope has been recently extended to\u000a      support fitting of a wide range of trans-dimensional models, including\u000a      variable selections, automatic curve-fitting using splines, Bayesian\u000a      Multivariate adaptive regression splines (MARS) and Classification and\u000a      regression trees (CART), normal mixture analysis, spatial epidemiology\u000a      clustering models and variable-order Markov chains. All of these\u000a      additional functions are based on [1].\u000a    3. LIS: NASA's Land Information System (LIS) [h] is a software\u000a      framework for high performance land surface modelling and data\u000a      assimilation. LIS is led by the Hydrological Sciences Branch at NASA's\u000a      Goddard Space Flight Center. LIS software tools are used to develop\u000a      customized Land Data Assimilation Systems at NASA's Goddard Space Flight\u000a      Centre, NOAA's National Centres for Environmental Prediction and the Air\u000a      Force Weather Agency. MCMC methods including [1] are currently being\u000a      implemented and incorporated into the system.\u000a    ","ImpactSummary":"\u000a    Reversible Jump Markov chain Monte Carlo, introduced by Peter Green [1]\u000a      in 1995, was the first generic technique for conducting the computations\u000a      necessary for joint Bayesian inference about models and their parameters,\u000a      and it remains by far the most widely used, 18 years after its\u000a      introduction. The paper has been (by September 2013) cited over 3800 times\u000a      in the academic literature, according to Google Scholar, the vast majority\u000a      of the citing articles being outside statistics and mathematics. This case\u000a      study, however, focusses on substantive applications outside academic\u000a      research altogether, in the geophysical sciences, ecology and the\u000a      environment, agriculture, medicine, social science, commerce and\u000a      engineering.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Bristol\u000a    ","Institutions":[{"AlternativeName":"Bristol (University of)","InstitutionName":"University of Bristol","PeerGroup":"A","Region":"South West","UKPRN":10007786}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"5419384","Name":"Denver"}],"References":"\u000a    \u000a*[1] Green, P. J. (1995). Reversible jump Markov chain Monte Carlo\u000a      computation and Bayesian model determination, Biometrika, 82,\u000a      711-732. DOI: 10.1093\/biomet\/82.4.711\u000a    \u000a\u000a*[2] Richardson, S. and Green, P. J. (1997). On Bayesian analysis of\u000a      mixtures with an unknown number of components (with discussion). Read to\u000a      the Royal Statistical Society on 15 January 1997. Journal of the Royal\u000a        Statistical Society (B), 59, 731-792. DOI:\u000a      10.1111\/1467-9868.00095\u000a    \u000a\u000a*[3] Giudici, P. and Green, P. J. (1999). Decomposable graphical Gaussian\u000a      model determination, Biometrika, 86, 785-801. DOI:\u000a      10.1093\/biomet\/86.4.785\u000a    \u000a* reference that best indicates the quality of the underpinning research\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    Major correspondents include:\u000a      [a] Research Geophysicist, USGS Crustal Geophysics and Geochemistry\u000a      Science Center, Denver, Colorado.\u000a      Corroborates item A2 in section 4.\u000a      [b] Scientist, Statistics &amp; Chemometrics, Shell Technology Centre,\u000a      Chester, UK.\u000a      Corroborates item A4 in section 4.\u000a      [c] President of Cox Associates Consulting, Denver, Colorado.\u000a      Corroborates item A5 in section 4.\u000a      [d] Project leader, Agriculture, Forestry and Fisheries Research Council\u000a      (Japan).\u000a      Corroborates item C in section 4.\u000a      [e] Research scientist, Physics division, Lawrence Livermore National\u000a      Laboratory, California.\u000a      Corroborates item F2 in section 4.\u000a    Links to software cited:\u000a      [f] http:\/\/mrbayes.net\u000a      Corroborates item G1 in section 4.\u000a      [g] http:\/\/www.mrc-bsu.cam.ac.uk\/bugs\/welcome.shtml\u000a      Corroborates item G2 in section 4.\u000a      [h] http:\/\/lis.gsfc.nasa.gov\/\u000a      Corroborates item G3 in section 4. \u000a    ","Title":"\u000a    Using the data to choose the best model for a statistical analysis, using\u000a      Reversible Jump Markov chain Monte Carlo: generic model choice for an\u000a      evidence-informed society\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Statistical analysis of data is a ubiquitously dominant ingredient in\u000a      evidence-based decision making across virtually all fields of human\u000a      endeavour; most of such analysis is based on statistical models, and much\u000a      of this either entails formal choice between models with differing numbers\u000a      of parameters, or requires models with variable-dimension parameters. The\u000a      research underpinning this impact case study consists of work carried out\u000a      from 1993 at the University of Bristol by Peter Green, culminating with\u000a      the 1995 publication of a paper [1] in Biometrika, which\u000a      introduced Reversible Jump Markov chain Monte Carlo (RJMCMC), a\u000a      simulation-based methodology for fitting Bayesian statistical models that\u000a      have variable-dimension parameters. Mathematically, Reversible Jump is\u000a      formalism for Metropolis-Hastings MCMC on a general state space consisting\u000a      of a countable union of Euclidean spaces of differing dimensions. The\u000a      paper included 3 illustrative applications. Over the following few years,\u000a      Green developed many more substantial applications of RJMCMC in\u000a      collaborative research projects, and several resulting publications\u000a      [including 2-3] have themselves all stimulated further research and are\u000a      well-cited.\u000a    "},{"CaseStudyId":"40330","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Development of models that engage and help policy makers make informed\u000a      decisions remains a\u000a      significant problem for complexity science research. The current case\u000a      study based on the Humber\u000a      region is a step in this direction. Managing the development of a new\u000a      bio-based economy in the\u000a      Humber region requires the understanding and modelling of complex,\u000a      interacting, socio-economic\u000a      factors.\u000a    The impact of the research is twofold. Firstly, the participants and\u000a      policy makers were introduced to\u000a      systems level and complexity-science based thinking about the key factors\u000a      influencing the start-up\u000a      of a bio-based economy, how these factors interact with each other, and\u000a      how to turn the\u000a      information into a mathematical model. The response of the HEM group and\u000a      other stakeholders to\u000a      this approach has been enthusiastic with over half the participants of the\u000a      original workshop sending\u000a      long responses suggesting improvements to the model highlighting their\u000a      important engagement in\u000a      the process. For example, the CEO of Global Biotechnology Transfer\u000a      Foundation, a not-for-profit\u000a      United Nations partner organisation, wrote \"I found the interaction\u000a        with the FCM workshop both\u000a        useful and enlightening and in particular the mathematical modelling\u000a        aspect. The FCM exercise\u000a        significantly affected my thinking ....\"\u000a    The Programme Manager for Humber Chemical Focus Ltd) wrote: \"The FCM\u000a        workshop was useful\u000a        and informative. This interaction between multiple organisations on the\u000a        same topic is useful and\u000a        cannot easily be replicated in normal business-public sector\u000a        interactions. The FCM modelling\u000a        approach had a clear impact on this interaction.\"\u000a    While the model is becoming progressively more complex, it is also\u000a      becoming easier for the\u000a      managers to use. The model explores the consequences of several scenarios\u000a      that might influence\u000a      the start-up of a new business sector. This output has flagged up key\u000a      points for the HEM group\u000a      and the South Humber gateway delivery board when discussing strategy\u000a      related to a bio-based\u000a      economy. Since several industries, NGOs and local authorities have taken\u000a      part in the modelling\u000a      process and are carrying out R&amp;D in various bio-based technologies or\u000a      are involved in regional\u000a      planning, the model automatically has engagement with the decision makers\u000a      &#8212; in particular the\u000a      SHGB and Local Authorities who carry out strategic planning for the\u000a      region. They do not make\u000a      policy, but make highly influential decisions about the region's future,\u000a      and therefore indirectly\u000a      impact public policy.\u000a    Secondly, a tool for investigating various policy\/strategy decisions\u000a      based on network-flows was\u000a      created. The main aim of the model is to generate a set of questions for\u000a      decision makers to think\u000a      about when designing policies or making strategic decisions. The ERIE\u000a      project now has an\u000a      established engagement with HEM and SHGB with additional evolution\u000a      expected to develop over\u000a      the next three years.\u000a    At present the principal target audience is the team in the Humber\u000a      region. As the methodology\u000a      develops and becomes more generic, it is planned to roll it out to other\u000a      regions. Preliminary\u000a      discussions with DEFRA (Department for Environment, Food and Rural\u000a      Affairs) have taken place.\u000a    ","ImpactSummary":"\u000a    Industrial regions around the UK are seeking to develop bio-based\u000a      economies in order to minimise\u000a      their CO2 emissions and stimulate economic regeneration.\u000a    Researchers at Surrey, in collaboration with key industrialists from the\u000a      Humber region, have\u000a      produced a mathematical model of the main factors influencing the\u000a      transition to, and establishment\u000a      of, a bio-based economy. This model has been used by the Humber\u000a      Environmental Managers\u000a      (HEM) group, and the Humber local authorities to help guide strategic\u000a      planning for the region. The\u000a      outcome is that the research has contributed to environmental improvement\u000a      and economic\u000a      regeneration of the Humber region, and has indirectly impacted on public\u000a      policy.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University of Surrey\u000a    ","Institutions":[{"AlternativeName":"Surrey (University of)","InstitutionName":"University of Surrey","PeerGroup":"B","Region":"South East","UKPRN":10007160}],"Panel":"B         ","PlaceName":[],"References":"\u000a    The development of the model has been on going and as new versions emerge\u000a      they have been\u000a      presented to the HEM group. Feedback from these presentations has then\u000a      been used to further\u000a      refine the model. A paper based on this sequence of interactions has\u000a      appeared in PLoS ONE\u000a      (Penn et al, 2013). Since the analysis is based on participatory\u000a      modelling, it is natural for the\u000a      paper to appear coincident with the impact. A second paper (Knight et al,\u000a      2013) gives a dynamical\u000a      systems analysis of the model.\u000a    \u000a&#8226; A.S. Penn, C.J.K. Knight, D.J.B. Lloyd, D. Avitabile, K. Kok, F.\u000a      Schiller, A Woodward, A.\u000a      Druckman, and L. Basson (2013) Participatory development and analysis\u000a        of a fuzzy\u000a        cognitive map of the establishment of a bio-based economy in the Humber\u000a        region, PLOS\u000a      ONE, DOI: 10.1371\/journal.pone.0078319.\u000a    \u000a\u000a&#8226; C.J.K. Knight, D.J.B. Lloyd, and A.S. Penn (2013) Linear and\u000a        sigmoidal fuzzy cognitive\u000a        maps: an analysis of fixed points, Applied Soft Computing (accepted,\u000a      in press).\u000a    \u000aThe principal funding is the EPSRC ERIE grant:\u000a    &#8226; EP\/H021779\/1 Evolution and Resilience of Industrial Ecosystems,\u000a      PI: GN Gilbert\u000a      (Sociology), co-investigators: R Hoyle (Maths), D Lloyd (Maths), A Skeldon\u000a      (Maths), P\u000a      Krause (Computing), L Basson (CES), S Moschoyiannis (Computing),\u000a      &#163;3,344,524\u000a    The HEM is made up of representatives of local heavy industries,\u000a      conservation organisations\u000a      (NGOs) and other key regional figures, for example the National Industrial\u000a      Symbiosis Programme.\u000a      It is a collaborative group which discusses issues relevant to industry\u000a      and sustainability in the\u000a      region. It allows industry, conservation bodies and local authorities to\u000a      communicate and come up\u000a      with strategies and solutions for common problems.\u000a    The SHGB is a strategic regional planning group whose members include\u000a      North Lincolnshire\u000a      Council chief executives. The board's terms of reference, include\u000a      \"providing strategic direction\" to\u000a      the South Humber Gateway and \"identifying constraints to investment and\u000a      development and\u000a      seeking their early resolution.\"\u000a      http:\/\/www.northlincs.gov.uk\/business\/investing-in-north-lincolnshire\/southhumbergateway\/shgb\/\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000a    The principal impact is on the strategy and implementation policy of the\u000a      range of government\u000a      groups (HEM, SHGB), heavy industry, and NGOs. The principal impact on the\u000a      HEM and SHGB\u000a      will be an impact on the environment and indirectly on the economy, since\u000a      one of the primary\u000a      focuses is the transition to a green economy.\u000a    The impact of the FCM exercise has been documented in statements from\u000a    \u000a      The Programme Manager at Humber Chemical Focus Ltd (www.hcfhumber.co.uk)\u000a      The Chief Executive Officer at Global Biotechnology Transfer\u000a        Foundation, an international\u000a        not-for-profit United Nations Foundation partner organisation. Their\u000a        mission is to promote\u000a        awareness of the potential for biotechnology to support sustainable,\u000a        long-term, socio-\u000a        economic development. (NB. The CEO has since joined the ERIE Advisory\u000a        Board, as a\u000a        result of his interest after participating in the FCM Workshop.)\u000a    \u000a    Additional impact is corroborated by :\u000a    \u000a      The Environmental Manager at Croda Europe Ltd. Contact details\u000a        provided.\u000a      A representative of the Economic Regeneration Department of the North\u000a        East Lincolnshire\u000a        Council. Contact details provided.\u000a      The Company Director of Link2Energy. Contact details provided.\u000a    \u000a    The above letters and emails establish (1) the importance of looking at\u000a      the bio-based economy for\u000a      the Humber region and further afield, (2) That they found the interaction\u000a      with the FCM\u000a      workshop\/ERIE useful and enlightening. (3) That the interaction has helped\u000a      change their thinking\u000a      or any decision pathways.\u000a    ","Title":"\u000a    Modelling the evolution of a bio-based economy in the Humber region\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The Surrey team, led by a mathematician (and consisting of environmental\u000a      scientists, sociologists,\u000a      engineers and computer scientists) was formed in 2010, as part of the\u000a      EPSRC-funded \"Evolution\u000a      and Resilience of Industrial Ecosystems\" (ERIE) project, to study the\u000a      application of complexity\u000a      science to social and economic systems.\u000a    The Centre for Environmental Strategy (CES) has run a longstanding case\u000a      study of the Humber\u000a      region, which is one of the UK's major energy generators and CO2 emitters,\u000a      and they introduced\u000a      ERIE to Humber. To initiate the project, researchers carried out 18\u000a      interviews with key\u000a      stakeholders in the region to survey the main issues, identify parameters,\u000a      and specifics that\u000a      needed to be addressed for the Humber region. The development and analysis\u000a      of these interviews\u000a      were underpinned by quantitative sociology and complexity science. An\u000a      important problem which\u000a      emerged was how to mathematically model the transition from a\u000a      predominantly fossil fuel based\u000a      economy to a bio-based economy. A mathematics-sociology team then set in\u000a      motion a strategy for\u000a      modelling the transition.\u000a    The Surrey team led by David Lloyd (Lecturer in Mathematics) and\u000a      Alexandra Penn (ERIE\u000a      Research Fellow) consists of Lauren Basson (Lecturer in CES), Angela\u000a      Druckman (Senior Lecturer\u000a      in CES), Frank Schiller (ERIE Research Fellow), Chris Knight (ERIE\u000a      Research Fellow), Amy\u000a      Woodward (ERIE PhD student, and ERIE Administrator) and Daniele Avitabile\u000a      (former ERIE\u000a      Research Fellow, now Lecturer at Nottingham). Kasper Kok (Lecturer,\u000a      University of Wageningen,\u000a      Netherlands) facilitated the participatory modelling exercise.\u000a    On the basis of the requirements identified via the aforementioned\u000a      interviews, the ERIE team\u000a      studied the \"participatory modelling\" literature to identify an\u000a      appropriate modelling strategy capable\u000a      of representing the interaction of the key factors influencing the Humber\u000a      region. The basic model\u000a      proposed was a Fuzzy Cognitive Mapping (FCM), which was formulated as a\u000a      dynamical system,\u000a      X(n+1) = f(AX(n)), where X\u000a      has m components, A is a weighted connectivity matrix, and f\u000a      is a\u000a      mapping. The connectivity matrix A was formulated via \"participatory\u000a      modelling\": the participants\u000a      identify key factors that will influence the development of a nascent\u000a      bio-based economy (land\u000a      availability, fossil fuel price, community acceptance, etc), and they are\u000a      weighted and linked by a\u000a      directed graph. Participatory modelling was used as it has the significant\u000a      advantage of establishing\u000a      the engagement and buy-in with stakeholders that are vital for any model\u000a      to have impact in\u000a      decision-making. FCM in particular was used as it is a well-established\u000a      modelling methodology for\u000a      exploring complex issues with stakeholders that is able to yield quick and\u000a      useful results during a\u000a      workshop, and is amenable to a dynamical systems analysis.\u000a    The HEM group welcomed the initiative and the first meeting was held in\u000a      November 2011 in\u000a      Humber with 12 local industrialists and policy makers, including members\u000a      of the regional HEM\u000a      group and the South Humber Gateway Board (SHGB), with the Surrey team\u000a      leading the meeting.\u000a      The meeting participants were guided to create links between these factors\u000a      and describe the links'\u000a      strengths. These factors were then connected with a cognitive map. The\u000a      connections were each\u000a      assigned a weight, and they formed the entries of the connectivity matrix.\u000a      By then choosing an\u000a      appropriate f the dynamical system could be implemented\u000a      immediately at the meeting and a range\u000a      of scenarios tested. The results of this meeting led to the paper of Penn\u000a      et al. (2013).\u000a    Once the main parameters of the model were proposed it was important for\u000a      the mathematicians to\u000a      test for robustness of the model. Dynamical systems theory, with\u000a      appropriate modification for the\u000a      constraints induced by the connectivity matrix, was applied to the model.\u000a      Different representations\u000a      for f were tested, the fixed points of the model were identified,\u000a      and stability and dynamics tested.\u000a      The principal nonlinear f tested was the sigmoidal model.\u000a      These results were reported in Knight et\u000a      al (2013). This research was then fed back into the next iteration of the\u000a      model development.\u000a    Since the initial workshop, the Surrey team has met regularly with the\u000a      HEM and SHBG groups and\u000a      members of the Local Authorities, with two subsequent workshops in\u000a      February 2012 and 2013,\u000a      firstly with HEM and then with the larger group, with the February 2012\u000a      meeting used for\u000a      verification of the FCM. The model continues to evolve taking into account\u000a      feedback from the HEM\u000a      group. Current improvements include coupling to take into account feedback\u000a      control and\u000a      exploration of regionally contentious feedstock supply and land use\u000a      scenarios.\u000a    "},{"CaseStudyId":"40534","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council","Natural Environment Research Council"],"ImpactDetails":"\u000d\u000a    The principal impact of the case study has been in the take-up by the Met\u000d\u000a      Office. It is using the simple model to identify key weaknesses in DA\u000d\u000a      algorithms. Following the publication of the results [1,3], the Met Office\u000d\u000a      DA group recognised the value of using the 3-body problem in an attempt to\u000d\u000a      reconcile the theoretical limitations of 4DVAR with its practical success\u000d\u000a      in situations far from those for which it is valid. The calculation of the\u000d\u000a      background error covariance statistics of the variables used in data\u000d\u000a      assimilation schemes is a crucial part of the algorithm. In an email\u000d\u000a      communication, the Leader of the Met Office DA Unit wrote to Ian Roulstone\u000d\u000a      \"The work has had a large influence on our 4DVAR strategy, as it\u000d\u000a        probably explains why some of our recalculations of Cov were successful\u000d\u000a        (and were implemented).\"\u000d\u000a    The Met Office recognised that the 4D VAR scheme for the 3-body problem,\u000d\u000a      developed by Roulstone et al., would serve as a useful test bed in which\u000d\u000a      \"challenging but realistic scenarios\" could be studied. The Met Office\u000d\u000a      team set out to examine whether the standard method of calculating the\u000d\u000a      background penalty was still optimal in the presence of model error. The\u000d\u000a      relative simplicity of the 3-body problem (and the fact that in an\u000d\u000a      idealised setting the notion of `truth' can be defined precisely) enabled\u000d\u000a      them to study this question in detail, and the outcome proved to be of\u000d\u000a      major significance. Analysis of the standard 4DVAR technique applied to\u000d\u000a      the 3-body problem revealed that the widely accepted notion of calculating\u000d\u000a      the background error covariance from the mismatch between forecasts to\u000d\u000a      estimates of the truth was flawed. Better forecasts could be obtained by\u000d\u000a      excluding the mismatch between the forecast and the truth resulting from\u000d\u000a      systematic model error.\u000d\u000a    The follow-up by the Met Office DA unit has had far-reaching\u000d\u000a      implications: not only is the Met Office now pursuing several lines of\u000d\u000a      research to improve the calculations of Cov, but they are also having to\u000d\u000a      review how they conduct forecast verification. Recognising that the\u000d\u000a      current calculation of Cov in the operational model was therefore flawed,\u000d\u000a      implementing even a simple correction, mimicking the techniques applied to\u000d\u000a      improve the simulation of the 3-body problem, led to marked improvements\u000d\u000a      in forecasts. The Leader of the DA unit at the Met Office states;\u000d\u000a    \"As a result of subsequent projects carried out by the Met Office\u000d\u000a        using this model, major investments have been made in improving the\u000d\u000a        operational covariance model, which have had an impact on forecast\u000d\u000a        accuracy, and new methods of generating forecast and analysis ensembles\u000d\u000a        are being actively studied.\"\u000d\u000a    The 3-body problem facilitated a thorough mathematical analysis of the\u000d\u000a      implementation of the background error covariance term in 4DVAR. The two\u000d\u000a      timescales of the 3-body problem, which play a key role in the definition,\u000d\u000a      and representation, of short-range and long-range forecast errors, the\u000d\u000a      inherent Hamiltonian properties, and the fact that the 'true state' of the\u000d\u000a      system can be defined precisely in a toy model, enabled the Met Office DA\u000d\u000a      unit to identify weaknesses in the standard formulation of 4DVAR in which\u000d\u000a      the background is designed to approximate the 'truth'. The salient new\u000d\u000a      idea, namely that the background term should be replaced by a\u000d\u000a      'regularization factor' (reported in the follow-up papers, Cullen (2010a,\u000d\u000a      2010b), cited below), the optimal choice of which minimises the short\u000d\u000a      range forecast errors, was formulated on the basis of the study of the\u000d\u000a      3-body problem. This regularization procedure has also been employed by\u000d\u000a      Delahaies et al [7] in a 4DVAR approach to modelling the terrestrial\u000d\u000a      carbon cycle.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Data assimilation is playing an ever increasing role in weather\u000d\u000a      forecasting. Implementing four- dimensional variational data\u000d\u000a        assimilation (4DVAR) is part of the long term strategy of the UK Met\u000d\u000a      Office.\u000d\u000a    In this case study, an idealised 4DVAR scheme, developed by a team from\u000d\u000a      the Universities of Surrey and Reading working with the UK Met Office,\u000d\u000a      based on the integration of Hamiltonian dynamics and nonlinearity into\u000d\u000a      data assimilation, has now been taken up by the Met Office. It is being\u000d\u000a      used to evaluate options for improving operational 4DVAR. The simplicity\u000d\u000a      of the scheme developed by this team has facilitated careful analyses of\u000d\u000a      some generic problems with the operational model. The outcome includes\u000d\u000a      direct impact on the environment and indirect impact on the economy, both\u000d\u000a      through improvements in weather forecasting.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Surrey\u000d\u000a    ","Institutions":[{"AlternativeName":"Surrey (University of)","InstitutionName":"University of Surrey","PeerGroup":"B","Region":"South East","UKPRN":10007160}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1. L.R. Watkinson, A.S. Lawless, N.K. Nichols &amp; I. Roulstone (2005) Variational\u000a        data assimilation for Hamiltonian problems, International Journal\u000d\u000a      Numerical Methods in Fluids, 47 1361-1367. DOI: 10.1002\/fld.844\u000d\u000a    \u000a\u000a2. M. Wlasak, N.K. Nichols &amp; I. Roulstone (2006) Use of potential\u000d\u000a        vorticity for incremental data assimilation, Quarterly J Royal\u000d\u000a      Meteorological Society, 132, 2867-2886. DOI: 10.1256\/qj.06.02\u000d\u000a    \u000a\u000a3. L.R. Watkinson, A.S. Lawless, N.K. Nichols &amp; I. Roulstone (2007) Weak\u000a        constraints in four- dimensional variational data assimilation,.\u000d\u000a      Meteorologische Zeitschrift, 16, 767- 776. DOI: 10.1127\/0941-2948\/2007\/0249\u000d\u000a    \u000a\u000a4. I. Roulstone (2006) Data assimilation and the 2- and 3-body\u000d\u000a        problems, Oberwolfach Reports 39, 2356-2359.\u000d\u000a    \u000a\u000a5. A.C. Rudd, I. Roulstone, &amp; J.R. Eyre (2012) A simple column\u000d\u000a        model to explore anticipated problems in variational assimilation of\u000d\u000a        satellite observations, J Env. Mod. &amp; Software 27-28,\u000d\u000a      23-29. DOI:\u000d\u000a        10.1016\/j.envsoft.2011.10.001\u000d\u000a    \u000a\u000a6. D. Fairbairn, S.R. Pring, A.C. Lorenc, &amp; I. Roulstone (2013) A\u000d\u000a        comparison of 4D-Var with ensemble data assimilation methods, Q.J.\u000d\u000a      Roy. Met. Soc. (in press, published online in May 2013). DOI: 10.1002\/qj.2135\u000d\u000a    \u000aThe research on DA has been supported by EPSRC (EP\/C0006208\/1, which\u000d\u000a      looked at stochastic perturbations in DA), NERC through the National\u000d\u000a      Centre for Earth Observation (&#163;550k, 2008-13): Ian Roulstone is national\u000d\u000a      co-theme leader of the DA Theme in NCEO. http:\/\/www.nceo.ac.uk\u000d\u000a    Roulstone presented a talk and report [4] on Hamiltonian methods in Data\u000d\u000a      Assimilation at the Oberwolfach programme on the \"Mathematical Theory\u000d\u000a        and Modelling in Atmosphere-Ocean Science\" in August 2006. Although\u000d\u000a      that meeting was focussed on potential improvements to forecasting\u000d\u000a      techniques, Roulstone's talk precipitated a discussion about a future\u000d\u000a      Oberwolfach programme on the mathematics of data assimilation. That\u000d\u000a      programme was approved and the meeting was held in December 2012.\u000d\u000a    An example of secondary impact is the application to modelling of the\u000d\u000a      terrestrial carbon cycle. A recent talk on this was given by Delahaies at\u000d\u000a      the EGU in April.\u000d\u000a    \u000d\u000a      S. Delahaies, I. Roulstone &amp; N. Nichols (2013) A\u000d\u000a          regularization of the carbon-cycle data- fusion problem,\u000d\u000a        Geophysical Research Abstracts 15, EGU2013-4087-1.\u000d\u000a    \u000d\u000a    The impact was facilitated by the visit of Gordon Inverarity (of the UK\u000d\u000a      Met Office) to participate in a workshop on data assimilation held at the\u000d\u000a      University of Surrey in October 2007.\u000d\u000a    \u000d\u000a      G. Inverarity Theoretical foundations of data assimilation using\u000d\u000a          nonlinear forecast models, Talk at the Surrey themed seminar\u000d\u000a        series, 10th October 2007\u000d\u000a    \u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"4","Level2":"1","Subject":"Atmospheric Sciences"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a    The impact is corroborated by emails from the Leader of the Met Office\u000d\u000a      unit, tasked with exploitation of satellite data in numerical weather\u000d\u000a      prediction, and emails plus a letter from the Leader of the Data\u000d\u000a      Assimilation unit at the Met Office.\u000d\u000a    \u000d\u000a      Leader of the DA unit at the Met Office. Provided statement.\u000d\u000a    \u000d\u000a    Following on from the results of the case study, results of the Met\u000d\u000a      Office DA unit research give practical guidance as to ways of treating\u000d\u000a      analysis error, and the choice of regularisation that should be adopted in\u000d\u000a      operational 4D-VAR schemes. This follow up Met Office work is published in\u000d\u000a      the open literature.\u000d\u000a    \u000d\u000a      Cullen, MJP. A demonstration of 4D-Var using a time-distributed\u000d\u000a          background term, Q. J. R. Meteorol. Soc. 136 1301-1315\u000d\u000a        (2010a).\u000d\u000a      Cullen, MJP. A demonstration of cycled 4D-Var in the presence of\u000d\u000a          model error, Q. J. R. Meteorol. Soc. 136 1379-1395\u000d\u000a        (2010b).\u000d\u000a    \u000d\u000a    and can be corroborated by;\u000d\u000a    \u000d\u000a      Leader of the unit tasked with exploitation of satellite data in\u000d\u000a        numerical weather prediction at the Met Office. Contact details\u000d\u000a        provided.\u000d\u000a    \u000d\u000a    The implications of the research in the context of climate modelling can\u000d\u000a      be found in the following technical report.\u000d\u000a    \u000d\u000a      D. Pearson (2011) A Primer on data assimilation, parameter\u000d\u000a          estimation and automatic differentiation with examples, Hadley\u000d\u000a        Centre Tech Report\u000d\u000a        85, http:\/\/www.metoffice.gov.uk\/media\/pdf\/k\/o\/HCTN_85.pdf\u000a\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Exploiting nonlinearity in operational data assimilation for weather\u000d\u000a        prediction\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Data assimilation (DA) is a technique for combining mathematical models\u000d\u000a      of physical systems with measurements of those systems, in order to\u000d\u000a      establish either the state of the system, or the parameters in the models.\u000d\u000a      Such techniques have been used extensively in weather and climate\u000d\u000a      prediction. 4DVAR calculates a forecast that best fits the available\u000d\u000a      observations of weather, to within the observational error over a period\u000d\u000a      of time. Since there will inevitably be insufficient data to calculate the\u000d\u000a      present state of the atmosphere with certainty, data assimilation research\u000d\u000a      often focuses on ways of using auxiliary information in the forecasting\u000d\u000a      algorithms.\u000d\u000a    Surrey's Ian Roulstone (Professor of Mathematics) has been working in the\u000d\u000a      area of DA, in the context of weather prediction, for over ten years. The\u000d\u000a      motivation for this particular project was twofold: how to utilise\u000d\u000a      conservation laws to mollify the problem of sparse data coverage in\u000d\u000a      situations where nonlinearity becomes important, and how to rectify the\u000d\u000a      inability of the operational scheme to represent rapidly growing modes.\u000d\u000a      The approach taken, motivated by a Met Office strategy for evaluating new\u000d\u000a      research directions by testing ideas on systems that are simpler than the\u000d\u000a      full operational forecasting model, was to study a simple nonlinear system\u000d\u000a      with the key attributes of nonlinearity and conservation laws as well as\u000d\u000a      unstable modes, namely the 2- and 3- body problem [4].\u000d\u000a    It had been known for some time that conservation laws provide a rational\u000d\u000a      basis for incorporating new observational data into forecast models, but\u000d\u000a      the methodology was hitherto somewhat ad hoc, and often difficult to\u000d\u000a      implement. In an earlier study of Wlasak, Nichols &amp; Roulstone [2],\u000d\u000a      potential vorticity (PV), an important conserved quantity, was exploited\u000d\u000a      in a DA scheme for a simplified shallow water model, and improvements were\u000d\u000a      found when it was used to project observational data onto the important\u000d\u000a      modes of atmospheric motion. Hence, a more systematic study was in order.\u000d\u000a    The aim of the underpinning research [1,3,4] was to establish whether\u000d\u000a      Hamiltonian properties of nonlinear dynamical systems could be exploited\u000d\u000a      more generally in the formulation of 4DVAR. The results presented in [1,3]\u000d\u000a      demonstrated conclusively that invariants of dynamical systems could be\u000d\u000a      systematically incorporated into 4DVAR schemes. In particular, the\u000d\u000a      inability of the operational scheme to represent rapidly growing modes and\u000d\u000a      the problem of forecasting poorly observed modes have been studied in the\u000d\u000a      simplified model, and new ways to improve the operational models have been\u000d\u000a      formulated. The model developed in [1,3] supports rapidly growing\u000d\u000a      perturbations, so it is suitable for investigating why the so-called\u000d\u000a      analysis error does not project strongly onto the rapidly growing modes of\u000d\u000a      the forecast.\u000d\u000a    The underpinning research continued with the EPSRC-CASE funded 2009 PhD\u000d\u000a      thesis of Alison Rudd on \"The effect of nonlinearity on the variational\u000d\u000a        assimilation of satellite observations using a simple column model\",\u000d\u000a      where nonlinearity, in the context of DA, was studied in a columnar model\u000d\u000a      which was still simplified but closer to the meteorological context. The\u000d\u000a      results were published in [5], showing that nonlinearity also dramatically\u000d\u000a      affected the \"tangent linear\" assumption. The Leader of the Met Office\u000d\u000a      unit, tasked with exploitation of satellite data in numerical weather\u000d\u000a      prediction, wrote \"Rudd's work showed that we needed to sharpen our\u000d\u000a        approach to understanding the basis for assimilating satellite radiances\u000d\u000a        from cloudy regimes.\" In current work with EngD student David\u000d\u000a      Fairbairn (joint between Surrey and the Met Office) the aim is to apply\u000d\u000a      the theory to models closer to the operational system [6].\u000d\u000a    The Surrey team is composed of Ian Roulstone (Professor), Sylvain\u000d\u000a      Delahaies (NERC-funded Postdoc, 2008-present), Andrew Lorenc (Visiting\u000d\u000a      Professor), Alison Rudd (EPSRC-CASE supported PhD student, completed in\u000d\u000a      2009), David Fairbairn (EPSRC supported EngD, started in 2010). The\u000d\u000a      Reading team consists of Nancy Nichols (Professor), Amos Lawless\u000d\u000a      (Lecturer) and Laura Watkinson (EPSRC-CASE supported PhD, completed in\u000d\u000a      2006).\u000d\u000a    "},{"CaseStudyId":"40535","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The interaction between OWEL and Surrey began in August 2011, when a\u000a      member of the research\u000a      and team at OWEL contacted Surrey. They had seen the sloshing website,\u000a      with its published\u000a      papers and reports on shallow water sloshing in three-dimensional rotating\u000a      and translating vessels.\u000a      It was exactly what was needed for their project.\u000a    At the time, OWEL was using CFD (Computational Fluid Dynamics) for fully\u000a      3D simulations.\u000a      However CFD is very time consuming. The Chief Technical Officer at OWEL\u000a      writes \"It takes\u000a        between 3 and 9 days to get a simple CFD model to run and, as we have 60\u000a        odd load cases, it is\u000a        too long by far. CFD is also a bit of an unknown in terms of quality,\u000a        and validating it against\u000a        physical models has been tricky and is on-going.\" The principal CFD\u000a      researcher at OWEL adds \"...\u000a      and we need to increase the real time in simulations by a factor of 10!\u000a        This is clearly impractical.\"\u000a    \"The interaction with Surrey has changed our research programme into\u000a        the modelling of the OWEL\u000a        WEC. This new approach to modelling is being used to (a) optimise the\u000a        power conversion\u000a        performance by simulating various naval architectural layouts and\u000a        control strategies ...; (b) cross-validate\u000a        the CFD models; (c) provide direction for new experiments .... These\u000a        tasks ... are\u000a        essential to the commercial success of the machine.\"\u000a    OWEL was interested in a more refined mathematical model built around the\u000a      shallow water\u000a      equations (SWEs) for the interior flow. By comparison the Surrey shallow\u000a      water code has run\u000a      times of the order of minutes. The OWEL CFD researcher writes \"With the\u000a        speedup obtained\u000a        using the SWEs, it will become the basis of an optimisation tool, which\u000a        is needed to identify optimal\u000a        physical geometry, mooring characteristics, and implementing control\u000a        systems.\"\u000a    Initially the Surrey team sent the SWE codes to OWEL and advised them on\u000a      shallow water\u000a      hydrodynamics. This interaction was low key for the first year. When one\u000a      of the main research\u000a      engineers departed from the OWEL project, a plan was set in motion to\u000a      obtain funding for a large-scale\u000a      Surrey input into the project. An EPSRC grant, awarded in Spring 2013, has\u000a      provided the\u000a      infrastructure to make this happen. The department also provided bridging\u000a      funds, and the post-doc\u000a      (Alemi Ardakani) starting working on the project in March 2013.\u000a    The principal impact, going back to the initial interaction, has been\u000a      that the Surrey team has\u000a      changed the way OWEL\/ITPower approach the problem of using theory and\u000a      simulation to\u000a      understand the problem. CFD has enormous value, but is insufficient as a\u000a      design tool. The\u000a      approach based on the shallow water equations is now a firm part of the\u000a      OWEL R&amp;D strategy.\u000a      The inventor of the original OWEL WEC configuration and founder member of\u000a      the company OWEL\u000a      remarked: \"This is just the kind of underpinning research that I wish\u000a        we had when we started the\u000a        project ten years ago!\"\u000a    \"I believe development of the OWEL WEC would have progressed at a much\u000a        faster rate if the\u000a        modelling capabilities of the Surrey team had been available ten years\u000a        ago. However, their study\u000a        is still timely in that it is complementary to the extensive\u000a        tank-testing result already collected by the\u000a        OWEL team.\" And \"In my view, the improvement in efficiency that I expect\u000a        to follow from the Surrey\u000a        study is likely to lead to the design of the first wave energy device\u000a        that is commercially viable.\"\u000a    The research interaction is now fully developed. In addition to the\u000a      shallow water approach, the\u000a      project has expanded in two other directions: linear and nonlinear\u000a      implications of resonance in the\u000a      OWEL WEC coupled system, the stabilisation and control (the more stable\u000a      the OWEL WEC is\u000a      when at sea, the higher the efficiency in the PTO).\u000a    The Surrey-OWEL interaction is planned for the long term. The EPSRC grant\u000a      runs to 2016, the\u000a      Surrey team has secured funding for a 3.5 year PhD studentship starting in\u000a      October 2013, and\u000a      OWEL has a 20 year plan for implementing successively more refined wave\u000a      energy extraction\u000a      devices, so the Surrey team is impacting R&amp;D at an early stage of the\u000a      project. A test prototype\u000a      (called the \"Marine Demonstrator\") has been designed and, subject to\u000a      construction scheduling, is\u000a      expected to be deployed at \"WaveHub\" in summer 2014. The current OWEL WEC\u000a      has\u000a      deficiencies, and it is the analysis and optimisation to remove these\u000a      deficiencies that is the\u000a      principal aim of the Surrey and OWEL teams, in preparation for the next\u000a      generation prototype.\u000a    The impact on the environment and economy is expected to be significant.\u000a      Approximately 35% of\u000a      the wave energy available to Europe is directed at the UK, which is\u000a      therefore well-positioned to\u000a      lead wave energy extraction research in the region. A strong combined\u000a      industry-government-academia\u000a      initiative is underway and gathering momentum. As of March 2011 the UK has\u000a      3.4MW of\u000a      installed marine energy capacity, with an additional 23MW in planning:\u000a      potentially 2.17GW of\u000a      marine energy projects can be in place by 2020. Indeed, it is predicted\u000a      that marine energy could\u000a      ultimately provide 20% of UK electricity consumption. In preparation the\u000a      government has\u000a      supported the design and construction of \"WaveHub\" which is an offshore\u000a      test bed which WECs\u000a      can plug into and feed generated electricity into the grid.\u000a    On-going impact on the ocean industry includes a \"Water Waves in Industry\u000a      Day\" organised by\u000a      Bridges which will be part of the Newton Institute programme on \"Theory of\u000a      Water Waves\" in July\u000a      2014, where a range of speakers from the ocean industry are invited to\u000a      speak about the role of the\u000a      theory of water waves in their industry. The principal speakers are from\u000a      the ocean wave energy\u000a      industry.\u000a    ","ImpactSummary":"\u000a    Extraction of energy from ocean waves is a high-priority\u000a      sustainable-energy initiative in the UK.\u000a      The OWEL wave-energy convertor involves a floating rectangular box which\u000a      captures waves and\u000a      extracts their energy. This configuration dovetails with research at the\u000a      University of Surrey on fluid\u000a      sloshing in rotating-translating rectangular containers.\u000a    The Surrey team is providing underpinning mathematics for the modelling\u000a      and has led to the\u000a      development of a suite of algorithms that are being tailored for use to\u000a      optimise system parameters.\u000a      The outcome is direct impact on the wave energy industry and indirect\u000a      impact on the environment\u000a      and the economy.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University of Surrey\u000a    ","Institutions":[{"AlternativeName":"Surrey (University of)","InstitutionName":"University of Surrey","PeerGroup":"B","Region":"South East","UKPRN":10007160}],"Panel":"B         ","PlaceName":[],"References":"\u000a    The principal underpinning research is reported in the following five\u000a      papers.\u000a    \u000a1. M.R. Turner, T.J. Bridges &amp; H. Alemi Ardakani. Dynamic\u000a        coupling in Cooker's sloshing\u000a        experiment with baffles, Phys Fluids 25 112102 (2013b).\u000a      http:\/\/dx.doi.org\/10.1063\/1.4827203\u000a    \u000a\u000a2. M.R. Turner &amp; T.J. Bridges. Nonlinear energy transfer between\u000a        fluid sloshing and vessel\u000a        motion, J Fluid Mech 719 606-636 (2013a). \u000a        http:\/\/dx.doi.org\/10.1017\/jfm.2013.29\u000a    \u000a\u000a3. H. Alemi Ardakani, T.J. Bridges &amp; M.R. Turner. Resonance in a\u000a        model for Cooker's\u000a        sloshing experiment, Euro J Mech B\/Fluids 36 25-38 (2012).\u000a      http:\/\/dx.doi.org\/10.1016\/j.euromechflu.2012.04.007\u000a    \u000a\u000a4. H. Alemi Ardakani &amp; T.J. Bridges. Shallow-water sloshing in\u000a        vessels undergoing prescribed\u000a        rigid-body motion in three dimensions, J Fluid Mech 667\u000a      474-519\u000a      (2011). http:\/\/dx.doi.org\/10.1017\/S0022112010004477\u000a    \u000a\u000a5. H. Alemi Ardakani &amp; T.J. Bridges. Dynamic coupling between\u000a        shallow-water sloshing and\u000a        horizontal vehicle motion, Euro J Appl Math 41 479-517\u000a      (2010).\u000a      http:\/\/dx.doi.org\/10.1017\/S0956792510000197\u000a    \u000aThe project has also been supported by four grants.\u000a    &#8226; Leverhulme Trust Fellowship, \"Three-dimensional shallow-water\u000a        sloshing in rotating\u000a        vessels\", 2009-10, &#163; 42K; PI: Bridges\u000a    &#8226; ORS award for PhD studies of H. Alemi Ardakani, 2007-10 (the last ORS\u000a      award made to\u000a      Maths at Surrey by Universities UK before the scheme was discontinued);\u000a      approx &#163;30K;\u000a      PI: Bridges\u000a    &#8226; Internal grant from the department for a one-month postdoc in Spring\u000a      2013.\u000a    &#8226; EPSRC grant EP\/K008188\/1: \"Dynamics of floating water-wave energy\u000a        extraction devices,\"\u000a      PI: Bridges, Co-I: Turner, 2013-2016, &#163;291K,\u000a      http:\/\/gow.epsrc.ac.uk\/NGBOViewGrant.aspx?GrantRef=EP\/K008188\/1\u000a    There is also a wide range of internal reports and preprints and they can\u000a      be found on the\u000a      website: http:\/\/personal.maths.surrey.ac.uk\/st\/T.Bridges\/SLOSH\/\u000a    A Non-disclosure agreement has been signed between the University of\u000a      Surrey and OWEL and\u000a      ITPower Ltd (ITPower Ltd is the largest shareholder and parent company of\u000a      OWEL). This covers\u000a      the exchange of information and data between OWEL\/ITPower and the Surrey\u000a      team. The content\u000a      of this impact case study is general enough that it is not affected by the\u000a      NDA; concomitantly\u000a      discussion of the detailed data is not needed for this impact case study.\u000a      The website for OWEL is http:\/\/www.owel.co.uk\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"11","Subject":"Maritime Engineering"},{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"}],"Sources":"\u000a    The various aspects of the impact are corroborated by letters and emails\u000a      from\u000a    \u000a      The Chief Technical Officer on the OWEL project. Provided statement.\u000a      Marine Engineer and principal CFD researcher on the OWEL project.\u000a        Contact details\u000a        provided.\u000a      The inventor (and original patent holder) of the OWEL WEC prototype\u000a        and founder member\u000a        of the company OWEL. Provided statement.\u000a    \u000a    ","Title":"\u000a    Modelling and analysis of ocean wave energy extraction devices\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The device under development at OWEL (Offshore Wave Energy Ltd) is\u000a      rectangular in shape,\u000a      floats on the surface secured by mooring cables, and it traps waves\u000a      inside. A novel power take off\u000a      (PTO) system extracts the energy from the waves. Hence the interior fluid\u000a      motion is similar to\u000a      shallow water sloshing, with oscillatory mass input and output at the\u000a      ends, in an almost rectangular\u000a      vessel undergoing fully three-dimensional rotation and translation. At\u000a      Surrey, research in the area\u000a      of shallow water sloshing in moving vessels has been on-going since 2008.\u000a      The Surrey theory,\u000a      although developed independently, is a perfect match for the required\u000a      underpinning theory for\u000a      OWEL, and an alternative to CFD.\u000a    The research team at Surrey consists of Tom Bridges (Professor), Matt\u000a      Turner (Lecturer) and\u000a      Hamid Alemi Ardakani (former PhD student, now Postdoc). This project\u000a      discovered a new set of\u000a      shallow water equations for sloshing in a vessel undergoing fully\u000a      three-dimensional motion. In\u000a      addition, the project has produced\u000a    (a) A numerical method for shallow-water sloshing in 3D rectangular\u000a      vessels with prescribed\u000a      motion of the vessel (e.g. from ocean wave forcing). The numerical method\u000a      is implicit, robust\u000a      and fast (compared with fully 3D simulations). Results on a number of\u000a      configurations and\u000a      forcing have been published in [4].\u000a    (b) A theory and numerical framework for dynamic coupling between the\u000a      vessel motion and the\u000a      fluid motion. Coupled simulations are difficult because a proper energy\u000a      partition needs to be\u000a      maintained. Using a Lagrangian formulation, an approach was developed at\u000a      Surrey for\u000a      maintaining accurate energy partition for long times. Initial results are\u000a      published in [5].\u000a    (c) The linear and nonlinear implications of resonance between the fluid\u000a      and vessel motion.\u000a      Resonances can create physical transfer of energy between the vessel and\u000a      fluid motion.\u000a      This energy transfer can be positive (used to control vessel motion) or\u000a      negative (transfer of\u000a      energy from the wave to vessel motion, rather than PTO). The Surrey\u000a      project shows how to\u000a      identify these resonances and analyse the nonlinear implications. Recent\u000a      linear results are\u000a      reported in [3] and nonlinear results on (positive and negative) energy\u000a      transfer are published\u000a      in [2].\u000a    (d) The effect of baffles on resonance structure and control of sloshing\u000a      motion, including\u000a      coupling with vessel motion, recently published in [1].\u000a    (e) Motivated by the OWEL configuration, development of a two-layer\u000a      two-layer shallow-water\u000a      model with variable bottom and cross section, which includes a model for\u000a      the escape of the\u000a      upper fluid into the PTO.\u000a    (f) The OWEL team have provided the Surrey team with a vast amount of\u000a      (confidential) data,\u000a      including measurements and videos, obtained from experiments at HMRC (UCC,\u000a      Ireland),\u000a      University of Southampton wave basin, and the Plymouth University wave\u000a      basin, and this\u000a      data is being used for comparison with the modelling.\u000a    "},{"CaseStudyId":"40536","Continent":[{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    The impact had five facets.\u000d\u000a    \u000d\u000a      It affected a decision pathway at Pfizer: the research gave strong\u000d\u000a        doubts into the viability of an on-going project and on this basis\u000d\u000a        Pfizer decided to terminate this project and redeploy the resources\u000d\u000a        elsewhere. This impact had clear financial implications, but Pfizer has\u000d\u000a        not revealed the value.\u000d\u000a      Pfizer is part of a consortium in the US called the Centre for Protein\u000d\u000a        Therapeutics (which also includes most of the other major pharmaceutical\u000d\u000a        companies). The mathematical analysis, reported in the JTB paper, formed\u000d\u000a        the basis for a proposal by Pfizer for an experimental project by this\u000d\u000a        consortium. The proposal was ranked first out of all the proposals\u000d\u000a        competing for funding in this consortium. The project aims to exploit\u000d\u000a        the theoretical ideas by focusing on techniques to influence the\u000d\u000a        association constant of proteins such as antibodies.\u000d\u000a      The impact has sector-wide implications as the major drug companies\u000d\u000a        such as Pfizer are now aware of the importance of both the dissociation\u000d\u000a        and the association constant in the efficacy of proteins and antibodies.\u000d\u000a        As evidence of this secondary impact, a team at Bristol-Meyers Squibb Co\u000d\u000a        in the USA, has adapted the analysis and simulation techniques from this\u000d\u000a        project to their drug discovery process. They reported their results in\u000d\u000a        one of the highest impact industrial journals issued by the American\u000d\u000a        Association of Pharmaceutical Scientists: Chimalakonda et al, Amer.\u000d\u000a        Assoc. Pharma. Sci. J. (2013), doi: 10.1208\/s12248-013-9477-3.\u000d\u000a      In the design of therapeutics, the question arose as to whether a\u000d\u000a        longer half-life antibody would be more likely to cause rebound in\u000d\u000a        antigen levels after treatment cessation. However, the rebound analysis\u000d\u000a        showed that this was not the case, allowing the Pfizer project to move\u000d\u000a        forward.\u000d\u000a      On a more general level, this project gave new confidence to the idea\u000d\u000a        of using mathematical models as a guide in the early drug-discovery\u000d\u000a        process to develop and identify the most promising candidates.\u000d\u000a    \u000d\u000a    Pfizer stated;\u000d\u000a      \"As the pharmaceutical industry strives to improve decision-making at\u000d\u000a        all stages of drug discovery and development, one aspect that has gained\u000d\u000a        attention is the ability to make more objective decisions, especially at\u000d\u000a        an early stage of projects, using quantitative tools. This collaboration\u000d\u000a        is at the forefront of this shift in expectations. A key aspect of the\u000d\u000a        mathematical analysis practiced by Dr Derks and Dr Aston is the ability\u000d\u000a        to draw general conclusions about the \"design property space\" that is\u000d\u000a        not suitable for a particular project &#8212; a conclusion that can elude a\u000d\u000a        purely simulation-based analysis. While being elegant, this aspect has\u000d\u000a        the hidden advantage of condensing a lot of information into simple\u000d\u000a        outputs that can be more easily conveyed to a non-quantitative audience,\u000d\u000a        and hence used in decision-making. \"\u000d\u000a    The principal impact occurred in the period 2011-2012, with research\u000d\u000a      interaction continuing which will further the mathematical modelling of\u000d\u000a      the discovery process in drug manufacturing. The team is currently working\u000d\u000a      on part 2 of the rebound paper, in collaboration with a new industrial\u000d\u000a      partner, MedImmune, the worldwide biologics research and development arm\u000d\u000a      of the international biopharmaceutical company AstraZeneca, based in\u000d\u000a      Cambridge. Further research and industry interaction: (a) a MMath\u000d\u000a      placement student was embedded in Neusentis (a part of Pfizer) for 7\u000d\u000a      months in 2013, working on 2 compartment TMDD models; (b) an EPSRC-DTG PhD\u000d\u000a      student will start in October 2013 under the supervision of Drs Aston and\u000d\u000a      Derks, in collaboration with Pfizer (a new team at Pfizer), to work on the\u000d\u000a      analysis of extended TMDD models; (c) funding is being sought for an\u000d\u000a      academic-industry partnership in the area of \"Mechanistic Modelling of\u000d\u000a      Biologics\".\u000d\u000a    ","ImpactSummary":"\u000d\u000a    In the initial stages of the drug-discovery process, a range of synthetic\u000d\u000a      molecules are developed and the most promising ones are selected for\u000d\u000a      further development into potential drugs. The research of the Surrey team\u000d\u000a      in collaboration with a research team at Pfizer sheds new light on how to\u000d\u000a      achieve high efficacy, by using mathematical modelling to speed up this\u000d\u000a      selection process. The research has led the pharmaceutical company Pfizer\u000d\u000a      to terminate a discovery project and redeploy resources in a new\u000d\u000a      direction. This research has generated direct impact in the field of\u000d\u000a      early-stage pharmaceutical research, and indirect impact on the economy\u000d\u000a      and health.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Surrey\u000d\u000a    ","Institutions":[{"AlternativeName":"Surrey (University of)","InstitutionName":"University of Surrey","PeerGroup":"B","Region":"South East","UKPRN":10007160}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1. P.J. Aston, G. Derks, A. Raji, B.M. Agoram &amp; P.H. van der Graaf .\"Mathematical\u000a        analysis of the pharmacokinetic-pharmacodynamic (PKPD) behaviour of\u000d\u000a        monoclonal antibodies predicting in-vivo potency\", J. Theoretical\u000d\u000a      Biology 281, 113-121 (2011)\u000d\u000a      doi: 10.1016\/j.jtbi.2011.04.030.\u000d\u000a    \u000a\u000a2. P.J. Aston, G. Derks, B.M. Agoram &amp; P.H. van der Graaf. \"A\u000d\u000a        mathematical analysis of rebound in a target-mediated drug disposition\u000d\u000a        model: I. Without feedback\", J. Mathematical Biology (published\u000d\u000a      online April 2013) doi: 10.1007\/s00285-013-0675-5.\u000d\u000a    \u000aThe project was initially funded by a grant from the BioPharma Skills\u000d\u000a      project, which was a joint initiative between the Universities of Surrey\u000d\u000a      and Reading. It was funded by both universities as well as the Higher\u000d\u000a      Education Funding Council for England's Economic Challenge Investment Fund\u000d\u000a      (ECIF) and the South East England Development Agency (SEEDA). The\u000d\u000a      BioPharma Skills project awarded an 11 month internship. The intern worked\u000d\u000a      in 2010-2011 at the Pfizer offices with regular interaction with the\u000d\u000a      Surrey team.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"6","Level2":"1","Subject":"Biochemistry and Cell Biology"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000d\u000a    Corroboration has been obtained from the two principal scientists working\u000d\u000a      on the project at Pfizer.\u000d\u000a    \u000d\u000a      Director of Clinical Pharmacology\/DMPK, MedImuune. Provided statement.\u000d\u000a      Principal Scientist, Academic Center for Drug Research, University of\u000d\u000a        Leiden. Contact details provided.\u000d\u000a    \u000d\u000a    In addition, a file with the evidence of the award by the BioPharma\u000d\u000a      Skills project, and the outcome of the proposal to the Centre for Protein\u000d\u000a      Therapeutics (including evidence of its top ranking) is available. \u000d\u000a    ","Title":"\u000d\u000a    Guiding drug discovery by prediction of in vivo efficacy of monoclonal\u000d\u000a        antibodies\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The Surrey team (Dr Philip Aston and Dr Gianne Derks, both Readers in\u000d\u000a      Mathematics) had a combined 33+ years of experience working in applied\u000d\u000a      dynamical systems and mathematical modelling, when they were contacted by\u000d\u000a      a Research Scientist at Pfizer in 2009. Pfizer was interested in using\u000d\u000a      mathematical models consisting of small systems of ordinary differential\u000d\u000a      equations (ODEs) to get a better understanding of fundamental properties\u000d\u000a      of monoclonal antibodies, with an aim to improve and guide the early drug\u000d\u000a      discovery process through modelling.\u000d\u000a\u0009 Pfizer's problems involved monoclonal\u000d\u000a      antibodies and target-mediated drug disposition (TMDD) models that\u000d\u000a      describe their pharmacokinetic-pharmacodynamic (PKPD) interactions. One of\u000d\u000a      the questions was \"What is the relation between efficacy of monoclinic\u000d\u000a      antibodies and their affinity and elimination parameters?\" Another was\u000d\u000a      \"Can rebound occur and if so, what triggers it?\" The mathematical\u000d\u000a      underpinning involved systems of ODEs, representing the target-mediated\u000d\u000a      drug disposition (TMDD) models, with coefficients representative of\u000d\u000a      parameters in the pharmacokinetic-pharmacodynamic processes. These models\u000d\u000a      were then extensively analysed both methodologically and rigorously to\u000d\u000a      determine parametric effect and asymptotic behaviour. The principal\u000d\u000a      analysis to address the efficacy question was a study of the relationship\u000d\u000a      between the target affinity of a monoclonal antibody and its in-vivo\u000d\u000a      potency\/efficacy. As a measure of efficacy, the minimum level of the free\u000d\u000a      receptor following a single bolus injection of the ligand into the plasma\u000d\u000a      compartment was considered. It is known that the equilibrium dissociation\u000d\u000a      constant KD, which is the quotient of the dissociation constant\u000d\u000a      koff and the association constant kon, plays an\u000d\u000a      important role in the efficacy. Before this research, the different roles\u000d\u000a      played by the two constants in this quotient had not been realised.\u000d\u000a    The methodology in the underpinning research involved qualitative\u000d\u000a      analysis of ordinary differential equations, dynamical systems analysis\u000d\u000a      (invariant manifold theory, attracting sets, heteroclinicity), multi-scale\u000d\u000a      asymptotics, and numerical simulation. The initial stage\u000d\u000a      pharmacokinetic-pharmacodynamic implications of the analysis are discussed\u000d\u000a      in [1]. A rigorous mathematical analysis of the system of ODEs, focusing\u000d\u000a      on the full time course and the rebound question is considered in [2] ,\u000d\u000a      which is primarily a theorem-proof paper, and uses invariant manifold\u000d\u000a      theory, geometric analysis from dynamical systems, and heteroclinic orbits\u000d\u000a      to give a comprehensive description of the rebound phenomenon\u000d\u000a    From the ODEs in the model, two expressions for the efficacy were\u000d\u000a      obtained, in terms of the parameters of the problem, one of which is valid\u000d\u000a      over the full range of values of the equilibrium dissociation constant KD,\u000d\u000a      and the other which is valid only for a large drug dose or for a small\u000d\u000a      value of this constant. Both of these formulae show that the efficacy\u000d\u000a      achieved by increasing the association constant kon can be very\u000d\u000a      different from the efficacy achieved by decreasing the dissociation\u000d\u000a      constant koff. In particular, there is a saturation effect when\u000d\u000a      decreasing the dissociation constant koff, where the increase\u000d\u000a      in efficacy that can be achieved is limited. There is no such effect when\u000d\u000a      increasing the association constant kon.\u000d\u000a    Thus, for certain monoclonal antibodies, an increase in efficacy may be\u000d\u000a      better achieved by increasing the association constant kon than\u000d\u000a      by decreasing the dissociation constant koff. This observation\u000d\u000a      sheds new light on the drug-discovery process. The saturation of the\u000d\u000a      dissociation constant koff was an especially unpleasant\u000d\u000a      surprise as that one is easier to manipulate and hence usually the focus\u000d\u000a      of design trials.\u000d\u000a    While the efficacy question involved mainly the initial stages of the\u000d\u000a      PKPD interaction of the monoclonal antibody with its antigen target, the\u000d\u000a      rebound question involved the full time course. Rebound is a post-dose\u000d\u000a      rise in receptor (antigen\/cytokine) levels to higher than pre-dose\u000d\u000a      (baseline). The mathematical research, which involved the study of four\u000d\u000a      different parameter regions, showed that rebound can happen if and only if\u000d\u000a      the elimination rate of the antibody-receptor complex is smaller than the\u000d\u000a      elimination rates of both the antibody and the receptor on their own.\u000d\u000a    "},{"CaseStudyId":"41140","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000d\u000a    The Cassini Missions\u000d\u000a    Cassini-Huygens is a flagship-class NASA-ESA-ASI robotic spacecraft sent\u000d\u000a      to the Saturn system. It has studied the planet and its many natural\u000d\u000a      satellites since arriving there in 2004, also observing Jupiter, the\u000d\u000a      Heliosphere, and testing the theory of relativity. Sixteen European\u000d\u000a      countries and the United States make up the team responsible for\u000d\u000a      designing, building, flying and collecting data from the Cassini orbiter\u000d\u000a      and its Huygens probe. Cassini completed its initial four-year mission to\u000d\u000a      explore the Saturn System in June 2008 and the first extended mission,\u000d\u000a      called the Cassini Equinox Mission, in September 2010.\u000d\u000a    Cassini's discovery of an icy plume shooting from one of Saturn's moons,\u000d\u000a      and subsequent observations of the spray containing complex organic\u000d\u000a      chemicals have caused much debate and conjecture on the part of\u000d\u000a      astronomers. Brilliantov's work has helped to identify warm, liquid water\u000d\u000a      ocean encased beneath the surface of Enceladus' moon as being the most\u000d\u000a      likely cause of the icy geysers.\u000d\u000a    Evaluation of flightpath\u000d\u000a    Brilliantov's work was used by NASA in the planning of the Cassini\u000d\u000a      mission in 2009-2010 in an absolutely fundamental way. The contribution is\u000d\u000a      summarised in a statement by one of the Principal Investigators of the\u000d\u000a      Cassini Mission, Prof. L. Esposito, who wrote that:\u000d\u000a    \"... the model of Schmidt et al 2008 (J. Schmidt, N. Brilliantov, F.\u000d\u000a        Spahn, and S. Kempf, Nature, 451 (2008) 685) has been used to evaluate\u000d\u000a        the planned flybys of the spacecraft of the Cassini mission in\u000d\u000a        2009-2010, allowing the mission designers to calculate the danger of the\u000d\u000a        Cassini Spacecraft colliding with dust in the Enceladus plumes and jets.\u000d\u000a        This has allowed NASA's Cassini Project to reduce the hazardous impacts\u000d\u000a        of the probe with the ice grains within the Enceladus plume.\"\u000d\u000a    Economic impact\u000d\u000a    The total cost of the mission was about $3.26 billion, which was\u000d\u000a      paid for by the US, the European and the Italian Space Agencies. Had the\u000d\u000a      Cassini spacecraft collided with the plumes due to failure to take account\u000d\u000a      of the research underpinning this impact case, this significant investment\u000d\u000a      would have been largely lost.\u000d\u000a    Cultural impact\u000d\u000a    The team's research enabled the Cassini mission to carry out a very close\u000d\u000a      sweep of the moon and capture unique data that provided further evidence\u000d\u000a      of the presence of a subterranean sea and verification of the chemical\u000d\u000a      composition of the moon's atmosphere.\u000d\u000a    The significance of the presence of the liquid water sea has implications\u000d\u000a      for mankind. Tidal heating is keeping Enceladus warm and hotspots\u000d\u000a      associated with the fountains have been pinpointed. With heat, organic\u000d\u000a      chemicals and, potentially liquid water, Enceladus could be a place where\u000d\u000a      primitive life forms might evolve. Questions surrounding Enceladus'\u000d\u000a      \"astrobiological potential\" are at the heart of many investigations being\u000d\u000a      conducted in the Solstice Mission. Because of its large astrobiological\u000d\u000a      potential a forthcoming European mission \"Enceladus Explorer\" is\u000d\u000a      planned including a base station on the Enceladus surface and ice drilling\u000d\u000a      to examine the liquid ocean for any traces of microorganisms.\u000d\u000a    The mission has inspired extensive media and public interest on a global\u000d\u000a      scale. The discovery of the underground ocean and potential life in outer\u000d\u000a      space has, understandably, been the source of widespread media coverage\u000d\u000a      reaching millions of people (e.g. New York Times articles reaching almost\u000d\u000a      2 million readers plus online hits).\u000d\u000a    Social media statistics also illustrate how inspiring this story is to\u000d\u000a      the general population. As of June 2013 #Enceladus had almost 35,000\u000d\u000a      mentions (76% of which were retweets showing that the stories have gone\u000d\u000a      viral) and Life on Saturn is searched for by 5,400 people a month on\u000d\u000a      Google.\u000d\u000a    NASA has a comprehensive outreach programme and has disseminated the\u000d\u000a      findings of the Cassini mission to millions of people including well used\u000d\u000a      teaching materials in schools. One of the top documentary films of 2010 \"7\u000d\u000a        Wonders of the Solar System \"annotates\" The seven wonder of our\u000d\u000a        solar system are discussed: Enceladus' geysers, Rings of Saturn,\u000d\u000a        Jupiter's Great Red Spot, the Asteroid Belt, Mars' Olympus Mons, the\u000d\u000a        Surface of the Sun, and planet Earth\"; it has been watched by\u000d\u000a      several hundred thousand people.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    NASA's Cassini mission to Saturn's icy moon Enceladus in 2009-10\u000d\u000a      investigated the presence of explosive ice geysers towering over the south\u000d\u000a      pole of the planet. The geysers consist of vapour and ice particles which\u000d\u000a      rise up to 1,000 kilometres above Enceladus' surface. The source of these\u000d\u000a      jets has been hotly contested. Cassini's mission was to fly as close as\u000d\u000a      possible to the plumes to search for evidence of sub-surface water\u000d\u000a      containing the building blocks of life.\u000d\u000a    Mathematical modelling, conducted at Leicester, allowed the mission\u000d\u000a      designers to calculate the possibility of the Cassini Spacecraft colliding\u000d\u000a      with dust from the Enceladus jets, with potentially catastrophic results,\u000d\u000a      enabling the craft to be manoeuvred as close as safely possible to the\u000d\u000a      moon's surface to capture the images it required.\u000d\u000a    The mission, with an estimated $3.26 billion cost, was successful &#8212;\u000d\u000a      gathering evidence that the research team's hypothesis of a subterranean\u000d\u000a      sea on Enceladus was correct &#8212; a revelation which has inspired public\u000d\u000a      interest around the world.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Leicester\u000d\u000a    ","Institutions":[{"AlternativeName":"Leicester (University of)","InstitutionName":"University of Leicester","PeerGroup":"A","Region":"East Midlands","UKPRN":10007796}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1. N. Brilliantov, J.Schmidt and F. Spahn, Geysers of Enceladus:\u000d\u000a      Quantitative analysis of Qualitative models, Planetary and Space Science,\u000d\u000a      56 (2008), pp. 1596-1606.\u000d\u000a    \u000a\u000a2. J. Schmidt, N. Brilliantov, F. Spahn, and S. Kempf, Slow dust in\u000d\u000a      Enceladus' plume from condensation and wall collisions in tiger stripe\u000d\u000a      fractures, Nature, 451 (2008), pp. 685-688.\u000d\u000a    \u000a\u000a3. F. Postberg, S. Kempf, J. Schmidt, N. Brilliantov, A. Beinsen, B.\u000d\u000a      Abel, U. Buck &amp; R. Srama, Sodium salts in E-ring ice grains from an\u000d\u000a      ocean below the surface of Enceladus, Nature, 459 (2009) pp. 1098 - 1101\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"4","Level2":"3","Subject":"Geology"},{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"},{"Level1":"4","Level2":"5","Subject":"Oceanography"}],"Sources":"\u000d\u000a    \u000d\u000a      Factual statement from the Principal Investigator of the Cassini\u000d\u000a        Ultraviolet Imaging Spectrograph (LASP, Boulder, USA).\u000d\u000a      Article from the New York Times, \"Saturn Imitates Louis XIV\", February\u000d\u000a        28, 2008:\u000d\u000a        http:\/\/tierneylab.blogs.nytimes.com\/2008\/02\/28\/saturn-imitates-louis-xiv\/\u000a\u000d\u000a      Report from the BBC on the underground sea on Enceladus on 25 June\u000d\u000a        2009:\u000d\u000a        http:\/\/news.bbc.co.uk\/1\/hi\/sci\/tech\/8115148.stm\u000a\u000d\u000a      Statement from the Co-Investigator of the Cosmic Dust Analyser on the\u000d\u000a        Cassini Spacecraft.\u000d\u000a      Article in Sciencedaily http:\/\/www.sciencedaily.com\/releases\/2008\/02\/080222112324.htm\u000a\u000d\u000a      Article in Physorg http:\/\/www.physorg.com\/news122898790.html\u000a\u000d\u000a      Report by The Cheers News Agency\u000d\u000a        http:\/\/newsagency.thecheers.org\/Science\/news_12848_Scientists-unravel-secret-behind-Saturns-moons-mysterious-plumes-of-dust-and-water-vapour.html\u000a\u000d\u000a      Report in Thaindian News http:\/\/www.thaindian.com\/newsportal\/health\/scientists-unravel-secret-behind-saturns-moons-mysterious-plumes-of-dust-and-water-vapour-2_10020607.html\u000a\u000d\u000a      JPL News Feature http:\/\/saturn.jpl.nasa.gov\/news\/cassinifeatures\/\u000a\u000d\u000a      On Space.Com http:\/\/www.space.com\/4935-mystery-saturn-watery-moon-solved.html\u000a\u000d\u000a      NASA report http:\/\/www.nasa.gov\/mission_pages\/cassini\/whycassini\/cassinif-20080207.html\u000a\u000d\u000a\u0009\u000d\u000a    ","Title":"\u000d\u000a    Mathematical modelling contributes to NASA space mission and inspires\u000d\u000a      public interest in science\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    In 2008, Nikolay Brilliantov (University of Leicester since 2007, now\u000d\u000a      Chair, Professor in Applied Mathematics) collaborated with Jurgen Schmidt\u000d\u000a      and Frank Spahn (both Institute of Physics, University of Potsdam,\u000d\u000a      Germany) publishing research in Planetary and Space Science [1]. The\u000d\u000a      research analysed previously published qualitative models of the geysers\u000d\u000a      of Enceladus and examined two hypotheses for the underlying cause of\u000d\u000a      Enceladus' geysers already proposed by astronomers, in the light of data\u000d\u000a      gathered by Cassini.\u000d\u000a    The first `Cold Faithful hypothesis' assumes an explosive boiling\u000d\u000a      of subsurface liquid water, when pressure exerted by the ice crust is\u000d\u000a      suddenly released due to an opening crack. In the second hypothesis, `Frigid\u000d\u000a        Faithful', the existence of a deep shell of clathrates below\u000d\u000a      Enceladus' south pole is conjectured; clathrates can decompose explosively\u000d\u000a      when exposed to vacuum through a fracture in the outer icy shell.\u000d\u000a    Development of a new hypothesis\u000d\u000a    Brilliantov elaborated mathematical models that quantified both\u000d\u000a      hypotheses and his collaborators performed the numerical computations. It\u000d\u000a      was found that, for the Cold Faithful model, the explosive boiling can't\u000d\u000a      provide the velocities of icy particles observed in the plume. It also\u000d\u000a      found that the low temperatures of the Frigid Faithful model implied a too\u000d\u000a      dilute vapour to support the observed high particle fluxes in Enceladus'\u000d\u000a      plume.\u000d\u000a    In the same year, the same team published a novel hypothesis [2],\u000d\u000a      underpinned by the new mathematical model by Brilliantov, to explain the\u000d\u000a      icy geysers. The explanation was a liquid water ocean with a large vapour\u000d\u000a      reservoir below Enceladus' south pole. Smaller velocities for the geyser's\u000d\u000a      grains than for the vapour had been difficult to understand. The gas and\u000d\u000a      dust were too dilute in the plume to interact, so the difference had to\u000d\u000a      arise below the surface. The team published a model for grain condensation\u000d\u000a      and growth in channels of variable width showing that repeated wall\u000d\u000a      collisions of grains, with re-acceleration by the gas, induced an\u000d\u000a      effective friction, offering a natural explanation for the reduced grain\u000d\u000a      velocity. The gas seemed to form near the triple point of water; gas\u000d\u000a      densities corresponding to sublimation from ice are generally too low to\u000d\u000a      support the measured particle fluxes. This in turn suggested liquid water\u000d\u000a      ocean below Enceladus' south pole. This theory was in a quantitative\u000d\u000a      agreement with data gathered so far by Cassini.\u000d\u000a    In 2009, Brilliantov elaborated a theory quantifying the sodium content\u000d\u000a      in the icy geysers. He provided the mathematical modelling expertise , in\u000d\u000a      a the team comprising colleagues from Potsdam, Heidelberg, Gottingen and\u000d\u000a      Leicester, providing strong evidence [3] that supported the theory that\u000d\u000a      liquid water, rather than frozen water, below Enceladus' south pole was\u000d\u000a      responsible for the icy geysers. After studying data from the Cosmic Dust\u000d\u000a      Analyzer (CDA) on board the Cassini spacecraft and combining this data\u000d\u000a      with laboratory experiments, the team reported the detection of sodium\u000d\u000a      salts among the dust ejected in the Enceladus plume, hinting at the salty\u000d\u000a      ocean deep below and thus confirming their theory. The results of the\u000d\u000a      study imply that the concentration of sodium chloride in the ocean can be\u000d\u000a      as high as that of Earth's oceans.\u000d\u000a    Brilliantov's mathematical model was combined with computational\u000d\u000a      modelling of the grain sizes and laboratory experiments performed by the\u000d\u000a      rest of the team. This explained the observed concentration of salt in icy\u000d\u000a      grains, especially, for the part of grains with a low salt concentration.\u000d\u000a      Their conclusion through analysis of the distribution of grain size, was\u000d\u000a      that the type and speed of particles could only have been produced from\u000d\u000a      liquid water, hence the hypothesis of the existence of a subterranean\u000d\u000a      ocean and the conclusion that the grain sizes and speed were unlikely to\u000d\u000a      damage Cassini.\u000d\u000a    This work enabled NASA to conclude that the risks were such that it\u000d\u000a      deemed `safe' for the mission to fly closer to the moon's surface and thus\u000d\u000a      undertake the exploration leading to the subsequent important discoveries.\u000d\u000a    "},{"CaseStudyId":"41218","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3175395","Name":"Italy"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The impact of the research described above has been both on the work of\u000a      large industrial\u000a      companies (Unilever PLC), as well as industrial research institutes\u000a      (I2T3). In both cases the\u000a      methods developed in the research were directly applied to problems of\u000a      industrial relevance.\u000a    I2T3 (Innovazione Industriale Tramite Trasferimento Tecnologico Onlus) is\u000a      based in Florence and is\u000a      an industrial research and technology transfer organisation. This was\u000a      funded for start up, by the\u000a      Chamber of Commerce of Florence, the Cassa di Risparmio di Firenze, the\u000a      Fraunhofer Institut and\u000a      the University of Florence, and brought together industrial and academic\u000a      partners. Its Vice-President\u000a      describes direct impact of the methods developed in this research on a\u000a      technologically relevant\u000a      project called \"MAC-GEO\". The project concerned modelling of the evolution\u000a      of geothermal\u000a      reservoirs and I2T3's contribution focused on phase equilibria of\u000a      geothermal mixtures. The project\u000a      was worth 800k Euro overall, but involved also data collecting, database\u000a      organization so that this\u000a      modelling part had a budget of ca. 200k Euro. Polydispersity came in both\u000a      as part of the main\u000a      dynamic model and, of course, in solving the phase equilibrium conditions.\u000a      I2T3 had a contract to\u000a      work on phase equilibrium calculation and chemical kinetics between rock\u000a      matrix and geothermal\u000a      fluid. I2T3's Vice-President emphasizes that the phase equilibrium\u000a      calculations were possible only\u000a      because of the efficient numerical methods arising out of the research in\u000a      this case study. He is also\u000a      conducting a feasibility analysis for a project for a company that makes\u000a      compressors and might be\u000a      interested in developing modelling software in which liquid\/gas phase\u000a      transitions of polydisperse\u000a      fluids are taken into account.\u000a    I2T3's Vice-President also reports several other instances of impact,\u000a      including modelling of\u000a      blowdown processes in hydrocarbon pressure vessels (with I2T3 and\u000a      Snamprogetti spa, at that time\u000a      part of the ENI group, now sold to SAIPEM), and a project with ENI to\u000a      model wax formation in\u000a      pipelines where again hydrocarbon mixtures were involved. The results for\u000a      the impact at\u000a      Snamprogetti were presented at a SIMAI (Societa Italiana di Matematica\u000a      Applicata e Industriale)\u000a      conference, in a symposium about oil and gas-related mathematical\u000a      modelling that had significant\u000a      attendance from industry. More significantly, the moment method was\u000a      incorporated directly in\u000a      blowdown process simulation software delivered to Snamprogetti, where it\u000a      was then used in the pre-design\u000a      of hydrocarbon pressure vessels.\u000a    Unilever is the world's third largest consumer goods company. It owns\u000a      over 400 brands, including\u000a      several with annual sales exceeding one billion euros. Its products\u000a      include foods, beverages,\u000a      cleaning agents and products for personal care and hygiene. Many of these\u000a      products are\u000a      polydisperse in nature. Food products such as mayonnaise or margarine are\u000a      typically colloidal\u000a      mixtures of several different ingredients (including plant oil, fatty\u000a      acids, water and plant-sterols). The\u000a      same is true for home and personal care products, many of which are\u000a      sophisticated blends of\u000a      surfactants, polymers and colloids. It is essential for processability and\u000a      usability of these substances\u000a      that they can be designed to be stable against demixing of their\u000a      components, for the entire range of\u000a      ambient temperatures at which they are used, stored, or processed.\u000a    A substantial amount of work is therefore invested at the Unilever\u000a      research labs to formulate\u000a      compositions that ensure such stability of their products. The\u000a      polydispersity research described in\u000a      section 3 has provided concepts and insights which have streamlined this\u000a      work [see for example\u000a      \"Flory-Huggins theory for the solubility of heterogeneously-modified\u000a      polymers\", P. B. Warren,\u000a      Macromolecules 40, 6709 (2007)]. Particularly in the area of computer\u000a      aided formulation, the ideas\u000a      are expected to lead to significant savings in research efforts, faster\u000a      development cycles, shorter\u000a      time-to-market, and improved flexibility and efficiency in the supply\u000a      chain.\u000a    In a supporting letter, a senior scientist at Unilever describes the\u000a      impact of the research,\u000a      emphasizing the conceptual impact as follows: \"Indeed, I may add that the\u000a      polydispersity work now\u000a      shapes my thinking in a deep way. In computer-aided formulation we are\u000a      investigating of the phase\u000a      behaviour of alkyl chain terminated surfactants. Polydispersity in the\u000a      alkyl chain length is a concern,\u000a      but I think a key insight from the polydispersity work is that it is\u000a      legitimate to estimate the effect by a\u000a      2- or 3-component mixture....\" In this way the conceptual insights from\u000a      the moment free energy have\u000a      an impact on Unilever's day to day work. Concerning the resulting savings\u000a      in research efforts for a\u000a      typical characterisation project, the senior scientist further comments:\u000a      \"... It is difficult to quantify the\u000a      impact in financial terms though if we say it saved 6 months of\u000a      characterisation effort, we can make\u000a      a very rough justification that this corresponds to &#163;50m. This is\u000a      indicative of the scale of savings that\u000a      have continued to accrue since.\"\u000a    ","ImpactSummary":"\u000a    Research by Prof Sollich and collaborators has led to new ways of looking\u000a      at the problem of\u000a      understanding the phase behaviour (phase transitions like freezing and\u000a      melting, or demixing in oil-water\u000a      mixtures) of systems which are polydisperse in that they contain an\u000a      effectively infinite number\u000a      of different particle species. This is the situation with many\u000a      industrially important materials: e.g. in\u000a      emulsion paint, the colloidal paint particles have an essentially\u000a      continuous spread of diameters.\u000a      Beyond conceptual progress, the research has resulted in efficient\u000a      numerical algorithms for\u000a      predicting phase equilibria. Specifically, it has led to significant\u000a      savings in industrial research\u000a      processes and thus has had both economic impact and impact on\u000a      practitioners and professional\u000a      services.\u000a    ","ImpactType":"Technological","Institution":"\u000a    King's College London\u000a    ","Institutions":[{"AlternativeName":"King's College London","InstitutionName":"King's College London","PeerGroup":"A","Region":"London","UKPRN":10003645}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"3176959","Name":"Florence"}],"References":"\u000a    \u000a1) P. Sollich, P. B. Warren and M. E. Cates, Moment free energies for\u000a      polydisperse systems.\u000a      Advances in Chemical Physics (I Prigogine and S A Rice, editors),\u000a      116:265-336, 2001.\u000a      DOI:10.1002\/9780470141762.ch4, Citation counts as of 09\/2012: 57 (google\u000a      scholar), 52 (ISI)\u000a    \u000a\u000a2) P. Sollich, Predicting phase equilibria in polydisperse systems\u000a      (invited topical review). Journal of\u000a        Physics: Condensed Matter, 14:R79-R117, 2002.\u000a      DOI:10.1088\/0953-8984\/14\/3\/201, Citation counts as of 09\/2012: 129 (google\u000a      scholar), 104 (ISI)\u000a    \u000a\u000a3) A. Speranza and P. Sollich, Simplified Onsager theory for\u000a      isotropic-nematic phase equilibria of\u000a      length polydisperse hard rods. Journal of Chemical Physics,\u000a      117:5421-5436, 2002.\u000a      DOI:10.1063\/1.1499718, Citation counts as of 09\/2012: 51 (google scholar),\u000a      45 (ISI)\u000a    \u000aGrant support:\u000a    &#8226; EPSRC Fast Stream Grant, awarded to Peter Sollich (sole investigator),\u000a      Polydispersity\u000a      effects on colloidal phase behaviour, Oct 2001- Sep 2004, &#163;63K (PhD\u000a      studentship; funding\u000a      amount limited by rules for Fast Stream Grants). In the assessment at the\u000a      end of the grant,\u000a      both assessors rated the research overall as \"outstanding\" (highest\u000a      rating), and described it\u000a      as an \"outstanding project\" producing \"research ... of the very highest\u000a      quality\" and leading to\u000a      \"significant ... advances [in] new theoretical and computational\u000a      developments\".\u000a    ","ResearchSubjectAreas":[{"Level1":"3","Level2":"6","Subject":"Physical Chemistry (incl. Structural)"}],"Sources":"\u000a    Information on I2T3 can be found on its website at http:\/\/www.i2t3.unifi.it\/\u000a      Link to KCL-mirror\u000a        of I2T3 web-site.\u000a    Published information about the MAC-GEO project can be found at\u000a    \u000a      DOI: 10.1685\/2010CAIM589\u000a      DOI:10.1016\/j.cageo.2011.03.018\u000a      DOI:10.1016\/j.ijengsci.2011.05.003\u000a    \u000a    Of these the first document is most relevant as it deals directly with\u000a      the polydispersity effects on\u000a      phase equilibria. (documents available on request)\u000a    Published information on the impact at Unilever:\u000a      \"Flory-Huggins theory for the solubility of heterogeneously-modified\u000a      polymers\", P. B. Warren,\u000a      Macromolecules 40, 6709 (2007), DOI: 10.1021\/ma070809x.\u000a      http:\/\/pubs.acs.org\/doi\/abs\/10.1021\/ma070809x\u000a    Individual sources:\u000a    \u000a      Senior scientist at Unilever (testimonial received and available on\u000a        request).\u000a      Vice-President of I2T3 (also project manager and general coordinator\u000a        of the Foundation for\u000a        Research and Innovation, http:\/\/www.fondazionericerca.unifi.it),\u000a        now at KBC Advanced\u000a        Technologies, London (testimonial received and available on request).\u000a        Link to KCL-mirror\u000a          of Fondazione Ricerca site.\u000a    \u000a    Impact on blowdown processes:\u000a    \u000a      \u000ahttp:\/\/www.worldscibooks.com\/mathematics\/5854.html\u000a        (ISBN: 978-981-256-368-2)\u000a        Link to KCL-mirror\u000a          of World Scientific page for the Proceedings\u000a\u000a      \u000ahttp:\/\/www.worldscibooks.com\/mathematics\/6554.html\u000a        (ISBN: 978-981-270-938-7)\u000a        Link to KCL-mirror\u000a          of World Scientific web-page for the Proceedings\u000a\u000a    \u000a    ","Title":"\u000a    Conceptual insights and numerical methods for polydisperse phase\u000a      behaviour\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underlying research relates to understanding and predicting phase\u000a      behaviour: transitions\u000a      between different states of matter or \"phases\", like ice, water, steam or\u000a      the various phases of\u000a      anisotropic molecules used in liquid crystal displays (LCDs); or demixing\u000a      transitions into two or more\u000a      phases, as in liquid mixtures like oil and water that may separate at room\u000a      temperature but remain\u000a      mixed at different temperature. The challenge is to predict, from the\u000a      properties of the constituent\u000a      molecules, the number of coexisting phases and their properties, and how\u000a      these change when one\u000a      varies external control parameters like temperature or pressure, or in\u000a      typical \"soft matter\" systems\u000a      like colloidal suspensions (paint) or emulsions (mayonnaise) the overall\u000a      dilution of the system.\u000a    There are well established approaches for predicting phase behaviour in\u000a      simple systems: a \"free\u000a      energy\" is calculated as a function of the concentrations of all of the\u000a      species of molecules present,\u000a      and phase separation regions can be found by constructing tangent planes\u000a      to this free energy\u000a      surface. However, in soft systems one has the added complication that e.g.\u000a      colloidal particles are\u000a      never all identical but typically have variations in size (or shape,\u000a      electrical charge etc). This makes\u000a      them very different from atoms or small molecules. In the colloidal case,\u000a      as particle sizes are fixed\u000a      by the process of chemical synthesis, the number of particles in each\u000a      small range of sizes is fixed,\u000a      and so each such range has to be treated as a separate particle species.\u000a      Because the size\u000a      distribution is continuous (rather than a mixture of a few distinct\u000a      sizes), the system is polydisperse: it\u000a      is a mixture of an effectively infinite number of particle species. The\u000a      traditional procedures for\u000a      predicting phase behaviour then become unmanageable both conceptually and\u000a      numerically.\u000a    Polydisperse systems are very widespread in industrial applications. Most\u000a      commercially available\u000a      surfactants (soaps) are polydisperse, and so are polymer additives, often\u000a      highly so. Home and\u000a      personal care product formulations are sophisticated blends of\u000a      surfactants, polymers and colloids,\u000a      and control of phase behaviour is essential for processability and\u000a      usability.\u000a    The research described here successfully tackled the problem of\u000a      predicting phase behaviour in\u000a      polydisperse systems. It was carried out by Peter Sollich in collaboration\u000a      with Michael Cates and\u000a      Patrick Warren from early 1997 to March 2000. The main research\u000a      underpinning the impact,\u000a      particularly as regards the development of efficient numerical algorithms,\u000a      was done by Peter Sollich\u000a      with Alessandro Speranza from September 1999 until September 2002.\u000a    The key insight was that the free energy expressions for many\u000a      polydisperse mixtures contain the full\u000a      details of the polydispersity (the size distribution, in the colloids\u000a      example) only in the \"entropy of\u000a      mixing\" term. The remainder of the free energy represents the interactions\u000a      between molecules of\u000a      different species and can normally be written as a function of only a few\u000a      moments of the size\u000a      distribution. The research proposed a method of constructing for such free\u000a      energies a \"moment free\u000a      energy\", which depends on only as many concentration variables as the\u000a      number of moments\u000a      required. Remarkably, this can then be treated like the free energy of a\u000a      simple mixture of a few\u000a      effective particle species, while preserving exactly many properties of\u000a      the original free energy and\u000a      giving accurate approximations for others. Peter Sollich was instrumental\u000a      in developing these\u000a      theoretical insights (with Cates and Warren), which provide a new and\u000a      conceptually powerful way of\u000a      looking at polydisperse phase equilibria. He also developed computer code\u000a      for evaluating the\u000a      predictions of the moment free energy method efficiently. Crucially, this\u000a      code is generic in that in can\u000a      be applied to any free energy with the required moment structure. The\u000a      further research with\u000a      Speranza led to an efficient computational method for extending the\u000a      approach to be numerically\u000a      essentially exact, overcoming the need to approximate in certain regions\u000a      of the phase diagram.\u000a    Key researchers\u000a    \u000a      Professor Peter Sollich\u000a        - King's College London since 01\/1999\u000a        initially as Lecturer, promoted to Reader Sept 2002, promoted to\u000a        Professor Oct 2004\u000a      Dr Alessandro Speranza\u000a        - King's College London Oct 1999 to Sept 2002, PhD student\u000a      Professor Michael E Cates\u000a        - University of Edinburgh\u000a      Dr Patrick B Warren\u000a        - scientist at Unilever PCL at Port Sunlight (UK)\u000a    \u000a    "},{"CaseStudyId":"41219","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2658434","Name":"Switzerland"}],"Funders":[],"ImpactDetails":"\u000a    The research described in section 2 has generated two instances of\u000a      impact. It has been used by a Swiss biotechnology company to validate\u000a      principles underlying the construction of a knowledge based approach that\u000a      allowed the discovery of patterns connecting a certain set of peptides\u000a      with the occurrence of a set of rare diseases, and it has led to\u000a      consultancy work done for the Financial Services Authority (FSA) in a\u000a      drive by the FSA to improve their tool-kit used to carry out market\u000a      cleanliness event studies.\u000a    TDM's consultancy work for the FSA can be understood as a consequence of\u000a      the fact that she is acknowledged as one of the leading experts in\u000a      Econophysics, the theory of complex systems, the analysis of financial\u000a      markets using techniques of statistical physics, network theory and\u000a      numerical methods, and in particular on the specific information filtering\u000a      techniques she developed since joining KCL.\u000a    The recent financial crisis has led financial institutions to rethink the\u000a      proper methodologies implemented at that time. In this context, TDM was\u000a      approached by the Financial Services Authority (FSA) in 2010 to provide\u000a      advice for a project to strengthen their toolbox used in so called market\u000a      cleanliness event studies, specifically to test whether new techniques\u000a      from Econophysics could help to improve the accuracy and diagnostic power\u000a      of such studies. The cleanliness of markets is important for London as a\u000a      financial centre. Therefore the FSA undertakes and publishes market\u000a      cleanliness studies annually, and provides a measure indicative of the\u000a      level of suspicious trading activity (insider trading) in the London stock\u000a      market by detecting anomalous trading and price- movement patterns which\u000a      occur ahead of the release of important information, such as announcements\u000a      of takeovers or regulatory changes.\u000a    TDM's work on network-based filtering has been used by the FSA to\u000a      cross-validate the analysis of a set of financial events about its\u000a      cleanliness. The work done by TDM within a consultancy project at the FSA\u000a      was to provide advice on the methodological correctness and suitability of\u000a      Econophysics techniques applied to such market cleanliness event studies;\u000a      this included advice on coding and interpretation of results. Feedback\u000a      received from the FSA suggests that TDM's contribution to the project was\u000a      regarded as very valuable in enhancing the FSA's methodical awareness, and\u000a      that TDM's network based filtering techniques in particular could be used\u000a      to refine the FSA standard market cleanliness indicator.\u000a    A second instance of impact has been generated in the biotechnology\u000a      industry. In 2009, Ruggero Gramatica (RG) contacted KCL's Financial\u000a      Mathematics group, to study towards a PhD in Econophysics under the\u000a      supervision of TDM. After joining KCL in 2010, RG was appointed CEO of the\u000a      Swiss biotechnology company mondoBIOTECH AG, now THERAMetrics Holding AG,\u000a      and he quickly realized that the network-related tools and techniques\u000a      pioneered by TDM and co-workers could be generalized and fruitfully\u000a      applied to the data-analysis problems of concern to his company, dealing\u000a      with the discovery of drugs for rare diseases via a knowledge based\u000a      process of repurposing already existing drugs.\u000a    Specifically, THERAMetrics was looking for an inferential methodology\u000a      that could validate their line of research, which deals with automatically\u000a      extracting bio-medical information on human physiology provided in\u000a      published works of biochemists or physicians, which would allow the\u000a      discovery of new Mechanisms of Action (MoA). While biochemists will refer\u000a      to proteins, receptors, genes and biochemical processes, physicians and\u000a      health practitioners will mention symptoms, clinical tests, diseases, body\u000a      organs, tissues, and drugs available for treatment. The central task is to\u000a      combine such unstructured and dispersed information in a manner that\u000a      allows relating, for instance, information about biochemical processes\u000a      with diseases, symptoms and treatments discussed by clinicians.\u000a    More than 10 man-years of research were invested at THERAMetrics,\u000a      starting from the original model proposed by RG, where a knowledge based\u000a      graph derived from more than 3 million scientific publications, and made\u000a      of hundreds of thousand of nodes with a very dense set of correlated\u000a      information is provided, with the aim to search for the non-obvious paths\u000a      connecting certain molecules to certain diseases, stepping through a\u000a      number of biological pathways (i.e. new MOAs). TDM's research and\u000a      expertise, described in section 2, was instrumental to extract such\u000a      emerging patterns and create new bio-mathematical tools. Using these\u000a      tools, RG and the scientific team of THERAMetrics have been able to\u000a      validate a number of the molecules-disease relations that had been present\u000a      in their candidate pipeline, and in so doing were able to reinforce the\u000a      scientific foundations of THERAMetrics' drug-discovery platform. Indeed,\u000a      RG has meanwhile filed IP protection for the general semantic and\u000a      mathematical model underlying this research.\u000a    THERAMetrics Holding AG has been loss-making in the past. However,\u000a      amongst other factors related to the Company's restructuring plan, the\u000a      successful results obtained by the above-mentioned methodology helped the\u000a      company to outline a proper research platform which became a valuable\u000a      asset in the recent business combination realized by means of a reverse\u000a      merger with Pierrel's Contract Research International with THERAMetrics\u000a      Holding AG adding an innovative element in the drug rescuing and\u000a      repurposing strategy. The takeover\/merger was concluded in September 2013\u000a      and thanks to such business combination THERAMetrics Holding AG has\u000a      significantly increased its market capitalization.\u000a    ","ImpactSummary":"\u000a    Research of Tiziana Di Matteo on network-based filtering techniques has\u000a      lead to powerful new tools for the characterization of dependencies in\u000a      large complex data sets. This has generated impact on practitioners and\u000a      professional services in the biotechnology industry and with financial\u000a      regulators. The Swiss biotechnology firm THERAMetrics Holding AG has used\u000a      Di Matteo's techniques for developing a quantitative methodology to\u000a      validate their knowledge based research platform for drug repositioning\u000a      research. Within a consultancy project awarded to her by the Financial\u000a      Services Authority (FSA), the information filtering techniques where used\u000a      to provided advice on methodological correctness of Econophysics\u000a      techniques applied to a market cleanliness event study.\u000a    ","ImpactType":"Technological","Institution":"\u000a    King's College London\u000a    ","Institutions":[{"AlternativeName":"King's College London","InstitutionName":"King's College London","PeerGroup":"A","Region":"London","UKPRN":10003645}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a1. T. Aste, W. Shaw, T. Di Matteo, \"Correlation structure and dynamics in\u000a      volatile markets\", New J. Phys. 12 (2010) 085009.\u000a      DOI:10.1088\/1367-2630\/12\/8\/085009.\u000a    \u000a\u000a2. T. Di Matteo, F. Pozzi, T. Aste, \"The use of dynamical networks to\u000a      detect the hierarchical organization of financial market sectors\", The\u000a      European Physical Journal B 73 (2010) 3-11. DOI:\u000a      10.1140\/epjb\/e2009-00286-0\u000a    \u000a\u000a3. Won-Min Song, T. Di Matteo, T. Aste, \"Nested hierarchies in planar\u000a      graphs\", Discrete Applied Mathematics 159 (2011) 2135-2146. DOI:\u000a      10.1016\/j.dam.2011.07.018.\u000a    \u000a\u000a4. * Won-Min Song, T. Di Matteo, T. Aste, \"Hierarchical information\u000a      clustering by means of topologically embedded graphs\", PLoS One 7(3)\u000a      (2012) e31929. DOI: 10.1371\/journal.pone.0031929.\u000a    \u000a\u000a5. * T. Aste, Ruggero Gramatica, T. Di Matteo, \"Exploring complex\u000a      networks via topological embedding on surfaces\", Physical Review E\u000a      86 (2012) 036109. DOI: 10.1103\/PhysRevE.86.036109.\u000a    \u000a\u000a6. * R. Gramatica, D. Bevec, T. Di Matteo, M. Barbiani, S. Giorgetti and\u000a      T. Aste, \"Graph theory enables drug repurposing - How a mathematical model\u000a      can drive the discovery of hidden Mechanisms of Action\", submitted to Plos\u000a      One (2013); available at\u000a      http:\/\/arxiv.org\/abs\/1306.0924.\u000a    \u000aArticles marked with an asterisk best indicate the quality of the\u000a      underpinning research.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000a    Financial\/economic background concerning THERAMetrics including\u000a      information about details of the corporate takeover\/merger with Pierrel\u000a      SpA in 2013 at\u000a      http:\/\/www.therametrics.com\/investor\/investors\/key-information\u000a    KCL mirror of THERAMetrics site\u000a    Personal sources to corroborate impact at THERAMetrics Holding AG\u000a    \u000a      Former CEO of THERAMetrics Holding AG (testimonial received and\u000a        available on request).\u000a      Chief Scientific Officer at THERAMetrics Holding AG\u000a    \u000a    Personal source to corroborate impact at the Financial Services Authority\u000a      (FSA), now Financial Conduct Authority (FCA)\u000a    \u000a      Manager, Economics of Financial Regulation, FSA (testimonial received\u000a        and available on request)\u000a    \u000a    ","Title":"\u000a    New tools to study complex data sets\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Tiziana Di Matteo's (TDM) main research area is Complex Systems and\u000a      Econophysics, including the application of methods from statistical\u000a      physics and network theory to economic modelling, and the analysis of\u000a      financial markets and social problems.\u000a    She was, in particular, the first scientist to propose analysing complex\u000a      financial datasets (correlation and autocorrelation matrices of interest\u000a      rates and stock market indices) from the perspective of geometrical and\u000a      topological properties of metric graphs, embedded in spaces of appropriate\u000a      dimensions and curvature.\u000a    This proposal addresses a problem one generally faces when observing the\u000a      behaviour of large complex systems, namely that relevant features in such\u000a      systems are typically both local and global, and that these\u000a      different levels of organization emerge at different scales in a way that\u000a      is intrinsically not reducible. It is therefore essential to detect\u000a      clusters together with the different hierarchical patterns of dependencies\u000a      both above and below the cluster levels. Graph embedding techniques have\u000a      provided efficient tools to solve this task for a wide variety of complex\u000a      systems, including financial and biological systems.\u000a    Specifically, in the last few years, TDM and collaborators have been\u000a      focusing on the dynamical characterization of correlated financial\u000a      data in terms of graphs [1,2], studies that, apart from their scientific\u000a      interest, are very relevant to risk estimation and portfolio selection.\u000a      One of the main research interests addressed in these studies concerns the\u000a      extraction of meaningful information concerning the behaviour and\u000a      interactions between variables describing a system under study, often from\u000a      data sets containing a high level of redundancy, and to use such\u000a      information to model and forecast their collective evolution. The\u000a      main methodological outcome of these studies has been the discovery of a\u000a      new method to filter information out of complex datasets [5]. Recent\u000a      developments have shown that this approach can be used to extract clusters\u000a      and hierarchies from high-dimensional complex data sets in an unsupervised\u000a      and deterministic manner, without the use of any prior information\u000a      [3,4]. This network-based approach to information filtering has opened new\u000a      ways to study financial systems and also several other fields where a\u000a      large number of interrelated variables are concerned, such as inference in\u000a      biomedicine [6].\u000a    Key Researchers\u000a    1. Dr Tiziana Di Matteo\u000a    - King's College, since 01\/09, Reader in Financial Mathematics\u000a    2. Ruggero Gramatica\u000a    - King's College London, since 01\/10, PhD Student (PT)\u000a    - 04\/10 - 09\/13. CEO of MondoBiotech AG (now THERAMetrics Holding AG)\u000a    "},{"CaseStudyId":"41220","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3175395","Name":"Italy"}],"Funders":[],"ImpactDetails":"\u000a    In recent years, Damiano Brigo has been working on the formulation of\u000a      accurate pricing models for\u000a      the valuation of counterparty risk in an arbitrage free framework. Because\u000a      of the practical relevance\u000a      of this work, it generated substantial impact of various types, including\u000a      impact on practitioners and\u000a      professional services, and economic impact. Impact has been realised along\u000a      three different\u000a      dimensions as described below.\u000a    First and foremost, the significance and relevance of Brigo's research\u000a      for the financial industry is\u000a      shown by the fact that many of the counterparty risk models and\u000a      techniques, introduced by him, are\u000a      nowadays implemented in banks and related institutes. However, given the\u000a      relative secrecy of the\u000a      front office environments in banks, it is hard to document this in\u000a      writing. Nevertheless, a few cases\u000a      can be testified to by a former colleague (see Section 5).\u000a    A second dimension of Brigo's influence relates to his impact on public\u000a      policy and services. This is\u000a      demonstrated by the fact that he co-authored a report (see article [A] in\u000a      Section 5), together with a\u000a      regulator for the German Federal Financial Supervisory Authority (BAFIN),\u000a      who is also a\u000a      representative at the Basel III panel. This report was published in the\u000a      official Bundesbank discussion\u000a      paper series, which testifies to the significance attached by the\u000a      Bundesbank to the findings of this\u000a      paper. Another report in a working paper series of a similar calibre has\u000a      been published by the Bank\u000a      of International Settlements (BIS) as a response to the consultative\u000a      document \"Fundamental review\u000a      of the trading book\" issued by BIS earlier on, see article [B] in Section\u000a      5.\u000a    Moreover, Brigo's research on the calibration of structural first-passage\u000a      stochastic models to credit\u000a      default swap data and the related application to Lehman Brothers' data,\u000a      has had impact on public\u000a      policy in the form of a verdict by an Italian Court of Law. In 2011, the\u000a      court in Novara, Italy, retried a\u000a      case of financial intermediation after the spectacular bankruptcy of\u000a      Lehman Brothers Holdings Inc in\u000a      September 2008. The court based its analysis on research of Brigo. In\u000a      particular, the reasons for the\u000a      judgement explicitly refer to Brigo's research article on credit\u000a      calibration, which was an online\u000a      published preprint version of article [1] in Section 3. The relevant part\u000a      of the sentence translates as\u000a    \"... in a recent study two different mathematical models (AT1P and SBTV)\u000a      have been applied\u000a      to the CDS trend of Lehman, and this shows that, despite a worsening of\u000a      the estimate, even\u000a      from a mathematical point of view, based on the CDS patterns, the survival\u000a      probability of\u000a      Lehman, even near the default event, was still high.\"\u000a    Third, not in small part because of the fact that Brigo's models are\u000a      implemented in banks, he has\u000a      received numerous and regular invitations as plenary speaker to key\u000a      industry events, as participant\u000a      of round tables and as teacher of master-classes at major industry\u000a      conferences. These master-\u000a      classes are very specialized, and participation is expensive. Attendance\u000a      is typically in the range 10 -\u000a      20, with participants attending to learn first hand about theory related\u000a      to models implemented at their\u000a      employers' institutions.\u000a    Although Brigo's research led to publications in leading academic\u000a      journals, he has also been\u000a      working on the application of his ideas in the financial industry.\u000a      Relevant results have been\u000a      published and discussed in the top industry magazines with the highest\u000a      impact on practitioners and\u000a      professional services (see Section 5). Brigo's reputation in the financial\u000a      industry is further\u000a      underlined by a variety of interviews in relevant newspapers and by his\u000a      award as the most cited\u000a      author in Risk Magazine 2010, one of the most influential industry\u000a      magazines in the area of financial\u000a      risk.\u000a    To put publications in industry magazines, and invitations to industry\u000a      events into proper perspective,\u000a      one should bear in mind that nowadays the interests of practitioners from\u000a      the financial industry and\u000a      academics working on financial mathematics diverge significantly. Hence,\u000a      active involvements of\u000a      academics in industry discussions are very rare. Indeed, Brigo is one of\u000a      only a few examples of\u000a      mathematicians whose expertise is not only highly rated in academia but\u000a      also much sought after in\u000a      the financial industry.\u000a    ","ImpactSummary":"\u000a    Research of Professor Brigo in the areas of credit risk, pricing models\u000a      for the valuation of\u000a      counterparty risk, and the development of accurate calibration methods of\u000a      various credit risk models\u000a      has generated significant impact both on public policy and on\u000a      practitioners and professional\u000a      services. His models were implemented and his calibration methods adopted\u000a      in the financial\u000a      industry. The significance attached to his work by the industry also\u000a      resulted in a collaboration with\u000a      the German regulator (BAFIN). Further evidence of his impact can be found\u000a      in the fact that a Court\u000a      of Law based its analysis in a financial intermediation case on Brigo's\u000a      research.\u000a    ","ImpactType":"Economic","Institution":"\u000a    King's College London\u000a    ","Institutions":[{"AlternativeName":"King's College London","InstitutionName":"King's College London","PeerGroup":"A","Region":"London","UKPRN":10003645}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a1. D. Brigo, M. Morini, M. Tarenghi, Credit calibration with structural\u000a      models and equity return\u000a      swap valuation under counterparty risk. In: Bielecki, Brigo and Patras\u000a      (Editors), Credit Risk\u000a        Frontiers: Subprime crisis, Pricing and Hedging, CVA, MBS, Ratings and\u000a        Liquidity,\u000a      Wiley\/Bloomberg Press, 457-484, 2011. DOI: 10.1002\/9781118531839.ch14.\u000a    \u000a\u000a2. D. Brigo, A. Capponi, A. Pallavicini, Arbitrage-free bilateral\u000a      counterparty risk valuation under\u000a      collateralization and application to Credit Default Swaps. Mathematical\u000a        Finance, 2012.\u000a      DOI: 10.1111\/j.1467-9965.2012.00520.x.\u000a    \u000a\u000a3. D. Brigo, M.Morini, No-armageddon measure for arbitrage-free pricing\u000a      of index options in a\u000a      credit crisis. Mathematical Finance, 21, 573-593, 2011.\u000a      DOI: 10.1111\/j.1467-9965.2010.00444.x.\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"15","Level2":"2","Subject":"Banking, Finance and Investment"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000a    In order not to exceed the maximum, the following list is very much\u000a      restricted to a few references.\u000a      Further details can be provided on request.\u000a    Publications in research paper series of central banks and regulators:\u000a    A. C. Albanese, D. Brigo, F. Oertel, Restructuring counterparty credit\u000a      risk,\u000a      Deutsche Bundesbank Discussion Paper 14\/2013.\u000a      Affiliations of Oertel (BaFin) and Brigo in this article are new\u000a      affiliations at time of printing\u000a      rather than of submission. (corroborating document available on request)\u000a    B. D. Brigo, C. Nordio, Comments received on the consultative document\u000a      \"Fundamental review\u000a      of the trading book\", Bank for International Settlements, 2010. Document\u000a      available on\u000a      request; published at: http:\/\/www.bis.org\/publ\/bcbs219\/cacomments.htm\u000a      Link to KCL-mirror\u000a        of Bank for International Settlements 2010 Comments page.\u000a    Testimonial on Brigo's impact on practitioners and professional services:\u000a    \u000a      Head of Credit Models in Banca IMI (testimonial received and available\u000a        on request).\u000a    \u000a    Awards\u000a    \u000a      Most cited author in Risk Magazine (corroborating document available\u000a        on request)\u000a    \u000a    Evidence of influence in the financial industry (examples available on\u000a      request):\u000a    \u000a      Invited (plenary) speaker at various industry events\u000a      Teaching numerous training courses for practitioners\u000a      Several invitations to round tables\u000a    \u000a    Newspaper interviews and articles:\u000a    \u000a      Has Basel got its numbers wrong? Interview with Prof. Brigo, The\u000a          Banker, 21\/06\/2011 (article\u000a        available on request). The Banker is the Financial Times monthly\u000a        international financial\u000a        affairs publication and it is read in 150 countries around the world.\u000a      The risk-free myth, profile interview with Prof. Brigo, Risk\u000a          Magazine, March 2011 (article\u000a        available on request).\u000a    \u000a    Court sentence:\u000a    \u000a      Document (in Italian) available on request; relevant citation is on\u000a        page 17.\u000a    \u000a    ","Title":"\u000a    Credit risk modelling\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Professor Brigo has been applying credit risk and pricing models to the\u000a      valuation of counterparty\u000a      credit risk, the so-called Credit Valuation Adjustment (CVA). This\u000a      describes the modification in the\u000a      price of a financial product due to the fact that a counterparty may\u000a      default. CVA can be\u000a      conceptualised as an option on a complex portfolio with a random maturity\u000a      given by default time. Its\u000a      valuation requires sophisticated hybrid models that need to be calibrated\u000a      as accurately as possible\u000a      to liquid market data. Prior to Prof. Brigo's research, CVA would be\u000a      estimated through actuarial or\u000a      basic statistical techniques, building on the apparatus for risk\u000a      measurement. Since risk\u000a      measurement is typically much less precise than pricing, such techniques\u000a      were inadequate. The\u000a      novel approach proposed by Brigo instead used the methodology of risk\u000a      neutral valuation to study\u000a      CVA. This allowed to model the possible statistical dependence between\u000a      market risk and credit risk.\u000a      As time evolved, new types of valuation adjustments emerged, including\u000a      debit and funding\u000a      adjustments, and the challenge has shifted in modelling all such risks\u000a      consistently. Emphasis has\u000a      been on the proper and precise calibration of dynamical models of\u000a      different nature to market data,\u000a      analysing both models in the reduced form framework and models in the firm\u000a      value area.\u000a    In particular, one of his papers considered the calibration of firm value\u000a      models introduced by Brigo et\u000a      al. (see [1] in Section 3) to Lehman's Brothers' Credit Default Swap (CDS)\u000a      data. Firm value models\u000a      allow for an economic interpretation of default in terms of a default\u000a      barrier, representing safety\u000a      covenants. Typically, such models do not exhibit enough flexibility to\u000a      calibrate the CDS data with\u000a      precision, and are rarely used in the pricing and hedging arena for\u000a      counterparty credit risk. However,\u000a      in this paper Brigo and co-authors showed that this is not necessarily the\u000a      case, since firm value\u000a      models can be extended to curved barrier models that are fully\u000a      analytically tractable and lead to\u000a      robust and precise calibration of market CDS data. The analysis of the\u000a      calibration of such models\u000a      along Lehman's history, up to the default points, illustrated the economic\u000a      information which could be\u000a      extracted from the CDS market on the default dynamics of Lehman Brothers.\u000a      Prior to Brigo's work, it\u000a      was not known how to precisely calibrate firm value models to CDS data.\u000a    This kind of research has been part of the strategy of the financial\u000a      mathematics group at King's\u000a      College to close the gap between academic research in financial modelling\u000a      and industry\u000a      applications. Prof. Brigo took on the challenge at the level of\u000a      counterparty credit, funding and\u000a      liquidity risk, whereas other colleagues such as Dr Di Matteo engaged in\u000a      collaborations with\u000a      regulators (e.g. FSA) and asset managers such as Winton Capital.\u000a    Key researchers\u000a    \u000a      Professor Damiano Brigo\u000a        - King's College London, 07\/2010 - 09\/2012\u000a        as Gilbart Professor and Head of the Financial Mathematics group\u000a      Dr Andrea Pallavicini\u000a        - Head of Financial Engineering, Banca Leonardo, 2007-2011\u000a        - Head of Equity, FX and Commodities models, Banca IMI, since 2011\u000a      Dr Massimo Morini\u000a        - Head of Credit Models, Banca IMI, since 2007.\u000a      Marco Tarenghi\u000a        - Banca Leonardo, 2008-2011\u000a        - Mediobanca, since 2011\u000a    \u000a    "},{"CaseStudyId":"41221","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6251999","Name":"Canada"}],"Funders":[],"ImpactDetails":"\u000d\u000a    R K&#252;hn and P Neu's cooperation on risk was triggered by questions on\u000d\u000a      operational risk that arose\u000d\u000a      in Neu's risk control work at Dresdner Bank, Frankfurt. It resulted in the\u000d\u000a      formulation of a first\u000d\u000a      interacting-processes model of operational risks inspired by statistical\u000d\u000a      physics. When Neu and\u000d\u000a      K&#252;hn generalised their ideas to cover credit risk [1], this was apparently\u000d\u000a      regarded as an advance by\u000d\u000a      practitioners. It triggered, among other things, an invitation for RK to\u000d\u000a      explain his ideas to a group of\u000d\u000a      Economists and Mathematicians of the Macro-Prudential Risks Division at\u000d\u000a      the Bank of England\u000d\u000a      (BoE), very soon after Neu and K&#252;hn had posted their first results as a\u000d\u000a      preprint on the Gloria Mundi\u000d\u000a      web-site [A] (letters refer to sources listed in Sect 5). Since then RK\u000d\u000a      has attended several further\u000d\u000a      informal discussion meetings and given seminars to wider audiences at the\u000d\u000a      BoE to explain progress\u000d\u000a      in his credit risk modelling efforts [2,5].\u000d\u000a    As the Bank of England stepped up their efforts in network-oriented risk\u000d\u000a      modelling with the onset of\u000d\u000a      the financial crisis, they asked RK whether there was a suitable candidate\u000d\u000a      from his group who\u000d\u000a      would be able to support these efforts as an intern. This resulted in RK's\u000d\u000a      PhD student Kartik Anand\u000d\u000a      spending several months at the Bank, to implement and calibrate an\u000d\u000a      expanded and more detailed\u000d\u000a      version of the Neu-K&#252;hn model to specifically assess risk in the British\u000d\u000a      banking system. The study\u000d\u000a      was to be one of the first calibrated network models developed at the BoE\u000d\u000a      to highlight the\u000d\u000a      propagation of economic shocks through networks of claims and obligations\u000d\u000a      and to illustrate\u000d\u000a      amplifying effects of asset fire sales. In its course, and with contagion\u000d\u000a      through networks of financial\u000d\u000a      exposures becoming to be recognized as one of the main mechanisms\u000d\u000a      responsible for the\u000d\u000a      unfolding of the crisis, RK was also contacted specifically to share his\u000d\u000a      views and expertise on\u000d\u000a      network-oriented research on risk [B]. One of the important outcomes of\u000d\u000a      this line of research was to\u000d\u000a      support the usefulness of its methodology for stress testing. Not in small\u000d\u000a      part due to this aspect,\u000d\u000a      network-oriented approaches to the analysis of systemic risk have now\u000d\u000a      become firmly established\u000d\u000a      as part of the research tool-kit at the Bank and other key policy\u000d\u000a      institutions internationally (see\u000d\u000a      Haldane, 2009; Haldane and May, 2011 [F]).\u000d\u000a    The importance of RK's recent work on the influence of CDS trading on\u000d\u000a      systemic risk (with Heise)\u000d\u000a      was quickly realized by practitioners, and resulted in an invitation to\u000d\u000a      present these results at the\u000d\u000a      2011 Global Derivatives Trading &amp; Risk Management Conference &#8212; the\u000d\u000a      world's largest industry\u000d\u000a      conference of its kind &#8212; and at the 2nd Annual Conference of the\u000d\u000a      Macro-Prudential Research\u000d\u000a      Network (MaRs) at the European Central Bank [D], as well as in an\u000d\u000a      opportunity to present them in a\u000d\u000a      seminar at the BoE in 2012. Finally, the Financial Stability Department of\u000d\u000a      the Bank of Canada\u000d\u000a      recently decided to use RK's CDS model, and calibrate it on real market\u000d\u000a      data in one of its future\u000d\u000a      policy projects aimed at creating tools to facilitate systemic stress\u000d\u000a      testing of over-the-counter\u000d\u000a      derivatives markets. As one of the originators of that model, RK was\u000d\u000a      invited to lend his expertise to\u000d\u000a      this enterprise by participating in the project.\u000d\u000a    Concerning appreciation by practitioners of RK's results and insights on\u000d\u000a      collective effects in\u000d\u000a      financial risks, P Neu, BCG's Head of Risk Practice Area, in a supporting\u000d\u000a      letter [E] states that\u000d\u000a      \"[RK's] .. results and insights have informed our discussions with\u000d\u000a        regulators and our clients.\u000d\u000a        Moreover, his techniques allowed us to gain a better understanding of\u000d\u000a        the underlying collective\u000d\u000a        effects and to create transparency and processes for mitigation measures\u000d\u000a        regarding operational\u000d\u000a        and credit risk management\". In 2013, Neu also spent part of his\u000d\u000a      sabbatical from BCG with RK at\u000d\u000a      King's College London with the specific aim of exploring one of the most\u000d\u000a      important new forms of\u000d\u000a      systemic financial risks and collective effects arising from a recent\u000d\u000a      decision by G20 states to move\u000d\u000a      substantial parts of financial derivatives trading away from\u000d\u000a      over-the-counter markets and replace it\u000d\u000a      by a system where such trades are conducted via central clearing houses.\u000d\u000a    Further evidence for the visibility of RK's work on risk in professional\u000d\u000a      circles derives from the fact\u000d\u000a      that the risk papers of RK and co-authors have between them had over 4600\u000d\u000a      downloads from the\u000d\u000a      Gloria Mundi web-site [A], and that his risk modelling web page is very\u000d\u000a      highly ranked in popular\u000d\u000a      search engines: e.g., a Google-search for the generic search term \"risk\u000d\u000a      modelling\" currently lists his\u000d\u000a      page second only to Wikipedia (in 2nd non-sponsored position).\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research by Reimer K&#252;hn (RK) and collaborators has produced a framework\u000d\u000a      to study and quantify\u000d\u000a      the influence of interactions on risk in complex systems, including\u000d\u000a      default risk in economy-wide\u000d\u000a      networks of financial exposures. This work has had impact on practitioners\u000d\u000a      and professional\u000d\u000a      services dealing with financial risk, including research groups at central\u000d\u000a      banks, who &#8212; partly in\u000d\u000a      response to the recent financial crisis &#8212; have adopted such network\u000d\u000a      oriented approaches to analyse\u000d\u000a      and quantify systemic risk. The Financial Stability Division at the Bank\u000d\u000a      of England has, for instance,\u000d\u000a      developed refined versions of the network-oriented models proposed by K&#252;hn\u000d\u000a      and collaborators to\u000d\u000a      specifically assess risk in the British banking system.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    King's College London\u000d\u000a    ","Institutions":[{"AlternativeName":"King's College London","InstitutionName":"King's College London","PeerGroup":"A","Region":"London","UKPRN":10003645}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1. P. Neu and R. K&#252;hn, Credit Risk Enhancement in a Network of\u000d\u000a        Interdependent Firms,\u000d\u000a      Physica A 342, 639-655(2004), DOI: 10.1016\/j.physa.2004.05.062\u000d\u000a    \u000a\u000a2. *J.P.L. Hatchett and R. K&#252;hn, Effects of Economic Interactions on\u000d\u000a        Credit Risk, J. Phys. A\u000d\u000a      39, 2231-2251 (2006), DOI: 10.1088\/0305-4470\/39\/10\/001\u000d\u000a    \u000a\u000a3. J.P.L. Hatchett and R. K&#252;hn, Credit Contagion and Credit Risk, Quant.\u000d\u000a      Fin 9, 373-382\u000d\u000a        (2009) DOI: 10.1080\/14697680802464162\u000d\u000a    \u000a\u000a4. *K. Anand and R K&#252;hn, Phase Transitions in Operational Risk,\u000d\u000a      Phys. Rev. E 75, 016111\u000d\u000a      (2007), DOI: 10.1103\/PhysRevE.75.016111\u000d\u000a    \u000a\u000a5. *S. Heise and R. K&#252;hn, Derivatives and Credit Contagion in\u000d\u000a        Interconnected Networks, Eur.\u000d\u000a      Phys. J. B 85, 115 (2012), DOI: 10.1140\/epjb\/e2012-20740-0\u000d\u000a    \u000aArticles marked with an asterisk best indicate the quality of the\u000d\u000a      underpinning research.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"15","Level2":"2","Subject":"Banking, Finance and Investment"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a    [A] http:\/\/gloria-mundi.com One\u000d\u000a      of the main online repositories of papers consulted by practitioners,\u000d\u000a      which hosts papers on various aspects of risk; includes download\u000d\u000a      statistics of submitted papers.\u000d\u000a      Figures given in Sect 4 concerning numbers of downloads from this site can\u000d\u000a      be checked by\u000d\u000a      going to the site, searching for Author \"Kuhn, Reimer\" and checking\u000d\u000a      downloads individually for\u000d\u000a      listed papers. Link to KCL-mirror\u000d\u000a        of gloria-mundi search form.\u000d\u000a    [B] E-mail trail and questions document Analytical_6033092v1.pdf received\u000d\u000a      from the Bank (2008);\u000d\u000a      (documents available on request).\u000d\u000a    [C] http:\/\/www.bankofengland.co.uk\/research\/Pages\/workingpapers\/default.aspx\u000d\u000a      Repository of\u000d\u000a      working papers of the BoE dating back to 1992. A report of the study by\u000d\u000a      Anand et al. was\u000d\u000a      recently included in the repository as paper No 458 (document available on\u000d\u000a      request).\u000d\u000a      Link to KCL-mirror\u000d\u000a        of BoE working paper series site.\u000d\u000a    [D] http:\/\/www.ecb.int\/home\/html\/researcher_mars.en.html\u000d\u000a      MaRs website, includes link to the\u000d\u000a      2012 conference programme (programme available on request).\u000d\u000a      Link to KCL-mirror\u000d\u000a        of MaRs web site and KCL-mirror\u000d\u000a        of 2012 MaRs Conference Programme.\u000d\u000a    [E] Supporting letter from BCG, received and available on request.\u000d\u000a    [F] Haldane, A. 2009, Rethinking the Financial Network, Speech at\u000d\u000a      the Financial Student\u000d\u000a      Association, Amsterdam, April 2009; Haldane A. and May R. 2011, Systemic\u000d\u000a        Risk in Banking\u000d\u000a        Ecosystems, Nature 469, 351-355 (2011) (documents available on\u000d\u000a      request).\u000d\u000a    \u000d\u000a      First contact with the Bank of England initiated by the then Senior\u000d\u000a        Manager of its Macro\u000d\u000a        Prudential Risks Division.\u000d\u000a      Later meetings and consultations involved members of the Financial\u000d\u000a        Stability Division at the\u000d\u000a        Bank of England (testimonial about impact at BoE received and available\u000d\u000a        on request).\u000d\u000a      The source [C] above could be consulted to verify the statement made\u000d\u000a        in Sect 4 concerning\u000d\u000a        the appearance of network-oriented risk research within the Bank of\u000d\u000a        England.\u000d\u000a      Researchers at the Financial Stability Department of the Bank of\u000d\u000a        Canada could confirm\u000d\u000a        adoption of the Heise-K&#252;hn CDS model for one of their projects to\u000d\u000a        develop tools for\u000d\u000a        systemic stress-testing of OTC derivatives markets.\u000d\u000a      BCG's former head of Risk Practice area, now at DZ Bank, confirmed the\u000d\u000a        importance\u000d\u000a        attached by his team to understanding collective effects in financial\u000d\u000a        risks (see [E] above).\u000d\u000a      Page rank in google searches can be checked directly by performing the\u000d\u000a        search indicated.\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Effects of Interactions on Risk\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    From 2002 onwards, RK has been applying methods originally developed for\u000d\u000a      the study of glassy\u000d\u000a      systems to analyse problems of risk in networks of interacting processes\u000d\u000a      or networks of mutual\u000d\u000a      financial exposures. The main challenge here is to properly model the role\u000d\u000a      of interactions (networks\u000d\u000a      of functional dependencies) on risk for these systems, which are typically\u000d\u000a      characterised by high\u000d\u000a      levels of heterogeneity.\u000d\u000a    Through their ability to trigger avalanches of risk events, interactions\u000d\u000a      can significantly increase the\u000d\u000a      probabilities for the occurrence of systemic high-loss events in\u000d\u000a      comparison to those foreseen in\u000d\u000a      conventional approaches to risk. These typically take an essentially\u000d\u000a      static point of view. They\u000d\u000a      usually involve estimating the likelihood for the occurrence of individual\u000d\u000a      risk events over a certain\u000d\u000a      time horizon, as well as the distribution of losses incurred by such risk\u000d\u000a      events. However losses\u000d\u000a      generated by several such events are then aggregated, as if they were\u000d\u000a      independent, or at best\u000d\u000a      correlated. Such approaches are fundamentally incapable of describing\u000d\u000a      avalanches of risk events,\u000d\u000a      as would be associated with blackouts in power-grids or with significant\u000d\u000a      clustering of credit-defaults\u000d\u000a      as seen in the past financial crisis. This fundamental deficiency is, for\u000d\u000a      instance, shared by the Basel\u000d\u000a      II document which regulates the methods by which banks analyse their\u000d\u000a      risks, as a consequence of\u000d\u000a      which systemic risk is severely underestimated in this central regulatory\u000d\u000a      document.\u000d\u000a    To overcome the deficiency, a proper dynamical approach has to be taken\u000d\u000a      which allows one to\u000d\u000a      describe the mechanisms by which one risk event is able to induce another\u000d\u000a      via \"direct contagion\",\u000d\u000a      using methods which are powerful enough to capture the full heterogeneity\u000d\u000a      of mutual dependencies\u000d\u000a      and to allow studying the dynamics at system level.\u000d\u000a    For the important problem of credit risk, this was first achieved\u000d\u000a      by K&#252;hn and Neu in 2004 [1]\u000d\u000a      (numbers refer to references in Sect 3) in a model which describes the\u000d\u000a      stochastic dynamics of a\u000d\u000a      heterogeneous network of interacting financial positions, where\u000d\u000a      start-of-year wealth positions and\u000d\u000a      economic impacts of defaults could be related to unconditional and\u000d\u000a      conditional default probabilities\u000d\u000a      of firms in the network. The results are based on earlier joint work with\u000d\u000a      Neu on operational risk. A\u000d\u000a      fully analytic solution of the credit risk problem, which allowed a more\u000d\u000a      comprehensive overview over\u000d\u000a      collective effects in contagion dynamics was obtained with Hatchett [2] in\u000d\u000a      2006. Finally, the first\u000d\u000a      ever model to include the effect of credit default swap (CDS)\u000d\u000a      markets on contagion dynamics was\u000d\u000a      proposed and solved in 2009\/10 (with Heise), clearly showing the\u000d\u000a      potentially destabilising nature of\u000d\u000a      CDS markets at a systemic level [5]. A fully analytical solution to an operational\u000d\u000a        risk model for\u000d\u000a      networks of interacting processes [4] was obtained in 2007 (with Anand),\u000d\u000a      pointing out the possibility\u000d\u000a      of first order phase transitions to complete break-down in process\u000d\u000a      networks, when mutual\u000d\u000a      dependencies between processes are increased, e.g. in moves to increase\u000d\u000a      system-efficiency.\u000d\u000a    Key Researchers\u000d\u000a    \u000d\u000a      Dr Reimer K&#252;hn: since 01\/03 at King's College London, initially as\u000d\u000a        Lecturer, 09\/05 promoted\u000d\u000a        to Reader, 03\/11 promoted to Professor\u000d\u000a      Dr Peter Neu: 08\/97-07\/05 at Dresdner Bank Frankfurt, Director, Head\u000d\u000a        of Liquidity Risk\u000d\u000a        Control; 08\/05-02\/13 at Boston Consulting Group Frankfurt, Partner and\u000d\u000a        Managing Director,\u000d\u000a        Topic Leader Risk Practice Area; since 04\/13 at DZ Bank Frankfurt,\u000d\u000a        Division Head of\u000d\u000a        Financial Controlling, Strategy and Investments\u000d\u000a      Dr Kartik Anand: 09\/05-12\/08 at King's College London, PhD student;\u000d\u000a        since Nov 2012 at the\u000d\u000a        Central Bank of Canada. (The risk work of Kartik Anand at KCL was\u000d\u000a        supported by a\u000d\u000a        Departmental DTA doctoral training grant.)\u000d\u000a      Dr Jonathan Hatchett: 11\/04 - 03\/06 at RIKEN Lab for Mathematical\u000d\u000a        Neuroscience,\u000d\u000a        Saitama, Japan, Post-Doc; since 08\/06 at Hymans and Robertsons LLP\u000d\u000a        London, Strategic\u000d\u000a        Risk Consultant\u000d\u000a      Sebastian Heise: 09\/08-07\/10 at Bank of England, Economist, Financial\u000d\u000a        Stability Division,\u000d\u000a        and Graduate Diploma Student (PT) at King's College; since 09\/10 at Yale\u000d\u000a        University, New\u000d\u000a        Haven, PhD Student\u000d\u000a    \u000d\u000a    "},{"CaseStudyId":"41222","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"660013","Name":"Finland"}],"Funders":[],"ImpactDetails":"\u000d\u000a    We have applied our risk management techniques to several financial\u000d\u000a      institutions in Finland\u000d\u000a      including the Ministry of Social Affairs and Health, the State Pension\u000d\u000a        Fund, and the Bank of Finland.\u000d\u000a      In the UK, we are developed an Asset-Liability Management (ALM) model for\u000d\u000a      the Pension Policy\u000d\u000a      Institute (PPI) to study the risks associated with variable annuity\u000d\u000a      pension contracts. Consultations\u000d\u000a      with the Ministry as well as with the Bank of Finland were initiated by\u000d\u000a      them, after they had heard\u000d\u000a      about our earlier work with pension insurers in Finland. Consultation with\u000d\u000a      the State Pension Fund\u000d\u000a      started after development of a pilot model of the state pension\u000d\u000a      liabilities. Collaboration with the PPI\u000d\u000a      was initiated when we contacted them and described our earlier work with\u000d\u000a      Finnish pension insurers.\u000d\u000a      Our consulting for the Ministry of Social Affairs and Health, the State\u000d\u000a      Pension Fund, and the Bank of\u000d\u000a      Finland was done through Quantitative Solvency Analysts (QSA), which is a\u000d\u000a      company founded by\u000d\u000a      Pennanen and his collaborators as a spin-off of their research projects.\u000d\u000a      The collaboration with the\u000d\u000a      PPI is a joint project aiming at developing computational tools for\u000d\u000a      quantifying long term uncertainties\u000d\u000a      in the UK pensions industry.\u000d\u000a    The models and computational techniques produced for the Ministry of\u000d\u000a      Social Affairs and Health\u000d\u000a      address the Finnish private sector pension system as a whole. Our models\u000d\u000a      build on the actual cash-flows\u000d\u000a      of both the assets and liabilities and it avoids many problems (most\u000d\u000a      notably pro-cyclicality)\u000d\u000a      associated with traditional accounting standards. The models' outputs have\u000d\u000a      led to many discoveries\u000d\u000a      concerning the financial risks of the pension system. The discoveries are\u000d\u000a      described in reports [A], [B]\u000d\u000a      and [C] the publication of which was funded by the Ministry. These reports\u000d\u000a      also include new\u000d\u000a      recommendations and guidelines for the financial risk management in\u000d\u000a      pension insurance companies\u000d\u000a      and in their regulation. The findings were widely recognised by the\u000d\u000a      pension industry and the media,\u000d\u000a      see for example [E,F,G,H].\u000d\u000a    Report [D] uses our asset-liability model to study how individual income\u000d\u000a      generated in the UK\u000d\u000a      automatic enrolment pension system compare to an income that might be\u000d\u000a      considered adequate in\u000d\u000a      an uncertain investment environment. Once automatic enrolment into\u000d\u000a      workplace pensions is fully\u000d\u000a      implemented in 2018, it is estimated that there could be between 6 and 9\u000d\u000a      million new savers into\u000d\u000a      workplace pensions. The simulation studies conducted with our model\u000d\u000a      suggest several possibilities\u000d\u000a      to improve the likelihood of achieving adequate retirement income [D].\u000d\u000a    The consultation provided for the State Pension Fund includes risk\u000d\u000a      analysis of their annual strategic\u000d\u000a      investment plan as well as a long term analysis of their funding ratio.\u000d\u000a      The modeling project started in\u000d\u000a      the fall of 2012, and some of the computational work is still under way.\u000d\u000a      The analyses have been\u000d\u000a      used by the management and the board of directors to asses financial risks\u000d\u000a      of the Fund. The funding\u000d\u000a      ratio is the key variable used in defining the targets of the State\u000d\u000a      Pension Fund set in Finnish law.\u000d\u000a      The ALM-based funding ratio is currently used in risk reporting together\u000d\u000a      with traditional actuarial\u000d\u000a      valuations. The analyses are based on models and computational techniques\u000d\u000a      developed by\u000d\u000a      Pennanen and his research team.\u000d\u000a    The consultation done for the Bank of Finland focused on the risk\u000d\u000a      analysis of the bank's reserves.\u000d\u000a      The work included the development of a stochastic model for the\u000d\u000a      investments of the bank as well as\u000d\u000a      a computational analysis of the bank's investment strategies. Our analysis\u000d\u000a      covered currency risk,\u000d\u000a      interest rate risk, and credit risk of the banking reserves in a dynamic\u000d\u000a      multiperiod setting. The results\u000d\u000a      produced by the models were used by the board of directors of the Bank in\u000d\u000a      the analysis of the\u000d\u000a      strategic investment plan.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Prof. Pennanen and collaborators have developed mathematical models and\u000d\u000a      computational\u000d\u000a      techniques for financial risk management. The techniques allow for\u000d\u000a      quantitative analysis and\u000d\u000a      optimization of financial risk management actions in an uncertain\u000d\u000a      investment environment. The\u000d\u000a      techniques have been used by the State Pension Fund, Ministry of Social\u000d\u000a      Affairs and Health, Bank\u000d\u000a      of Finland and Pension Policy Institute. The techniques have significant\u000d\u000a      impact on practitioners and\u000d\u000a      professional services in increasing the awareness and understanding of\u000d\u000a      long-term financial risks that\u000d\u000a      are difficult to quantify with more traditional techniques. Beneficiaries\u000d\u000a      of the developed risk\u000d\u000a      management techniques include future pensioners and tax payers.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    King's College London\u000d\u000a    ","Institutions":[{"AlternativeName":"King's College London","InstitutionName":"King's College London","PeerGroup":"A","Region":"London","UKPRN":10003645}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    \u000a1*. T. Pennanen, Convex duality in optimal investment under illiquidity,\u000d\u000a      Mathematical Programming,\u000d\u000a      (2013). DOI:10.1007\/s10107-013-0721-5.\u000d\u000a    \u000a\u000a2*. H. Aro, T. Pennanen, Stochastic modeling of mortality and financial\u000d\u000a      markets, Scandinavian\u000d\u000a        Actuarial Journal, 1-27, 2012. DOI:10.1080\/03461238.2012.724442.\u000d\u000a    \u000a\u000a3. T. Pennanen, A.-P. Perkki&#246;, Stochastic programs without duality gaps,\u000d\u000a      Mathematical\u000d\u000a        Programming, 136:91-110, 2012. DOI: 10.1007\/s10107-012-0552-9.\u000d\u000a    \u000a\u000a4*. T. Pennanen, Introduction to convex optimization in financial\u000d\u000a      markets, Mathematical\u000d\u000a        Programming, 134:157-186, 2012. DOI: 10.1007\/s10107-012-0573-4.\u000d\u000a    \u000a\u000a5. T. Pennanen, Dual representation of superhedging costs in illiquid\u000d\u000a      markets, Mathematics and\u000d\u000a        Financial Economics, 5:233-248, 2012. DOI:\u000d\u000a      10.1007\/s11579-012-0061-x.\u000d\u000a    \u000a\u000a6. P. Malo, T. Pennanen, Reduced form modeling of limit order markets, Quantitative\u000d\u000a        Finance,\u000d\u000a      12:1025-1036, 2012. DOI:10.1080\/14697688.2011.589402.\u000d\u000a    \u000aArticles marked with an asterisk best indicate the quality of the\u000d\u000a      underpinning research.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"15","Level2":"2","Subject":"Banking, Finance and Investment"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000d\u000a    Individual sources:\u000d\u000a    \u000d\u000a      Managing Director, State Pension Fund (testimonial received and\u000d\u000a        available on request)\u000d\u000a      Director, Ministry of Social Affairs and Health, (testimonial received\u000d\u000a        and available on request)\u000d\u000a    \u000d\u000a    Reports for practitioners:\u000d\u000a    A. P. Hilli, T. Pennanen, El&#228;kevakuuttaminen ep&#228;varmassa\u000d\u000a      sijoitusymp&#228;rist&#246;ss&#228;, Unigrafia,\u000d\u000a      2012; this 94-page report describes the results of a research project\u000d\u000a      funded by the Ministry\u000d\u000a      of Social Affairs and Health (in Finnish, document available on request).\u000d\u000a    B. P. Hilli, T. Pennanen, El&#228;kevakuuttaminen ep&#228;varmassa\u000d\u000a      sijoitusymp&#228;rist&#246;ss&#228; Laskelmia\u000d\u000a      ty&#246;el&#228;kkeiden rahastoinnin tehostamisesta, Aalto University, 2012;\u000d\u000a      this 50-page report\u000d\u000a      describes the results of a research project funded by the Ministry of\u000d\u000a      Social Affairs and Health\u000d\u000a      (in Finnish, document available on request).\u000d\u000a    C. P. Hilli, T. Pennanen, Ty&#246;el&#228;kkeiden rahastoinnin uudistamistarpeet, Ty&#246;el&#228;kelehti,\u000d\u000a      4, 2012;\u000d\u000a      invited article in Ty&#246;el&#228;kelehti, a magazine directed at experts in the\u000d\u000a      field of earnings-related\u000d\u000a      pension. It is published by the Finnish Centre for Pensions and comes out\u000d\u000a      five times per year\u000d\u000a      (in Finnish, document available on request).\u000d\u000a    D. J. Armstrong, L. Carrera, D. Redwood, T. Pennanen, What level of\u000d\u000a      pension contribution is\u000d\u000a      needed to obtain an adequate retirement income, Pension Policy Institute,\u000d\u000a      October 2013; full\u000d\u000a      report (ISBN 978-1-906284-27-5), and press release (documents available on\u000d\u000a      request).\u000d\u000a      Link to KCL-mirror\u000d\u000a        of PPI page for full report.\u000d\u000a    Articles in newspapers (in Finnish, documents available on request):\u000d\u000a    E. Tutkijat: El&#228;kelaitosten sijoitukset liian lyhytn&#228;k&#246;isi&#228;, Taloussanomat,\u000d\u000a      20\/06\/2012\u000d\u000a    F. Tutkijoiden hurja ehdotus: Suomeen vain yksi el&#228;kelaitos, Taloussanomat,20\/06\/2012\u000d\u000a      Link to KCL-mirror\u000d\u000a        of the site.\u000d\u000a    G. Avoimuus tekisi hyv&#228;&#228; el&#228;kkeille, Helsingin Sanomat 28.12.2012\u000d\u000a    H. Raikas tuulahdus: Tutkijakaksikko ehdottaa mielenkiintoisia\u000d\u000a      uudistuksia el&#228;kerahoitukseen\u000d\u000a      Suomen Kuvalehti 12.11.2012\u000d\u000a    ","Title":"\u000d\u000a    Convex optimisation in financial risk management\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The developed risk management techniques are based on the simple but\u000d\u000a      often neglected fact that\u000d\u000a      most pricing and valuation problems in the financial industry can be\u000d\u000a      treated as problems of asset-liability\u000d\u000a      management which, in turn, can be treated under convex optimisation. This\u000d\u000a      observation adds\u000d\u000a      consistency to various approaches in financial mathematics and it\u000d\u000a      simplifies many problems that are\u000d\u000a      often viewed as separate. A general overview is given in reference [4]\u000d\u000a      below. Many earlier models\u000d\u000a      concentrate solely on pricing of financial products without explicit\u000d\u000a      account of the asset management\u000d\u000a      side. Accounting standards, both in banking and insurance suffer from the\u000d\u000a      same problem. They are\u000d\u000a      often based on so called risk-neutral valuation principles or even simpler\u000d\u000a      rules of thumb that ignore\u000d\u000a      the interplay between assets and liabilities. This may lead to problems\u000d\u000a      when the pricing models fail,\u000d\u000a      as has been seen in the past.\u000d\u000a    Traditionally, the main tools in mathematical finance have come from\u000d\u000a      probability theory and\u000d\u000a      stochastics but convex analysis is turning out to be equally useful. For\u000d\u000a      example, general\u000d\u000a      characterisations of the no-arbitrage property of a perfectly liquid\u000d\u000a      market model in terms of\u000d\u000a      martingale measures are largely based on separation theorems for convex\u000d\u000a      sets. The optimisation\u000d\u000a      perspective brings in variational and computational techniques that have\u000d\u000a      been successful in more\u000d\u000a      traditional fields of applied mathematics such as partial differential\u000d\u000a      equations and operations\u000d\u000a      research. Techniques of convex optimisation allow for significant\u000d\u000a      generalisations to the classical\u000d\u000a      models of perfectly liquid financial markets. We have applied convex\u000d\u000a      analysis techniques to extend\u000d\u000a      fundamental results in mathematical finance to markets with transaction\u000d\u000a      costs and portfolio\u000d\u000a      constraints that are often encountered in practical applications. New\u000d\u000a      models with nonlinear illiquidity\u000d\u000a      effects are developed in references [1,3,5,6] below.\u000d\u000a    Techniques of convex optimisation provide new possibilities also for\u000d\u000a      financial risk management in\u000d\u000a      practice beyond the techniques of stochastic analysis alone. The presence\u000d\u000a      of stochastic elements in\u000d\u000a      a model results in difficult, often infinite-dimensional optimisation\u000d\u000a      problems that require specialised\u000d\u000a      optimisation techniques. We have developed new computational techniques\u000d\u000a      for such problems by\u000d\u000a      combining simulation techniques with the classical Galerkin method which\u000d\u000a      is widely used in\u000d\u000a      numerical analysis of partial differential equations and other problems in\u000d\u000a      physics and engineering. A\u000d\u000a      general description of this approach can be found in [4].\u000d\u000a    Practical applications of the convex optimisation techniques in risk\u000d\u000a      management require statistical\u000d\u000a      and econometric models of the underlying risk factors that affect the\u000d\u000a      investment and the liabilities of\u000d\u000a      a financial institution. Some of the models developed by our group are\u000d\u000a      reported in [2,6]. Applications\u000d\u000a      of the models and associated computational techniques to the Finnish\u000d\u000a      pension industry are reported\u000d\u000a      in articles [A,B] in Section 5 below.\u000d\u000a    The mathematical side of the research is based on earlier work of Teemu\u000d\u000a      Pennanen on\u000d\u000a      mathematical optimisation. The major part of it has been produced since\u000d\u000a      2008 by Pennanen at\u000d\u000a      King's College London.\u000d\u000a    The underpinning research is part of the general strategy of the\u000d\u000a      Financial Mathematics research\u000d\u000a      group in extending the applicability of mathematical finance. The strategy\u000d\u000a      is motivated by the well\u000d\u000a      documented failures of more traditional models of mathematical finance\u000d\u000a      during the recent financial\u000d\u000a      crises.\u000d\u000a    Key researchers\u000d\u000a    \u000d\u000a      Professor Teemu Pennanen\u000d\u000a      - King's College London since 10\/2011\u000d\u000a\u0009\u0009\u000d\u000a \u0009  \u000d\u000a      Dr John Armstrong\u000d\u000a      - Lecturer, King's College London\u000d\u000a\u0009  \u000d\u000a      \u000d\u000a\u0009  Dr Petri Hilli,\u000d\u000a      - QSA Quantitative Solvency Analysts Ltd\u000d\u000a    \u000d\u000a     \u000d\u000a\u0009Helena Aro,\u000d\u000a     - PhD student, Aalto University\u000d\u000a\u0009\u000d\u000a    "},{"CaseStudyId":"41518","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"690791","Name":"Ukraine"},{"GeoNamesId":"3017382","Name":"France"}],"Funders":[],"ImpactDetails":"\u000a    Joint work with Institute Curie (Paris, France) started in 2004. This is\u000a      one of the top European cancer research and treatment centres. Together\u000a      with the Bioinformatics Unite of Institute Curie, we have developed a\u000a      software library which implements most of our methods. This software is\u000a      now open for non-commercial use worldwide [5.2]. Institute Curie\u000a      uses this software in various projects for visualization and analysis of\u000a      microarrays for various types of cancer, for visualization of clinical and\u000a      biochemical data [5.2].\u000a    Publication [5.3] demonstrates knowledge transfer impact as the\u000a      IC-MSQUARE conference is dedicated to application of mathematics in other\u000a      science and technology, and the author list of the paper has two member of\u000a      the University (Gorban and Mirkes) and three colleagues from [text removed\u000a      for publication] (Alexandris, Slater and Tuli).\u000a    Use in Humans\u000a      Many institutions and clinics in various countries have reported\u000a      successful use of these methods and software for clinical purposes [5.2]:\u000a    \u000a      The Ukrainian Medical Almanac [5.6] reported two new\u000a        applications: (i) Prediction of treatment result of long bones fracture\u000a        for diabetes patients, (ii) Pain management and quantitative estimation\u000a        of pain.\u000a      Dr. Arndt Benecke (joint affiliation at Institut de G&#233;n&#233;tique et de\u000a        Biologie Mol&#233;culaire et Cellulaire, CNRS\/INSERM\/ULP, Coll&#232;ge de France\u000a        and Institut des Hautes Etudes Scientifique, France) used the method of\u000a        elastic maps for analysis of microarray data in cancer. This experience\u000a        was reflected in the subsequent publication [5.8].\u000a    \u000a    Use in Animals\u000a      The treatment of dogs is a vast and recession-resistant business: there\u000a      are 80 million dogs in the United States alone, and even in recession most\u000a      people keep spending on their pets. Research into the treatment of cancer\u000a      in dogs also has relevance to the treatment of cancer in humans,\u000a      particularly because it relates to spontaneous cancer which occurs in a\u000a      domestic environment. \"Lymphoma is one of the most common canine cancers,\u000a      representing 5% of all malignancies. It has an annual incidence on 25\u000a      cases per 100,00 dogs\" [5.7].\u000a    [text removed for publication] has developed a lymphoma blood test, [text\u000a      removed for publication], [5.4, 5.5] which gives vets an\u000a      easier, less stressful, cheaper and quicker way of testing for lymphoma.\u000a      This means that dogs are more likely to be tested for lymphoma when any\u000a      suspicious symptoms show, and that results of the tests are available\u000a      quickly &#8212; generally the same day. If lymphoma is caught early on it can be\u000a      treated quickly. While researchers do not talk of a \"cure\" for lymphoma,\u000a      early treatment can produce a healthier dog for longer, adding 12 months\u000a      to two years to a dog's average 12-year lifespan.\u000a    The blood test was developed from serum samples collected from several\u000a      veterinary practices. The samples were fractionated and analysed by mass\u000a      spectrometry. Two protein peaks, with the highest diagnostic power, were\u000a      selected and further identified as acute phase proteins, C-Reactive\u000a      Protein and Haptoglobin. Data mining methods were then applied to the\u000a      collected data for the development of our online computer-assisted\u000a      veterinary diagnostic tool.\u000a    After testing of more than 2,000,000 versions of the combinations of the\u000a      known and original data mining approaches, the best solutions were found.\u000a      It is tested on the clinical data of several veterinary clinics worldwide.\u000a      The generated software is a tool for diagnostic, monitoring and screening.\u000a      Initially, the diagnosis of lymphoma was formulated as a classification\u000a      problem and then later refined as a lymphoma risk estimation. Three\u000a      classical methods, decision trees, advanced kNN and probability density\u000a      evaluation, were used in combinations with original approaches for\u000a      classification and risk estimation and several pre-processing approaches\u000a      were implemented to create the diagnostic system.\u000a    For the differential diagnosis the best solution gave a sensitivity and\u000a      specificity of 83.5% and 77%, respectively (using three input features,\u000a      CRP, Haptoglobin and standard clinical symptom). For the screening task,\u000a      the decision tree method provided the best result, with sensitivity and\u000a      specificity of 81.4% and &gt;99%, respectively (using the same input\u000a      features). Furthermore, the development and application of new techniques\u000a      for the generation of risk maps allowed the visualisation of risk maps in\u000a      a more user-friendly manner.\u000a    This is a potentially useful tool for explanatory data analysis and\u000a      testing other theoretical input features in the final diagnosis. The risk\u000a      maps provide early diagnosis of lymphoma return several weeks before the\u000a      clinical symptoms are developed. In this monitoring lymphoma return the\u000a      risk maps perform significantly better than most of the veterinary\u000a      practitioners. The generated lymphoma software (JAVA) has the potential of\u000a      being web-accessible.\u000a    In a letter to the Vice-Chancellor of the University of Leicester from\u000a      [text removed for publication] reports \"The new treatment monitoring test\u000a      has the potential to add a further [text removed for publication] to our\u000a      projected turnover. It has also bought forward the collaboration with the\u000a      largest veterinary corporation in the UK who were specifically interested\u000a      in the treatment monitoring application of our test. They are now planning\u000a      to launch the new test developed with University of Leicester which will\u000a      have an immediate impact on both our short and long term revenues\" [5.1].\u000a    In short &#8212; this system is significantly changing veterinary practice in\u000a      the UK.\u000a    ","ImpactSummary":"\u000a    Advanced technologies for data visualisation and data mining, developed\u000a      in the Unit in collaboration with national and international teams, are\u000a      widely applied for development of medical services. In particular, a\u000a      system for canine lymphoma diagnosis and monitoring developed with [text\u000a      removed for publication] has now been successfully tested using clinical\u000a      data from several veterinary clinics. The risk maps produced by our\u000a      technology provide early diagnosis of lymphoma several weeks before the\u000a      clinical symptoms develop. [text removed for publication] has estimated\u000a      the treatment test, named [text removed for publication], developed with\u000a      the Unit to add [text removed for publication] to the value of their\u000a      business. Institute Curie (Paris), applies this data mapping technique and\u000a      the software that has been developed jointly with Leicester in clinical\u000a      projects.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Leicester\u000a    ","Institutions":[{"AlternativeName":"Leicester (University of)","InstitutionName":"University of Leicester","PeerGroup":"A","Region":"East Midlands","UKPRN":10007796}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Publications\u000a    \u000a(1) A.N. Gorban, A. Zinovyev, Principal manifolds and graphs in practice:\u000a      from molecular biology to dynamical systems, International Journal of\u000a        Neural Systems 20 (3) (2010), 219-232.\u000a    \u000a\u000a(2) A.N. Gorban, A. Y. Zinovyev, Principal Graphs and Manifolds, Chapter\u000a      2 in: Handbook of Research on Machine Learning Applications and\u000a        Trends: Algorithms, Methods, and Techniques, Emilio Soria Olivas et\u000a      al. (eds), IGI Global, Hershey, PA, USA, 2009, pp. 28-59.\u000a    \u000a\u000a(3) A.N. Gorban, N.R. Sumner, and A.Y. Zinovyev, Topological grammars for\u000a      data approximation, Applied Mathematics Letters, 20 (4) (2007),\u000a      382-386.\u000a    \u000a\u000a(4) A. Zinovyev, E. Mirkes, Data complexity measured by principal graphs,\u000a      Computers &amp; Mathematics with Applications, Volume 65, Number\u000a      10, 1471-1482.\u000a    \u000a\u000a(5) A.N. Gorban, B. Kegl, D. Wunsch, A. Zinovyev (Eds.), Principal\u000a        Manifolds for Data Visualisation and Dimension Reduction, Lecture\u000a      Notes in Computational Science and Engineering, Vol. 58, Springer, Berlin\u000a      &#8212; Heidelberg &#8212; New York, 2008. (ISBN 978-3-540-73749-0).\u000a    \u000a\u000a(6) A. Gorban, A. Zinovyev, Elastic Principal Graphs and Manifolds and\u000a      their Practical Applications, Computing 75 (2005), 359-379.\u000a    \u000a\u000a(7) Furness\u000a        PN, Levesley\u000a        J, Luo\u000a        Z, Taub\u000a        N, Kazi\u000a        JI, Bates\u000a        WD, Nicholson\u000a        ML., A neural network approach to the biopsy diagnosis of early\u000a      acute renal transplant rejection, Histopathology, Volume 35\u000a      (1999), 461-467.\u000a    \u000aGrant\u000a    Data Mining for Lymphoma Differential Diagnosis, A University of\u000a      Leicester Innovation Partnership with [text removed for publication],\u000a      2012. European Regional Development Fund.\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000a    \u000a      Factual statement by [text removed for publication]\u000a      Factual statement from Director of U900 Institut Curie and references\u000a        to the clinical projects.\u000a      E. M. Mirkes, I. Alexandrakis, K. Slater, R. Tuli, A. N. Gorban,\u000a        Computational Diagnosis of Canine Lymphoma, Presented at the conference\u000a        IC-MSQUARE 2013, Prague September 2013 (Short version is published in\u000a        the Book of Abstracts IC-MSQUARE 2013), Accepted for publication in\u000a        IC-MSQUARE 2013 Proceedings (IOP Conference series), extended version is\u000a        invited to the Special Issue of Physics in Medicine and Biology.\u000a        Preprint version is published in arXiv: arXiv:1305.4942 [q-bio.QM]\u000a      Canine lymphoma blood tests &#8212; results explained, [text removed for\u000a        publication], internal publication.\u000a      Guidance notes for [text removed for publication], the canine lymphoma\u000a        blood test system.\u000a      Ivchenko V.K., Galchenko V.Ya., Ivchenko A.V.: Part I: Prediction of\u000a        treatment result of long bones fracture for diabetes patients by means\u000a        of intellectual and statistical data analysis. Part I. Visual data\u000a        mining for multidimensional data, Ukrainian Medical Almanac , 2013, Vol.\u000a        16, Iss. 2 (Supplement), pp. 4-7; Part II. Production of prognostic\u000a        classification rules, Ukrainian Medical Almanac , 2013, Vol. 16, Iss. 2\u000a        (Supplement), pp. 8-11; Part III. Analysis of efficiency of produced\u000a        prognostic classification rules, Ukrainian Medical Almanac , 2013, Vol.\u000a        16, Iss. 2 (Supplement), pp. 12-15.\u000a      [text removed for publication]\u000a      B&#233;cavin C, Benecke A. New dimensionality reduction methods for the\u000a        representation of high dimensional 'omics' data. Expert Rev Mol\u000a          Diagn. 11(1) (2011), 27-34.\u000a    \u000a    ","Title":"\u000a    Data maps with applications to medical diagnostics and monitoring\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The problems related to large data set analysis and visualisation,\u000a        model reduction and the struggle with complexity of data sets are\u000a      important for many areas of human activity. The identification of hidden\u000a      geometry and topology in noisy data sets is a challenging task. Many\u000a      branches of data analysis aim to solve such problems under some additional\u000a      assumptions that simplify the problem. However, the verification of these\u000a      assumptions may be more complicated than the solutions of the problems. A\u000a      universal technology for uncovering the hidden structure is very\u000a      desirable. An answer to this challenge cannot be simple because it must\u000a      potentially cover the majority of situations.\u000a    In the 1990s Levesley and Light produced theoretical results concerning\u000a      the approximation power of neural networks. This work led to Levesley's\u000a      involvement in a simple neural network model for the prediction of acute\u000a      rejection of kidney transplants together with pathologists from the\u000a      University of Leicester [3.7]. The Unit recognised the potential\u000a      for impact of research in this area, leading to the development of a team\u000a      under the leadership of Gorban with more specific expertise in the theory\u000a      and practical application of neural networks.\u000a    In summary, we have developed a universal technology for revealing and\u000a      visualising the hidden structure in data. For this purpose, we have used\u000a      ideas both old and new:\u000a    \u000a      The oldest of them is the idea of self-consistency introduced by H.\u000a        Steinhaus in 1957 (k-means) and then recognized as a very general and\u000a        productive idea that can be used for construction of many principal\u000a        objects like principal manifolds and principal graphs (Husty at al,\u000a        1984). This idea is an intrinsic part of the self-organizing maps (SOM)\u000a        and many data approximation approaches also.\u000a      The application of quadratic elastic energy functionals is a basic\u000a        idea in spline approximation and is used by us for construction of\u000a        principal manifolds, in the elastic maps technology [3.6].\u000a      Gorban and Zinovyev (Curie) developed the topological grammars\u000a        approach for data analysis [3.3] based on the idea of graph\u000a        grammars.\u000a      We use the pluriharmonic embeddings of graphs into data space as the\u000a        ideal approximators [3.5] and developed optimization methods to\u000a        minimize the deviation of data approximants from the pluriharmonic\u000a        graphs.\u000a      The idea of robust growth makes the whole approach more efficient. For\u000a        the organization of robust grows, we use truncated energy functionals.\u000a        In the splitting algorithms of optimization they also produce systems of\u000a        linear equations, and make the construction of the approximators much\u000a        more stable in presence of noise and outliers.\u000a    \u000a    Most of the ideas are implemented in user-friendly software and can be\u000a      applied to many real-life problems.\u000a    For the development of applied systems we combine our original technology\u000a      with more classical approaches like decision trees, advanced kNN method\u000a      and Bayesian networks. For example, for the canine lymphoma diagnosis we\u000a      have tested more than 2,000,000 versions of combinations of known and our\u000a      novel data mining approaches, and the best solutions have been implemented\u000a      in JAVA (web-accessible) software. It is shown that for the differential\u000a      diagnosis of clinically vulnerable patients, the sensitivity (proportion\u000a      of correct prediction of positive results) of the system is 83.5%, and\u000a      specificity (proportion of correct prediction of negative results) is 77%.\u000a      For caninelymphoma screening purposes, the best data mining solution we\u000a      found has sensitivity 81.4% and specificity &gt;99%.\u000a    On base of case-study, which has been done, the best solution for each\u000a      problem has been selected. The results obtained from case-study are\u000a      extremely favourable compared to many current human cancer screening tests\u000a      that rely upon single biomarkers. These include the current CA-125 screen\u000a      for human ovarian cancer (sensitivity approximately 50% and specificity\u000a      98% [3.1]) and the male PSA test (sensitivity approximately 65% and\u000a      specificity 35% [3.2]).\u000a    "},{"CaseStudyId":"41583","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2017370","Name":"Russia"}],"Funders":[],"ImpactDetails":"\u000a    The research has impacted on the provision of healthcare in the Far North\u000a      of Russia, involving thousands of patients. This is the main source of\u000a      impact. However, the research has also received media attention [5.4]\u000a      and [5.5] in explaining how economic stress leads to financial\u000a      crisis [3.3].\u000a    Healthcare\u000a      The provision of healthcare to the people of the Far North is complex. The\u000a      health of the population is influenced by many factors, including the\u000a      physical environment, climate, genetics, health-related behaviours, and\u000a      living conditions. Healthcare provision in the Far North faces serious and\u000a      unique challenges, and has led to calls within Russia to recognise a\u000a      distinct branch of science called Arctic Medicine, in the same way that\u000a      Tropical Medicine is globally recognised. Identification of this need for\u000a      specialist healthcare provision, further fuelled by plans for intensive\u000a      industrial development in the Far North, has led to the establishment of\u000a      the Scientific Research Institute of Medical Problems of the North.\u000a    The official mission of the Institute is the \"preservation and\u000a      development of the health, longevity and active life of the human\u000a      population of the Far North and Siberia''. This institute not only has a\u000a      research function but also a huge clinical mission, It has been directly\u000a      involved in a number of activities, based on the work of the Leicester\u000a      team, discussed below, which have improved the health and well-being of\u000a      inhabitants of the Far North. To fulfil its mission, the Institute carries\u000a      out research, provides medical treatment and organises expeditions to the\u000a      region to monitor the health of the population, consult with practitioners\u000a      and prepare recommendations for local authorities and medical service\u000a      providers.\u000a    The Unit's analysis of dynamical models of adaptation research has been\u000a      used by the Institute to reveal the mechanisms of human adaptation to the\u000a      harsh living conditions in the Far North.\u000a    The director of the Institute, VT Manchuk, one of the most prominent\u000a      Russian experts in polar medicine, has testified to the value of the\u000a      research findings in guiding the organisation's expeditions and monitoring\u000a      work. In a letter, he stated that: \"the developed methods of adaptation\u000a      monitoring and control are now widely used in the practice of health\u000a      monitoring in the Russian Far North. Many groups and regions and thousands\u000a      of people have been monitored with the help of this technique.'' [5.1]\u000a    Professor Manchuk has also provided several specific examples of the ways\u000a      in which the research had been applied. These include:\u000a    \u000a      A study of 4,770 inhabitants of Evenkia (a district of Krasnoyarsk)\u000a        including 1,248 natives and 3,522 migrants showed the difference in\u000a        response to various allergic disorders. The results have been used in\u000a        recommendations to the local authorities concerning treatment of the\u000a        different groups of people [5.2].\u000a      A study of 305 Eastern Siberians between the ages of 15 and 79 of the\u000a        interaction between intracellular pathogens with allergic diseases.\u000a        Adaptometric methodology developed in Leicester allowed for estimation\u000a        of the strength of the immune system to such diseases, and diagnosis of\u000a        the adaptation of the system at the cellular level [5.3].\u000a      The monitoring of gastric secretory function in children living in\u000a        northern conditions (500 patients) which led to treatment and\u000a        recommendations for differentiated treatment according to ethnicity and\u000a        diet [5.1].\u000a      Development of differentiated interventions for different ethnic\u000a        groups of the Far North, who demonstrate different adaptation response\u000a        to environmental change (pollution, global warming, translocation). A\u000a        study of 388 patients using adaptometry identified the different factors\u000a        which are most harmful to each group.\u000a    \u000a    Not only has the research from Leicester allowed for specific\u000a      intervention such as detailed above, but it has also led to a systematic\u000a      approach to the monitoring of patients: (i) identification of groups who\u000a      need treatment; (ii) identification of high risk groups; (iii) evaluation\u000a      of new treatments and interventions. This more systematic approach has\u000a      been applied across many thousands of patients in addition to the cases\u000a      mentioned above.\u000a    Manchuk states in his letter to the University that: \"there are many\u000a      other examples of applications involving in total thousands of patients\u000a      for monitoring, consultation and treatment. The methods for adaptation\u000a      modelling, monitoring and control developed by Professor Gorban and his\u000a      team already have valuable impact on the health monitoring and management\u000a      in Russian Far North and Siberia\". [5.1]\u000a    ","ImpactSummary":"\u000a    Russia's Far North region, located mainly north of the Arctic Circle, is\u000a      home to around seven million people. Living conditions are harsh and a\u000a      combination of extreme climate, genetics, diet and behaviour mean\u000a      delivering healthcare has multiple challenges [3.7]. Mathematical\u000a      methods developed within the Unit have been used to monitor the health of\u000a      the population of Russia's Far North, and thereby enabled Government\u000a      bodies to improve the planning and provision of healthcare, resulting in\u000a      increased well-being within the population, and efficiency gains for the\u000a      administration.\u000a    ","ImpactType":"Health","Institution":"\u000a    University of Leicester\u000a    ","Institutions":[{"AlternativeName":"Leicester (University of)","InstitutionName":"University of Leicester","PeerGroup":"A","Region":"East Midlands","UKPRN":10007796}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"1502020","Name":"Krasnoyarskiy Kray"}],"References":"\u000a    Grant\u000a    \"Development of Systems of Optimal Control of Adaptation by Controllable\u000a      Crises\", the Russian Federal Ministry of Education and Science, Russian\u000a      Federal Program \"Scientific and scientific-pedagogical personnel of\u000a      innovative Russia\", State Contract # 02.740.11.5086, Oct 2009 &#8212; Sept 2011.\u000a      This grant was awarded for research and development under the supervision\u000a      of foreign scientists. The foreign supervisor and PI of this project was\u000a      Prof. Gorban. Duration of the project: 2009-2010. The value was 2,000,000\u000a      RUR.\u000a    Publications\u000a    \u000a1. A.N. Gorban, E.V. Smirnova, T.A. Tyukina, Correlations, risk and\u000a      crisis: From physiology to finance, Physica A, Vol. 389, Issue 16, 2010,\u000a      3193-3217. DOI: 10.1016\/j.physa.2010.03.035\u000a    \u000a\u000a2. I. Tyukin. Adaptation in Dynamical Systems, Cambridge University\u000a      Press, 2011. ISBN:9780521198196\u000a    \u000a\u000a3. A.N. Gorban, E.V. Smirnova, T.A. Tyukina, General Laws of Adaptation\u000a      to Environmental Factors: from Ecological Stress to Financial Crisis.\u000a      Math. Model. Nat. Phenom. Vol. 4, No. 6, 2009, 1-53. DOI:\u000a      10.1051\/mmnp\/20094601\u000a    \u000a\u000a4. A.N. Gorban, L.I. Pokidysheva, E,V. Smirnova, T.A. Tyukina. Law of the\u000a      Minimum Paradoxes, Bull Math Biol, 73(9) (2011), 2013-2044. DOI:\u000a      10.1007\/s11538-010-9597-1\u000a    \u000a\u000a5. V. N. Razzhevaikin, M. I. Shpitonkov, The model of correlation\u000a      adaptometry and its use for estimation of obesity treatment efficiency,\u000a      Russian Journal of Numerical Analysis and Mathematical Modelling 26 (6),\u000a      565-574 (2011), DOI: 10.1515\/rjnamm.2011.033\u000a    \u000a\u000a6. L. Pokidysheva, I. Ignatova, Principal Component Analysis Used in\u000a      Estimation of Human's Immune System, Suffered from Allergic\u000a      Rhinosinusopathy Complicated with Clamidiosis or without it, Advances in\u000a      Intelligent Analysis of Medical Data and Decision Support Systems, Studies\u000a      in Computational Intelligence, Vol. 473, Springer 2013, pp. 147-156.\u000a    \u000a\u000a7. Is Arctic medicine a distinct science? A Russian perspective, Dimitrii\u000a      G. Tikhonov, Int J Circumpolar Health 2013, 72: 21248 - http:\/\/dx.doi.org\/10.3402\/ijch.v72i0.21248\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"11","Level2":"9","Subject":"Neurosciences"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"6","Level2":"4","Subject":"Genetics"}],"Sources":"\u000a    \u000a      Factual statement from Member of Russian Academy of Medical Sciences,\u000a        Director of the State Research Institute for Medical Problems of\u000a        Northern Regions, Krasnoyarsk, Russia.\u000a      I.A. Ignatova, S.V. Smirnova, L.I. Pokidysheva. Allergic\u000a          Rhinosinusopathy in Inhabitants of Siberia. System Analysis,\u000a        Germany: Lambert Academic Publishing, 2012, 168 pp. ISBN: 978\u000a        -3-659-17861-0.\u000a      L. Pokidysheva, I. Ignatova, Principal Component Analysis Used in\u000a        Estimation of Human's Immune System, Suffered from Allergic\u000a        Rhinosinusopathy Complicated with Clamidiosis or without it, Advances in\u000a        Intelligent Analysis of Medical Data and Decision Support Systems,\u000a        Studies in Computational Intelligence, Vol. 473, Springer 2013, pp.\u000a        147-156.\u000a      ScienceDaily: Plants and Animals Under Stress May Provide the Key to\u000a        Better Stock Market Predications (Nov. 3, 2010), http:\/\/www.sciencedaily.com\/releases\/2010\/11\/101103082312.htm.\u000a      NewsRoom America: Stressed Plants And Animals May Help Predict Stock\u000a        Market\u000a        http:\/\/www.newsroomamerica.com\/story\/71963\/stressed_plants_and_animals_may_help_predict_stock_market.html.\u000a    \u000a    ","Title":"\u000a    Efficient planning of healthcare for people living in Russia's Far North\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The Department of Mathematics at the University of Leicester has a strong\u000a      research group in analysis of dynamical models of adaptation [3.1,\u000a        3.2]. The group is led by Professor Alexander N Gorban and Dr Ivan\u000a      Tyukin, both of whom have been in Leicester throughout the REF period, and\u000a      includes research assistants (Tyukina, Mirkes, and Penkova) and PhD\u000a      students.\u000a    The group has created a system of dynamic models of physiological\u000a      adaptation based on the quantitative formalisation of endocrinologist Hans\u000a      Selye's ideas about the general adaptation syndrome and adaptation\u000a      resources. Studying how systems facing stress react in terms of becoming\u000a      more interdependent and volatile reveals patterns that help to predict\u000a      when a crisis may occur and the likelihood of death or recovery. A key\u000a      finding is that, as the crisis approaches, systems become more dependent\u000a      on each other but at the same time more likely to react differently.\u000a    Research in this area began in 1987 and explored critical conditions for\u000a      development of babies in the first week of life, and the work of the Unit\u000a      in Leicester has built on this early work. Criteria for early evaluation\u000a      of the risk of such crises are developed on the base of these dynamic\u000a      models [3.1, 3.3,3.4]. These criteria are based on\u000a      the analysis of the dynamics of correlations between physiological\u000a      attributes in ensembles of similar systems.\u000a    In 2009, Gorban and Tyukina, working with Professor Elena Smirnova from\u000a      the Siberian Federal University, Russia, published [3.3] which\u000a      studied ensembles of similar systems under load of environmental factors.\u000a      It showed that, typically, when the load increases above some threshold,\u000a      then the adapting systems become more different (variance increases), but\u000a      the correlation increases too. If the stress continues to increase then\u000a      the second threshold appears: the correlation achieves maximal value, and\u000a      start to decrease, but the variance continue to increase. It proposed\u000a      that, in many applications, this second threshold is a signal of the\u000a      approach of a fatal outcome.2029This effect was supported by experiments\u000a      and observation of groups of humans, mice, trees, grassy plants, and on\u000a      financial time series.\u000a    In 2010 the same academic team published [3.1], in which a general\u000a      approach to the explanation of the effect through dynamics of individual\u000a      adaptation of similar non-interactive individuals to a similar system of\u000a      external factors was developed. Qualitatively, this approach followed\u000a      Selye's idea concerning adaptation energy.\u000a    In 2011, the same researchers, joined by L.I. Pokidysheva, Professor of\u000a      Computer Science, Siberian Federal University, Russia, published [3.4]\u000a      which exploits, criticizes and develops further the concept of the \"Law of\u000a      the Minimum\". This concept, originally applied to plant or crop growth\u000a      (Justus von Liebig, 1840) states that growth is controlled by the scarcest\u000a      resource (limiting factor). Violations of this law in natural and\u000a      experimental ecosystems were also reported.\u000a    Leicester's research studied models of adaptation in ensembles of similar\u000a      organisms under the load of environmental factors and proved that\u000a      violation of Liebig's law follows from adaptation effects. If the fitness\u000a      of an organism in a fixed environment satisfies the Law of the Minimum\u000a      then adaptation equalizes the pressure of essential factors and,\u000a      therefore, acts against Liebig's law. This is the Law of the Minimum\u000a      paradox: if for a randomly chosen pair \"organism-environment\" the Law of\u000a      the Minimum typically holds, then in a well-adapted system, we have to\u000a      expect violations of this law.\u000a    For the opposite interaction of factors (a synergistic system of factors\u000a      which amplify each other), adaptation leads from factor equivalence to\u000a      limitations by a smaller number of factors. Some other typical forms of\u000a      organization of the system of factors are studied. The most important of\u000a      them is synergetic interaction of factors and combinations of synergetic\u000a      and Liebig's systems.\u000a    For analysis of adaptation, the research developed a system of mean-field\u000a      multi-agent models of adaptation based on Selye's idea of the universal\u000a      adaptation resource (adaptation energy). These models predict that under\u000a      the load of an environmental factor a population separates into two groups\u000a      (phases): a less correlated, well adapted group and a highly correlated\u000a      group with a larger variance of attributes, which experiences problems\u000a      with adaptation. Some empirical data were presented and evidences of\u000a      interdisciplinary applications to econometrics were discussed. These\u000a      models and criteria are intensively used in medical applications [3.5,\u000a        3.6] and in financial econometric studies of crises [3.3].\u000a    "},{"CaseStudyId":"42094","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"690791","Name":"Ukraine"}],"Funders":[],"ImpactDetails":"\u000a    The research on PR and lattices by L. Slepyan and Mishuris has been\u000a      conducted in close\u000a      collaboration with V. Slepyan, Chief Designer at the Loginov and Partner\u000a      Mining Company\u000a      (LPMC), where a vibrating screen was designed and built based on the idea\u000a      of PR.\u000a    LPMC now makes and sells equipment for size-sorting based on PR for field\u000a      testing, currently only\u000a      within the Ukraine; the equipment is known as GEPARD. They are popular\u000a      because they are\u000a      effective at separating wet granular material: in a traditional\u000a      vibrating lattice, the dissipation is\u000a      highest at the centre, and the material gathers there, but with PR not\u000a      only is the amplitude larger\u000a      for a given amount of input energy, but the dissipation is better\u000a      distributed so material does not\u000a      clump. This is expected to give LPMC a distinct market advantage.\u000a    At first, the screens broke too frequently (roughly every few hours), due\u000a      to cracks developing both\u000a      longitudinally and around the edges. As the Director General of LPMC\u000a      writes: \"Close\u000a      communication of V.I. Slepyan with Professors Leonid Slepyan and Gennady\u000a      Mishuris\u000a      (Aberystwith University) and the experiments carried out by the Company\u000a      resulted in better\u000a      understanding of the screening machine operation process and developing\u000a      solutions to increase\u000a      the fatigue strength of the screen and stabilize the parametric resonance.\u000a      As a result of the\u000a      solutions, the screening machine service life has been increased\". By the\u000a      end of July 2013, the\u000a      operating time for the vibrating screen had been increased to a period of\u000a      several days.\u000a    The key improvements due to the mathematical modelling are:\u000a    \u000a      a more careful delineation of the parameter space in which the stable\u000a        PR modes are to be\u000a        found;\u000a      a suggestion to change the way in which the screen boundary is clamped\u000a        at its edges, so\u000a        as to eliminate edge cracks in the lattice;\u000a      changing the lattice structure of the screens, to have a mesh with\u000a        elliptical or circular holes,\u000a        and a distribution of hole sizes which decrease towards the edges of the\u000a        screen;\u000a      most significantly, inserting a small gap between the screen and the\u000a        exciter motor is\u000a        remarkably effective in preventing cracking of the lattice.\u000a    \u000a    Each screen costs between one and eight-hundred dollars, depending on its\u000a      country of\u000a      manufacture and the quality of the material. The whole vibrating machine\u000a      sells for only a few\u000a      thousand dollars. Thus the screens represent a significant proportion of\u000a      the running costs. The\u000a      result of the modelling &#8212; that is, the reduction in the frequency at which\u000a      the screens fail due to\u000a      cracking &#8212; is that there is less manual intervention required by the\u000a      operators, and a reduction in the\u000a      amount spent on replacement screens, currently by a factor of about four.\u000a    To increase the longevity of the screens, they are often used below\u000a      maximum efficiency, i.e.\u000a      outside the optimal PR regime. The industry standard is that a screen\u000a      should last for about 10\u000a      million cycles, roughly two weeks, and this is now the goal for modelling\u000a      and development of\u000a      refinements to the equipment.\u000a    Mathematical models developed for the oscillations and waves in lattice\u000a      structures and the\u000a      determination parameter space of the regular PR oscillations resulted in\u000a      the improvement of the\u000a      machine and opened a way for other PR applications.\u000a    ","ImpactSummary":"\u000a    A novel application of parametric resonance (PR) is described, which has\u000a      improved the\u000a      effectiveness of a vibrating screen used for size-sorting of crushed rock.\u000a      These improvements\u000a      have had an economic impact on the Ukrainian company that makes the\u000a      screens: the mathematics\u000a      developed in Aberystwyth permits a stable, high amplitude PR-regime to be\u000a      found, reducing the\u000a      damage to the screen mesh and increasing its longevity. This new\u000a      technology is allowing the\u000a      company to reduce costs and equipment downtime and is enabling them to\u000a      gain a market\u000a      advantage by being able to sort wetter materials than previously.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Aberystwyth University\u000a    ","Institutions":[{"AlternativeName":"Aberystwyth University","InstitutionName":"Aberystwyth University","PeerGroup":"B","Region":"Wales","UKPRN":10007856}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[3.1] V.I. Slepyan, I.G. Loginov and L.I. Slepyan, The method of\u000a      resonance excitation of a vibrating\u000a      sieve and the vibrating screen for its implementation. Ukrainian patent on\u000a      invention No. 87369,\u000a      2009.\u000a    \u000a\u000a[3.2] Slepyan, L.I., Mishuris, G.S., Movchan, A.B. (2010) Crack in a\u000a      lattice waveguide. Int. J. Fract.,\u000a      162, 91-106.\u000a      DOI: 10.1007\/s10704-009-9389-5\u000a    \u000a\u000a[3.3] Mishuris, G; Movchan, A; Slepyan, L. (2009) Localised knife waves\u000a      in a structured interface.\u000a      Journal of the Mechanics and Physics of Solids, 57, 1958-1979.\u000a      DOI: 10.1016\/j.jmps.2009.08.004\u000a      REF2 Submitted.\u000a    \u000a\u000a[3.4] Slepyan, L.I., and Slepyan, V.I., (2013) Modeling of parametrically\u000a      excited vibrating screen. J.\u000a      Phys.: Conf. Ser. 451, 012026.\u000a      DOI: 10.1088\/1742-6596\/451\/1\/012026\u000a    \u000a[3.5] PARM-2, Vibro-impact machines based on parametric resonance:\u000a      concepts, mathematical\u000a      modelling, experimental verification and implementation,\u000a      PIAP-GA-2012-284544-PARM2, 1.8m\u000a      euro, 01\/01\/2012 - 31\/12\/2015.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"5","Subject":"Mathematical Physics"}],"Sources":"\u000a    [5.1] Letter and contact details for the Director General of LPMC.\u000a    \u000a    ","Title":"\u000a    Improved parametric resonance of a vibrating screen\u000a    ","UKLocation":[{"GeoNamesId":"2657782","Name":"Aberystwyth"}],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Vibrating screens are used to separate crushed stone into different\u000a      sizes, for example in coal and\u000a      ore mining and various chemical industries. In the past, parametric\u000a      resonance has been mainly\u000a      considered as an undesirable phenomenon, but in 2007, L. Slepyan and V.\u000a      Slepyan realized that a\u000a      PR-based vibrating screen could eliminate many of the drawbacks of\u000a      existing screens. A PR-based\u000a      screen compares favourably with conventional size-sorting machines, in\u000a      which transverse\u000a      oscillations are excited directly, because it has not only a larger\u000a      amplitude of vibration, but also\u000a      insensitivity to dissipation over a rather wide range. During L. Slepyan's\u000a      visit to the UK in 2007, he\u000a      worked with Movchan and Mishuris to start an intensive programme of\u000a      research on the topic, using\u000a      mathematical models of oscillations and waves in lattice structures.\u000a    In 2009, a patent was issued to the Slepyans on the excitation method and\u000a      structure of the screen\u000a      [3.1]. At that time, the nonlinear dynamics of such a machine had been\u000a      numerically simulated and\u000a      the first PR-based screen was built in Loginov and Partner Mining Company\u000a      (Kiev, Ukraine). L.\u000a      Slepyan subsequently visited Aberystwyth in 2010-12, and is now employed\u000a      here. He and Mishuris\u000a      have been actively working together within the EU FP7 IAPP project PARM-2\u000a      [3.5], coordinated by\u000a      Mishuris, to improve the screen and develop related theory [3.2,3.3].\u000a    The stable operation of a PR-based machine for the grading of granular\u000a      materials requires careful\u000a      design and set-up, which can be achieved on the basis of the mathematical\u000a      analysis of its\u000a      dynamics. Numerical simulations allow for the refinement of the domains of\u000a      optimal parameters,\u000a      where the parametric oscillations are excited and where the\u000a      analytically-obtained steady oscillation\u000a      regimes are stable. The models developed are also of use in other PR\u000a      applications, although this is\u000a      probably the largest machine to use PR at this time.\u000a    The modelling problem is described by a system of two coupled nonlinear\u000a      equations [3.4], which\u000a      permit an exact solution in the case that there is no damping associated\u000a      with the transverse\u000a      oscillations. In the fully nonlinear regime, harmonic analysis is used to\u000a      give explicit expressions for\u000a      the amplitudes of longitudinal and transverse oscillations as functions of\u000a      the external force\u000a      amplitude and frequency. It is remarkable that in the case of the resonant\u000a      excitation, where the\u000a      external force frequency coincides with the frequency of the free\u000a      longitudinal oscillations, the\u000a      amplitudes are independent of the viscosity of the granular material. In\u000a      addition, lattice waves and\u000a      pre-stress were studied, to induce particle fracture off-lattice.\u000a    The determination of the boundaries of the PR domain in the\u000a      frequency-amplitude plane is based\u000a      on the linear formulation. The PR arises in the nonlinear problem in\u000a      almost the same frequency\u000a      region predicted by the linear analysis, slightly shifted towards the\u000a      higher frequencies. The\u000a      transverse oscillations, both regular and irregular, abruptly decay on the\u000a      boundaries of the PR\u000a      region.\u000a    "},{"CaseStudyId":"42095","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Aberystwyth researchers Cox and Davies have developed a number of talks\u000a      and demonstrations, aimed mainly at children of secondary school age, that\u000a      describe soap bubble geometry and the mathematics of minimization. These\u000a      have been given under various names, including \"Show me the (shortest) way\u000a      to go home\", \"Mathematics of Soap Bubbles\" and \"Bubble Magic\". The\u000a      material is based upon\u000a    (i) demonstrations of Plateau's laws in wire frames, including the\u000a      hysteretic transition between different minima;\u000a    (ii) seeking least area arrangements of soap films, comparing the\u000a      conjectured solutions in [3] with children's intuition;\u000a    (iii) a discussion of soap-film solutions of Steiner-like problems in the\u000a      plane (for example the shortest road network joining 3 cities, 4 cities,\u000a      and 5 towns\/cities in Wales) and local minima in complicated energy\u000a      landscapes. Here numerical solutions are used to indicate the relevance of\u000a      constraints (mountains!).\u000a    Interaction with participants is increased by offering the opportunity\u000a      for attendees to do experiments themselves, and by running a quiz on\u000a      Plateau's laws, with prizes of book tokens.\u000a    Exemplars of this activity include:\u000a    - Cox gave an invited evening talk for the Lancashire and North-west\u000a      Branch of the Institute of Mathematics and its Applications and the\u000a      Lancashire and Cumbria Branch of the Institute of Physics, hosted by the\u000a      University of Central Lancashire, in December 2010. The audience was a\u000a      mixture of academics and members of the public, including A-level pupils\u000a      [5.1].\u000a    - Cox brought Frank Morgan (Williams College, US) to the UK to give a\u000a      talk \"Soap Bubbles and Mathematics\" associated with an ICMS workshop\u000a      [5.2], and publicised on his Huffington post Blog [5.3]. The event was\u000a      held at Dynamic Earth in Edinburgh, attended by about sixty 14-15 year\u000a      olds [5.4]. He used Cox's research results [3.2] in the talk. The children\u000a      also tried out experiments for themselves (from the IMA's Big Box). A\u000a      report and photograph were used as an exemplar of ICMS outreach in the LMS\u000a      newsletter [5.5], and feedback from teachers and pupils included \"the\u000a      speaker was excellent; enthusiastic, sense of humour, involved the\u000a      audience\" and \"Showing the applications of maths is incredibly useful\"\u000a      [5.6].\u000a    - Davies talks and gives demonstrations at the annual Welsh\u000a      \"Eisteddfod\"s, both for young people (the Urdd Eisteddfod) and adults (the\u000a      National Eisteddfod), which attract over 10,000 people every year. In 2012\u000a      his work was publicised by amgylchedd.com [5.7] and, in May 2013, 55\u000a      children completed a questionnaire which indicated their increased grasp\u000a      of the material.\u000a    - School visits to Aberystwyth to hear soap film talks have covered much\u000a      of the country, including Llanfyllin (2009) and Bro Ddyfi, Machynlleth\u000a      (2013). The feedback from Bro Ddyfi included the following comment on\u000a      Twitter: \"Thanks for having us, the presentations were great, very\u000a      interesting\" [5.8] and an email from a teacher: \"The pupils enjoyed the\u000a      experience, and it has certainly enriched their understanding of\u000a      Mathematics in higher education\" [5.9].\u000a    ","ImpactSummary":"\u000a    Research on the optimal arrangements of soap bubbles and soap films has\u000a      been used as a vehicle for public engagement in mathematics. Presentations\u000a      and demonstrations have been given in both Welsh and English at various\u000a      events. These have had an impact on the awareness and interest of school\u000a      children in geometry and mathematics.\u000a    ","ImpactType":"Societal","Institution":"\u000a    Aberystwyth University\u000a    ","Institutions":[{"AlternativeName":"Aberystwyth University","InstitutionName":"Aberystwyth University","PeerGroup":"B","Region":"Wales","UKPRN":10007856}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2650225","Name":"Edinburgh"}],"References":"\u000a    \u000a[3.1] S. Hutzler, D. Weaire, S.J. Cox, A. Van der Net, and E. Janiaud\u000a      (2007) Pre-empting Plateau: the nature of topological transitions in foam.\u000a      Europhys. Lett. 77: 28002.\u000a      DOI: 10.1209\/0295-5075\/77\/28002\u000a    \u000a\u000a[3.2] S.J. Cox and E. Flikkema (2010) The minimal perimeter for N\u000a      confined deformable bubbles of equal area. Elec. J. Combinatorics\u000a      17:R45.\u000a      www.combinatorics.org\/Volume_17\/v17i1toc.html\u000a      REF2 submitted to UoA9.\u000a    \u000a\u000a[3.3] S.J. Cox, F. Morgan and F. Graner (2013) Are large\u000a      perimeter-minimizing two-dimensional clusters of equal-area bubbles\u000a      hexagonal or circular? Proc. Roy. Soc A 469: 20120392.\u000a      DOI: 10.1098\/rspa.2012.0392\u000a      REF2 submitted.\u000a    \u000a\u000a[3.4] S.J Cox, and E.L Whittick (2006) Shear modulus of two-dimensional\u000a      foams: The effect of area dispersity and disorder. Euro. Phys. J. E\u000a      21:49-56.\u000a      DOI: 10.1140\/epje\/i2006-10044-x\u000a    \u000a\u000a[3.5] I.T. Davies and S.J. Cox (2010) Sedimentation of an elliptical\u000a      object in a two-dimensional foam. J. Non-Newt. Fl. Mech. 165:793-799.\u000a      DOI: 10.1016\/j.jnnfm.2010.04.005\u000a    \u000a[3.6] EPSRC Strategic Partnership (P&amp;G), EP\/F000049\/1\u000a      Characterisation, Modification and Mathematical Modelling of Sudsing,\u000a      2007-2011, &#163;189,788.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"},{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000a    [5.1] http:\/\/www.ima.org.uk\/_db\/_documents\/lancs_nw_8_dec_2010.pdf\u000a    [5.2] http:\/\/icms.org.uk\/workshops\/soapbubble\u000a    [5.3] http:\/\/www.huffingtonpost.com\/frank-morgan\/international-centre-for-mathematical-sciences_b_1368994.html\u000a    [5.4] http:\/\/sites.williams.edu\/Morgan\/2012\/03\/21\/icms-isoperimetric-problems-19-23-march-2012-edinburgh\/\u000a    [5.5] LMS Newsletter No. 415 June 2012, pg 7. http:\/\/newsletter.lms.ac.uk\/415\/415_issue.pdf\u000a    [5.6] Email from the Knowledge Transfer Officer at ICMS.\u000a    [5.7] http:\/\/www.amgylchedd.com\/2012\/07\/y-coleg-cymraeg-cenedlaethol-yn-eisteddfod-yr-urdd\/\u000a      (no longer active). [Welsh]\u000a    [5.8] https:\/\/twitter.com\/tudurdavies\u000a      [Welsh]\u000a    [5.9] Email from the numeracy coordinator at Ysgol Bro Ddyfi.\u000a    ","Title":"\u000a    Optimal geometry of soap films\u000a    ","UKLocation":[{"GeoNamesId":"2657782","Name":"Aberystwyth"}],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Soap films minimize surface energy, which is directly proportional to\u000a      surface area (or perimeter, in two dimensions), if the surface tension is\u000a      constant, as is usually the case. The usefulness of wire frames dipped in\u000a      soap solution to demonstrate fundamental concepts in minimization and\u000a      geometry, building on Plateau's work in the 19th century,\u000a      continues to this day, acting as a spur to both scientific endeavour and\u000a      public engagement. It is of particular relevance to the development of\u000a      mathematical models of the structure and dynamics of aqueous foams, which\u000a      consist of collections of soap films, of relevance to industries include\u000a      ore separation and oil recovery.\u000a    In a wire frame in the shape of a triangular prism, there is a transition\u000a      between different film structures as the length of the prism changes. The\u000a      transition is hysteretic, occurring at different lengths in extension and\u000a      compression, and Cox and co-authors were able to show that the transition\u000a      occurs \"pre-emptively\", that is, before energetic arguments suggest that\u000a      it should [3.1].\u000a    Mathematics, and in particular Plateau's rules, is also an important\u000a      feature of calculations that explore the energy landscape of\u000a      perimeter-minimizing clusters of bubbles. Similar to problems of shortest\u000a      distance (cf. Steiner networks, or the Travelling Salesman problem), soap\u000a      bubbles give rise to the following question: given N bubbles of\u000a      given areas, which arrangement has least perimeter, and hence energy. This\u000a      is a packing problem, the solution to which can indicate how best\u000a      (in the sense of least interface, and possibly least deformation) to fit\u000a      deformable objects within some given boundary. Ongoing effort in\u000a      Aberystwyth seeks to provide candidate solutions to this problem [3.2],\u000a      and to explore the conjectured solutions in the large N limit\u000a      [3.3].\u000a    The elementary geometrical rules can even be used as the basis of\u000a      predictions of the rheology of foams consisting of many bubbles. More\u000a      usually, the Aberystwyth group uses energy minimization to simulate the\u000a      structure, the static rheology (e.g. calculations of shear modulus [3.4]),\u000a      and the slow (quasi-static) flow of foams [3.5], for example in their\u000a      EPSRC\/P&amp;G-funded work [3.6].\u000a    "},{"CaseStudyId":"42262","Continent":[{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"3996063","Name":"Mexico"},{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"3572887","Name":"Bahamas"},{"GeoNamesId":"3562981","Name":"Cuba"}],"Funders":[],"ImpactDetails":"\u000a    NLOM\u000a      The collaboration with Dr Alan Wallcraft at the NRL resulted in the NRL\u000a      Layered Ocean Model (NLOM) - the world's first scalable portable ocean\u000a      model. The model was released in 1995 and was available through the NRL\u000a      [A]. The Navy Layered Ocean Model (NLOM) is a nearly global (72S to 65N)\u000a      mesoscale ocean model. It was run successfully as the US Navy's Ocean\u000a      Forecasting and Nowcasting system from 1995 until 28 Feb 2013 [A]. The\u000a      model has 6 layers in the vertical direction and during its 18 year\u000a      lifespan it increased its horizontal resolution from 1\/8 of a degree to\u000a      1\/32 of a degree. It changed hardware successfully every 3-5 years running\u000a      efficiently and interchangeably on massively parallel computers\u000a      (distributed memory - CM5, CRAY T3D\/T3E, IBM SP2, SGI Power Challenge\u000a      Array, Convex Exemplar), multi-processor shared memory computers (CRAY\u000a      YMP\/C90\/T90, SGI Power Challenge, Convex Exemplar), or scalar computers\u000a      (single processor workstations) [A, B].\u000a    NLOM is a layered model that covers regions of the ocean deeper than\u000a      200m. The complimentary Navy Coastal Ocean Model (NCOM) is maximised for\u000a      shallow seas or continental shelves and when it hits deep water is patched\u000a      onto NLOM. NLOM and NCOM are free and available software.\u000a    HYCOM\u000a      On 1\/3\/2013 NLOM was formally superceded by the Hybrid Coordinate Ocean\u000a      Model (HYCOM) [C]. Earlier versions of HYCOM had been available, in\u000a      addition to NLOM, for the previous decade. HYCOM is a sophisticated, high\u000a      resolution system for simulating ocean physics and describes the effects\u000a      of the tides, winds, earth's rotation, and many other factors on the flow\u000a      of water. HYCOM produces daily 3D snapshots of oceanographic variables\u000a      such as temperature, salinity, and current velocity and it runs daily at\u000a      the Navy DoD Supercomputing Resource Center. As a hybrid model it, unlike\u000a      NLOM, works in both deep and shallow waters.\u000a    The prediction systems produced by HYCOM are being transitioned for\u000a      operational use by the US Navy at the Naval Oceanographic Office, the US\u000a      Fleet Numerical Meteorology and Oceanography Centre and by NOAA at the\u000a      National Centers for Environmental Prediction. Improved open-ocean\u000a      nowcasts and forecasts are being applied to search and rescue operations,\u000a      shipping routes, tracking of icebergs and major pollutants, commercial\u000a      fisheries, etc [C].\u000a    Oil Spills\u000a      A major application of NLOM and HYCOM has been their use in tracking oil\u000a      spills such as the Deepwater Horizon disaster in the Gulf of Mexico\u000a      (2010), the Montara Well release in the Timor Sea (2009) and the Prestige\u000a      Oil Tanker spill in the Bay of Biscay (2002). These models provide\u000a      valuable forecasts of where the oil will be carried by local ocean\u000a      currents, which is of great assistance for deployment of clean-up vessels.\u000a    Deepwater Horizon: During the BP Deepwater Horizon disaster in\u000a      2010, the largest oil spill in US history, the Naval Oceanographic Office\u000a      (NAVO) supplied the National Oceanic and Atmospheric Administration (NOAA)\u000a      with daily trajectory forecasts. NOAA officially managed the response to\u000a      the disaster, in conjunction with BP. Beginning on 21 April 2010, and\u000a      lasting for 107 days, different forecasts were supplied to NOAO, three of\u000a      which were NLOM, NCOM and HYCOM. Forecasts for 24 hours, 48 hours, and 72\u000a      hours were produced for surface oil in the nearshore to support daily\u000a      response planning. These forecasts continued to be produced until no\u000a      recoverable oil was seen in overflights of the area for about three weeks\u000a      [D]. In mid-May, when a \"tail\" of oil was observed which created a\u000a      potential pathway for oil to be transported to the Florida Keys, Cuba, or\u000a      the Bahamas, forecasts began being produced for two regions: nearshore and\u000a      offshore. The offshore forecasts also supported daily response planning,\u000a      predicting where surface oil impacted by the Gulf current systems would be\u000a      in the next 24, 48, and 72 hours. Weeks later, when recoverable oil was no\u000a      longer observed in overflights or satellite analyses, the offshore\u000a      forecasts were phased out. The archive of the daily oil spill forecast\u000a      maps produced by NOAA [D] refer to models from NAVO\/NRL (nearshore\u000a      forecasts in May and June), NAVO\/NCOM (offshore forecasts in May and June)\u000a      and NRL\/IASNFS (offshore forecasts in May and June) [D]. The NAVO\/NRL\u000a      models included NLOM and NCOM. NRL\/IASNFS refers to the NRL Intra-Americas\u000a      Sea Ocean Nowcast\/Forecast System (IASNPS) which is a 1\/24 degree (~6 km),\u000a      41-level sigma-z ocean model based on NCOM. The open boundary conditions\u000a      for the model, which include sea surface elevation, transport,\u000a      temperature, salinity and currents, are provided by NCOM [E].\u000a    There are many examples of the use of NLOM and HYCOM in forecasting the\u000a      movement of the Deepwater Horizon oil spill. Many academic and private\u000a      research institutions partnered NOAA in the response to the spill,\u000a      contributing personnel, expertise and equipment [F]. Examples of the\u000a      extensive use of HYCOM or NLOM by these institutions are provided by The\u000a      Ocean Circulation Group (OCG) and the Optical Oceanography Laboratory at\u000a      the University of South Florida [G], and the Deepwater Horizon Oil Spill\u000a      Portal hosted by the Rutgers University Coastal Ocean Observation Lab [H].\u000a      Partners in the Rutgers University Coastal Ocean Observation Lab's\u000a      response to the Deepwater spill included several US federal agencies such\u000a      as the US Department of Homeland Security, Environmental Protection\u000a      Agency, and the US Integrated Ocean Observing System (IOOS) [I]. The\u000a      `Deepwater Horizon Oil Spill Principal Investigator Workshop Final Report'\u000a      from 2011, sponsored by the US National Science and Technology Council's\u000a      Subcommittee on Ocean Science and Technology, credits HYCOM in reference\u000a      to modelling the warm ocean current in the Gulf of Mexico [J].\u000a    Since the disaster, there have been numerous publications detailing the\u000a      events and the response, many of which credit HYCOM for hydrodynamic\u000a      modelling [K]. There have also been several books published, again\u000a      crediting HYCOM [L] or NCOM, NLOM and HYCOM [M].\u000a    Montara Well Release: The Montara oil spill was an oil and gas\u000a      leak and subsequent slick that took place in the Montara oil field in the\u000a      Timor Sea, off the northern coast of Western Australia. It is considered\u000a      one of Australia's worst oil disasters. The slick was released following a\u000a      blowout from the Montara wellhead platform in Aug 2009, and continued\u000a      leaking until early Nov 2009 (in total 74 days), when the leak was stopped\u000a      by pumping mud into the well and the wellbore cemented thus \"capping\" the\u000a      blowout. NLOM, NCOM and HYCOM were used to assess the spill [N,O].\u000a    ","ImpactSummary":"\u000a    In the 1990s Dr D Moore, who has extensive experience in fluid dynamics,\u000a      worked with collaborators at the US Naval Research Laboratory (NRL) on\u000a      parallelising an ocean modelling code. This resulted in the Navy Layered\u000a      Ocean Model (NLOM) and later the Hybrid Coordinate Ocean Model (HYCOM).\u000a      NLOM and HYCOM, which were\/are distributed through the NRL and HYCOM\u000a      consortium, are open access ocean modelling codes that are used to\u000a      forecast ocean currents. They have proved particularly impactful for the\u000a      forecasting of ocean oil spills and the corresponding management of the\u000a      environmental risk. NLOM and\/or HYCOM have been used extensively in the\u000a      Deepwater Horizon oil spill in 2010 as well as the Montara Well Release\u000a      oil spill in Australia in 2009, providing valuable forecasts to assist\u000a      with the response to the disasters.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    Imperial College London\u000a    ","Institutions":[{"AlternativeName":"Imperial College London","InstitutionName":"Imperial College London","PeerGroup":"A","Region":"London","UKPRN":10003270}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2058645","Name":"State of Western Australia"}],"References":"\u000a    (* References that best indicate quality of underpinning research)\u000a    \u000a[1] * Wallcraft, Alan J. &amp; Moore, Daniel R., \"The NRL layered\u000a        ocean model\", Parallel Computing, 23, 2227-2242 (1997). DOI.\u000a      [N.B. Author affiliation for DM is listed as Stennis Space Center but the\u000a      research was carried out while DM was employed as a Reader in\u000a      Computational Applied Mathematics at Imperial College London.]\u000a    \u000a\u000a[2] * Moore, D.R., Wallcraft, A.J., \"Formulation of the NRL Layered\u000a        Ocean Model in spherical coordinates\", NRL Contractor Report\u000a      7323-96-0005 24 pp. (1998). DOI.\u000a      [N.B. Author affiliation for DM and AW is listed as Planning Systems Inc\u000a      (who paid for DM's visits to the NRL) but the research was carried out\u000a      while DM was employed as a Reader in Computational Applied Mathematics at\u000a      Imperial College London.]\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"10","Level2":"6","Subject":"Computer Hardware"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000a    [A] NRL Global NLOM website, http:\/\/www7320.nrlssc.navy.mil\/global_nlom\/\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/8mf\u000a      on 8\/7\/13)\u000a    [B] Office of Naval Research NLOM page (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/9mf\u000a      on 8\/7\/13)\u000a    [C] HYCOM webpage, http:\/\/hycom.org\/\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/0mf\u000a      on 8\/7\/13) and http:\/\/hycom.org\/about\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/bnf\u000a      on 8\/7\/13)\u000a    [D] NOAA Deepwater Horizon archive, Oil Trajectory Maps,\u000a      http:\/\/www.noaa.gov\/deepwaterhorizon\/maps\/traj_maps.html\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/lnf\u000a      on 9\/7\/13)\u000a    [E] IASNFS webpage, http:\/\/www7320.nrlssc.navy.mil\/IASNFS_WWW\/IASNFS_intro.html\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/mnf\u000a      on 9\/7\/13)\u000a    [F] NOAA Deepwater Horizon Archive - About the response,\u000a      http:\/\/www.noaa.gov\/deepwaterhorizon\/about\/index.html\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/cnf\u000a      on 8\/7\/13)\u000a    [G] OCG Deepwater Horizon webpage, http:\/\/ocg6.marine.usf.edu\/~liu\/oil.html\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/dnf\u000a      on 8\/7\/13) and OCG HYCOM forecast\u000a      http:\/\/ocg6.marine.usf.edu\/~liu\/Drifters\/latest_gomhycom.htm\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/fnf\u000a      on 8\/7\/13)\u000a    [H] IOOS\/Rutgers University Deepwater Horizon Blog,\u000a      http:\/\/rucool.marine.rutgers.edu\/deepwater\/category\/deepwater-blog\/\u000a      (see 28, 25, 23, 22 July)\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/gnf\u000a      on 8\/7\/13) and\u000a      http:\/\/rucool.marine.rutgers.edu\/deepwater\/2010\/06\/another-amazing-hycom-forecast\/\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/hnf\u000a      on 8\/7\/13),\u000a      http:\/\/rucool.marine.rutgers.edu\/deepwater\/2010\/06\/surface-analysis-hycom-is-back\/\u000a      (archived\u000a      at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/jnf\u000a      on 8\/7\/13)\u000a    [I] IOOS\/Rutgers University Deepwater Horizon Oil Spill Portal,\u000a      http:\/\/rucool.marine.rutgers.edu\/deepwater\/\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/knf\u000a      on 8\/7\/13),\u000a    [J] Deepwater Horizon Oil Spill Principal Investigator Workshop Oct\u000a      25-26, 2011 - Final Report (available here)\u000a    [K] Selected publications using HYCOM and\/or NLOM for Deepwater Horizon\u000a      analysis: DOI-1, DOI-2,\u000a      DOI-3, PDF-1\u000a    [L] `Monitoring and Modeling the Deepwater Horizon Oil Spill: A Record\u000a      Breaking Enterprise', Yonggang Liu, Amy MacFadyen, Zhen-Gang Ji, Robert H.\u000a      Weisberg, Wiley, 2 May 2013, ISBN13 9781118671825 (available here)\u000a    [M] `Oil Spill Science and Technology', Mervin Fingas, Gulf Professional\u000a      Publishing, 3 Dec 2010, ISBN13 9781856179447 (available here)\u000a    [N] B.A. Brushett, B.A. King, and C.J. Lemckert, \"Evaluation\u000a        of met-ocean forecast data effectiveness for tracking drifters deployed\u000a        during operational oil spill response in Australian waters\", Journal\u000a      of Coastal Research, 2011, SPI 64, pp 991-994 (available here)\u000a    [O] `Ocean nowcasting and forecasting for the Montara oil spill',\u000a      Brassington and King, 2010, http:\/\/www.cawcr.gov.au\/staff\/gbb\/presentations\/agu_ocean_sciences_2010_brassington.pdf\u000a      (archived here)\u000a    ","Title":"\u000a    C10 - Forecasting Ocean Oil Spill movements, facilitating Oil Spill\u000a      clean-ups\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Dr Dan Moore worked extensively on fluid dynamics earlier in his career\u000a      and had a large set of fluid dynamics code that was optimised for the, now\u000a      obsolete, CRAY supercomputer. By the early 1990s he had the motivation to\u000a      consider how to move large Computational Fluid Dynamics (CFD) programs\u000a      from the CRAY environment to the Massively Parallel Computing Environment.\u000a      Crucially, he had also established a research link with Dr. Alan\u000a      Wallcraft, a former Imperial Mathematics PhD Student, who worked in the\u000a      Ocean Modeling Division of the US Naval Research Laboratory (NRL). Moore\u000a      had previously supplied numerical software in the 1980s (optimized for the\u000a      CRAY supercomputer) for the NRL's numerical models of the Gulf of Mexico,\u000a      and had worked with Dr. Wallcraft in the early 1990s trying to scale up\u000a      the NRL's efforts from the Gulf of Mexico to the rest of the World's\u000a      oceans (minus the Arctic Ocean).\u000a    From 1991 to present Moore spent every summer in the US working with\u000a      Wallcraft on a real `production' Ocean Modelling Code [2] (the Navy\u000a      Layered Ocean Model: NLOM), trying out various ideas developed and tested\u000a      at Imperial during the academic year to allow it to run efficiently on\u000a      several hundred processors simultaneously. The collaboration resulted in\u000a      paper [1]. This paper developed a scalable methodology for covering a two\u000a      dimensional domain (such as the surface of the earth) with overlapping\u000a      tiles. The fluid flow equations could be solved locally on each tile by a\u000a      single processor. After each time step, each tile would exchange\u000a      information with the adjacent overlapping tiles. This enabled flows to\u000a      move through the tiles. Moore and Wallcraft devised an efficient mapping\u000a      from this 2-D tiling to a 1-D striping to allow the Elliptic Equation for\u000a      the surface pressure field to be solved by spectral methods on many\u000a      processors simultaneously. The intention was to develop code that could\u000a      work on any number of processors efficiently. This was achieved by using a\u000a      standard numerical research computing language, FORTRAN, for all of the\u000a      code and a standard mechanism for exchanging data between tiles, Message\u000a      Passing Protocol MPI. This disconnected the Fluid Dynamics Code from the\u000a      precise details of the computer it was running on. The number of\u000a      processors to use and the exact details of the size and number of tiles\u000a      were made as input data for the code to `read' at the start of its run.\u000a    The concepts behind producing a scalable World Ocean model can be applied\u000a      to both existing 2-D CFD codes and to the design of new codes. This tiling\u000a      strategy was applied successfully to other NRL models such as the Polar\u000a      Ice Prediction System (PIPS) and the Navy Coastal Model, NCOM. Moore\u000a      continued his summer research associations with the Ocean Modelling\u000a      Division of the NRL and worked on developing their current operational\u000a      model: Hybrid Coordinate Ocean Model (HYCOM, [Journal of Marine Systems,\u000a      Vol 65, Issue 1&#8212;4, 2007, p60-83]). This too uses the tiling concepts\u000a      developed by Moore and Wallcraft in the mid-1990s to achieve efficient\u000a      execution of several thousand processors simultaneously.\u000a    Key personnel:\u000a    \u000a      Dr Dan Moore, Reader in Computational Applied Mathematics,\u000a        1\/9\/1977-31\/7\/2011 and 1\/10\/2011- present, Mathematics Department,\u000a        Imperial College.\u000a      Dr Alan Wallcraft, Mathematics PhD Student 1977-1981, Imperial\u000a        College, Computer Scientist Contractor 1981-1997, Planning Systems Inc,\u000a        Computer Scientist, 1997-present, NRL- Stennis Space Center (NRL-SSC),\u000a        USA\u000a    \u000a    "},{"CaseStudyId":"42264","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    Credit is an important, driving mechanism in the economy. Not giving it\u000d\u000a      when we should has impact on the individual and impact on wider society.\u000d\u000a      The advances in the three evaluation areas described in section 2 benefit\u000d\u000a      consumers and the economy by helping to ensure that consumers are not\u000d\u000a      denied credit when they do indeed qualify and, conversely, preventing\u000d\u000a      people from obtaining credit when they shouldn't qualify.\u000d\u000a    Since the challenges motivating our research in this area arise from the\u000d\u000a      industry itself, dissemination of our work and results across the industry\u000d\u000a      is an integral part of our activity. We achieve this dissemination in a\u000d\u000a      number of ways, such as formal consultancy projects, industry-funded PhD\u000d\u000a      studentships (sometimes part time, with company employees), postdocs,\u000d\u000a      invited presentations to corporations, and through industry conferences\u000d\u000a      (where we are regularly invited to give keynote presentations). Examples\u000d\u000a      of bodies which have funded our work include GMAC, (evaluating\u000d\u000a      scorecards), HBOS (pattern discovery in retail banking data), British\u000d\u000a      Credit Trust (developing new scorecards), Fair Isaac (evaluating\u000d\u000a      scorecards), Capital One (evaluating scorecards), Goldman Sachs (fraud\u000d\u000a      detection), and many others. We have worked with most of the major\u000d\u000a      industry players, and many minor ones.\u000d\u000a    Turning to each of the three areas of research described in Section 2 we\u000d\u000a      now describe the impact:\u000d\u000a    (i) The H-measure: These matters were first presented (by Hand)\u000d\u000a      to the industry at the Henry Stewart Conference on Predictive\u000d\u000a        Analytics (an important conference for industry users of marketing\u000d\u000a      analytics) in December 2008. Since then Hand has been presented this work\u000d\u000a      in many commercial and industrial contexts (as well as more academic\u000d\u000a      meetings), for example, Credit Scoring and Control XI (August\u000d\u000a      2009, the premier conference on retail finance, with 90% industrial\u000d\u000a      participants), IMA Conference on Mathematics and its Applications\u000d\u000a      (March 2011), an invited seminar to Opera Solutions (November\u000d\u000a      2011, a leading data analytics company), a two-hour keynote presentation\u000d\u000a      at the Capital One Allstat and Quants conference (October 2013),\u000d\u000a      and many many others. The H-measure is being increasingly adopted in the\u000d\u000a      retail credit industry, as a performance measure which overcomes the\u000d\u000a      incoherence problem of the current industry standard Gini and KS measures.\u000d\u000a      The CEO of a leading US credit modelling company, Prescient Models,\u000d\u000a      writes: \"Over the last decade I have found the research papers from the\u000d\u000a        team at Imperial College to be very valuable. For the topics of adverse\u000d\u000a        selection, reject inference, the proper use of statistics like KS and\u000d\u000a        Gini, and survival models, I have read several excellent papers\u000d\u000a        that have assisted in my product development and general understanding\"\u000d\u000a      [A]. He also notes that paper [2] \"has alerted practitioners to how\u000d\u000a        they can make more useful models instead of simply chasing improvements\u000d\u000a        in arbitrary measures\" [B].\u000d\u000a    (ii) A KS comparative test: This new test [4] was first\u000d\u000a      described, by Hand, to the industry at the Credit Scoring and Control\u000d\u000a        XII meeting (August, 2011). Of this work, the Director of the Credit\u000d\u000a      Risk Analysis Division of a US banking regulator, the US Office of the\u000d\u000a      Comptroller of Currency, commented in 2012: \"The statistical test you\u000d\u000a        outlined in your paper is exactly what is needed to add statistical\u000d\u000a        rigor to the decision process. It is a test we will recommend banks\u000d\u000a        adopt as part of their model selection and validation process\" [C].\u000d\u000a      In 2013, the CEO of the US credit modelling company, Prescient Models,\u000d\u000a      noted that the work in [4] provided \"further weight and clarity\" to\u000d\u000a      the issue, allowing \"models to be judged realistically\" [B]. In\u000d\u000a      praising the robust K S comparative test in [4], he commented that \"[s]ubstituting\u000a        one model for another in pursuit of spurious accuracy can be a\u000d\u000a        multi-million dollar waste of money and distraction\" [B].\u000d\u000a    (iii) The illusion of progress: The illusion of progress work is\u000d\u000a      fundamental, with wide applications, and can be considered a cautionary\u000d\u000a      tale for the industry, warning it against inflated claims of enhanced\u000d\u000a      performance for complicated classifiers. The Director of the Credit Risk\u000d\u000a      Analysis Division of a US banking regulator, the US Office of the\u000d\u000a      Comptroller of Currency, wrote about the work described in [5]: \"I\u000d\u000a        would also like to note the contribution your paper [5] has had on our\u000d\u000a        discussions internally and with modelers at the larger institutions we\u000d\u000a        supervise ... Your insight on this issue has helped us solidify our\u000d\u000a        thoughts on this issue and develop methods of assessing the process\u000d\u000a        banks use to develop and implement their new models. We have used your\u000d\u000a        paper as a \"discussion paper\" for an internal workshop on modeling\u000d\u000a        methods after the crash, and as a recommended research paper for\u000d\u000a        modelers at large and mid-size banks...\" [C]. Similarly, \"This\u000a        article has had a direct impact on the creation of models in retail\u000d\u000a        lending with some of its terminology quickly becoming part of standard\u000d\u000a        conversation within the industry. It has provided the needed support for\u000d\u000a        analysts to avoid wasting time on spurious improvements, thereby saving\u000d\u000a        moving and allowing analysts to investigation deeper issues\" [B].\u000d\u000a    Further impact\u000d\u000a    In addition to the specific impact that has arisen from our research on\u000d\u000a      the H-measure, KS comparative test and illusion of progress, the more\u000d\u000a      general impact made by the statistics group is demonstrated by the\u000d\u000a      following statements from people we have worked with in the financial\u000d\u000a      industry, and the major industry award citation given below:\u000d\u000a    \u000d\u000a      One of the leaders of the industry, a former CEO of Experian (Experian\u000d\u000a        is a FTSE100 company, and a leading international provider of credit\u000d\u000a        information) writes \"You and your team's research has helped\u000d\u000a          enormously in the development of credit scoring and the credit\u000d\u000a          industry\" [D].\u000d\u000a      This general point is also made by an employee of one of world's\u000d\u000a        largest credit scoring organisations, Fair Isaac, who says \"it is\u000d\u000a          more valuable than ever to hear your voice which cuts straight through\u000d\u000a          to the scientific underpinnings of predictive modelling and\u000d\u000a          classification, while at the same time being concerned with the\u000d\u000a          important issues of practicality and usefulness of the resulting\u000d\u000a          models\" and \"We continue to look very much forward to your\u000d\u000a          highly relevant contributions in the field of credit scoring and\u000d\u000a          statistics\" [E].\u000d\u000a    \u000d\u000a    Finally, in 2012, the Consumer Credit Research Group at Imperial was\u000d\u000a        awarded the Credit Collections and Risk (CCR) industry award for\u000d\u000a        Contributions to the Industry. This is the first time this, or\u000d\u000a      indeed any of the CCR awards, has been made to an academic unit. The award\u000d\u000a      was presented by Gary Brooks, Group Credit Manager at Hitachi Europe, and\u000d\u000a      the citation read: \"... our winners have contributed significantly to\u000d\u000a        improving decision-making strategies for the retail credit industry.\u000d\u000a        They have worked with regulators, the banking and finance sectors and\u000d\u000a        scoring and ratings agencies worldwide in a wide range of areas to\u000d\u000a        improve scoring\" [F]. Prof Hand has also been asked to join the\u000d\u000a      Editorial Advisory Board of Credit Collections and Risk, the\u000d\u000a      leading industry magazine in the area.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    This case study describes impact resulting from research on assessing the\u000d\u000a      performance of credit scoring models conducted by the Consumer Credit \/\u000d\u000a      Retail Banking Research Group of the Mathematics Department at Imperial\u000d\u000a      College. The group's work has influenced both high-level industry\u000d\u000a      strategies for developing scoring models, and also low-level performance\u000d\u000a      measures for which such models are developed, refined and evaluated. We\u000d\u000a      describe examples of companies or bodies that have benefitted from\u000d\u000a      improved credit scoring models, including Prescient Models (a US credit\u000d\u000a      scoring company), Experian and the US Office of the Comptroller of\u000d\u000a      Currency. The group has established a very significant reputation for a\u000d\u000a      wide range of commercially valuable work in this area &#8212; to the extent that\u000d\u000a      the group received the major Credit Collections and Risk industry\u000d\u000a      award for Contributions to the Credit Industry in 2012.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    Imperial College London\u000d\u000a    ","Institutions":[{"AlternativeName":"Imperial College London","InstitutionName":"Imperial College London","PeerGroup":"A","Region":"London","UKPRN":10003270}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    (* References that best indicate quality of underpinning research)\u000d\u000a    \u000a[1] Hand D.J., Anagnostopoulos C., `When is the area\u000d\u000a        under the receiver operating characteristic curve an appropriate measure\u000d\u000a        of classifier performance?', Pattern Recognition Letters, 34,\u000d\u000a      492-495 (2013). DOI.\u000d\u000a    \u000a\u000a[2] *Hand D.J., `Measuring classifier performance: a coherent\u000d\u000a        alternative to the area under the ROC curve', Machine Learning, 77,\u000d\u000a      103-123 (2009). DOI.\u000d\u000a    \u000a\u000a[3] *Hand D.J., `Evaluating diagnostic tests: the area under\u000d\u000a        the ROC curve and the balance of errors', Statistics in Medicine,\u000d\u000a      29, 1502-1510 (2010). DOI.\u000d\u000a    \u000a\u000a[4] Krzanowski W.J., Hand D.J., `Testing the\u000d\u000a        difference between two Kolmogorov-Smirnov values in the context of\u000d\u000a        Receiver Operating Characteristic curves', Journal of Applied\u000d\u000a      Statistics, 38, 437-450 (2011). DOI.\u000d\u000a    \u000a\u000a[5] *Hand D.J., `Classifier Technology and the Illusion of\u000d\u000a        Progress', Statistical Science, 21, 1-14 (2006). DOI.\u000d\u000a    \u000aGrants:\u000d\u000a    [G1] EPSRC, EP\/C532589\/1,\u000d\u000a      1\/10\/05\/-31\/3\/08, `Statistical and machine learning tools for plastic card\u000d\u000a      and other personal banking fraud detection', PI: DJ Hand, CoI: NM Adams,\u000d\u000a      &#163;233,935\u000d\u000a    [G2] EPSRC, EP\/D505380\/1,1\/3\/06-28\/2\/09,\u000a      `Risk Management in the Personal Financial Services Sector', PI: DJ\u000d\u000a      Hand, Project partners: Fair, Isaac &amp; Company Incorporated, Lloyds Tsb\u000d\u000a      Bank Plc, &#163;540,154\u000d\u000a    [G3] Link Financial, MATH_P06509, 1\/10\/06-30\/9\/09, `Creating a\u000d\u000a        predictive portfolio review model', PI:DJ Hand, &#163;83,492\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"15","Level2":"1","Subject":"Accounting, Auditing and Accountability"}],"Sources":"\u000d\u000a    [A] Letter from CEO, Prescient Models LLC, 2\/1\/2012 (letter available\u000d\u000a      from Imperial College on request)\u000d\u000a    [B] Letter from CEO, Prescient Models LLC, 4\/4\/2013 (letter available\u000d\u000a      from Imperial College on request)\u000d\u000a    [C] Letter from Director, Credit Risk Analysis Division, US Office of the\u000d\u000a      Comptroller of Currency, 9\/01\/2012 [The Comptroller of Currency is the US\u000d\u000a      Federal Agency responsible for chartering, regulating, and supervising all\u000d\u000a      national banks and the federal branches and agencies of foreign banks]\u000d\u000a      (letter available from Imperial College on request)\u000d\u000a    [D] Letter from Former CEO of Experian, now PDG of Scoresoft, 25\/10\/2011\u000d\u000a      (letter available from Imperial College on request)\u000d\u000a    [E] Letter from Analytic Science-Senior Director, FICO Research, FICO\u000d\u000a      (Fair Isaac Corporation), one of the world's biggest players in the\u000d\u000a      area,16\/10\/13 (letter available from Imperial College on request)\u000d\u000a    [F] Contribution To The Industry Credit Excellence Award 2012, Winner:\u000d\u000a      Professor David Hand and the Consumer Credit Research Group at Imperial\u000d\u000a      College, http:\/\/www.ccr-\u000a        interactive.co.uk\/index.php?option=com_content&amp;task=view&amp;id=1416&amp;Itemid=117\u000d\u000a      (Archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/tkf\u000d\u000a      on 21\/5\/13)\u000d\u000a    ","Title":"\u000d\u000a    C2 &#8212; Improved scorecard evolution methods impacting financial services\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The research impact described in this Case Study is concerned with\u000d\u000a      predictive models for guiding decisions concerning individual applicants\u000d\u000a      and users of financial products, such as bank loans, credit cards, car\u000d\u000a      finance, store cards, debit cards, mortgages, student loans, and so on. We\u000d\u000a      focus on retail credit &#8212; that is, financial services for\u000d\u000a      individual people, not corporations or investors, etc. &#8212; and our aim is to\u000d\u000a      model and predict likely behaviour.\u000d\u000a    A `scorecard' is a statistical model purporting to measure\u000d\u000a      someone's riskiness, creditworthiness or other attribute. Such models are\u000d\u000a      used as the basis of loan decisions, to monitor credit card transactions\u000d\u000a      patterns, for fraud detection and for a host of other reasons. Measuring\u000d\u000a      the performance of scorecards lies at the heart of their construction\u000d\u000a      (e.g. parameter estimation to maximise performance), their selection (e.g.\u000d\u000a      which one should be used) and their effectiveness (e.g. is it good enough\u000d\u000a      for purpose). In short, sound evaluation methods are central to this\u000d\u000a      trillion dollar industry's effectiveness and progress.\u000d\u000a    We describe three of the evaluation areas where we have had significant\u000d\u000a      impact:\u000d\u000a    (i) The H-measure. The most widely used measure of scorecard\u000d\u000a      performance in the UK is the Gini coefficient (GC), which is applied in\u000d\u000a      situations where the aim is to assign people to classes (e.g. good risk or\u000d\u000a      bad risk). The GC is a chance-standardised version of the area under the\u000d\u000a      Receiver Operating Characteristic curve. It is also very widely used in\u000d\u000a      other areas, such as diagnostic medicine, fault detection and signal\u000d\u000a      detection. Currently, this measure is used as the choice of performance\u000d\u000a      measure in around 6,000 papers per year [1]. As the culmination of an\u000d\u000a      extended piece of work, dating from 1999 and continuing to 2009 [2], 2010\u000d\u000a      [3], 2012 [1] and beyond, we characterised two distinct situations under\u000d\u000a      which the GC may be used. One ignores the classification of other people\u000d\u000a      when assigning a particular individual, and the other takes other\u000d\u000a      classifications into account. We showed that the former usage implies a fundamental\u000a        incoherence in the GC. That is, when used in such situations, it\u000d\u000a        is equivalent to using different performance measures for different\u000d\u000a        scorecards &#8212; contravening the fundamental performance assessment\u000d\u000a      tenet that the same instrument must be used to measure different\u000d\u000a      scorecards. This contribution therefore identified, and provided a\u000d\u000a      solution to, a deep conceptual problem at the core of all practical\u000d\u000a      scorecard assessment. This solution is a new statistic (the H-measure)\u000d\u000a      which overcomes the problem. Public domain code for this is available in\u000d\u000a      the R statistical computing language (http:\/\/www.hmeasure.net\/).\u000d\u000a    (ii) A KS comparative test. Paralleling the H-measure, the most\u000d\u000a      widely used measure of scorecard performance in the US is the Kolmogorov\u000d\u000a      Smirnov KS test statistic (not the statistical test, per se, but\u000d\u000a      the statistic itself). However, we recognised there was no formal\u000d\u000a      statistical test to compare KS statistics: comparisons in the industry had\u000d\u000a      hitherto been based on ranking, or on informal assessments of the relative\u000d\u000a      size of the statistics. This was a serious shortcoming; it compromised\u000d\u000a      credit-granters' legal obligation to claim that credit-granting decisions\u000d\u000a      are objective, and it risked poor decisions about the choice of scoring\u000d\u000a      model, with adverse implications for both lenders and borrowers. Having\u000d\u000a      identified the problem, to meet the industry needs we developed, described\u000d\u000a      and implemented a test for comparing KS statistics [4].\u000d\u000a    (iii) The illusion of progress. Although scorecards are measuring\u000d\u000a      devices, not classifiers, they are often used as the basis for classifiers\u000d\u000a      by comparing the score with a threshold. More general research on\u000d\u000a      comparative evaluation of classifiers, culminating in [5], demonstrated\u000d\u000a      that there was a pronounced tendency to exaggerate new results, and\u000d\u000a      elucidated the various mechanisms behind this tendency. This contribution\u000d\u000a      is important to banks, since it provides a balancing view to overstated\u000d\u000a      claims of improved performance: it helps them make informed decisions\u000d\u000a      about when new models should be adopted, or when performance claims were\u000d\u000a      (usually accidentally) inflated. [N.B. This work has also had a wide\u000d\u000a      impact in other areas, not least the quantitative algorithmic trading\u000d\u000a      hedge fund industry.]\u000d\u000a    The Consumer Credit\/Retail Banking research group has been located at\u000d\u000a      Imperial College since 1999. The group at Imperial has included over 20\u000d\u000a      researchers. Key personnel are:\u000d\u000a    \u000d\u000a      Prof David Hand, Professor, group leader, 1999-present.\u000d\u000a      Dr Niall Adams, PDRA, Lecturer, and now Reader in Statistics,\u000d\u000a        1999-present.\u000d\u000a      Dr Christoforos Anagnostopoulos, PhD student, then lecturer at\u000d\u000a        Imperial, 2006-present.\u000d\u000a    \u000d\u000a    The research was supported by EPSRC [e.g. G1, G2]. Close collaborations\u000d\u000a      with industry (e.g. Barclaycard, Experian, Fair Isaac, Equifax, Capital\u000d\u000a      One &amp; GMAC) also funded the research [e.g. G3], identified relevant\u000d\u000a      problems, and provided data and other resources.\u000d\u000a    "},{"CaseStudyId":"42265","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The impact from the above described work on Bayesian computation has been\u000a      varied and wide, impacting a number of sectors.\u000a    Finance:\u000a    Particle filters and MCMC are used in many financial institutions. One\u000a      particular case study is the development of algorithmic trading strategies\u000a      at Maple-Leaf Capital LLC. Algorithmic trading strategies use mathematical\u000a      models and rules to decide upon buying and selling financial instruments,\u000a      such as equities and futures. Often these models and rules are combined to\u000a      generate a trading position. Prior to the work to be described,\u000a      understanding the statistical interaction between strategies was\u000a      challenging as estimation of these properties was often very difficult.\u000a    The head of quantitative analysis at Maple-Leaf Capital LLP in the period\u000a      2004-2010 confirms the use of the MCMC techniques in finance. The methods\u000a      developed at Imperial College were used by the quantitative analysis team\u000a      \"to infer a probability distribution associated to the statistical\u000a        interaction between strategies...Once this information [was] available,\u000a        it was used to calibrate the position of the overall combination of\u000a        trading strategies\" which was then used \"to assist trades\"\u000a      [A]. The exact impact of the algorithm (e.g. in terms of profits) is not\u000a      publicly available, but the improvement in estimation (of statistical\u000a      interaction of the strategies) was adopted due to apparent failures of\u000a      existing methodology. The quantitative analysis team \"did not know of\u000a        any method at that time, other than MCMC, which could have fitted this\u000a        type of model\". The algorithms were thus \"critical in this part\u000a        of the work\" [A].\u000a    This example of financial impact is presented in detail, but the\u000a      methodology is used generically across the field: MCMC and particle\u000a      filters are further utilized as methods to analyse and predict financial\u000a      positions and complement and enhance existing methods. There have been\u000a      numerous papers devoted to this application of particle filtering\/SMC\u000a      methods (e.g. [B]).\u000a    Credit research initiative (CRI), National University of Singapore\u000a        (NUS):\u000a    This is a non-profit undertaking by the Risk Management Institute (RMI)\u000a      at NUS that uses MCMC and particle filters for assistance in predicting\u000a      credit risk rate in a \"public good\" approach to credit rating. NUS\u000a      launched the CRI in 2009 to output predictions of probability of default\u000a      (PDs) using advanced statistical models and intending to \"give the big\u000a        rating agencies like Standard &amp; Poor's, Moody's Investor Service and\u000a        Fitch Ratings a run for their money\" [C] (see also [D, E]). As is\u000a      well known in the popular literature, the credit prediction system melted\u000a      down in 2007\/2008, leading to the credit crunch and resulting financial\u000a      crisis. There are, of course, a wide range of reasons for this, but one\u000a      must be attributed to the problems of existing models and methods for\u000a      prediction of PDs. One solution is to \"leverage open source models with\u000a        fully transparent inputs and outputs\" with \"software and\u000a        data...open to a worldwide peer review process...[to] facilitate their\u000a        rapid improvement\" [F]. Such \"open source, transparent credit\u000a        models and methodologies would eliminate conflicts of interest and bring\u000a        the benefits of mass collaboration to the world of credit ratings\"\u000a      [G].\u000a    In order to accurately fit the models, the SMC sampler technique of [2],\u000a      co-developed at Imperial College London, is utilized by the CRI to provide\u000a      online predictions. The CRI's methodology for parameter estimation is\u000a      described in Duan, J.C. &amp; A. Fulop (2013) which references [2] in\u000a      describing the parameter estimation by SMC [H, I]. The SMC method is used\u000a      to \"deal with the problem of high dimension of the parameter space\",\u000a      allowing uncertainty to be properly assigned to the parameters [J]. The\u000a      online predictions provided by the CRI would not be possible without such\u000a      methods. The predictions are publicly available to anyone (subject to the\u000a      decision of NUS).\u000a    The CRI website [K] offers daily predictions from a probability of\u000a      default (PD) model for defaults of about 60,400 listed firms in 106\u000a      economies in Asia Pacific, North America, Europe, Latin America, the\u000a      Middle East and Africa [L]. This web portal presents the outputs from this\u000a      model, including daily updated PDs for individual firms in the\u000a      aforementioned regions and aggregate PDs for different economies and\u000a      sectors. The CRI has \"agreed to provide [the] Probability of Default to\u000a        a number of financial institutions for their internal risk management\u000a        and analysis\" and its website has over 2000 registered users [J].\u000a      The CRI initiative shows the \"potential for open source credit models\u000a        to take their place next to proprietary software and agency ratings\"\u000a      in the credit ratings industry [G].\u000a    Defence:\u000a    Target tracking is the notion of trying to estimate or predict the\u000a      position and\/or velocity of targets simultaneously given noisy sensor\u000a      measurements. This has particular applications in the defence industry,\u000a      where the `targets' could be enemy tanks\/aircraft\/submarines and the\u000a      measurements are 'noisy' measurements recorded by sensors. These phenomena\u000a      are often modelled by a state-space model.\u000a    Until the development of particle filtering methodology, one could only\u000a      apply the most basic of state-space models, which are often unrealistic\u000a      representations of the real data phenomena encountered in target tracking.\u000a      The development of the bootstrap particle filter by Adrian Smith and\u000a      co-workers in [1] was one of the most fundamental methods to allow one to\u000a      fit, online, realistic state-space models. This work has become integral\u000a      in the target tracking work of the UK defence industry, such as QinetiQ.\u000a      The bootstrap particle filter has been routinely applied at QinetiQ and\u000a      BAE systems since 1993 and plays a fundamental role in the defence of the\u000a      United Kingdom and the ability to predict or estimate the position of the\u000a      enemy.\u000a    Confirmation of the impact of the work in the defence sector comes from\u000a      the current Principal Consultant (National Security) at BAE Systems Detica\u000a      who is able to confirm the use of particle filters based around paper [1]\u000a      in the period 2008-2012 whilst employed at QinetiQ [M]. Particle filters\u000a      allowed QinetiQ to \"tackle problems that typically had weak or no\u000a        existing solution\" [M]. As an example, in the context of\u000a      multi-target tracking, \"particle filters enabled [QinetiQ] to constrain\u000a        objects to be on the road, improving localisation accuracy, use\u000a        interacting models to constrain objects motion by other objects, and to\u000a        perform inference in bearings only tracking problems\" [M]. These\u000a      problems are \"routinely found in the defence industry\" and \"the\u000a        particle filter played an important role in [QinetiQ's] work\" [M].\u000a      Unfortunately it is not possible to receive confirmation about precisely\u000a      what was implemented in real systems however it can be confirmed that \"particle\u000a        filters had a massive impact on the breath of problems that could be\u000a        solved [in defense], allowing tracking systems to be deployed in\u000a        scenarios that were previously impossible (or unreliable)\" [M].\u000a    Navigation and wireless networks:\u000a    Similar to the application for target tracking in the defence industry,\u000a      particle filters\/SMC are also used in navigation (GPS) and for tracking\u000a      in, the now standard problem of, wireless sensor networks. Frameworks for\u000a      positioning, navigation, and tracking problems have been developed and\u000a      particle filters can be used for positioning based on cellular phone\u000a      measurements, for integrated navigation in aircraft, and for target\u000a      tracking in aircraft and cars. The particle filter enables a promising\u000a      solution to the combined task of navigation and tracking, with possible\u000a      application to collision avoidance systems in cars [e.g. N].\u000a    ","ImpactSummary":"\u000a    In recent years there has been an explosion of real data from areas as\u000a      diverse as bioinformatics, genetics, engineering and finance. Coupled with\u000a      this has been the development of complex and realistic Bayesian\u000a      statistical models to represent these data. In order to use these models\u000a      to perform (Bayesian) statistical inference, one is required to calculate\u000a      integrals, which are unknown analytically. Most of the numerical methods\u000a      used to approximate these integrals are based upon Monte Carlo methods of\u000a      which some of the seminal work has been done at Imperial College London,\u000a      for instance the `particle-filter' developed in 1993 [4]. These methods\u000a      are now very widely used in finance for automated trading, calculating the\u000a      probability of default for economies, and for target tracking in the\u000a      defence sector and we give explicit exemplars of each. The numerical\u000a      methods developed at Imperial have been important in applying realistic\u000a      models to these varied application areas and have impacted companies and\u000a      organisations as diverse as Maple-Leaf Capital LLC, QinetiQ and the Credit\u000a      Research Initiative.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Imperial College London\u000a    ","Institutions":[{"AlternativeName":"Imperial College London","InstitutionName":"Imperial College London","PeerGroup":"A","Region":"London","UKPRN":10003270}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"1880252","Name":"Singapore"}],"References":"\u000a    (* References that best indicate quality of underpinning research)\u000a    \u000a[1] * Gordon, N., Salmond, D. &amp; Smith, A.F.M., `Novel\u000a        approach to nonlinear\/non-Gaussian Bayesian state estimation', Radar\u000a      and Signal Processing, IET Proceedings F, 140 (2), p107-113 (1993). DOI.'\u000a    \u000a\u000a[2] Del Moral, P., Doucet, A. &amp; Jasra, A., `Sequential\u000a        Monte Carlo samplers', J. R. Statist. Soc. B, 68, 411-436 (2006). DOI.\u000a      [The work by A Jasra was conducted at Imperial but his author affiliation\u000a      on the paper is University of Cambridge.]\u000a    \u000a\u000a[3] * Pitt, M.K., Shephard, N., `Filtering via simulation:\u000a        auxiliary particle filters', Journal of the American Statistical\u000a      Association, 94:446, 590-599 (1999). DOI.\u000a    \u000a\u000a[4] * Crisan, D., &amp; Doucet, A., `A survey of convergence\u000a        results on particle filtering methods for practitioners', IEEE\u000a      Trans. Sig. Proc., 50, 736-746 (2002). DOI.\u000a    \u000a\u000a[5] Denison, D.G.T., Holmes, C.C., Mallick, B.K. &amp;\u000a      Smith, A.F.M., `Bayesian Methods for Nonlinear Classification and\u000a        Regression', Publ. Wiley: New York, ISBN-13: 978-0471490364 (2002).\u000a    \u000a\u000a[6] Gander M.P.S., Stephens D.A., `Simulation and inference\u000a        for stochastic volatility models driven by Levy processes',\u000a      Biometrika, 94 (3), 627-646 (2007). DOI.\u000a    \u000aGrants:\u000a    [G1] EPSRC Grant: EP\/H000550\/1,\u000a      `Increasing the efficiency of numerical methods for estimating the state\u000a      of a partially observed system', PI: Dr D. Crisan, &#163;314,974,\u000a      01\/10\/09-30\/03\/13.\u000a    [G2] EPSRC Grant: GR\/G62103\/01,\u000a      `Studies in Bayesian Computation and Display methodology', PI: Prof A.\u000a      F.M. Smith, &#163;100,960, 13\/03\/92-12\/04\/95.\u000a    [G3] EPSRC Grant: GR\/L10437\/01,\u000a      'Bayesian population pharmacokinetic &amp; pharmacodynamics modelling:\u000a      implementation and model selection', PI: Dr J. Wakefield, Co-I: N Best, D\u000a      Spiegelhalter, &#163;123,811, 01\/10\/96-31\/03\/99.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    [A] Letter from Quantitative Analysis, Tudor Capital LLP, formerly Head\u000a      of Quantitative Analysis, Maple-Leaf Capital LLP (available from Imperial\u000a      on request)\u000a    [B] Examples of the financial applications of SMC\/particle filtering: DOI-1, DOI-2\u000a    [C] Today, Singapore article, `Ratings Systems: New Asian Kid on the\u000a      Block?', 17\/7\/09 (archived here).\u000a    [D] Reuters article, `Singapore university seeks to break hold of\u000a      credit-rating goliaths', 14\/10\/11 (archived here).\u000a    [E] Business Times article, `NUS offers free global credit ratings of\u000a      firms', 16\/7\/10 (archived here).\u000a    [F] \u000a        http:\/\/www.guardian.co.uk\/commentisfree\/2013\/feb\/25\/moodys-sp-credit-rating-agencies-need-reform\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/rmf\u000a      on 19\/6\/13)\u000a    [G] http:\/\/tabbforum.com\/opinions\/can-open-source-models-fix-the-credit-ratings-industry\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/smf\u000a      on 19\/6\/13)\u000a    [H] The methodology for the parameter estimation used in the CRI models,\u000a      which references [2], is described in Duan, J.-C., A. Fulop, 2013,\u000a      `Multiperiod Corporate Default Prediction with Partially-Conditioned\u000a      Forward Intensity' (archived here)\u000a    [I] Description of background documents for the CRI models,\u000a      http:\/\/www.rmicri.org\/about\/backgrounddocs.php\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/tmf\u000a      on 17\/5\/13)\u000a    [J] Letter from Deputy Director of Education and Industry Relations, RMI,\u000a      NUS, 9\/6\/13 (available from Imperial on request)\u000a    [K] http:\/\/www.rmicri.org\/home\/\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/wmf\u000a      on 19\/6\/13)\u000a    [L] http:\/\/www.rmicri.org\/about\/aboutcri.php\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/vmf\u000a      on 19\/6\/13)\u000a    [M] Letter from Principal Consultant, National Security, BAE Systems\u000a      (formerly at QinetiQ), 10\/6\/13 (available from Imperial on request)\u000a    [N] Examples of the use of SMC\/particle filtering in navigation and\u000a      wireless networks: DOI-1,\u000a      DOI-2, DOI-3,\u000a      DOI-4, DOI-5\u000a    ","Title":"\u000a    C3 -\u0009Applications of Bayesian methods in finance, credit scoring and\u000a      target tracking\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Bayesian computation can be split, roughly, into two main components:\u000a      Markov chain Monte Carlo (MCMC) and particle filtering\/sequential Monte\u000a      Carlo (SMC) methods. These are the two primary numerical techniques which\u000a      are the current state-of-the art to perform Bayesian statistical inference\u000a      (i.e. to draw conclusions that are interpretable to non-statisticians)\u000a      from complex and realistic statistical models; that is, those models which\u000a      reflect real-world phenomena. Due to a variety of demands in real\u000a      applications (for example high frequency trading in finance or the human\u000a      genome project in genetics), the need to be able to perform such inference\u000a      has greatly increased within the past 20 years.\u000a    The first appearance of an implementation of the particle filter was in\u000a      the seminal paper by Gordon, Salmond and Smith [1]; further important\u000a      developments were made by the group of Imperial researchers led by Adrian\u000a      Smith whilst he was at Imperial. The bootstrap particle filter is the\u000a      basis of almost every exact computational algorithm used for `filtering' a\u000a      problem appearing in finance and the defence industry. Some additional\u000a      important theoretical and methodological contributions were made,\u000a      individually, by Dan Crisan, Mike Pitt and Ajay Jasra (e.g. [2])\u000a      respectively. These Imperial-led projects (for instance under grant [G1])\u000a      include the widely used `auxiliary particle filter' [3] and `sequential\u000a      Monte Carlo samplers' [2] methods; also the foundations of the theoretical\u000a      understanding of these methods is in [4]. The SMC sampler method allows\u000a      for a different filtering problem (to that mentioned above) to be\u000a      addressed and this is something which is now used, for example, in the\u000a      calculation of probability of defaults on a daily basis. The auxiliary\u000a      particle filter is a substantial adaptation and improvement of the\u000a      particle filter.\u000a    MCMC techniques were also put to use and developed by Adrian Smith's\u000a      group. In particular, new MCMC methods and applications to important\u000a      statistical models were at the forefront of the work that was done. These\u000a      were some of the first researchers in the world to nurture this\u000a      methodology for real Bayesian models and included Jon Wakefield, Chris\u000a      Holmes, Dave Denison, David Stephens and Bani Mallick (culminating in the\u000a      book [5]). These works were funded by grants [G2, G3]. As an example, of\u000a      the resulting academic research that was done and applied, David Stephens\u000a      and Matthew Gander used MCMC for financial models in 2001-2004 (see [6]).\u000a    All of this Imperial research effort can be thought of as the foundations\u000a      of all of the current (and substantial) research being done in Bayesian\u000a      computation.\u000a    Key contributors:\u000a    \u000a      The key Imperial staff and students involved in the above research\u000a        include Adrian Smith (Professor, 1990-1998), David Stephens (RA then\u000a        lecturer, 1990-2006), Bani Mallick (lecturer, 1994-1998), Dan Crisan\u000a        (Professor, 2000-Present, RA 1995-1998), Jon Wakefield (lecturer,\u000a        1990-1996), Ajay Jasra (RF &amp; lecturer, 2006-2011, previously PhD\u000a        student, 2002-2005), Mike Pitt (RA, 1995-1999), Christopher C. Holmes\u000a        (PhD, RA &amp; lecturer, 1996-2004), Neil Gordon (PhD, 1990-1993, then\u000a        Defense Research Agency), Matthew Gander (PhD, 2001-2004).\u000a      Key external collaborators include A Doucet (University of Cambridge,\u000a        University of Melbourne, University of British Columbia) and D Salmond\u000a        (Defence Research Agency)\u000a    \u000a    "},{"CaseStudyId":"42266","Continent":[{"GeoNamesId":"6255146","Name":"Africa"},{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2245662","Name":"Senegal"},{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"2134431","Name":"Vanuatu"},{"GeoNamesId":"3144096","Name":"Norway"},{"GeoNamesId":"2963597","Name":"Ireland"},{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"660013","Name":"Finland"},{"GeoNamesId":"6251999","Name":"Canada"}],"Funders":["Economic and Social Research Council","Engineering and Physical Sciences Research Council","Medical Research Council"],"ImpactDetails":"\u000a    WinBUGS is an established and stable, stand-alone version of the\u000a      BUGS software, which remains\u000a      available [A] but is no longer being further developed. WinBUGS is still\u000a      used extensively (searching\u000a      for `WinBUGS' on Google returns over 205,000 hits, with 42,400 since 2008\u000a      and 11,300 in 2013)\u000a      and has been described as \"the most widely accepted Bayesian modelling\u000a        package\" [B]. Since\u000a      2005, development of the BUGS project has focussed on the OpenBUGS\u000a      project [C], the open-source\u000a      version of the BUGS code. OpenBUGS was first released in 2005 and the\u000a      latest versions of\u000a      OpenBUGS (from v3.0.7 onwards) have been designed to be at least as\u000a      efficient and reliable as\u000a      WinBUGS over a wide range of test applications, but with greater\u000a      flexibility and extensibility [A].\u000a    The impact of the BUGS software is summed up by Prof Brad Carlin, Head of\u000a      Biostatistcs\u000a      University of Minnesota: \"MCMC freed Bayes from the shackles of\u000a        conjugate priors and the curse\u000a        of dimensionality; BUGS then brought MCMC-Bayes to the masses, yielding\u000a        an astonishing\u000a        explosion in the number, quality, and complexity of Bayesian inference\u000a        over a vast array of\u000a        application areas, from finance to medicine to data mining\" [D].\u000a    Books and Training:\u000a    A demonstration of the popularity and wide applicability of the BUGS,\u000a      WinBUGS and OpenBUGS\u000a      software has been the wide number of books published on them since their\u000a      launch. Since 2008\u000a      there have been over 10 dedicated books published about WinBUGS and\u000a      OpenBUGS, including\u000a      one by Nicky Best and colleagues. Some examples are:\u000a    1) Bayesian\u000a        Modeling Using WinBUGS (2009), Wiley, I Ntzoufras\u000a    2) Introduction\u000a        to WinBUGS for Ecologists: Bayesian approach to regression, ANOVA, mixed\u000a        models and related analyses\u000a      (2010), Academic Press, M Kery ([text removed for publication]\u000a      copies sold, 21\/11\/13, [E]).\u000a    3) Bayesian\u000a        Population Analysis using WinBUGS: A Hierarchical Perspective\u000a      (2011), Academic\u000a      Press, Mark Kery ([text removed for publication], 21\/11\/13, [E]).\u000a    4) Bayesian\u000a        Analysis Made Simple: An Excel GUI for WinBUGS (2011), Chapman &amp;\u000a      Hall\/CRC\u000a      Biostatistics Series, Phil Woodward ([text removed for publication] as at\u000a      21\/11\/13, [F]).\u000a    5) Statistics\u000a        for Bioengineering Sciences: With MATLAB and WinBUGS Support (2011),\u000a      Springer\u000a      Texts in Statistics, B Vidakovic\u000a    6) The\u000a        BUGS Book: A Practical Introduction to Bayesian Analysis D Lunn, C\u000a      Jackson, N Best, A\u000a      Thomas, and D Spiegelhalter. (2012), Chapman and Hall. ([text removed for\u000a      publication] copies\u000a      sold, 21\/11\/13, [F]).\u000a    7) Applied\u000a        Bayesian Statistics: With R and OpenBUGS Examples (2013), Springer,\u000a      MK Cowles\u000a    8) R\u000a        Tutorial with Bayesian Statistics Using OpenBUGS, Amazon Media EU\u000a      (2012), Chi Yau\u000a    Sales for four of the books above have been made available to us [E, F]\u000a      totalling [text removed for\u000a      publication] copies sold worldwide. The BUGS Book, 6), co-authored by\u000a      Best, has sold [text\u000a      removed for publication] copies in its first year since publication, and\u000a      in November 2013 was\u000a      ranked #42 on the Amazon \"best sellers in Mathematical Probability and\u000a      Statistics\" list (this was the\u000a      top ranked Bayesian text book) and ranked #28,188 overall (out of\u000a      39,995,344) in their books\u000a      bestsellers. The descriptions for books 2) and 3) state \"Bayesian\u000a        statistics has exploded into\u000a        biology and its sub-disciplines [...]. WinBUGS and its open-source\u000a        sister OpenBugs is currently the\u000a        only flexible and general-purpose program available with which the\u000a        average ecologist can conduct\u000a        standard and non-standard Bayesian statistics\" [G].\u000a    The BUGS software is also used widely for the teaching of Bayesian\u000a      modelling ideas to students\u000a      and researchers the world over, and several texts (as demonstrated by the\u000a      list above) use\u000a      WinBUGS and OpenBUGS extensively for illustrating the Bayesian approach\u000a      across both distinct\u000a      and general application areas. For example, `The BUGS Book' (book 6) has\u000a      been adopted as\u000a      material for Bayesian courses at 19 universities across the world in its\u000a      first year since publication,\u000a      with a further 34 universities reviewing the book for courses beginning in\u000a      2014. Countries include\u000a      the UK, Ireland, USA, Canada, Germany, Norway, Finland, the Netherlands\u000a      and Singapore [F].\u000a    Selected Applications of WinBUGs\/OpenBUGS:\u000a    To provide a flavour of the impact that the BUGS software has had on the\u000a      practice of Bayesian\u000a      statistics outside of the academic arena, three exemplars are provided\u000a      below:\u000a      &#8226; Pharmaceutical Industry: For Pfizer Neusentis WinBUGS has been\u000a        the software of choice when\u000a        undertaking Bayesian analyses and has been used in numerous Phase 2 and\u000a        3 studies to\u000a        analyse data, adopting informative prior distributions for the placebo\u000a        and dose response, and\u000a        sometimes the standard of care response, saving time, money and, more\u000a        importantly,\u000a        unnecessary patient recruitment. The most notable example is one in\u000a        which Pfizer reduced the\u000a        placebo arm of the trial by 100 patients, from 300 to 200. As a result\u000a        the duration of the trial\u000a        was reduced by approximately 12 months and saved about $7.5M. In a\u000a        further exemplar, the\u000a        results of a BUGS-based meta-analysis of dose response integrating seven\u000a        phase 2 and 3\u000a        studies proved central in a recent discussion supporting dose selection\u000a        that resulted in approval\u000a        of a new compound for the treatment of rheumatory arthritis [H].\u000a      More generally, Pfizer have stated that \"Bayesian methods have\u000a          contributed a great deal to the\u000a          efforts being made by Pfizer to improve the efficiency of drug\u000a          discovery and development. It is\u000a          only due to the invention of MCMC methods, and their practical\u000a          implementation in the BUGS\u000a          software, that we have been able to apply Bayesian methods as widely\u000a          as we now do. By\u000a          making MCMC methods for Bayesian analysis both free and relatively\u000a          easy to program, BUGS\u000a          is a major factor in overcoming the inertia that exists in the\u000a          adoption of new methodologies\" [H].\u000a      &#8226; WinBUGS is also the only software package discussed by name in the\u000a        report by the FDA on\u000a        Guidance for the Use of Bayesian Statistics in Medical Device Clinical\u000a        Trials (2010) U.S.\u000a        Department of Health and Human Services, Food and Drug Administration\u000a        [I].\u000a      &#8226; Informing national disease control programmes in developing\u000a          countries: The geostatistical\u000a        model functionality in add on package GeoBUGS has been used to produce\u000a        spatial predictive\u000a        infectious disease risk maps to aid implementation of national disease\u000a        control programmes in\u000a        developing countries across Africa and the Asia-Pacific region. These\u000a        maps, modelled by Prof\u000a        Clements (U. Queensland), have informed the allocation of resources for\u000a        various diseases\u000a        including schistosomiasis, soil-transmitted helminth infections, malaria\u000a        and rift valley fever.\u000a        Examples include generating maps for the planning of mass drug\u000a        administration campaigns to\u000a        control schistosomiasis in Africa, and maps of malaria risk at the\u000a        baseline stage of a malaria\u000a        elimination programme in Vanuatu, forming the basis of a decision to\u000a        limit indoor residual\u000a        spraying of insecticide to within 2km of the coastline of Tanna Island.\u000a        Work on Rift Valley Fever\u000a        (RVF) was done in collaboration with Prof Best [J], and created maps to\u000a        support the planning of\u000a        the siting of sentinel surveillance sites for RVF activity in northern\u000a        Senegal. Clements states\u000a        that WinBUGs \"overcomes a number of limitations associated with\u000a          traditional geostatistics\u000a          allowing for robust spatial predictions that incorporate information\u000a          from a range of sources\" [K].\u000a      &#8226; Fisheries stock assessments: Fisheries stock assessments are\u000a        conducted to evaluate the\u000a        consequences of different management actions. In 1999, a seminal paper\u000a        by Meyer and Millar\u000a        [L] recognised the potential of WinBUGS for Bayesian fish stock\u000a        assessments: \"we report on\u000a        significant progress made in facilitating the routine implementation\u000a        that may have a\u000a        revolutionary effect on Bayesian stock assessment in everyday practice.\u000a        This is achieved\u000a        through BUGS, a recently developed software package\" [p. 1078]. They\u000a        conclude their article\u000a        with the prediction that \"the routine implementation of Bayesian\u000a        inference that is now possible\u000a        will `almost surely' have an impact on fisheries stock assessment\" [p.\u000a        1084]. Fourteen years\u000a        later, a Google search on the terms \"winbugs + fish + stock +\u000a        assessment\" yields over 1,700\u000a        hits since 2008. These include stock assessments of sword fish for the\u000a        Western and Central\u000a        Pacific Fisheries Commission [M], Chinook salmon for the Alaska\u000a        Department of Fish and\u000a        Game [N] and Bottomfish for the NOAA Pacific Islands Fisheries Science\u000a        Center [O].\u000a    ","ImpactSummary":"\u000a    The WinBUGS software (and now OpenBUGS software), developed initially at\u000a      Cambridge from\u000a      1989-1996 and then further at Imperial from 1996-2007, has made practical\u000a      MCMC Bayesian\u000a      methods readily available to applied statisticians and data analysts. The\u000a      software has been\u000a      instrumental in facilitating routine Bayesian analysis of a vast range of\u000a      complex statistical problems\u000a      covering a wide spectrum of application areas, and over 20 years after its\u000a      inception, it remains the\u000a      leading software tool for applied Bayesian analysis among both academic\u000a      and non-academic\u000a      communities internationally. WinBUGS had over 30,000 registered users as\u000a      of 2009 (the software\u000a      is now open-source and users are no longer required to register) and a\u000a      Google search on the term\u000a      `WinBUGS' returns over 205,000 hits (over 42,000 of which are since 2008)\u000a      with applications as\u000a      diverse as astrostatistics, solar radiation modelling, fish stock\u000a      assessments, credit risk assessment,\u000a      production of disease maps and atlases, drug development and healthcare\u000a      provider profiling.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Imperial College London\u000a    ","Institutions":[{"AlternativeName":"Imperial College London","InstitutionName":"Imperial College London","PeerGroup":"A","Region":"London","UKPRN":10003270}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"5879092","Name":"Alaska"},{"GeoNamesId":"1880252","Name":"Singapore"}],"References":"\u000a    \u000a[1] *Lunn, D.J., Thomas, A., Best, N. and Spiegelhalter, D., `WinBUGS\u000a        &#8212; A Bayesian modelling\u000a        framework: Concepts, structure, and extensibility', Statistics and\u000a      Computing, 10, 325-337\u000a      (2000). DOI.\u000a    \u000a\u000a[2] *Lunn, D.J., Best, N., Thomas, A, Wakefield, J. and\u000a      Spiegelhalter, D., `Bayesian Analysis of\u000a        Population PK\/PD Models: General Concepts and Software', Journal of\u000a      pharmacokinetics and\u000a      pharmacodynamics, 29, 271-307 (2002). DOI.\u000a    \u000a\u000a[3] Lunn, D.J., Best, N. and Whittaker, J., `Generic\u000a        reversible jump MCMC using graphical models',\u000a      Statistics and Computing, 19, 395-408 (2009). DOI.\u000a    \u000a\u000a[4] Lunn, D.J., Whittaker, J. C. and Best, N., `A Bayesian\u000a        toolkit for genetic association studies',\u000a      Genetic Epidemiology, 30, 231-247 (2006). DOI.\u000a    \u000a\u000a[5] *Best N, Richardson S and Thomson A., `A\u000a        comparison of Bayesian spatial models for disease\u000a        mapping', Stat Methods Med Res, 14(1), 35-59 (2005). DOI.\u000a    \u000a\u000a[6] Thomas, A., Best, N., Arnold, R.A., and Spiegelhalter, D.J.,\u000a      \"GeoBUGS User Manual,\u000a      Demonstration Version 1.2\" Imperial College and MRC Biostatistics Unit,\u000a      2004, available from\u000a      http:\/\/www.mrc-bsu.cam.ac.uk\/bugs\/winbugs\/geobugs12manual.pdf,\u000a      and also here.\u000a    \u000aSelected Grants:\u000a    [G1] EPSRC, GR\/L10437\/01,\u000a      `Bayesian Population Pharmacokinetic &amp; Pharmadynamic modelling:\u000a      Implementation and model selection', PI: J Wakefield, co-Is: N Best, D\u000a      Spiegelhalter, Project\u000a      partners: GlaxoSmithkline, Pfizer Global R&amp;D, 01\/10\/96-31\/03\/99,\u000a      &#163;123,811\u000a    [G2] ESRC, H519255036,\u000a      `Statistical Analysis of Large Geographical Health and Environmental\u000a      Databases: Methodology Software &amp; Application', PI: N Best,\u000a      01\/02\/98-31\/01\/00, &#163;155,550\u000a    [G3] MRC, `Modelling complexity in biomedical research', PI: N Best,\u000a      01\/04\/99-31\/03\/04, &#163;473,917\u000a    [G4] MRC, `Computational Tools for Bayesian Bioinformatics', PI: Lunn,\u000a      6\/10\/03-30\/9\/06, &#163;135,825\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    [A] The BUGS Project, http:\/\/www.mrc-bsu.cam.ac.uk\/bugs\/\u000a      (archived here\u000a      on 19\/11\/13)\u000a    [B] Reuters article 27\/4\/11, http:\/\/www.reuters.com\/article\/2011\/04\/27\/idUS152367+27-Apr-2011+BW20110427\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/t8f\u000a      on 19\/11\/13)\u000a    [C] OpenBUGS webpage, http:\/\/www.openbugs.info\/w\/\u000a      (archived here\u000a      on 19\/11\/13)\u000a    [D] Review from Prof Carlin, http:\/\/statistics.crcpress.com\/reviews\/the-bugs-book\/\u000a      (archived here)\u000a    [E] Elsevier, Customer Service [statement received, available from\u000a      Imperial on request]\u000a    [F] Senior Acquisitions Editor, Statistics, CRC [statement available from\u000a      Imperial on request]\u000a    [G] http:\/\/store.elsevier.com\/product.jsp?isbn=9780123870209&amp;locale=en_UK(archived\u000a      here).\u000a    [H] Email from VP Head of Pharma Therapeutics Statistics, Pfizer\u000a      Neusentis, Nov 2013 (available\u000a      from Imperial on request)\u000a    [I] http:\/\/www.fda.gov\/MedicalDevices\/DeviceRegulationandGuidance\/GuidanceDocuments\/ucm071072.htm\u000a      (archived here\u000a      on 25\/11\/2013).\u000a    [J] Clements ACA, Pfeiffer DU, Martin V, Pittliglio C, Best N, Thiongane\u000a      Y. \"Spatial risk\u000a      assessment of Rift Valley fever in Senegal\". Vector-Borne Zoonot 7(2),\u000a      (2007), 203-216, DOI.\u000a    [K] Email from Head of Infectious Disease Epidemiology Unit, University\u000a      of Queensland, November\u000a      2013 (available from Imperial on request)\u000a    [L] Meyer R and Millar M. \"BUGS\u000a        in Bayesian stock assessments\". Can. J. Fish. Aquat. Sci. 56:\u000a      1078-1086 (1999), also available here.\u000a    [M] Sam McKechnie, Simon Hoyle (2013). Western and Central Pacific\u000a      Fisheries Commission\u000a      Report http:\/\/www.wcpfc.int\/system\/files\/SA-IP-08-SWO-CPUE-NZ.pdf,\u000a      also available here.\u000a    [N] Alaska Department of Fish and Game, Fishery\u000a        Manuscript Series No. 13-02, also here.\u000a    [O] NOAA Pacific Islands Fisheries Science Center, Bottomfish\u000a        stock assessment 2012, also here.\u000a    ","Title":"\u000a    C4 - BUGS (Bayesian inference using Gibbs sampling)\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Bayesian statistical approaches have several advantages over conventional\u000a      statistical inference\u000a      methods, particularly in situations with sparse data, complex hierarchical\u000a      structure, missing\u000a      information and multiple comparisons, and can result in substantial gains\u000a      in efficiency through the\u000a      formal inclusion of all relevant prior information. Unlike conventional\u000a      statistical analyses, Bayesian\u000a      methods also provide direct probability statements about quantities of\u000a      interest, which enables\u000a      results of complex statistical modelling to be more easily communicated to\u000a      policy makers and end\u000a      users. They also offer a method for formally combining prior information\u000a      with current data to allow\u000a      learning from evidence as it accumulates. However, application of Bayesian\u000a      methods to real-world\u000a      problems was delayed by several decades due to computational difficulties,\u000a      until the development\u000a      of Markov Chain Monte Carlo (MCMC) computational methods in the early\u000a      1990s. Even then,\u000a      applied scientists were still constrained by the need for purpose-written\u000a      computer code to\u000a      implement the MCMC algorithms for each particular problem. This changed\u000a      with the WinBUGS\u000a      software, developed initially in Cambridge from 1989-1996 and then greatly\u000a      expanded at Imperial\u000a      from 1996 onwards, which aimed to make practical MCMC methods available to\u000a      applied\u000a      statisticians.\u000a    In 1996, Nicky Best, Andrew Thomas and the project moved from Cambridge\u000a      to Imperial, and work\u000a      began under Best's direction on expanding the software's capabilities [1].\u000a      In particular, Jon\u000a      Wakefield and Dave Lunn joined the project at this stage to work on\u000a      implementing non-linear\u000a      models, and development of WinBUGS gained momentum. In subsequent years, a\u000a      number of\u000a      other challenging model types were tackled and targeted to application\u000a      areas with many extensions\u000a      to the basic package to ensure wide dissemination [e.g. 1-6, G1-G4]. These\u000a      include (i) GeoBUGS\u000a      that fits spatial models and produces a range of maps as output [6,\u000a      G2-G3], (ii) PKBUGS that fits\u000a      pharmacokinetic\/dynamic models [2, G1], (iii) JumpBUGS that implements\u000a      variable-dimension\u000a      models fitted using reversible jump MCMC [3, G4], (iv) WBDiff\u000a      which allows the numerical solution\u000a      of arbitrary systems of ordinary differential equations (ODEs) within the\u000a      fitted models and (v)\u000a      WBDev which\u000a      enables users to implement their own specialized functions and\u000a      (univariate)\u000a      distributions. The development has been underpinned by the theoretical\u000a      work of Best and her\u000a      group, who have actively implemented in WinBUGS new analysis techniques\u000a      which are at the\u000a      forefront of biostatistical research. This work included research on:\u000a    \u000a      disease mapping and spatial regression, implemented in GeoBUGS [6] &#8212; Bayesian spatial and\u000a        spatio-temporal hierarchical models are now widely used for smoothing\u000a        small area disease\u000a        rates based on sparse data, in order to identify disease clusters or\u000a        general (spatial and\/or\u000a        temporal) trends in disease risk related to possible variations in risk\u000a        factors or\u000a        provision\/access\/uptake of health services, and for spatial prediction\u000a        of health outcomes [5].\u000a      population pharmacokinetic\/dynamic (PK\/PD) models, implemented in\u000a        PKBUGS &#8212; PK\/PD\u000a        models estimate the relationship between a drug dosing regimen, the\u000a        body's exposure to the\u000a        drug as measured by the nonlinear concentration time curve, and the\u000a        drug's efficacy. Such\u000a        analyses are often based on combining a limited number of measurements\u000a        from several\u000a        individuals, and are naturally estimated using Bayesian non-linear\u000a        hierarchical models to\u000a        characterize inter and intra-individual variation, and to enable the\u000a        inclusion of prior information\u000a        based on experience with similar compounds, and for predicting the\u000a        effects of different\u000a        schedules, doses and infusion times [2].\u000a      genetic association studies, implemented in JumpBUGS &#8212; such studies\u000a        involve selecting which\u000a        combination of genotypes out of a typically very large set of candidates\u000a        best predict a given\u000a        phenotype. Standard hypothesis tests and regression methods have high\u000a        error rates and low\u000a        power in such settings, and approaches based on Bayesian model averaging\u000a        (implemented\u000a        using reversible jump MCMC) are a popular alternative that can overcome\u000a        problems of multiple\u000a        hypothesis testing and estimate the probabilities of association\u000a        averaged over a number of\u000a        different model structures [3, 4].\u000a    \u000a    Since 2005, development of the BUGS software has focussed on the OpenBUGS\u000a          project, which is\u000a      an open-source version of the core BUGS code with a variety of interfaces.\u000a      It can run under\u000a      Windows with a very similar graphical interface to WinBUGS, run on Linux\u000a      with a plain-text\u000a      interface, or embedded in R as BRugs. The OpenBUGS project is supported by\u000a      a formal\u000a      Collaboration Agreement between Imperial, MRC Biostatistics Unit and Dr\u000a      Andrew Thomas.\u000a    Key contributors:\u000a    \u000a      Nicky Best, Professor of Statistics and Epidemiology, Imperial College\u000a        London (1996-present).\u000a      Jon Wakefield, Reader in Statistics, Imperial (1990-1999), now Prof at\u000a        U. Washington.\u000a      David Lunn, Research Fellow, Imperial (1996-2007), now at MRC\u000a        Biostatistics Unit, Cambridge.\u000a      Andrew Thomas, Senior Computing Officer, Department of Epidemiology\u000a        and Biostatistics,\u000a        Imperial (1996-2004), now at MRC Biostatistics Unit, Cambridge.\u000a    \u000a    "},{"CaseStudyId":"42267","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The research described in section 2 has increased the use of data and\u000d\u000a      statistics in the\u000d\u000a      management and monitoring of healthcare in the UK. Imperial's work has led\u000d\u000a      to the development of\u000d\u000a      innovative statistical and computational methods for processing large data\u000d\u000a      sets derived from\u000d\u000a      electronic medical records and NHS databases.\u000d\u000a    Findings and recommendations arising from our research for the Bristol\u000d\u000a      Inquiry were reflected in\u000d\u000a      the inquiry outputs, with the importance of routinely collected hospital\u000d\u000a      data highlighted in Ian\u000d\u000a      Kennedy's final Bristol Royal Infirmary Inquiry Report in 2001, `The\u000d\u000a        Report of the Public Inquiry into\u000d\u000a        children's heart surgery at the Bristol Royal Infirmary 1984-1995'\u000d\u000a      [A]:\u000d\u000a    \"From the start of the 1990s a national database existed at the\u000d\u000a        Department of Health (the\u000d\u000a        Hospital Episode Statistics database) which among other things held\u000d\u000a        information about deaths\u000d\u000a      in hospital. It was not recognised as a valuable tool for analysing the\u000d\u000a        performance of hospitals.\u000d\u000a        It is now, belatedly.\"\u000d\u000a    As a direct consequence of our work final recommendations of the Inquiry\u000d\u000a      included:\u000d\u000a    \"Steps should be taken nationally and locally to build the confidence\u000d\u000a        of clinicians in the data\u000d\u000a        recorded in the Patient Administration Systems in trusts (which is\u000d\u000a        subsequently aggregated\u000d\u000a        nationally to form the Hospital Episode Statistics). Such steps should\u000d\u000a        include the establishment\u000d\u000a        by trusts of closer working arrangements between clinicians and clinical\u000d\u000a        coding staff.\"\u000d\u000a\u0009\u0009\"The Hospital Episode Statistics database should be supported as a\u000d\u000a        major national resource\u000d\u000a        which can be used reliably, with care, to undertake the monitoring of a\u000d\u000a        range of healthcare\u000d\u000a        outcomes.\" [A]\u000d\u000a    The Commission for Healthcare Improvement (CHI, now called the Care\u000d\u000a      Quality Commission or\u000d\u000a      CQC) took forward the recommendations of the Bristol Inquiry from July\u000d\u000a      2001, which to this day\u000d\u000a      uses Hospital Episode Statistics to monitor healthcare performance (for\u000d\u000a      example, `CQC indicators\u000d\u000a      for mortality and emergency readmissions using Hospital Episode Statistics\u000d\u000a      (HES)', May 2013 [B]).\u000d\u000a    Our contribution to the Shipman Inquiry was recognised in the final\u000d\u000a      report by Dame Janet Smith [C,\u000d\u000a      paragraph 14.27]:\u000d\u000a    \"I am most grateful to Dr Aylin and his colleagues for the work that\u000d\u000a        they have done for the\u000d\u000a        Inquiry. It is innovative and, as I had hoped, it has made a real\u000d\u000a        contribution to the debate about\u000d\u000a        the feasibility and the value of setting up a system for the routine\u000d\u000a        monitoring of mortality rates\u000d\u000a        among the patients of GPs.\"\u000d\u000a    A number of recommendations arose as a direct result of this work [C,\u000d\u000a      Recommendations,\u000d\u000a      paragraphs 22-44, page 53] :\u000d\u000a    The Department of Health (DoH) must take the lead in developing a\u000d\u000a        national system for\u000d\u000a        monitoring GP patient mortality rates. The system should be supported by\u000d\u000a        a well-organised,\u000d\u000a        consistent and objective means of investigating those cases where a GP's\u000d\u000a        patient mortality\u000d\u000a        rates signal as being above the norm.\u000d\u000a    Every GP practice should keep a death register in which particulars of\u000d\u000a        the deaths of patients of\u000d\u000a        the practice should be recorded for use in audit and for other purposes.\u000d\u000a    PCTs should undertake reviews of the medical records of deceased\u000d\u000a        patients, either on a routine\u000d\u000a        periodic basis (if resources permit) or on a targeted basis limited to\u000d\u000a        those GPs whose\u000d\u000a        performance gives rise to concern.\u000d\u000a    The above recommendations are detailed further in Chapter 27, `Proposals\u000d\u000a      for Change &#8212; The Use\u000d\u000a      of Mortality Data as a Clinical Governance Tool: A National System of\u000d\u000a      Monitoring' (27.105-27.107,\u000d\u000a      [C]), and Chapter 14, `The Monitoring of Mortality Rates among the\u000d\u000a      Patients of General\u000d\u000a      Practitioners' [C]. Chapter 14 details the exact contribution of Aylin,\u000d\u000a      Best, Bottle, Marshall and the\u000d\u000a      Imperial team to the Inquiry (14.23-14.71, [C]). Chapter 14 described how\u000d\u000a      \"CUSUM charts could be\u000d\u000a        used to monitor patient mortality rates at GP level and that they would\u000d\u000a        have been capable of\u000d\u000a        detecting Shipman if they had been in use at the relevant time\"\u000d\u000a      (14.65, [C]).\u000d\u000a    The methodology developed and published in [4] and [5] is the methodology\u000d\u000a      that now underpins\u000d\u000a      both (i) our Real Time Monitoring System which is currently used by 70% of\u000d\u000a      English NHS acute\u000d\u000a      trusts to assist them in monitoring a variety of casemix adjusted outcomes\u000d\u000a      at the level of diagnosis\u000d\u000a      group and procedure group [D], and (ii) the CQC mortality outliers\u000d\u000a      programme that looks at\u000d\u000a      patterns of death rates within NHS trusts and is used to generate the\u000d\u000a      quarterly alerts of trusts with\u000d\u000a      high mortality [E]. The process involves analysing data that suggests\u000d\u000a      concerning trends in the\u000d\u000a      death rate for specific conditions or operations, with the trends being\u000d\u000a      calculated using SPC charts\u000d\u000a      [F]. All of the outliers are calculated using patient-level data from\u000d\u000a      hospitals which become part of a\u000d\u000a      national HES system. Some outliers are calculated by Imperial College (Dr\u000d\u000a      Foster Unit), while\u000d\u000a      others are calculated by the CQC [F].\u000d\u000a    Imperial's mortality alerting system has also been pivotal in alerting\u000d\u000a      the then Healthcare\u000d\u000a      Commission (HCC) to problems at the Mid Staffordshire NHS Foundation Trust\u000d\u000a      [G], which has\u000d\u000a      been the centre of a number of investigations and national inquiries.\u000d\u000a      Through the HCC's\u000d\u000a      programme to analyse mortality rates in England, it received an\u000d\u000a      unprecedented 11 alerts about\u000d\u000a      high mortality at the trust, four of which were after the investigation\u000d\u000a      was launched. Of the seven\u000d\u000a      alerts that were received prior to the launch of the investigation, four\u000d\u000a      came from the Dr Foster\u000d\u000a      Research Unit at Imperial College as part of its analysis of data [G,\u000d\u000a      Appendix E].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Statistical analysis and methodological development carried out by\u000d\u000a      Imperial College London on\u000d\u000a      data from the Bristol Royal Infirmary Inquiry and the Shipman Inquiry have\u000d\u000a      led to new monitoring\u000d\u000a      systems in healthcare. Using routinely collected healthcare information,\u000d\u000a      we have highlighted\u000d\u000a      variations in performance and safety, impacting the NHS through direct\u000d\u000a      interventions and\/or policy\u000d\u000a      change. For example: (i) findings and recommendations arising from our\u000d\u000a      research for the Bristol\u000d\u000a      Inquiry were reflected in the final inquiry outputs, which highlighted the\u000d\u000a      importance of routinely\u000d\u000a      collected hospital data to be used to undertake the monitoring of a range\u000d\u000a      of healthcare outcomes,\u000d\u000a      (ii) a range of monitoring recommendations have arisen as a direct result\u000d\u000a      of the research on data\u000d\u000a      from the Shipman Inquiry, (iii) analytical tools based on our\u000d\u000a      methodological research are used by\u000d\u000a      managers and clinicians in over two thirds of NHS hospital trusts, (iv)\u000d\u000a      Imperial's monthly mortality\u000d\u000a      alerts to the Care Quality Commission were major triggers leading to the\u000d\u000a      Healthcare Commission\u000d\u000a      investigation into the Mid Staffordshire NHS Trust.\u000d\u000a    ","ImpactType":"Health","Institution":"\u000d\u000a    Imperial College London\u000d\u000a    ","Institutions":[{"AlternativeName":"Imperial College London","InstitutionName":"Imperial College London","PeerGroup":"A","Region":"London","UKPRN":10003270}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    (* References that best indicate quality of underpinning research)\u000d\u000a    \u000a[1] * Aylin, P., Alves, B., Best, N., Cook, A., Elliott,\u000d\u000a        P., Evans, S.J., Lawrence, A.E., Murray, G.D.,\u000d\u000a      Pollack, J., Spiegelhalter, D., \"Comparison of UK paediatric cardiac\u000d\u000a        surgical performance by\u000d\u000a        analysis of routinely collected data 1984-96: was Bristol an outlier?\",\u000d\u000a      Lancet, 358, 181-187\u000d\u000a      (2001). DOI.\u000d\u000a    \u000a\u000a[2] * Spiegelhalter, D.J., Aylin, P., Best, N.G., Evans,\u000d\u000a      S.J.W., and Murray, G.D., \"Commissioned\u000d\u000a        analysis of surgical performance using routine data: lessons for the\u000d\u000a        Bristol Inquiry\", Journal of\u000d\u000a      the Royal Statist. Soc. A, 165, 191-231 (2002). DOI.\u000d\u000a    \u000a\u000a[3] Aylin, P., Best, N., Bottle, A., Marshall,\u000d\u000a        C., \"Following Shipman: a pilot system for monitoring\u000d\u000a        mortality rates in primary care\", Lancet, 362, 485-491 (2003). DOI.\u000d\u000a    \u000a\u000a[4] * Marshall, C., Best, N. G., Bottle, A. and Aylin,\u000d\u000a        P., \"Statistical issues in the prospective\u000d\u000a        monitoring of health outcomes across multiple units\", Journal of the\u000d\u000a      Royal Statist. Soc. A, 167,\u000d\u000a      541-559 (2004). DOI.\u000d\u000a    \u000a\u000a[5] Bottle, A., &amp; Aylin, P. Intelligent\u000d\u000a        information: A national system for monitoring clinical\u000d\u000a        performance. Health Services Research, 43, 10-31 (2008). DOI.\u000d\u000a    \u000aGrants:\u000d\u000a    [G1] Bristol Royal Infirmary Inquiry (1999-2000; &#163;72,080), Principal\u000d\u000a      Investigator (PI) P. Aylin,\u000d\u000a      \"Analysis of HES data\".\u000d\u000a    [G2] The Shipman Inquiry (2001-2002; &#163;96,190), PI P. Aylin, \"Monitoring\u000d\u000a      of mortality rates in\u000d\u000a      Primary Care, The Shipman Inquiry.\"\u000d\u000a    [G3] Dr Foster Intelligence (2006-2010; &#163;2,034,235), PI P. Aylin,\u000d\u000a      \"Explaining variations in outcome\u000d\u000a      in healthcare across England\"\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    [A] `The Report of the Public Inquiry into children's heart surgery at\u000d\u000a      the Bristol Royal Infirmary\u000d\u000a      1984-1995: learning from Bristol',\u000d\u000a      http:\/\/www.tsoshop.co.uk\/bookstore.asp?Action=Book&amp;ProductId=9780101520720\u000d\u000a      (report\u000d\u000a      available here)\u000d\u000a    [B] CQC guidance document, `CQC indicators for mortality and emergency\u000d\u000a      readmissions using\u000d\u000a      Hospital Episode Statistics (HES)', May 2013,\u000d\u000a      http:\/\/www.cqc.org.uk\/sites\/default\/files\/media\/documents\/nhs_hes_qrp_data_item_guidancefo\u000d\u000a        r_publication.pdf (archived here)\u000d\u000a    [C] Dame Janet Smith, `The Shipman Inquiry. Fifth Report &#8212;\u000d\u000a        Safeguarding Patients: Lessons from\u000d\u000a        the Past &#8212; Proposals for the Future', 9\/12\/04,\u000d\u000a      http:\/\/webarchive.nationalarchives.gov.uk\/20090808154959\/http:\/\/www.the-shipman-inquiry.org.uk\/fifthreport.asp (PDF archived here).\u000d\u000a      See Recommendations (pp49-65), Chapter\u000d\u000a      14 (The Monitoring of Mortality Rates among the Patients of General\u000d\u000a      Practitioners, The\u000d\u000a      Inquiry's Approach: The Commissioning of Work from Dr Paul Aylin and His\u000d\u000a      Team, pp411-423),\u000d\u000a      and Chapter 27 (Proposals for Change, The Use of Mortality Data as a\u000d\u000a      Clinical\u000d\u000a      Governance Tool: A National System of Monitoring, pp1123-1178)\u000d\u000a    [D] Real Time Monitoring (RTM). Enabling providers and commissioners to\u000d\u000a      benchmark and\u000d\u000a      monitor clinical outcomes. http:\/\/drfosterintelligence.co.uk\/solutions\/nhs-hospitals\/real-time-monitoring-rtm\/\u000d\u000a      (archived here\u000d\u000a      on 26\/11\/13)\u000d\u000a    [E] CQC quarterly mortality outlier reports, http:\/\/www.cqc.org.uk\/public\/reports-surveys-and-reviews\/reports\/mortality-outlier-reports (archived at\u000d\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/z1f\u000d\u000a      on 30\/10\/13).\u000d\u000a    [F] CQC mortality outliers programme (archived here)\u000d\u000a      and explanatory text (June 2012, archived\u000d\u000a      here).\u000d\u000a    [G] Investigation into Mid Staffordshire NHS Foundation trust, Healthcare\u000d\u000a      Commission, March\u000d\u000a      2009,\u000d\u000a      http:\/\/www.midstaffspublicinquiry.com\/sites\/default\/files\/Healthcare_Commission_report_on_Mid_Staffs.pdf\u000d\u000a      (archived here).\u000d\u000a    ","Title":"\u000d\u000a    C5 - Improving the safety and quality of healthcare delivery using\u000d\u000a      routine\u000d\u000a      data: improved statistical monitoring techniques\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2654675","Name":"Bristol"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    We have used routinely collected clinical and administrative data to\u000d\u000a      examine variations in quality\u000d\u000a      and safety in healthcare.\u000d\u000a    Work by Jarman et al published in 1999 first established that there was\u000d\u000a      substantial variation in\u000d\u000a      mortality between hospitals in England. Starting from work to look at\u000d\u000a      paediatric cardiac surgical\u000d\u000a      outcomes commissioned by the Bristol Royal Infirmary Inquiry in 1999 [G1],\u000d\u000a      we confirmed serious\u000d\u000a      concerns around the surgical outcomes at Bristol, and established the\u000d\u000a      usefulness of routine\u000d\u000a      administrative data (Hospital Episode Statistics) in helping to identify\u000d\u000a      quality of care issues [1].\u000d\u000a      Three levels of analysis of increasing sophistication were carried out.\u000d\u000a      The reasonable consistency\u000d\u000a      of the results arising from different sources of data, together with a\u000d\u000a      number of sensitivity analyses,\u000d\u000a      led to conclusion that there had been excess mortality in Bristol in open\u000d\u000a      heart operations on\u000d\u000a      children under one year of age [1]. Paper [2] developed the underlying\u000d\u000a      statistical methodology\u000d\u000a      used for the Bristol Royal Infirmary work, including techniques to\u000d\u000a      identify `divergent' as opposed to\u000d\u000a      just `extreme' performance, and estimation of uncertainty intervals on\u000d\u000a      hospital ranks. The potential\u000d\u000a      statistical role in future programmes for monitoring clinical performance\u000d\u000a      was also highlighted in this\u000d\u000a      paper, including use of cumulative sums risk adjusted outcomes and the\u000d\u000a      need for appropriate\u000d\u000a      statistical adjustment when a large number of comparisons are made, to\u000d\u000a      avoid the danger of\u000d\u000a      excessive false positive results arising from the naive use of\u000d\u000a      significance tests[2].\u000d\u000a    In further research commissioned by the Shipman Inquiry in 2001 [G2], we\u000d\u000a      established the role that\u000d\u000a      statistical process control (SPC) charts (specifically log-likelihood\u000d\u000a      CUSUM, or cumulative sum\u000d\u000a      control charts), and other routinely collected data (from death\u000d\u000a      certificates) could play in the\u000d\u000a      continuous surveillance of healthcare outcomes, and in this specific case,\u000d\u000a      the detection of unusual\u000d\u000a      patterns of patient mortality within General Practices [3]. This work\u000d\u000a      required developing underlying\u000d\u000a      statistical methodology for detecting unusual patterns of mortality [4].\u000d\u000a      We considered some of the\u000d\u000a      methodological and practical aspects that surround the routine\u000d\u000a      surveillance of health outcomes\u000d\u000a      and, in particular, we focussed on two important methodological issues\u000d\u000a      that arise when attempting\u000d\u000a      to extend SPC charts to monitor outcomes at more than one unit\u000d\u000a      simultaneously: the need to\u000d\u000a      acknowledge the inevitable between-unit variation in `acceptable'\u000d\u000a      performance outcomes due to\u000d\u000a      the net effect of many small unmeasured sources of variation (e.g.\u000d\u000a      unmeasured case mix and data\u000d\u000a      errors) and the problem of multiple testing over units as well as time. We\u000d\u000a      addressed the former by\u000d\u000a      using quasi-likelihood estimates of over dispersion, and the latter by\u000d\u000a      using methods based on\u000d\u000a      estimation of false discovery rates. An application of this approach to\u000d\u000a      annual monitoring `all-cause'\u000d\u000a      mortality data between 1995 and 2000 from 169 National Health Service\u000d\u000a      hospital trusts in England\u000d\u000a      and Wales was presented [4].\u000d\u000a    Building on the statistical foundations established in [4], we have also\u000d\u000a      developed a national\u000d\u000a      surveillance tool, the Real-Time Monitoring System (RTM as it is known),\u000d\u000a      designed to monitor\u000d\u000a      hospital outcomes across a range of diagnosis and procedure groups in near\u000d\u000a      real time with data\u000d\u000a      updated monthly [5][G3]. RTM implements statistical procedures for setting\u000d\u000a      alarm thresholds based\u000d\u000a      on false alarm rates within CUSUM charts for multiple institutions, with\u000d\u000a      automated multiple risk\u000d\u000a      adjustment methods.\u000d\u000a    Key personnel:\u000d\u000a    \u000d\u000a      Professor N Best, Professor of Statistics and Epidemiology, Faculty of\u000d\u000a        Medicine, School of\u000d\u000a        Public Health, Imperial College London, 1996-present.\u000d\u000a      Dr P Aylin, Clinical Reader in Epidemiology &amp; Public Health,\u000d\u000a        Faculty of Medicine, School of\u000d\u000a        Public Health, Imperial College London, 1997-present\u000d\u000a      Dr Alex Bottle, Senior Lecturer in Statistics, Faculty of Medicine,\u000d\u000a        School of Public Health,\u000d\u000a        Imperial College London, 1998-present\u000d\u000a    \u000d\u000a    "},{"CaseStudyId":"42268","Continent":[{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2186224","Name":"New Zealand"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The above research, in particular the WMTSA book [5], has had widespread\u000a      impact in a variety of sectors, as is detailed below.\u000a    There have been four software implementations of the highly cited\u000a      (&gt;2000 citations, [A]) WMTSA book that have had widespread use:\u000a    (1) TIBCO Spotfire S+ Wavelets: This software was developed\u000a    primarily by Bill Constantine at Insightful (owners of S+). It is based very\u000a    heavily on WMTSA. This is acknowledged in [B]: \"The new methodology\u000a    implemented in S+Wavelets 2.0 stems almost entirely from Don Percival's book\u000a    entitled, Wavelet Methods for Time Series Analysis, co-authored by Andrew\u000a    Walden and published by Cambridge University Press in 2001\" [B].\u000a    Subsequently the S+ software was sold to TIBCO [C] and v8.1 incorporated\u000a      into their Spotfire system in 2008 (see under Powerful New Statistics)\u000a      [D]. The press release announcing the release of TIBCO Spotfire S+&#174;\u000a      version 8.1 states \"Spotfire S+&#174; is the only statistical programming\u000a        platform that delivers a fully integrated development environment, a\u000a        commercially supported analytic packaging system, and the ability to\u000a        scale a desktop to manipulate gigabyte class data sets. Spotfire S+\u000a        enables statisticians and business analysts to prototype, test, and\u000a        deploy analytics much faster than with alternative statistical modeling\u000a        environments. It delivers a wider range of robust statistics tools, as\u000a        well as improved deployment and integration capabilities that help\u000a        business analysts and researchers make informed and reliable decisions\u000a        at critical points across the organization\" [D]. Specifically the\u000a      Wavelets Package is described as providing \"advanced signal and image\u000a        analysis, time series analysis, statistical signal estimation, and data\u000a        compression analysis\" [E]. Spotfire customers span the Life\u000a      Sciences, Financial Services, Energy, Government, Consumer Goods,\u000a      Manufacturing and Telecommunications sectors [F]. Companies include GE,\u000a      Chevron, PerkinElmer Inc., GlaxoSmithKline, Cisco, the BNP Paribas,\u000a      Aberdeen Group, Salesforce, and BUCS Analytics. 65 US Government agencies\u000a      including the CIA, NSA, Defense Advanced Research Projects Agency (DARPA)\u000a      and the Defense Threat Reduction Agency (DTRA) use Spotfire S+ [F].\u000a    (2) WMTSA: a MATLAB toolkit developed by Charlie Cornish\u000a      (Department of Atmospheric Sciences, University of Washington).This is\u000a      widely used in fMRI (functional Magnetic Resonance Imaging). For example,\u000a      it is used in the processing pipeline by the Brain Mapping Unit at\u000a      Cambridge [G] and its Neuroscience use is frequently acknowledged (e.g.,\u000a      three papers in 2010 and 2012 are given in [H]). It is recommended\u000a      software (2010) used by the international company Disha Life Sciences Ltd\u000a      for Metabolomics (regulation and fluxes in cells) [p. 88 of I]).\u000a    (3) Waveslim: R package for \"basic wavelet routines for time\u000a      series (1D), image (2D) and array (3D) analysis\" [J] developed by Brandon\u000a      Whitcher. The waveslim code is based in part \"on wavelet methodology\u000a        developed in Percival and Walden (2000)...\" [J]. This has been\u000a      extensively used with applications ranging from a Reserve Bank of New\u000a      Zealand document on measuring core inflation in New Zealand [K] to studies\u000a      of copy number alterations in breast cancer led by Fred Hutchinson Cancer\u000a      Research Centre [L].\u000a    (4) wmtsa: a package developed by Bill Constantine and Don\u000a      Percival in the R programming language [M]. As with the MATLAB\u000a      distribution, this software has also found use in fMRI (e.g. [N]).\u000a    As a further specific example of the use of WMTSA in the biomedical\u000a      sector, in an email Dr. Brandon Whitcher (now Senior R Consultant, Mango\u000a      Solutions) confirms the use of methodology from WMTSA to analyse\u000a      univariate and bivariate time series in support of early-phase drug\u000a      development at GlaxoSmithKline during the period 2005-2009. The\u000a      methodology was applied \"in the quantitative analysis of both\u000a        functional and pharmacological MRI (magnetic resonance imaging)\u000a        experiments in a variety of pre-clinical models for the neurology and\u000a        psychiatry therapeutic areas. It facilitated the rapid and efficient\u000a        processing of time series data, and produced easily interpretable\u000a        results to the imaging scientists that supported our internal\u000a        decision-making process.\" [O]\u000a    Monitoring of Santa Maria del Fiore Dome in Florence\u000a    Based on the MODWPT algorithm introduced by Walden &amp; Contreras\u000a      Cristan, and included in WMTSA, Gabbanini, Vannucci et al [P] used wavelet\u000a      packet variances to analyse crack widths in the famous dome of the Santa\u000a      Maria del Fiore Cathedral in Florence. Their analysis revealed \"interesting\u000a        aspects regarding the dynamics of crack evolutions and the structural\u000a        functions of the different elements of the dome\". The influence of\u000a      this work is confirmed in the OPA Workshop on Monitoring of Great\u000a      Historical Structures (Florence, January 2012) discussion \"60 Years\u000a      Results of the Monitoring System on Santa Maria del Fiore Dome in\u000a      Florence\" by Blasi &amp; Ottoni which cites Gabbanini, Vannucci et al as\u000a      one of the \"essential references for Santa Maria del Fiore monitoring\u000a      issue\" [Q].\u000a    Ontario Ministry of Natural Resources\u000a    Methodology from WMTSA was used in the modelling of tree taper of jack\u000a      pine (Pinus banksiana) trees grown in the Canadian boreal forest region in\u000a      a study carried out by the Ontario Ministry of Natural Resources [R]. A\u000a      key point in their model fitting analysis is the use of reflecting\u000a      boundary conditions: \"Following Percival and Walden (2000, p. 140)...\"The\u000a      results of the study \"opens new possibilities for analysing\u000a        longitudinal or taper data collected across time or space.\"\"\u000a    Finance\u000a    Marco J. van der Burgt [S] of Atradius Group Risk Management (Amsterdam)\u000a      used the MODWT algorithm and wavelet variance results as given in WMTSA to\u000a      analyse monthly observed default rates to answer the question \"how long\u000a        is a business cycle and where are we in the business cycle?\" This\u000a      enabled the inclusion of business cycle effects in default probability\u000a      validation.\u000a    ","ImpactSummary":"\u000a    Methodological, algorithmic and interpretational advances in wavelet\u000a      techniques for time series analysis are encapsulated in the research\u000a      monograph by Percival and Walden (2000): \"Wavelet Methods for Time Series\u000a      Analysis\" (WMTSA). Multiple language software packages have been developed\u000a      from the book's contents, including the Spotfire S+ package from the major\u000a      commercial software company TIBCO (2008-present). TIBCO Spotfire clients\u000a      span many sectors and include major companies such as GE, Chevron,\u000a      GlaxoSmithKline and Cisco. Further applications of the wavelet techniques\u000a      developed in WMTSA include in the biomedical, conservation and financial\u000a      sectors. WMTSA is used, for example, in functional Magnetic Resonance\u000a      Imaging by GlaxoSmithKline, to monitor cracks in the dome of the UNESCO\u000a      world heritage site Santa Maria del Fiore Cathedral in Florence, and by\u000a      the Reserve Bank of New Zealand in its analysis of measuring core\u000a      inflation.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Imperial College London\u000a    ","Institutions":[{"AlternativeName":"Imperial College London","InstitutionName":"Imperial College London","PeerGroup":"A","Region":"London","UKPRN":10003270}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"3176959","Name":"Florence"},{"GeoNamesId":"2759794","Name":"Amsterdam"}],"References":"\u000a    (* References that best indicate quality of underpinning research)\u000a    \u000a[1] E. J. McCoy and A. T. Walden, `Wavelet analysis\u000a        and synthesis of stationary long-memory processes', Journal of\u000a      Computational and Graphical Statistics, 5, 26-56 (1996). DOI\u000a    \u000a\u000a[2] Walden, A.T. and Contreras Cristan, A., `The\u000a        phase-corrected undecimated discrete wavelet packet transform and its\u000a        application to interpreting the timing of events', Proc. R. Soc.\u000a      Lond. A, 454, 2243-2266 (1998). DOI\u000a    \u000a\u000a[3] *Walden, A.T., Percival, D.B. and McCoy, E.J., `Spectrum\u000a        estimation by wavelet thresholding of multitaper estimators', IEEE\u000a      Transactions on Signal Processing, 46, 3153-3165 (1998). DOI\u000a    \u000a\u000a[4] *Serroukh A., Walden, A.T. and Percival, D.B., `Statistical\u000a        properties and uses of the wavelet variance estimator for the scale\u000a        analysis of time series'. J. Am. Stat. Assoc., 95, 184-196 (2000). DOI\u000a    \u000a\u000a[5] *D. B. Percival and A. T. Walden, `Wavelet\u000a          Methods for Time Series Analysis', Cambridge University Press,\u000a      594pp, Hb, (2006).\u000a    \u000a\u000a[6] S. Olhede and A. T. Walden, `The Hilbert spectrum\u000a        via wavelet projections', Proc. R. Soc. Lond. A, 460, 955-975\u000a      (2004). DOI\u000a    \u000aGrants:\u000a    [G1] EPSRC, GR\/J62715\/01,\u000a      `Time Series Analysis using the Discrete Wavelet Transform', PI: A.T.\u000a      Walden, 01\/10\/94-31\/03\/98, &#163;124,878\u000a    [G2] EPSRC, GR\/L11182\/01,\u000a      `Advances in the theory and practice of multiwavelets', PI: A.T. Walden,\u000a      20\/01\/97-19\/07\/00, &#163;130,646\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000a    [A] WMTSA citation count, http:\/\/scholar.google.com\/citations?user=Ki2Ig9YAAAAJ&amp;hl=en\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/ktf)\u000a    [B] S+WAVELETS Version 2 information page, http:\/\/www.msi.co.jp\/splus\/addon\/wave2.html\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/ltf\u000a      on 9\/10\/13)\u000a    [C] TIBCO press release, 3\/9\/08,\u000a      http:\/\/www.tibco.com\/company\/news\/releases\/2008\/press924.jsp\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/ttf\u000a      on 9\/10\/13)\u000a    [D] `TIBCO Reveals Industry's Most Flexible Statistics-Driven Analytic\u000a      Platform' press release, 8\/12\/08, http:\/\/www.tibco.com\/company\/news\/releases\/2008\/press939.jsp\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/mtf\u000a      on 9\/10\/13)\u000a    [E] TIBCO Spotfire, `What's new in v8.1',http:\/\/spotfire.tibco.com\/~\/media\/content-center\/datasheets\/whats-new-splus-8-1.ashx\u000a      (available here)\u000a    [F] Who uses Spotfire? webpage, http:\/\/spotfire.tibco.com\/en\/discover-spotfire\/who-uses-spotfire.aspx\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/hwf\u000a      on 15\/10\/13) and Spotfire Case Studies webpage, http:\/\/spotfire.tibco.com\/en\/resources\/content-center.aspx?Content%20Type=Case%20Studies#content-center.aspx?Content%20Type=Case%20Studies%2C\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/jwf\u000a      on 15\/10\/13)\u000a    [G] Brain Mapping Unit webpage, University of Cambridge, https:\/\/wiki.cam.ac.uk\/bmuwiki\/FMRI\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/ntf\u000a      on 9\/10\/13)\u000a    [H] Three neuroscience papers using WTMSA for fMRI analysis: DOI(1),\u000a      DOI(2), DOI(3)\u000a    [I] Disha Life Sciences presentation, www.scribd.com\/doc\/27187414\/In-Silico-Analysis-to-Metabolomics\u000a      (pages 1 &amp; 88 available here)\u000a    [J] `waveslim' R software information page, http:\/\/cran.r-project.org\/web\/packages\/waveslim\/index.html\u000a      (archived here\u000a      on 9\/10\/13)\u000a    [K] `Using wavelets to measure core inflation: the case of New Zealand',\u000a      Reserve Bank of New Zealand, May 2009,\u000a      http:\/\/www.rbnz.govt.nz\/research_and_publications\/discussion_papers\/2009\/dp09_05.pdf\u000a      (archived here)\u000a    [L] Breast cancer paper using Waveslim, May 2011: DOI\u000a    [M] `wmtsa' R software information page, http:\/\/cran.r-project.org\/web\/packages\/wmtsa\/index.html\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/stf\u000a      on 9\/10\/13)\u000a    [N] Paper using WMTSA for fMRI analysis: DOI\u000a    [O] Email from Senior R Consultant, Mango Solutions, formerly of\u000a      GlaxoSmithKline, 17\/10\/12 (available from Imperial on request)\u000a    [P] `Wavelet Packet Methods for the Analysis of Variance of Time Series\u000a      With Application to Crack Widths on the Brunelleschi Dome', DOI\u000a    [Q] OPA Workshop abstract, (available here)\u000a    [R] `Applying wavelet-based functional approach in modelling tree taper',\u000a      2\/8\/11, DOI\u000a    [S] M. van der Burgt, `Wavelet analysis of business cycles for\u000a        validation of probability of default: what is the infb02uence of the\u000a        current credit crisis on model validation?', The Journal of Risk\u000a      Model Validation, Vol 3, No 1, 3-22 (2009) (available here)\u000a    ","Title":"\u000a    C6 - Wavelet analysis techniques developed into multiple software\u000a      packages and widely used internationally including in the biomedical,\u000a      conservation and financial sectors\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Professor Andrew Walden started working on wavelets initially via the\u000a      linkage between wavelets and long-memory time series [G1], resulting in\u000a      publication [1] with Dr Emma McCoy in 1996. With Alberto Contreras\u000a      Cristan, he then studied the maximal overlap (undecimated) discrete\u000a      wavelet transform (MODWT) and maximal overlap wavelet packet transforms\u000a      (MODWPT). In particular the optimum time shifts to apply to ensure\u000a      approximate zero phase fb01ltering at every level of the transform were\u000a      derived, and applied to the wavelet packet coefficients to give phase\u000a      corrections which ensure alignment with the original time series, making\u000a      the analysis methodology useful to physical scientists [2]. In addition,\u000a      in the context of matching pursuit, they showed how to expand the MODWT\u000a      dictionary in a physically sensible way while maintaining key theoretical\u000a      properties.\u000a    A wavelet thresholding scheme for multitaper spectral estimators was\u000a      derived with McCoy and Don Percival of the University of Washington [3]\u000a      and is much simpler and preferable to the previously proposed scheme\u000a      involving the periodogram. The `complete' pilot spectrum estimator of\u000a      Blackman and Tukey was updated and extended by using the MODWPT in work\u000a      with Eva Tsakiroglou. The statistical properties of the wavelet variance\u000a      estimator for the scale analysis of time series were studied with Abdeslam\u000a      Serroukh and Percival [4]. The asymptotic distribution of the MODWT-based\u000a      wavelet variance estimator was derived for a wide class of stochastic\u000a      processes, not necessarily Gaussian or linear. It was shown how to\u000a      estimate the variance of the estimator using spectral methods.\u000a    In 1998 Percival and Walden largely completed the research monograph\u000a      WMTSA [5]; this incorporated the aforementioned underpinning research.\u000a      Percival contributed introductory, qualitative and graphical explanatory\u000a      material on transforms, and his own research results, particularly\u000a      applications to long-memory processes. Both authors took a novel filtering\u000a      approach to the mathematical exposition (rendering results more easily\u000a      used in a statistical context) including a proper treatment of the\u000a      boundary wavelet coefficients, details and smooths showing how to\u000a      delineate reliable values free of end-effects, of great importance to\u000a      practitioners. The impact of the book led to demand for a Chinese\u000a      translation, published by the China Machine Press (2006).\u000a    Walden's research continued with a comparison of wavelets versus wavelet\u000a      packets for power spectrum estimation with Cristan and the new area of\u000a      wavelet analysis of matrix-valued times series, closely connected to\u000a      multiwavelets, with Abdeslam Serroukh (grant [G2]). Sofia Olhede joined\u000a      the research group in 2000 as focus moved to continuous wavelet\u000a      transforms, and particularly analysis of Morse wavelets, resulting in\u000a      articles showing applications to polarization in earthquakes and Doppler\u000a      ultrasound. Analysis of multicomponent signals and the development of\u000a      wavelet methodologies as an alternative to empirical mode decomposition\u000a      followed (e.g., [6]). Further work took place on the use of analytic\u000a      signals methodology for time series analysis, resulting in analytic\u000a      thresholding and novel directional denoising schemes.\u000a    Statistics Section contributors:\u000a    \u000a      Andrew Walden, Professor of Statistics, Imperial College\u000a        (1990-present)\u000a      Emma McCoy, initially an RA supported on grant [G1], now Senior\u000a        Lecturer in Statistics, Imperial College (1996-present)\u000a      Alberto Contreras Cristan, PhD student, Imperial College (1994-1998)\u000a      Evangelia Tsakiroglou, PhD student, Imperial College (1997-2000)\u000a      Abdeslam Serroukh, RA supported on grants [G1, G2], Imperial College\u000a        (1996-99)\u000a      Sofia Olhede, Senior Lecturer in Statistics, Imperial College\u000a        (2002-2007), now Pearson Professor of Statistics, UCL (2007-present).\u000a    \u000a    "},{"CaseStudyId":"42269","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    The tools developed by Hall, Ruban, Wu and Mughal at Imperial have become\u000a      a cornerstone of AIRBUS UK's laminar flow research and were used to\u000a      interpret flight test data from the TELFONA programme in 2009 [G2, A]. The\u000a      group is at present working closely with EADS\/Airbus on the design of the\u000a      `Smart Fixed Wing', to be flight tested in 2015 as part of the Clean Skies\u000a      initiative [B]. The Imperial group will also be fully involved with the\u000a      interpretation of the flight test data [C]. A major part of the\u000a      collaboration with EADS is the secondment of staff between academia and\u000a      industry to help translate the academic research to EADS, and onto Airbus,\u000a      and enable Imperial researchers to be fully aware of the complications\u000a      associated with real wings [D].\u000a    For commercial jets, the two major manufacturers are AIRBUS (owned by\u000a      EADS) and BOEING. In 2011, EADS\/AIRBUS invested ~&#163;1m pounds in LFC-UK [G4]\u000a      which, together with ~&#163;4m from EPSRC [G3], enabled us to provide the\u000a      theoretical tools to underpin the development of laminar flow wings. The\u000a      commitment of AIRBUS was a result of our previous successful\u000a      collaborations, which had, for example, involved the successful\u000a      interpretation of flight test data from the TELFONA project [5, C, E].\u000a    Within the period 2008-2013, EADS\/AIRBUS progressively deployed variants\u000a      of the Imperial group's PSE methods to predict transition rather than use\u000a      old-fashioned crude methods, and many areas of research from Imperial have\u000a      \"proved invaluable to EADS\" [D]:\u000a    \u000a      The work on receptivity theory has given EADS a rational method to\u000a        input disturbances into laminar flows. For example [1], shows how free\u000a        stream turbulence and acoustic waves generate disturbances in\u000a        predominantly 2D flows and, taken with earlier contributions by Ruban, \"provide\u000a          the cornerstone for modelling receptivity in the industrial\u000a          environment\" [D]. Paper [3] demonstrated how the receptivity ideas\u000a        could be used to model flows over randomly distributed surface roughness\u000a        typical of that found on airfoils. The results will play a \"key role\u000a          in the specification of manufacturing tolerances for the next\u000a          generation of wings developed at [EADS\/Airbus UK]\" [D].\u000a      The work on the 3D transition prediction method, paper [4], has had \"major\u000a          impact\". This paper \"opened up the way for industry to predict\u000a          transition within the RANS-Solar CFD method used for design purposes\"\u000a        [D]. EADS is supporting Mughal at Imperial to take the work forward in\u000a        order to understand the effect of 3D waviness induced by manufacturing\u000a        on in flight loading on transition. This work will contribute to the\u000a        flight test analysis of the EU JTI Clean Sky `BLADE' wing in 2015 and\u000a        represents \"the `state of the art' of transition prediction in 3D\u000a          flows\" [D].\u000a      EADS and Airbus have also taken great interest in the work of Hall and\u000a        Sherwin [6] on self-sustained processes and coherent structures. This\u000a        work is \"important for acoustic issues in aerodynamics\" [D].\u000a        Paper [5] was the first rational attempt to optimise the deployment of\u000a        suction on wings. Before that work, the `sledgehammer approach' of\u000a        sucking everywhere as hard as possible did indeed produce laminar flow,\u000a        but the equipment needed to produce it meant the aircraft was too heavy\u000a        to take off, let alone be commercially viable. The method provided the\u000a        basis for the development of optimisation strategies for all kinds of\u000a        instabilities on airfoils and produced a scenario where suction\u000a        deployment is commercially viable.\u000a    \u000a    In summary, EADS Innovation Works is \"already using a great deal of\u000a        the work\" of the Imperial College group in their research activities\u000a      \"directed at aerodynamic analysis and novel wing technology to support\u000a        the Airbus Business\" [D]. For example, the collaborative work with\u000a      EADS Innovation Works on \"a method for quantifying the effect of\u000a        surface finish uncertainties on the transition location has been\u000a        reported at the recent AIAA [American Institute of Aeronautics and\u000a        Astronautics] conference in San Diego\" (paper [3]) and is of a \"clear\u000a        world level standard\" [C].\u000a    Airbus considers the collaboration with Imperial and the LFC-UK project\u000a      to be mutually beneficial for all parties. For example, \"industry takes\u000a        on board the advanced and newer means of investigating complex flow\u000a        phenomenon resulting from manufacturing realities while academic\u000a        researchers benefit from exposure to real world problems\" [C].\u000a      Commenting on past collaboration, Airbus states \"the sharing of\u000a        roughness data, flight test data (Smart Fixed Wing, and Blade projects)\u000a        shows our commitment and belief in the excellence, novelty and\u000a        timeliness of the work being undertaken in the LFC-UK project. Major\u000a        aspects of the research highlighted in the September 2013 LFC-UK\u000a        industry workshop are of real and practical use to Airbus in the UK\"\u000a      [C]. As a final comment, Airbus states that it is \"fully supportive of\u000a        the research, which has already had an impact on our current work and we\u000a        fully expect the continuous stream of results coming from LFC-UK to\u000a        influence our approach to the aerodynamic design and manufacture of the\u000a        laminar flow wing concept\" [C].\u000a    In addition to the work already described above with EADS\/Airbus, the\u000a      Mathematics group has, or has had, collaborations with Qinetic [G1], BAE\u000a      and the Aircraft Research Association (ARA).\u000a    The active collaboration with BAE is being taken forward within LFC-UK to\u000a      develop a capability for BAE to design UCAVs having significant regions of\u000a      laminar flow. Of \"particular importance to BAE Systems has been the\u000a        work on the transition prediction on very 3D configurations\" [F].\u000a      Such configurations are \"relevant to UCAV design where increased range\u000a        can be achieved if laminar flow can be achieved over as much of the wing\u000a        as possible\" [F].\u000a    Here, the resultant reduced fuel burn enables vehicles to stay on mission\u000a      for longer periods. In the civilian aircraft context, the reduced burn is\u000a      primarily aimed at reducing fuel costs and the impact of emissions on the\u000a      environment. Indeed the fuel reductions are one minor step towards the\u000a      EU's planned goal of 50% reduction in aircraft emissions by 2050. LFC-UKs\u000a      role in this area was reported by the Economist in 2011: \"Understanding\u000a        what causes the transition from laminar to turbulent flow requires\u000a        massive mathematical and computing power. But if Dr Serghides's\u000a        colleague Philip Hall and his team can work out the details, they should\u000a        be able to design wings whose shape maintains laminar flow from front to\u000a        back, and thus lowers fuel consumption\" [G].\u000a    Scale of the impact:\u000a    It takes 20-25 years for an aircraft to go into service once a decision\u000a      to produce it has been made. Impact in the aeronautics industry therefore\u000a      occurs with long realisation timescales and it is difficult to assess the\u000a      scale of the current impact precisely. However, wing design and production\u000a      is a key capability for the UK and Airbus in the UK which, together with\u000a      its supply chain, provides supplies and services worth nearly &#163;1.5b\u000a      annually to the UK economy. EADS Innovation Works comments that \"each\u000a        contribution to the excellence of the product enables Airbus to complete\u000a        both within Europe and on the global stage and as such the work at\u000a        Imperial College is vital to the continued success of EADS and Airbus in\u000a        the UK\" [D]. Additionally, BAE states that UCAV development is \"an\u000a        activity involving tens of millions of pounds each year in the UK and\u000a        any technical superiority obtained using leading edge contributions from\u000a        academia helps to secure BAE Systems' position in this activity\"\u000a      [F].\u000a    ","ImpactSummary":"\u000a    Research at Imperial concerning the onset of turbulence in fluid flows\u000a      provided the key theoretical underpinning of the design tools needed to\u000a      produce the next generation of aircraft wings for both civil and military\u000a      aircraft. This work facilitates the development of laminar flow wings,\u000a      which, through reduced fuel consumption of up to 5%, has a significant\u000a      economic impact, together with a similar environmental impact, associated\u000a      with reduced engine noise. Carried out in conjunction with industry, most\u000a      notably EADS\/AIRBUS, the work is now part of the current design tools used\u000a      by AIRBUS and has already influenced the design of the wing developed by\u000a      AIRBUS for flight-testing in 2015. The financial impact in future for\u000a      AIRBUS-UK will be measured in billions if and when the technology becomes\u000a      part of future aircraft.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Imperial College London\u000a    ","Institutions":[{"AlternativeName":"Imperial College London","InstitutionName":"Imperial College London","PeerGroup":"A","Region":"London","UKPRN":10003270}],"Panel":"B         ","PlaceName":[],"References":"\u000a    (* References that best indicate quality of underpinning research)\u000a    \u000a[1] Wu, X., Choudhari, M., `Linear and nonlinear instabilities\u000a        of a Blasius boundary layer perturbed by streamwise vortices. Part 2.\u000a        Intermittent instability induced by long-wavelength Klebanoff modes',\u000a      J. Fluid Mech, 483, 249-286 (2003). DOI.\u000a    \u000a\u000a[2] Wu, X., `Receptivity of boundary layers with distributed\u000a        roughness to vortical and acoustic disturbances: a second order\u000a        asymptotic theory and comparison with experiments', J. Fluid Mech,\u000a      431, 91-133 (2001). DOI.\u000a    \u000a\u000a[3] Mughal, M. S. and Ashworth, R., `Uncertainty\u000a        Quantification Based Receptivity Modelling of Crossflow Instabilities\u000a        Induced by Distributed Surface Roughness in Swept Wing Boundary Layers'\u000a      , AIAA 2013-3106, 43rd AIAA Fluid Dynamics Conference (2013) (available here)\u000a    \u000a\u000a[4] Arthur, M. T., Horton, H.P. and Mughal, M., `Modelling of\u000a        natural transition in properly three-dimensional flows', AIAA,\u000a      2009-3556 (2009). DOI.\u000a    \u000a\u000a[5] Balakumar P, Hall P, `Optimum suction distribution for\u000a        transition control', Theoretical and Computational Fluid Dynamics,\u000a      Vol:13, Pages:1-19 (1999). DOI.\u000a    \u000a\u000a[6] Hall P, and Sherwin S, `Streamwise vortices in shear flows:\u000a        harbingers of transition and the skeleton of coherent structures',\u000a      J. Fluid Mech, 661, 178-205 (2010). DOI.\u000a    \u000aResearch grants: Within the period 2007-1013 support for transition\u000a      research includes:\u000a    [G1] Qinetiq (CU004-27172), PI: P Hall, 1\/2\/04-31\/8\/09, &#163;240,206,\u000a      `Advanced swept wing transition modelling and control'.\u000a    [G2] Airbus via European Commission, AST4-CT-2005-516109, PI: P Hall,\u000a      1\/5\/05-31\/10\/09, &#8364;220,568, `TELFONA - Testing for Laminar Flow on New\u000a      Aircraft'.\u000a    [G3] EPSRC (EP\/I037946\/1),\u000a      PI: P Hall, 1\/3\/11- 29\/2\/16, &#163;4,219,574, `LFC-UK: Development of\u000a      Underpinning Technology for Laminar Flow Control'.\u000a    [G4] EADS\/AIRBUS, PI: P Hall, 1\/3\/11- 29\/2\/16, &#163;1.08m, `LFC-UK:\u000a      Development of Underpinning Technology for Laminar Flow Control'.\u000a    ","ResearchSubjectAreas":[{"Level1":"9","Level2":"15","Subject":"Interdisciplinary Engineering"}],"Sources":"\u000a    [A] Telfona, AST4-CT-2005-516109, Final Technical Report, pages 2, 4,\u000a      &amp; 5 (available here)\u000a    [B] Flightglobal article, 26\/7\/11, \"Smart wing design takes shape for\u000a      next-generation narrowbody\", http:\/\/www.flightglobal.com\/news\/articles\/smart-wing-design-takes-shape-for-next-generation-narrowbody-359608\/\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/8rf\u000a      on 11\/9\/13)\u000a    [C] Letter from Transnational Senior Change Agent for Aerodynamics,\u000a      Airbus Operations Ltd, 21\/10\/13 (letter available from Imperial on\u000a      request)\u000a    [D] Letter from Head of Aeromechanics, EADS Innovation Works, 22\/10\/13\u000a      (letter available from Imperial on request)\u000a    [E] European Commission, Research &amp; Innovation, TELFONA page,\u000a      http:\/\/ec.europa.eu\/research\/transport\/projects\/items\/telfona_en.htm\u000a      (archived at\u000a      https:\/\/www.imperial.ac.uk\/ref\/webarchive\/9rf\u000a      on 11\/9\/13)\u000a    [F] Letter from Executive Scientist, BAE SYSTEMS, Advanced Technology\u000a      Centre, 7\/10\/13 (letter available from Imperial on request)\u000a    [G] Economist magazine article, 10\/3\/11, \"Plane truths: How to build\u000a      greener planes that airlines will actually want to fly\", http:\/\/www.economist.com\/node\/18329444\u000a      (Archived here)\u000a    ","Title":"\u000a    C7 - Research underpinning laminar airfoil design leading to revised\u000a      aircraft wing design\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning Imperial research concerned the onset of turbulence in\u000a      fluid flows and provided the key theoretical underpinning of the design\u000a      tools which are needed to produce the next generation of aircraft wings\u000a      for both civil and military aircraft. The relevant research primarily\u000a      concerned the following three areas:\u000a    (a) The Receptivity process. This is the mechanism by which\u000a      atmospheric fluctuations, surface imperfections and acoustic waves combine\u000a      to generate growing disturbances. In the absence of an understanding of\u000a      the receptivity phase, the transition prediction becomes empirical and the\u000a      consequent lack of precision historically meant that any drag savings were\u000a      lost by the necessity to `over-design' in order to avoid catastrophes. The\u000a      papers [1,2,3] have quantified the receptivity process to the extent that\u000a      the full benefits of flow control can be achieved.\u000a    A longstanding problem associated with transition was the question of\u000a      whether wind tunnel experiments were ever able to reproduce conditions in\u000a      flight. In [1], a theory to explain some long- standing experimental\u000a      observations of the role in streaks was given; this paper was one of the\u000a      first to explain the dangers of extrapolating wind tunnel results to the\u000a      flight situation. In 2001, a major contribution to our ability to account\u000a      for all possible disturbance generations was made [2]. The research in [2]\u000a      has been developed further in [3] to a level where we can account for how\u000a      the way in which wing surface is painted or rolled during manufacture\u000a      influences the transition process. [3] is a major breakthrough in the\u000a      deployment of our theoretical tools in a very applied industrial context.\u000a    (b) The Nonlinear Breakdown stage. This is the `endgame' of\u000a      disturbance growth where rapidly growing waves occur and turbulence is\u000a      quickly generated. Paper [6] uses the ideas from secondary instability\u000a      theory developed by Hall to describe the breakdown stage and to describe\u000a      coherent structures, which form the backbone of turbulent flows. Paper [5]\u000a      describes how suction can be used to prevent the nonlinear breakdown stage\u000a      occurring and was the crucial step in the development of a control\u000a      mechanism sufficiently compact to be deployed.\u000a    All the major aircraft manufacturers use the parabolised stability\u000a      equation method (PSE) to calculate the linear growth of disturbances in 2D\u000a      flows. Until recently, the approach was 2D whereas flows over real wings\u000a      are 3D. In 2010 [4], the first tools for transition prediction in\u000a      genuinely 3D flows were developed. Paper [4] was done in collaboration\u000a      with BAE and Qinetiq [G1] and concerned a very 3D geometry relevant to an\u000a      unmanned combat air vehicle (UCAV). The crucial input into this paper came\u000a      from Mughal who had developed the 3D PSE capability.\u000a    In situations where laminar flow cannot be maintained by careful design\u000a      of the wing, active means can be used to prevent disturbance growth. The\u000a      most viable active method is suction. However, test flights in the 1980's\u000a      showed that theoretical predictions of control by suction could be\u000a      verified but the weight of the machinery to deliver the suction meant that\u000a      no room would be left for passengers. Optimization methods that could be\u000a      used to find the best strategies for deployment without the punitive\u000a      weight penalties were demonstrated in [5]. That research was used for the\u000a      suction strategy used in the TELFONA flight tests [G2] to verify the\u000a      feasibility of suction control (see &#167;4).\u000a    (c) 3D PSE approach. The PSE approach was developed in the 1980's\u000a      following Hall's work on Gortler vortices. The development of variants of\u000a      that approach allowed flows of practical importance to be investigated\u000a      and, used in conjunction with (a) and (b), enabled us to provide accurate\u000a      transition prediction tools now being used by EADS\/AIRBUS as part of the\u000a      design process. Paper [4] represents the first practical implementation of\u000a      the approach for 3D flows.\u000a    The flow on swept wings breaks down to turbulence due to the growth and\u000a      then secondary instability of crossflow vortices. Secondary instability\u000a      theory used to predict the latter growth was developed by Hall and\u000a      Horseman in the early 1990's. In [6], it was found that the Hall-Horseman\u000a      theory described wave systems in turbulent flows, and so plays a role in\u000a      the control of turbulence. The Hall-Sherwin theory [6] provided the first\u000a      rational framework to describe coherent structures in turbulent flows.\u000a    Key personnel: The authors cited at Imperial College are Profs\u000a      Hall (1996-present), Ruban (2009-present) and Wu (1995-present) and senior\u000a      researcher Dr Mughal (1996-present). Hall is director of LFC-UK, a joint\u000a      research programme involving EPSRC [G3] and EADS\/AIRBUS [G4] created in\u000a      order to provide the underpinning theoretical technology for the\u000a      development of laminar flow wings.\u000a    "},{"CaseStudyId":"42270","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    The performance index for a magnetoresistive device, frequently called\u000d\u000a      the MR ratio, is conventionally cited as (RAP &#8212; RP)\/RP\u000d\u000a      2219 100% where RP and RAP are the resistances of\u000d\u000a      the device when the magnetizations of the two magnetic layers are parallel\u000d\u000a      and antiparallel respectively. In a read head, the larger the MR ratio the\u000d\u000a      smaller the magnetic bits on the disk which can be sensed and the higher\u000d\u000a      the disk storage density. Higher storage density means smaller portable\u000d\u000a      devices. The GMR sensor was used in HDD read heads from 1997 until 2005\u000d\u000a      but its MR ratio levelled out at 15-20%. It was gradually replaced by a\u000d\u000a      TMR read head from 2005 onwards when Seagate produced such a device with\u000d\u000a      an amorphous alumina barrier and an MR ratio of up to 70%. It seemed\u000d\u000a      possible that a larger MR ratio might be obtained with coherent tunnelling\u000d\u000a      through a crystalline barrier but no-one had any idea how large it would\u000d\u000a      be. Consequently we proposed to do an accurate calculation of the MR ratio\u000d\u000a      for a crystalline Fe\/MgO\/Fe junction, a good candidate due to a favourable\u000d\u000a      lattice match [G3]. The detailed calculation was carried out by Andrey\u000d\u000a      Umerski in collaboration with George Mathon and the remarkable result was\u000d\u000a      an MR ratio of over 1000% [3]. A calculation for the same system by a\u000d\u000a      different method was made at the same time independently by Prof William\u000d\u000a      Butler's group at Oak Ridge National Laboratory with a similar result.\u000d\u000a      Both of these calculations were published in 2001 and the huge predicted\u000d\u000a      MR ratio immediately presented a challenge to experimentalists. Umerski\u000d\u000a      and Mathon stressed the importance of a rather perfect Fe\/MgO interface\u000d\u000a      which was difficult to achieve in practice. The race to observe the\u000d\u000a      effect, with its obvious commercial application, ended in a dead heat in\u000d\u000a      2004 (S.S.P. Parkin et al, Nat. Mater. 3 862 (2004), S. Yuasa et\u000d\u000a      al, Nat. Mater. 3 868 (2004)), both of the successful groups\u000d\u000a      citing the theoretical work. This 3-year world-wide effort might not have\u000d\u000a      been sustained without the startling theoretical prediction of such a\u000d\u000a      large MR ratio.\u000d\u000a    There was still a long path to the market-place for the MgO-TMR read\u000d\u000a      head. Parkin had already taken an important step by preparing Fe\/MgO\/Fe\u000d\u000a      junctions of sufficient quality by sputter deposition which is suitable\u000d\u000a      for mass production, unlike the molecular beam epitaxy (MBE) method used\u000d\u000a      by Yuasa et al. The latter group subsequently collaborated with\u000d\u000a      Canon-Anelva to use their sputtering system. The MgO-TMR read head began\u000d\u000a      to reach the market in 2007 but its main impact occurred in 2008-9 since\u000d\u000a      when all manufactured HDDs are based on this technology. This is confirmed\u000d\u000a      by S.S.P. Parkin (IBM) who states: \"The work of Mathon and Umerski\u000d\u000a        clearly played an important role in the development of these materials\u000d\u000a        and their subsequent widespread application to recording read heads in\u000d\u000a        ~2007. All disk drives manufactured since about 2008-2009 use recording\u000d\u000a        read heads based on magnetic tunnel junctions.\" [B]\u000d\u000a    IBM subsequently sold its hard disk business to Hitachi Global Storage\u000d\u000a      Technologies who have recently (2012) been bought by Western Digital.\u000d\u000a      There are now only three major manufacturers of HDDs: Western Digital,\u000d\u000a      Seagate and Toshiba [A]. In 2008 Western Digital reported \"the\u000d\u000a        industry has made the transition to tunnel-junction magneto resistive\u000d\u000a        (\"TMR\") technology for the head reader function. We have completed the\u000d\u000a        transition to PMR [Perpendicular Magnetic Recording] and TMR in our\u000d\u000a        2.5-inch products and in the majority of our 3.5-inch products\"\u000d\u000a      (Western Digital 2008 Annual Report and Form 10-K, [C]). By 2009 they\u000d\u000a      reported \"We have completed the transition to PMR and TMR across all\u000d\u000a        product platforms\" (Western Digital 2009 Annual Report and Form\u000d\u000a      10-K, [D]). An example of the use of TMR technology by Toshiba is given in\u000d\u000a      their product information for internal notebook hard drives which \"use\u000d\u000a        proven state of the art .....TMR Head Recording technology for increased\u000d\u000a        capacity, reliability and performance\" [E].\u000d\u000a    The role of Mathon and Umerski's paper [3] in the emergence of\u000d\u000a      MgO-barrier magnetic tunnel junctions is emphasised in a review by S.\u000d\u000a      Ikeda for IEEE Transactions on Electron Devices [F]. The 20th\u000d\u000a      Tsukuba Prize was awarded to Drs Yuasa and Suzuki for \"Giant tunnel\u000d\u000a      magnetoresistance in MgO-based magnetic tunnel junctions and its\u000d\u000a      industrial applications\". The significance of the industrial application\u000d\u000a      and impact on society of TMR technology is clearly stated in the prize\u000d\u000a      citation: in addition to tracing a direct path from the theoretical\u000d\u000a      prediction to the industrial application the prize citation states \"The\u000a        giant TMR effect in MgO MTJs is expected to contribute to our society by\u000d\u000a        significantly reducing the power consumption of electronics devices and\u000d\u000a        improving the performance and security of computers\" [G]. Umerski's\u000d\u000a      theoretical prediction of 2001 has definitely had an impact on the huge\u000d\u000a      global HDD market, which was estimated to be $28 billion in 2012 [A].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    We demonstrate a strong influence on the design of the read head used in\u000d\u000a      the present state-of-the-art hard-disk drive (HDD) first produced\u000d\u000a      commercially in 2008. This much improved read head, enabling disk storage\u000d\u000a      density to increase by a factor of 5 to around 1 Tbit\/in2,\u000d\u000a      relies crucially on a magnetic tunnel junction with a MgO barrier whose\u000d\u000a      huge tunneling magnetoresistance was predicted theoretically in a 2001\u000d\u000a      paper co-authored by Dr A. Umerski [1], the RA on one of our EPSRC-funded\u000d\u000a      research grants. This prediction relied on techniques developed by us over\u000d\u000a      many years, specifically in refs [2] and [3]. Such magnetic tunnel\u000d\u000a      junctions are used in all computer HDDs manufactured today with predicted\u000d\u000a      sales in 2012 amounting to more than $28 billion [section 5, source A].\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    Imperial College London\u000d\u000a    ","Institutions":[{"AlternativeName":"Imperial College London","InstitutionName":"Imperial College London","PeerGroup":"A","Region":"London","UKPRN":10003270}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    (* References that best indicate quality of underpinning research)\u000d\u000a    \u000a[1] *J.Mathon, M.A.Villeret, R.B.Muniz, J.d'Albuquerque e Castro and D.M.Edwards,\u000d\u000a      \"Quantum well theory of the exchange coupling in Co\/Cu\/Co(001)\",\u000d\u000a      Phys. Rev. Lett., 74, 3696 (1995). DOI\u000d\u000a    \u000a\u000a[2] *A. Umerski, \"Closed-form solutions to surface Green's\u000d\u000a      functions\", Phys. Rev. B, 55, 5266 (1997). DOI\u000d\u000a    \u000a\u000a[3] *J.Mathon and A.Umerski, \"Theory of tunneling\u000d\u000a        magnetoresistance of an epitaxial Fe\/MgO\/Fe(001) junction\", Phys.\u000d\u000a      Rev B, 63, 220403(R) (2001). DOI\u000d\u000a      [N.B. Umerski's Imperial affiliation and grant GR\/L92594 were\u000d\u000a      inadvertently omitted on the paper but the bulk of the work reported was\u000d\u000a      done while employed 50% by Imperial (a statement by Umerski to this effect\u000d\u000a      is available upon request).]\u000d\u000a    \u000aRelevant Research Grants:\u000d\u000a    [G1] EPSRC GR\/J37263,\u000d\u000a      `Exchange coupling in magnetic multilayers; trends across the periodic\u000d\u000a      table and biquadratic exchange', &#163;36,313, 18\/04\/94 - 17\/05\/96 (PI: DM\u000d\u000a      Edwards, Imperial)\u000d\u000a    [G2] EPSRC GR\/L13292\u000d\u000a      ,`Quantum well theory of exchange, giant magnetoresistance and anisotropy\u000d\u000a      in magnetic multilayers', &#163;46,373, 06\/05\/96 - 05\/05\/98 (PI: DM Edwards,\u000d\u000a      Imperial)\u000d\u000a    [G3] EPSRC GR\/L92594,\u000d\u000a      `Theory of tunneling magnetoresistance and interface anisotropy', &#163;66,191,\u000d\u000a      06\/05\/98 - 05\/05\/2001 (RA Dr A Umerski, 50% Imperial, 50% City University,\u000d\u000a      PI: DM Edwards, Imperial College)\u000d\u000a    [G4] EPSRC GR\/N09039,\u000d\u000a      `Real space theory of magnetotransport and anisotropy in magnetic\u000d\u000a      nanostructures far from equilibrium', &#163;65,032, 01\/11\/2000 - 31\/10\/2003\u000d\u000a      (PI: DM Edwards, Imperial)\u000d\u000a    [G5] EPSRC EP\/D505798\/1,\u000d\u000a      `Spin@rt: Room temperature spintronics', &#163;87,504, 02\/05\/2006 - 01\/08\/2009\u000d\u000a      (PI: DM Edwards, Imperial). This was part of a &#163;2.3M grant for a\u000d\u000a      consortium of seven universities with Imperial and City responsible for\u000d\u000a      theory.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"3","Level2":"2","Subject":"Inorganic Chemistry"},{"Level1":"2","Level2":"2","Subject":"Atomic, Molecular, Nuclear, Particle and Plasma Physics"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000d\u000a    [A] Bizmology article: `Consolidation in the hard disk drive market: then\u000d\u000a      there were three', http:\/\/bizmology.hoovers.com\/2012\/03\/19\/consolidation-in-the-hdd-hard-disk-drive-market-then-there-were-three\/\u000d\u000a      (Archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/phf\u000d\u000a      on 22\/04\/13)\u000d\u000a    [B] Letter from Magnetoelectronics Manager, IBM Almaden Research Center,\u000d\u000a      confirming the important role of Umerski and Mathon in the development of\u000d\u000a      Fe\/MgO\/Fe TMR junctions (Sept 2012, available from Imperial on request).\u000d\u000a    [C] Western Digital 2008 Annual Report and Form 10-K, page 10,\u000d\u000a      https:\/\/materials.proxyvote.com\/Approved\/958102\/20080917\/AR_27910\/images\/Western_Digital-AR2008.pdf\u000d\u000a      (Archived here)\u000d\u000a    [D] Western Digital 2009 Annual Report and Form 10-K, page 11,\u000d\u000a      https:\/\/materials.proxyvote.com\/Approved\/958102\/20090916\/AR_46224\/HTML2\/default.htm.\u000d\u000a      (Archived here)\u000d\u000a    [E] Toshiba Storage Products `Internal Notebook Hard Drives' product\u000d\u000a      details webpage,\u000d\u000a      http:\/\/storage.toshiba.com\/storagesolutions\/archived-models\/internal-notebook-hard-drives.\u000d\u000a      (Archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/rhf\u000d\u000a      on 22\/04\/13)\u000d\u000a    [F] IEEE Transactions on Electron Devices 54 991 (2007) section 3A, DOI:\u000d\u000a      10.1109\/TED.2007.894617\u000d\u000a    [G] Citation for the 20th Tsukuba Prize: http:\/\/www.suzukiylab.mp.es.osaka-u.ac.jp\/Top\/tsukuba_english.pdf\u000d\u000a      (Archived here)\u000d\u000a    ","Title":"\u000d\u000a    C8 &#8212; A theoretical prediction leading to a redesigned read head used in\u000d\u000a      all hard-disk drives (HDDs) manufactured today\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    In 1989 Professors D.M. Edwards (Imperial) and J. Mathon (City\u000d\u000a      University) set up a collaboration on the theory of magnetic multilayers\u000d\u000a      which lasted for 20 years with almost continuous SERC and EPSRC funding\u000d\u000a      [G1-G5]. This followed the 1988 discovery of the giant magnetoresistance\u000d\u000a      (GMR) effect for which Fert and Gruenberg were awarded a Nobel Prize in\u000d\u000a      2007. In a metallic system consisting of two ferromagnetic layers,\u000d\u000a      separated by a non-magnetic spacer layer, the GMR effect is observed as a\u000d\u000a      change in electrical resistance when the angle between the magnetizations\u000d\u000a      of the two magnets is varied due to a magnetic field. In 1997 IBM brought\u000d\u000a      out a GMR hard-disk drive (HDD) in which the read head used this effect to\u000d\u000a      sense the magnetic `bits' of the disk. An increase in storage density from\u000d\u000a      0.1 to 100 Gbit\/in2 between 1991 and 2003 was largely enabled\u000d\u000a      by GMR.\u000d\u000a    In practice GMR in metallic systems was necessarily observed with the\u000d\u000a      electric current parallel to the layers and depended on defect scattering\u000d\u000a      of the electrons so that precise calculation of the effect was impossible.\u000d\u000a      Consequently theoreticians concentrated on another effect which was\u000d\u000a      observed in parallel with GMR by a group at IBM led by Stuart S.P. Parkin:\u000d\u000a      oscillations in interlayer exchange coupling (IEC) as a function of spacer\u000d\u000a      thickness. In 1990, the same year as its discovery, Edwards and Mathon\u000d\u000a      presented the first theory of this effect at the E-MRS Spring Meeting in\u000d\u000a      Strasbourg. Green's function techniques for multilayers were developed to\u000d\u000a      make the theory completely quantitative for real materials; this was done\u000d\u000a      for a Co\/Cu\/Co trilayer in 1995 [1, 2].\u000d\u000a    The techniques of [1, 2] were an essential underpinning for the\u000d\u000a      subsequent research which had a direct impact on tunnelling\u000d\u000a      magnetoresistance (TMR) read head technology. This subsequent research was\u000d\u000a      largely carried out by Dr A. Umerski, the RA employed at Imperial on the\u000d\u000a      grant [G3], in collaboration with Professor J. Mathon of City University.\u000d\u000a      A TMR read head is very similar to a GMR read head but with the metallic\u000d\u000a      spacer replaced by an insulating barrier through which the current tunnels\u000d\u000a      perpendicular to the layers. A TMR read head with an amorphous alumina\u000d\u000a      barrier was commercialised by Seagate (a manufacturer of HDDs) in 2005.\u000d\u000a      Meanwhile, in 2001 Mathon and Umerski [3] had published a paper,\u000d\u000a      simultaneously with one by a US group, showing with precise calculations\u000d\u000a      that a much larger TMR effect could be obtained from a crystalline\u000d\u000a      (001)-oriented Fe\/MgO\/Fe system. This paper has more than 400 citations.\u000d\u000a      The calculation combines the Kubo formula for conductance with the Green's\u000d\u000a      function techniques we had built up for our interlayer exchange coupling\u000d\u000a      work [1, 2]. It posed considerable technical problems and the high\u000d\u000a      accuracy required to calculate the very small tunnelling current could not\u000d\u000a      have been achieved without the prior work on surface Green's functions of\u000d\u000a      [2]. The large TMR predicted was verified experimentally in 2004 by S.S.P.\u000d\u000a      Parkin's IBM group and a Japanese group. MgO TMR HDDs have swept the\u000d\u000a      market since 2008.\u000d\u000a    Key Researchers:\u000d\u000a    \u000d\u000a      Dr. A. Umerski, RA, Department of Mathematics 1995-31st Oct\u000d\u000a        2000 (joint appointment with City University, 50% salary from Imperial,\u000d\u000a        50% from City).\u000d\u000a      Prof. D. M. Edwards, Senior Research Investigator, Department of\u000d\u000a        Mathematics, 1999-present, formerly Head of Mathematical Physics\u000d\u000a        Section.\u000d\u000a      Prof. J. Mathon, Professor of Mathematical Physics, City University.\u000d\u000a    \u000d\u000a    "},{"CaseStudyId":"42271","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    DISPERSE software:\u000a      The theory work of [1,2] is incorporated into the commercial package\u000a      DISPERSE [A] v2 which was released in 2002 and continues to be updated and\u000a      developed by Imperial College, led by Prof M Lowe, Mechanical Engineering.\u000a      DISPERSE was first released to external users in 1991 [text removed for\u000a      publication] [C]. DISPERSE is an interactive Windows program designed to\u000a      calculate dispersion curves for multi-layered flat or cylindrical\u000a      structures. The facility to model accurately circumferential modes (added\u000a      in 2002) is an attractive addition to the original code and has helped it\u000a      to maintain its pre-eminent position in this area. [text removed for\u000a      publication]. The spectral approach to generating dispersion curves\u000a      accurately, developed in paper [3] drove the development of a porous\u000a      elastic version by Karpfinger and colleagues at Schlumberger Oilfield\u000a      Services and Shell International. This allowed real time completion\u000a      modelling in deepwater oil wells with the technique now widely utilised in\u000a      those applications. An article in The Leading Edge, a magazine reporting\u000a      new geophysical advances to the Society of Exploration Geophysicists,\u000a      acknowledges the contribution of the spectral theory based on [3] to both\u000a      experiments and the development of real-time completion modelling for oil\u000a      fields [B].\u000a    Rolls-Royce Submarines:\u000a    Since 2004 Rolls-Royce has been working in collaboration with Imperial's\u000a      departments of Mathematics and Mechanical Engineering on the development\u000a      of the Finite Element Method (FEM) for the modelling of ultrasonic wave\u000a      propagation and defect interaction [G2]. The aim of the work was to model\u000a      capably the interaction of ultrasonic waves with small and geometrically\u000a      complex flaws typical of the types found in nuclear plants due to\u000a      manufacturing and service induced mechanisms. The limitations of existing\u000a      models placed unnecessary cost on the company due to \"extended\u000a        inspection durations and test piece trials\" [E]. An aim of the\u000a      collaboration with Imperial was to \"provide a generic modelling\u000a        capability to reduce inspection development costs through a greater\u000a        emphasis on modelling rather than test piece trials\" [E]. The\u000a      absorbing boundaries work [4,5] proved \"invaluable for the overall aims\u000a        of Rolls-Royce as it allowed small regions of a component to be modelled\u000a        using the FEM without wasting computing resources on unnecessarily large\u000a        models\" [E]. These developments have \"delivered significant\u000a        reductions in the model sizes required of typical inspection scenarios\"\u000a      [E]. Financial impact to Rolls-Royce can be summarised in two ways:\u000a    \u000a      The modelling improvements described above \"will significantly\u000a          reduce the number of ultrasonic test pieces required during inspection\u000a          development\" where a typical test piece adds in the region of\u000a        &#163;50,000-100,000 to inspection costs;\u000a      The \"production of component safety justifications requires\u000a          thousands of man hours that represents millions of pounds of cost\"\u000a        and a small benefit provided to this effort removes \"significant cost\u000a          from the submarine enterprise\" [E].\u000a    \u000a    The projects that the Imperial group delivered \"provided Rolls-Royce\u000a        with a significant improvement in modelling capability\" and have led\u000a      to two sponsored EngD projects to further refine the ideas and transfer\u000a      the technology into Rolls-Royce. Additionally, one of the collaborators\u000a      for this work, M. Drozdz, is now a fulltime employee at Rolls-Royce.\u000a    SIMPOSIUM:\u000a      The hybrid technique [6] has similarly been implemented in code by\u000a      Rolls-Royce and this successful implementation has created momentum with\u000a      the impact on-going and developing internationally. For instance, it led\u000a      CEA (the developers of CIVA, the pre-eminent ray code used in ultrasonics)\u000a      to invite Imperial College to join the EU project `Simulation Platform for\u000a      Non Destructive Evaluation of Structures and Materials' (SIMPOSIUM)\u000a      that it project manages [F]. SIMPOSIUM started in September 2011 and will\u000a      develop over a period of 3 years. The cost of Project is &#8364;5.99m and\u000a      involves Volkswagen, European Aeronautic Defense and Space Company (EADS),\u000a      SKF and Serco amongst others as industrial partners [D]. The key objective\u000a      of the SIMPOSIUM project is to \"build interoperable tools based on\u000a        hybrid modelling, for ultrasonic and electromagnetic NDE, in order to\u000a        solve very complex industrial cases from different fields (steel\u000a        nuclear, energy, aeronautics), software solution providers, and academic\u000a        teams\" [F]. The \"expertise, skills and know-how concerning both\u000a        NDE and mathematics from Imperial College\" is leading the\u000a      developments based on hybrid modelling for ultrasonic simulation, for\u000a      which it is \"necessary to combine mathematical and physics knowledge to\u000a        develop efficient hybrid and coupling formulations\" [F]. The core of\u000a      the work being delivered by Imperial College relates to \"a hybrid code\u000a        dedicated to ultrasonic simulation, based on the CIVA beam module\u000a        (prediction of the incident ultrasonic beam) and a \"scattering box\",\u000a        based on the well-known ABAQUS software, which contains a complex flaw\"\u000a      [F].\u000a    The advantage of the absorbing boundaries work combined with the hybrid\u000a      work is profound. For the first time, industrial companies such as\u000a      Rolls-Royce are able to perform realistic simulations of the scattering of\u000a      ultrasound from defects, using FE codes that are accepted in the industry.\u000a      The realism includes thick and complex-shaped components, containing\u000a      complex-shaped defects, including fatigue cracks with rough surfaces\u000a      (collaborative research is on-going on this aspect too). This means that\u000a      they can make cases to justify proposed inspection of safety-critical\u000a      components at much-reduced expense. The cost reductions include the\u000a      reduced need to make experimental test (justification) samples, and the\u000a      possibility to replace radiographical inspections with ultrasound\u000a      inspections, avoiding the need to evacuate personnel during inspection and\u000a      the attendant health and safety issues.\u000a    ","ImpactSummary":"\u000a    The waves group in the Mathematics Department at Imperial College London\u000a      has developed methodology in several areas, including novel absorbing\u000a      layer techniques and Hybrid methods (with Rolls Royce and CEA) for Finite\u000a      Element software, and efficient techniques for finding the properties of\u000a      waves in curved plates, bars and pipes. The impact is facilitated by a\u000a      long-standing research collaboration with the Non-destructive Evaluation\u000a      group in the Mechanical Engineering department, incorporated with\u000a      industrial partners through the UK Research Centre in Non- Destructive\u000a      Evaluation (https:\/\/www.rcnde.ac.uk).\u000a      Our work has been directly implemented in DISPERSE &#8212; the world leading\u000a      software modelling tool for guided stress waves (licensed by Imperial\u000a      Consultants) &#8212; and by Rolls-Royce. [text removed for publication].\u000a      Rolls-Royce use the models for improved inspection techniques, resulting\u000a      in reduced man hour costs and multiple &#163;50Ks of equipment savings.\u000a    ","ImpactType":"Technological","Institution":"\u000a    Imperial College London\u000a    ","Institutions":[{"AlternativeName":"Imperial College London","InstitutionName":"Imperial College London","PeerGroup":"A","Region":"London","UKPRN":10003270}],"Panel":"B         ","PlaceName":[],"References":"\u000a    (* References that best indicate quality of underpinning research)\u000a    \u000a[1] J. Fong, M.J.S. Lowe, D. Gridin and R.V.\u000a        Craster, \"Fast techniques for calculating dispersion relations of\u000a        circumferential waves in annular structures'', Review of Progress in\u000a      Quantitative NDE (American Institute of Physics conference proceedings),\u000a      22, 213-220 (2002). DOI.\u000a    \u000a\u000a[2] *D. Gridin, R.V. Craster, J. Fong, M.J.S.\u000a        Lowe and M. Beard, \"The high frequency asymptotic\u000a        analysis of guided waves in a circular elastic annulus'', Wave\u000a      Motion, 38, 67-90 (2003). DOI.\u000a    \u000a\u000a[3] A.T.I. Adamou and R.V. Craster, \"Spectral methods\u000a        for modelling guided waves in elastic media'', J. Acoust. Soc. Am.\u000a      116, 1524-1535 (2004). DOI.\u000a    \u000a\u000a[4] *E.A. Skelton, S.D.M. Adams, and R.V. Craster,\u000a      \"Guided Elastic Waves and perfectly matched layers\", Wave Motion,\u000a      44, 573-592 (2007). DOI.\u000a    \u000a\u000a[5] P. Rajagopal, M. Drozdz, E. Skelton, M.J.S.\u000a        Lowe and R.V. Craster, \"On the use of absorbing layers to\u000a        simulate the propagation of elastic waves in unbounded media using\u000a        commercially available Finite Element packages\", NDT &amp; E\u000a      International, 51, 30-40 (2012). DOI.\u000a    \u000a\u000a[6] *P. Rajagopal, E.A. Skelton, M.J.S. Lowe and R.V.\u000a        Craster, \"A generic hybrid model for bulk elastodynamics, with\u000a        application to ultrasonic Non-Destructive Evaluation\", IEEE Trans.\u000a      Ultrasonics, Ferroelectrics and Frequency Control, 59, 1239-252 (2012). DOI.\u000a    \u000aRelevant Research Grants:\u000a      [G1] EPSRC, GR\/R32031,\u000a      \"Distorted Elastic Waveguides: Theory and Application\", PI: R.\u000a      Craster, Co-Is: P. Cawley, M. Lowe, 01\/10\/01-30\/09\/04, &#163;236,682\u000a    [G2] Rolls-Royce and MoD direct funding of research programme in Mech Eng\u000a      (PIs: P Cawley, M Lowe), to develop ultrasonic inspection methods for\u000a      nuclear plant components. Total &gt;&#163;3M over period 2004-2013\u000a    [G3] EPSRC, EP\/I018948\/1,\u000a      \"Modelling of ultrasonic response from rough cracks\", PI: M. Lowe,\u000a      Co-I: R Craster, 1\/1\/11-31\/12\/13, &#163;292,750 + &#163;90K Industrial from project\u000a      partners (EDF- Energy, Rolls Royce Naval Marine), 01\/01\/11-31\/12\/13\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000a    [A] DISPERSE software website &#8212; An Interactive Program for Generating\u000a      Dispersion Curves http:\/\/www3.imperial.ac.uk\/nde\/products%20and%20services\/disperse\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/g9f\u000a      on 15\/11\/13)\u000a    [B] A. Bakulin, F. Karpfinger and B. Gurevich, `Understanding the\u000a        acoustic response of deepwater completions', The Leading Edge:\u000a      special section on permanent monitoring, smart oil fields and reservoir\u000a      surveillance, December 2008, 1646-1653. DOI.\u000a      (archived here)\u000a      [N.B. Reference to Craster in the acknowledgments]\u000a    [C] Letter from Imperial Consultants (ICON), June 2013. [text removed for\u000a      publication] (available from Imperial on request).\u000a    [D] Simposium project, partners webpage, http:\/\/www.simposium.eu\/imperial-college\u000a      (archived at https:\/\/www.imperial.ac.uk\/ref\/webarchive\/fpf\u000a      on 26\/7\/13)\u000a    [E] Letter from NDE Development Engineer, Rolls-Royce Submarines, May\u000a      2013 (available from Imperial on request)\u000a    [F] Letter from NDE Senior Expert and Coordinator of the SIMPOSIUM\u000a      project, CEA, 17\/7\/13 (available from Imperial on request)\u000a    ","Title":"\u000a    C9 &#8212; Modelling of bulk and guided waves in the Non-Destructive Evaluation\u000a      of structures; software and implementation by industry\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The initial collaboration between the Non-destructive Evaluation (NDE)\u000a      group (Mechanical Engineering) and Mathematics involved studying\u000a      dispersion curves which are vital in the interpretation and understanding\u000a      of guided elastic waves [G1]. A technical, but practically important,\u000a      issue is the efficient and accurate evaluation of such curves for\u000a      waveguide structures (such as pipelines) that is guaranteed not to miss\u000a      any curves and which remains reliable and stable for the industrial user.\u000a      The theory created in [1,2] does precisely this for the important class of\u000a      circumferential guided waves in pipelines and is incorporated in the\u000a      DISPERSE releases from 2002-present. DISPERSE is the world's leading\u000a      modelling tool for guided stress waves, developed in Imperial College and\u000a      licensed by Imperial Consultants. A further industry requirement is for\u000a      dispersion curves for porous elastic media, anisotropy for\u000a      fibre-reinforced composite pipelines, inhomogeneous media and buried\u000a      pipelines. Again, progress in the theory [3] has been made and, for porous\u000a      media, this has actively been taken up by the geophysics and petroleum\u000a      engineering communities.\u000a    In practical terms, ultrasonic modelling in industry is dominated by ray\u000a      modelling (the world leading code is CIVA, http:\/\/www-civa.cea.fr\/en\/,\u000a      developed by the Commissariat &#224; l'&#233;nergie atomique et aux &#233;nergies\u000a      alternatives, CEA, in France) but this is not good for scattering for\u000a      which finite element (FE) models are strongest; commercial FE codes such\u000a      as ABAQUS are widely-available and well established. When modelling\u000a      scattering by defects in practical structures of large size, the greatest\u000a      limitation in numerical modelling is unwanted reflections from model\u000a      boundaries that are manifest as additional reverberations in the\u000a      ultrasonic signal that contaminate and poison the results. Arbitrarily\u000a      extending the domain size to large extents requires high-cost computing\u000a      and long-time solutions, and indeed is unfeasible for all but the simplest\u000a      cases. The removal of unwanted reflections is a challenging practical\u000a      problem, and a research area in its own right, but a key issue ignored by\u000a      the academic literature is that implementation in industry must use finite\u000a      element codes that are professionally accredited and meet industry\u000a      standards. Contributions from Mathematics were to identify how perfectly\u000a      matched layers (PMLs) can be used to address elastic waveguide modes that\u000a      have negative group velocity [4], a technical point that destroyed\u000a      applications of bespoke FE codes to ultrasonics in waveguides, and then\u000a      use this knowledge to tackle the industry bottleneck. The primarily\u000a      academic work then grew into an investigation of absorbing layers versus\u000a      PMLs with the emphasis on how to actually implement these research ideas\u000a      into the codes (ABAQUS) that Rolls-Royce [G2] and many others in industry\u000a      actually use. We were successful in this, as described in [5], and our\u000a      techniques are now implemented by Rolls- Royce.\u000a    In practice, scattering from defects may occur in large complex\u000a      structures (rail tracks, components in nuclear plants etc, [G3]) with\u000a      considerable separation between excitation transducer, defect and receiver\u000a      and the subsequent numerical modelling can lead to vast memory\u000a      requirements, particularly in 3D. This industry bottleneck motivated the\u000a      group to develop hybrid methods whereby only the source, receiver and\u000a      scatterer are modelled numerically and these small \"boxes\" then\u000a      communicate mathematically with each other as described and implemented\u000a      for commercial FE in [6]. These advances enable large scale realistic\u000a      simulations, even in 3D, to be practical in CIVA and this is key to\u000a      applications of modern ultrasonics. This success led to the invitation by\u000a      CEA to join a consortium developing CIVA which is used by industry\u000a      worldwide (http:\/\/www-civa.cea.fr\/en\/partners\/).\u000a    Key Contributors:\u000a    \u000a      Professor R Craster, Head of Department of Mathematics, Imperial\u000a        College, Oct 1998-present\u000a      Professor M Lowe, Professor in Mechanical Engineering, Imperial\u000a        College, 1989-present\u000a      Dr E Skelton, RA, Department of Mathematics, Imperial College,\u000a        1990-present\u000a    \u000a    "},{"CaseStudyId":"42314","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"},{"GeoNamesId":"1814991","Name":"China"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"2782113","Name":"Austria"},{"GeoNamesId":"2661886","Name":"Sweden"},{"GeoNamesId":"2017370","Name":"Russia"},{"GeoNamesId":"3175395","Name":"Italy"},{"GeoNamesId":"2510769","Name":"Spain"}],"Funders":[],"ImpactDetails":"\u000a    Process from research to impact:\u000a    In 2002, a leading Swedish cataract surgeon reported low rates of\u000a      endophthalmitis following the use of an injection of the antibiotic\u000a      cefuroxime into the anterior chamber of the eye at the time of cataract\u000a      surgery. The European Society of Cataract and Refractive Surgeons (ESCRS)\u000a      required further evidence, and the President of ESCRS confirms that \"in\u000a      2002, Profs Gettinby and Revie were approached by a consultant\u000a      ophthalmologist acting on behalf of the European Society of Cataract and\u000a      Refractive Surgeons with a view to designing and implementing a large\u000a      scale randomised endophthalmitis study, and they were approached on the\u000a      basis of this work in this field and their expertise in the analysis of\u000a      large datasets\" [Source 1]. Gettinby and Revie recommended a controlled\u000a      clinical trial and were subsequently commissioned to implement what would\u000a      be referred to by leading European surgeons as \"one of the largest\u000a      prospective European clinical studies of antibiotic prophylaxis and the\u000a      largest in ophthalmology\" [Source 2].\u000a    The publication of results in 2006 and 2007 showed to the community of\u000a      ophthalmic surgeons that this use of cefuroxime as a prophylactic results\u000a      in an approximate 5-fold reduction in the risk of post-operative\u000a      endophthalmitis. At this point use of the published findings was having a\u000a      greater impact and many surgeons were adopting the use of cefuroxime\u000a      injection.\u000a    The President of ESCRS confirms that \"the results of the study had direct\u000a      influence on the ESCRS guidelines on `Prevention, Investigation and\u000a      Management of Post-Operative Endophthalmitis', which were revised in 2007\u000a      on the basis of Gettinby and Revie's published research\" [Source 1].\u000a      Central to these new guidelines [Source 3] was the recommendation that\u000a      surgeons consider the use of cefuroxime intracameral injection. Although\u000a      cefuroxime was unlicensed and the decision to use it was at the discretion\u000a      of the surgeon, this was the beginning of changing practices by cataract\u000a      surgeons in Europe.\u000a    Types of Impact: the publication of Gettinby and Revie's study has\u000a      led directly to impact on professional practice, patient welfare, and\u000a      wider benefits:\u000a    Changes to professional practice: The publication of the results\u000a      and changes to ESCRS guidelines has led to a change in the way that\u000a      cataract surgery is supported by antibiotic treatment since 2008 onwards.\u000a      The reach in Europe has been extensive. The European Society of Cataract\u000a      and Refractive Surgery [Source 4] has been successfully promoting the\u000a      findings, with uptake of their guidelines first reaching significant\u000a      levels from 2008 to 2010, [Source 5]. For example, the Austrian Society of\u000a      Ophthalmology recommended adoption of the ESCRS guidelines with its focus\u000a      on the use of the study findings concerning cefuroxime, and there was\u000a      evidence (Ocular Surgery News Europe\/ Asia &#8212; Pacific Edition December 1\u000a      2008) of cefuroxime being used in hospitals in Italy, Spain and Ireland.\u000a    The following are quotations which appeared in the ESCRS Eurotimes vol.\u000a      18, Issue 3, March 2013, pages 4-6 [Source 6]:\u000a    \"The survey has not been repeated, but since that time I think that more\u000a      American surgeons are using intracameral antibiotics following publication\u000a      of the study, compared to before.\" (Clinical Professor of Ophthalmology,\u000a      University of California &amp; Chinese University, Hong Kong)\u000a    \"In my country almost nobody has any doubts with regard to the cefuroxime\u000a      efficiency in endophthalmitis prophylaxis.\" (Citation from Boris Malyugin\u000a      MD, International Ophthalmologists Academy, Russia)\u000a    \"The ESCRS Endophthalmitis Study has made a revolutionary change not only\u000a      in my practice, but also many practices in India. The use of intracameral\u000a      moxifloxacin has become routine.\" (Keiki Mehta MD Medical Director and\u000a      Chief of the Mehta Charity Eye Hospital, Karja, India).\u000a    The president of ESCRS [Source 1] has stated that \"From 2008 the\u000a      guidelines significantly increased the use of antibiotic intracameral\u000a      injections following cataract surgery on an international scale. This\u000a      change in clinical practice has undoubtedly reduced the incidence of\u000a      blindness resulting from post-operative infections of the eye.\"\u000a    Benefits to patients: A cefuroxime injection to the eye at time of\u000a      surgery will prevent endophthalmitis occurring post operatively; and this\u000a      procedure has now been widely adopted by eye surgeons internationally. The\u000a      adoption of this simple process has saved the sight of thousands of\u000a      patients annually who may otherwise have developed infection of the eye.\u000a      Based on data from the Organisation of Economic Cooperation and\u000a      Development it is estimated that ophthalmologists undertake more than\u000a      three million cataract operations in Europe each year. It is estimated\u000a      that there are now 2500 fewer cases of loss of sight per 1,000,000\u000a      cataract patients, leading to 7500 fewer cases of blindness per year in\u000a      Europe alone. This estimate is conservative as uptake of the study\u000a      findings worldwide is expected to lead to thousands more patients avoiding\u000a      loss of sight.\u000a    Wider impact: The influence of the original research is still\u000a      gaining momentum. In 2013 one commercial company was granted a marketing\u000a      authorisation for an injectable cefuroxime product for the prevention of\u000a      endophthalmitis. The president of ECSRS has also confirmed that \"the\u000a      guidelines have been re-written by me and others and submitted to the\u000a      publishers for re-launching at our annual congress in Amsterdam in October\u000a      2013. They contain a further wealth of published evidence of the\u000a      world-wide benefits accruing from the original study\" [Source 1].\u000a    ","ImpactSummary":"\u000a    Research at Strathclyde has brought about a change in eye surgery\u000a      practice throughout Europe and worldwide. A four-year cross-Europe study\u000a      in collaboration with the European Society of Cataract and Refractive\u000a      Surgery (ESCRS) investigated antibiotic treatment to prevent\u000a      endophthalmitis, a complication arising during cataract operations which\u000a      typically results in loss of sight. The findings showed that when the\u000a      treatment is given at the start of surgery it leads to a 5-fold reduction\u000a      in the risk of endophthalmitis. The European Society of Cataract and\u000a      Refractive Surgery has endorsed the discovery and widely promoted the\u000a      uptake of the treatment through publications and guidelines, which over\u000a      the last 6 years has led to the prevention of loss of sight in thousands\u000a      of patients. In Europe alone it is estimated that each year there have\u000a      been 7500 fewer cases of blindness following cataract surgery as a result\u000a      of the ESCRS guidelines.\u000a    ","ImpactType":"Health","Institution":"\u000a    University of Strathclyde\u000a    ","Institutions":[{"AlternativeName":"Strathclyde (University of)","InstitutionName":"University of Strathclyde","PeerGroup":"B","Region":"Scotland","UKPRN":10007805}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"1819729","Name":"Hong Kong"},{"GeoNamesId":"2759794","Name":"Amsterdam"},{"GeoNamesId":"5332921","Name":"California"}],"References":"\u000a    References 2, 3 and 4 best indicate the quality of the underpinning\u000a        research\u000a    \u000a1. Seal, D.V., Barry, P., Gettinby, G., Lees, F., Peterson, M., Revie, C.\u000a      &amp; Wilhelmus, K.R. (2006) ESCRS study of prophylaxis of postoperative\u000a      endophthalmitis after cataract surgery: Case for a European multicentre\u000a      study. Journal of Cataract and Refractive Surgery, 32(3), 396-406\u000a    \u000a\u000a2. Barry, P., Seal, D.V., Gettinby, G., Lees, F., Peterson, M. &amp;\u000a      Revie, C. (2006) ESCRS study of prophylaxis of postoperative\u000a      endophthalmitis after cataract surgery: Preliminary report of principal\u000a      results from a European multicentre study. Journal of Cataract and\u000a      Refractive Surgery, 32(3), 407-410\u000a    \u000a\u000a3. Barry, P., Seal, D.V., Gettinby, G., Lees, F., Peterson, M. &amp;\u000a      Revie, C. (2007) ESCRS Endophthalmitis Study Group. Prophylaxis of\u000a      postoperative endophthalmitis following cataract surgery: results of the\u000a      ESCRS multicentre study and identification of risk factors. Journal of\u000a      Cataract Refractive Surgery, 33(6), 978-988\u000a    \u000a\u000a4. Barry, P., Gardner, S., Seal, D., Gettinby, G., Lees, F., Peterson, M.\u000a      &amp; Revie, C. (2009) Clinical observations associated with proven and\u000a      unproven cases in the ESCRS study of prophylaxis of postoperative\u000a      endophthalmitis after cataract surgery. Journal of Cataract &amp;\u000a      Refractive Surgery, 35, 1523-1531.\u000a    \u000a\u000a5. McKendrick. I.J., Gettinby, G., Gu, Y., Reid, S.W.J. and Revie, C.\u000a      (1999) Using a Bayesian belief network to aid differential diagnosis of\u000a      tropical bovine diseases. Preventive Veterinary Medicine , 47: 141-156\u000a    \u000a\u000a6. Gu, Y., Gettinby, G., McKendrick. I.J., Murray, M., Peregrine, A. and\u000a      Revie, C. (1999) Development of a Decision Support System for Trypanocidal\u000a      Drug Control of Bovine Trypanosomosis in Africa. Veterinary Parasitology,\u000a      87: 9-23\u000a    \u000aOther evidence for quality of research:\u000a    References 2, 3 and 4 were the journal's most cited papers in their\u000a      respective year of publication. Since publication, references 2 and 3 have\u000a      been cited 223 and 157 times respectively. Reference 3 provided further\u000a      in-depth findings following the completion of the study. All papers were\u000a      published in the same journal at the request of the funding body in order\u000a      to ensure the most rapid and effective dissemination of results to the\u000a      European community of cataract surgeons. A series of grants valued at\u000a      &#8364;314,000 were made to the University of Strathclyde between March 2002 and\u000a      Feb 2007 for the study of Antibiotic Prophylaxis in Cataract Surgery by\u000a      the European Society of Cataract and Refractive Surgery.\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"11","Level2":"13","Subject":"Ophthalmology and Optometry"},{"Level1":"11","Level2":"3","Subject":"Clinical Sciences"}],"Sources":"\u000a    \u000a      Statement from President of the European Society of Cataract and\u000a        Refractive Surgery and Consultant Ophthalmic Surgeon Royal Victoria Eye\u000a        and Ear and St Vincent's University Hospitals Dublin Ireland, supports\u000a        the claim(s) that Gettinby and Revie were approached on the basis of\u000a        their prior research, that their research influenced ESCRS guidelines\u000a        and there has been an international impact in reducing blindness in\u000a        cataract patients.\u000a      Izdebska J and Szaflik JP (2005) Levofloxacin (oftaquix) a\u000a        fluoroquinolone of a new generation in prevention of the postoperative\u000a        endophthalmitis following uncomplicated cataract surgery &#8212; the study of\u000a        the European Society of Cataract and Refractive Surgeons (ESCRS).\u000a        Klinika Oczna. 107: 344-347.\u000a      Document &#8212; ESCRS Guidelines on prevention, investigation and\u000a        management of post-operative endophthalmitis (August 2007)\u000a      European Society of Cataract and Refractive Surgery website http:\/\/www.escrs.org\/\u000a\u000a      Ocular Surgery News &#8212; shows increased used of antibiotic injections\u000a        across Europe http:\/\/www.healio.com\/ophthalmology\/cataract-surgery\/news\/online\/%7B7f141eb8-dfd9-4f2f-b341-18cb41bbdf7f%7D\/increasing-number-of-european-ophthalmologists-rely-on-intracameral-cefuroxime-for-endophthalmitis-prevention\u000a\u000a      ESCRS EUROTIMES vol 18, Issue 3, March 2013 pages 4-6:\u000a        http:\/\/issuu.com\/eurotimes\/docs\/et18-3\u000a        with testimonials from surgeons\u000a    \u000a    ","Title":"\u000a    Changes to European Society of Cataract and Refractive Surgery\u000a        guidelines reduces loss of sight for patients undergoing cataract\u000a        surgery\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Context: Cataract surgery with intra-ocular lens implantation is\u000a      the most commonly performed surgical procedure in the aging European\u000a      population. In 2003, almost 2.5 million operations were performed in\u000a      France, Germany, Italy, Spain and the United Kingdom [1]. With an\u000a      increasing elderly population, the number of people worldwide requiring\u000a      such surgery will rise in the decades to come. Although cataract surgery\u000a      procedures are usually successful in restoring failing eyesight, they were\u000a      responsible for permanent and significant loss of vision resulting from\u000a      severe postoperative infective endophthalmitis, acute inflammation of the\u000a      eye arising from bacterial infection, in up to 0.1% of patients. This\u000a      unacceptably high risk of post-operative infection with loss of vision was\u000a      an important unsolved problem of the procedure. In 2003, the only\u000a      prophylactic procedure against infection regarded as proven was the use of\u000a      povidone iodine (5%) in the conjunctival sac just before surgery. Numerous\u000a      retrospective, comparative or single-use studies had been published in\u000a      support of various proposed regimes of antibiotic use as prophylaxis. All,\u000a      however, could be suspected of bias, or were of insufficient power to\u000a      provide convincing evidence of benefit.\u000a    Key findings: Because of the rarity of the complication, the size\u000a      of study needed to establish the benefit of any proposed regime was beyond\u000a      the reach of any single ophthalmic unit. In recognition of this the\u000a      European Society of Cataract and Refractive Surgery approached the\u000a      Informatics and Epidemiology modelling group at the University of\u000a      Strathclyde to design and execute a large scale clinical European study,\u000a      together with state-of-the-art statistical analysis. This group was\u000a      internationally recognised for their work on integrating mathematical and\u000a      information modelling approaches in areas such as differential diagnosis\u000a      of disease using a Bayesian belief network [5] and a decision support\u000a      system on drug use for control of trypanosomiasis in Africa [6], and this\u000a      previous research led directly to the invitation from ESCRS.\u000a    The statistical design identified was a prospective randomised study\u000a      based on Fisher's classical 2x2 factorial design principle. Such a design\u000a      was novel for a cataract study and rarely possible in such large scale\u000a      multinational studies for ethical and logistical reasons, but optimum for\u000a      estimating treatment effects and their interaction. The study involved\u000a      twenty-four hospitals and ophthalmic clinics in Austria, Belgium, Germany,\u000a      Italy, Poland, Portugal, Spain, Turkey, and the United Kingdom. It tested\u000a      the use of the cefuroxime antibiotic and one other treatment, the\u000a      perioperative use of drops of the antibiotic levofloxacin. The study was\u000a      led by the statistical and data management unit at the University of\u000a      Strathclyde involving staff in the Department of Mathematics and\u000a      Statistics and the Department of Computer Science, and supported by an\u000a      administrative office in Ireland and a co-ordinating centre in England.\u000a    By December 2005 the study had recruited almost 14000 patients and it was\u000a      becoming clear that the study was almost certainly demonstrating that at\u000a      least one of the methods of prophylaxis under test was proving to be\u000a      effective. As a result, recruitment of patients into the study was brought\u000a      to an end in January 2006, by which date over 16000 patients had been\u000a      recruited. The principal result was that levofloxacin administered\u000a      topically was ineffective, whereas those patients receiving intracameral\u000a      cefuroxime injection had a large reduction in the incidence of\u000a      endophthalmitis. This led to publication of the preliminary results [2] in\u000a      the Journal of Cataract and Refractive Surgery in March 2006. This journal\u000a      was chosen by the European Society of Cataract and Refractive Surgeons so\u000a      that the results could be directly disseminated to a large proportion of\u000a      the community of European ophthalmic surgeons as quickly as possible. Full\u000a      publication of the definitive results took place in 2007 so that the\u000a      follow-up of late recruits to the Study could be completed [3]. This\u000a      showed to the community of ophthalmic surgeons that this use of cefuroxime\u000a      as a prophylactic results in an approximate 5-fold reduction in the risk\u000a      of post-operative endophthalmitis.\u000a    The use of intracameral cefuroxime led to a reduced endophthalmitis\u000a      incidence rate of 5 per 10000 patients compared to 35 per 10000 patients\u000a      in the control group. A further publication [4] investigated more fully\u000a      those cases of endophthalmitis which had occurred during the study and in\u000a      particular all clinical outcomes associated with the endophthalmitis\u000a      cases.\u000a    Key researchers: The key researchers were Professor George\u000a      Gettinby and Mr Magnus Peterson of the Department of Mathematics &amp;\u000a      Statistics who designed the study, undertook the analyses and prepared the\u000a      papers for publication, and Professor Crawford Revie and Ms Fiona Lees of\u000a      the Department of Computer and Information Sciences who developed the\u000a      multi-site database and undertook data acquisition and management\u000a      operations over the 5 year period of the study. Medical contributions were\u000a      provided by the Study Director, Dr Peter Barry, a consultant ophthalmic\u000a      surgeon based in Dublin and the Clinical Trial Coordinator, Dr David Seal,\u000a      a consultant ophthalmologist based at the University of London.\u000a    "},{"CaseStudyId":"42315","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2963597","Name":"Ireland"},{"GeoNamesId":"3144096","Name":"Norway"}],"Funders":["Natural Environment Research Council"],"ImpactDetails":"\u000a    Process from research to impact\u000a    In the early 1990's, the International Council for the Exploration of the\u000a      Sea (ICES) viewed cod stocks in the North Sea to be in a declining state\u000a      and consistently advised cuts in fishing, to the point of recommending\u000a      zero Total Allowable Catch (TAC). Nevertheless, the EU Council of\u000a      Ministers consistently agreed on TAC's in excess of the scientific advice.\u000a      The problem was that cod were caught in a mixed-species fishery so vessels\u000a      fishing for other species could not avoid catching cod regardless of\u000a      whether they were legally allowed to land them. Zero TAC for cod would\u000a      have effectively closed the North Sea for all demersal fisheries with\u000a      severe economic consequences. However, in 2000 the Commission was asked to\u000a      seek additional conservation measures that might protect cod whilst\u000a      enabling a continuing fishery for other species. The first of these, in\u000a      2001, was the emergency establishment of `closed areas' containing high\u000a      densities of cod, where fishing for all species was prohibited [Source A].\u000a      The Commission then sought scientific advice from member states as to\u000a      whether the 2001 closed areas were likely to be effective in promoting\u000a      stock recovery. In the UK, Defra then commissioned the research at\u000a      Strathclyde into spatial simulation modelling methods for cod in the North\u000a      Sea, as a means of analysing the effectiveness of the closed area\u000a      strategy. The resulting advice from Strathclyde was that simply closing\u000a      selected areas to fishing was unlikely to be effective due to displacement\u000a      of effort into neighbouring regions. The most effective measure was\u000a      permanent removal of fishing capacity from the system [Source C]. These\u000a      results formed part of the UK evidence supplied to the Commission [Source\u000a      D] and, on the basis of this evidence, the Commission abandoned the closed\u000a      area policy in 2004 and instead implemented the `Cod Recovery Plan'\u000a      [Source B].\u000a    Impact on the state of cod stocks, 2008-2012:\u000a    Although the Recovery Plan was implemented in 2004 significant impact on\u000a      cod numbers was not expected to be seen for a number of years, and the aim\u000a      was to reverse the decline of stocks by 2009. Annual stock assessments\u000a      showed that the decline was successfully halted by 2007 and by 2012 the\u000a      stock in the North Sea had recovered to just below the level at which its\u000a      viability is considered to be at risk [Source E].\u000a    Impact on fisheries policy, 2004-2012:\u000a    The Recovery Plan required EU member states to enforce drastic reductions\u000a      in catch quotas for cod and overall fishing capacity [Source F]. In the\u000a      case of the UK this was achieved by a scheme for decommissioning vessels\u000a      and restricting the permitted number of days a vessel was allowed to spend\u000a      at sea [Source G].\u000a    Economic impact:\u000a    The peak value of the North Sea cod fishery was more than &#163;450 million\u000a      per annum in the late 1970's (first sale value, standardised by the\u000a      Consumer Price Index to year 2000). However, this was not sustainable and\u000a      resulted in the overfishing which caused the decline in stocks. Current\u000a      estimates of the long-term sustainable value are around &#163;150 million per\u000a      year. Between 1978 and 2007 the first sale value of cod landings declined\u000a      by an average &#163;14 million per year to a minimum of less than &#163;35 million.\u000a      Since 2008 the quay-side value has increased, and by 2011 the Recovery\u000a      Plan and its successors had restored the annual value of the North Sea cod\u000a      landings to &#163;52 million. Hence, the Strathclyde research contributed to a\u000a      &#163;17 million per annum increase in the value of the cod fishery by 2011\u000a      compared to the low-point in 2007. The UK share of this international\u000a      fishery is around 45%.[Source H]\u000a    Public awareness of recovery of cod stocks:\u000a    The crisis in the fishing industry precipitated by the Closed Area Policy\u000a      and the Recovery Plan attracted media attention and raised public\u000a      awareness of the state of the stocks. High profile conservation campaigns\u000a      encouraged consumers to avoid buying cod. However there is evidence that\u000a      public opinion now recognises that cod stocks are recovering. Recent media\u000a      interest, for example via the BBC [Source I], reports that Barrie Deas,\u000a      the Chief Executive of the National Federation of Fishermen's\u000a      Organisations, which represents fisherman in England, Wales and Northern\u000a      Ireland, told Radio 4's Today programme that the recovery of stocks was a\u000a      \"dramatic turnaround ...I think a major part of it is there are fewer\u000a        vessels out there. There have been big decommissioning schemes. There's\u000a        also been a change in the mindset in the industry. We work very closely\u000a        with the scientists now.\" Another recent example of media interest\u000a      was an item in the Daily Telegraph (10th June 2013) quoting\u000a      Richard Benyon, the Fisheries Minister, as saying: \"We should not be\u000a        complacent, there is still a long way to go, but this is really good\u000a        news. People can eat cod without feeling guilty because there are large\u000a        quantities being caught further north, and our cod stocks in the North\u000a        Sea are recovering. Much of the credit for this must rest with the\u000a        fishermen who have introduced a vast number of [sustainable fishing]\u000a        measures\" [Source J]. This reflects both the impact of the Recovery\u000a      Plan, and the extent of public awareness of the issue.\u000a    Reach and significance: The impact extended through the UK\u000a      government fisheries agencies (Defra and Marine Scotland), to the EU\u000a      Commission [B,C,D]. Cod is the most important fish species targeted by\u000a      trawl fisheries in the North Sea [E], and the crisis in the fishing\u000a      industry associated with its decline and recovery raised public awareness\u000a      of the research that underpins fisheries policy [I,J]. The short-term\u000a      hardship (due to curtailed fishing opportunities [G]) and the accruing\u000a      economic benefits due to recovering cod stocks [H] are being felt by\u000a      fishing communities and industries throughout Europe and Norway which have\u000a      an economic interest in the North Sea demersal fisheries. The general\u000a      public are now more aware that cod stocks are recovering.\u000a    ","ImpactSummary":"\u000a    In 2012, cod stocks in the North Sea were assessed as having recovered\u000a      almost to a level at which their viability is considered to be safe. This\u000a      recovery followed 3 decades of progressive depletion to only 50% of the\u000a      safety threshold of abundance. Achieving this recovery required the EU to\u000a      abandon an earlier `closed area' policy banning fishing in selected areas\u000a      of the North Sea, and instead enforce drastic cuts in overall activity on\u000a      national fishing fleets. The policy change was prompted in part by\u000a      predictions from mathematical modelling of cod populations by researchers\u000a      at Strathclyde, showing that the `closed area' policy was unlikely to be\u000a      an effective strategy for recovery. The recovery has so far restored &#163;17\u000a      million in annual value to the fishery.\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University of Strathclyde\u000a    ","Institutions":[{"AlternativeName":"Strathclyde (University of)","InstitutionName":"University of Strathclyde","PeerGroup":"B","Region":"Scotland","UKPRN":10007805}],"Panel":"B         ","PlaceName":[],"References":"\u000a    References 1, 2 and 3 best exemplify the quality of the underpinning\u000a      research\u000a    \u000a1. Gurney, W.S.C., Speirs, D.C., Wood, S.N., Clarke, E.D. and\u000a      Heath, M.R. (2001). Simulating spatially and physiologically structured\u000a      populations. Journal of Animal Ecology 70, 881-894.\u000a    \u000a\u000a2. Speirs, D., Gurney, W.S.C., Heath, M.R., Horbelt, W., Wood, S.\u000a      and de Cuevas, A. (2006). Ocean-scale modelling of the distribution,\u000a      abundance, and seasonal dynamics of the copepod Calanus\u000a      finmarchicus. Marine Ecology Progress Series 131, 183-192.\u000a    \u000a\u000a3. Clarke, E.D., Speirs, D.C., Heath, M.R., Wood, S.N., Gurney,\u000a      W.S.G., Holmes, S.J. (2006). Calibrating remote sensed chlorophyll a\u000a      data using penalized regression splines. Journal of the Royal\u000a        Statistical Society: Series C (Applied Statistics) 55(3) 331-353.\u000a    \u000a\u000a4. Andrews, J.M., Gurney, W.S.C., Heath, M.R.,Gallego, A., O'Brien, C.M.,\u000a      Darby C. and Tyldesley, G. (2006). Modelling the spatial\u000a      demography of cod on the European continental shelf. Canadian Journal\u000a        of Fisheries and Aquatic Sciences, 63, 1027-1048.\u000a    \u000a\u000a5. Gurney W.S.C., Veitch R. (2007). The dynamics of size at age\u000a      variability. Bulletin of Mathematical Biology 69, 861-885.\u000a    \u000a\u000a6. Hedger, R., McKenzie, E., Heath, M., Wright, P., Scott, B., Gallego,A.\u000a      and Bridson, J. 2004. Analysis of the spatial distributions of mature cod\u000a      (Gadus morhua) and haddock (Melanogrammus aeglefinus) abundance in the\u000a      North Sea (1980-1999) using Generalised Additive Models. Fisheries\u000a        Research 70, 17-25.\u000a    \u000aOther evidence for quality of research (grants, patents etc.)\u000a    The research group at Strathclyde developed and supported its research\u000a      programme, and continues to do so, on the basis of competitive funding\u000a      from NERC and Defra. Development of the initial model methodology was\u000a      supported by two consortium grants (2000-2004, &#163;593k), led by Strathclyde,\u000a      from the NERC Marine Productivity Thematic Programme. Further development\u000a      of the models to investigate cod populations was carried out in a\u000a      Strathclyde led consortium project funded by Defra (2000-2003, &#163;500k). On\u000a      the basis of this, Defra commissioned the targeted research on closed\u000a      areas which ultimately yielded the impact on cod stock recovery.\u000a      Additional relevant funding has been a stake in an EU project on North\u000a      Atlantic cod (1998-2001, EU-FAIR-CT98-4122, consortium total award 1.13M\u000a      Euro), and the research has continued under a NERC consortium grant from\u000a      the Sustainable Marine Bioresources programme (2009-2012, &#163;114,363 to\u000a      Strathclyde).\u000a    ","ResearchSubjectAreas":[{"Level1":"6","Level2":"2","Subject":"Ecology"},{"Level1":"5","Level2":"2","Subject":"Environmental Science and Management"}],"Sources":"\u000a    A. Commission Regulation (EC) No 259\/2001 of 7 February 2001 establishing\u000a      measures for the recovery of the stock of cod in the North Sea (ICES\u000a      subarea IV) and associated conditions for the control of activities of\u000a      fishing vessels.\u000a      http:\/\/eur-lex.europa.eu\/LexUriServ\/LexUriServ.do?uri=OJ:L:2001:039:0007:0010:EN:PDF\u000a    B. EU Council Regulation (EC) No 423\/2004 of 26 February 2004\u000a      establishing measures for the recovery of cod stocks\u000a      http:\/\/eur-lex.europa.eu\/LexUriServ\/LexUriServ.do?uri=OJ:L:2004:070:0008:0011:EN:PDF\u000a    C. Darby C., Hutton T., Andrews J., Gurney W.S.C., Beveridge D., Hiddinck\u000a      J.G. (2006) Investigations into closed area management of the North Sea\u000a      cod. Cefas Contract report, p62-75. (Peer reviewed final report from a\u000a      research project commissioned by Defra to investigate the effectiveness of\u000a      closed area policies for conserving cod using the Strathclyde model &#8212;\u000a      Defra Reference: SFCD15, January-May 2005).\u000a    D. D.\u000a        http:\/\/www.cefas.co.uk\/publications\/files\/EU_Norway_expert_gp_codrecovery-may-2003.pdf\u000a      STECF meeting on cod assessment and technical measures, Brussels, 28\u000a      April-7 May 2003 127 pp.\u000a    E. ICES (2012). Advice Book 2012. Section 6.4.2 Cod in Subarea IV (North\u000a      Sea), Division VIId (Eastern Channel), and IIIa West (Skagerrak)\u000a      http:\/\/www.ices.dk\/sites\/pub\/PublicationReports\/ICES Advice\/2012\/ICES ADVICE 2012 BOOK 6.pdf\u000a    F. Council Regulation (EC) No 1342\/2008 of 18 December 2008 establishing\u000a      a long-term plan for cod stocks and the fisheries exploiting those stocks\u000a      and repealing Regulation (EC) No 423\/2004\u000a      http:\/\/eur-lex.europa.eu\/LexUriServ\/LexUriServ.do?uri=CELEX:32008R1342:EN:NOT\u000a    G. Almond, S &amp; Thomas, B. 2011. The UK fishing industry in 2010.\u000a      Structure and activity. UK Marine Management Organisation, 62pp.\u000a      http:\/\/marinemanagement.org.uk\/fisheries\/statistics\/documents\/ukseafish\/2010\/structure_ac\u000a        tivity.pdf\u000a    H. Evidence derived from: Almond, S &amp; Thomas, B. 2011. The UK Sea\u000a      Fisheries Statistics 2010. UK Marine Management Organisation, 158pp.\u000a      http:\/\/www.marinemanagement.org.uk\/fisheries\/statistics\/documents\/ukseafish\/2010\/final.pdf\u000a    I. http:\/\/www.bbc.co.uk\/news\/science-environment-22820162\u000a      BBC coverage of cod recovery\u000a    J. Article in The Telegraph, 10 June 2013: Britons \"Should not feel\u000a        guilty about eating cod\". (http:\/\/www.telegraph.co.uk\/earth\/wildlife\/10108952\/Britons-should-not-feel-guilty-about-eating-cod.html\u000a    ","Title":"\u000a    Recovery of cod stocks in the North Sea achieved by a change in EU\u000a        fisheries policy driven by evidence from mathematical models\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"},{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Context: Simulating spatial patterns in the demography of mobile\u000a      species is particularly challenging and a general problem in mathematical\u000a      ecology. However, validated models of this type are extremely powerful\u000a      tools since they provide a means of conducting virtual experiments to\u000a      diagnose the key factors affecting populations. This includes predicting\u000a      the consequences of climate change and, for commercially exploitable taxa,\u000a      changes in spatial patterns of harvesting. The research described here\u000a      provided a significant advance in capability in this area and was used to\u000a      support a policy change in fisheries management.\u000a    Key Research Findings: [Numbers in parentheses refer to\u000a        research articles listed in Section 3] A numerical technique for\u000a      modelling spatial populations was developed during two NERC research\u000a      grants between 2000 and 2006 [1]. The technique was used to simulate the\u000a      spatial distribution and population dynamics of a marine plankton species\u000a      (Calanus finmarchicus) which is particularly abundant in the North\u000a      Atlantic Ocean and is an indicator species for impacts of climate change\u000a      [2]. The life cycle involves spawning, development and dispersal by ocean\u000a      currents in the surface waters during spring and summer, alternating with\u000a      a dormant phase at depths of &gt;600 m in the winter. The modelling\u000a      technique was able to represent these developmental and dispersal\u000a      processes at a spatial resolution of a few tens of kilometres over the\u000a      whole North Atlantic, combining data on water currents and temperatures\u000a      from an ocean circulation model, and on the food of Calanus from\u000a      satellite remote sensing archives [3].\u000a    The modelling technique was then used to simulate the spatial population\u000a      dynamics of cod in the North Sea during a Defra funded research project\u000a      (2001-2005) [4], building on a body of work on the mathematics of growth\u000a      in fish [5]. It was fitted to cod distributions derived by statistical\u000a      analyses of survey data [6] as part of an EU funded project, and explained\u000a      changes in cod distribution in terms of temperature, migration behaviour,\u000a      and spatial patterns of fishing. On the basis of this capability, Defra\u000a      commissioned researchers at Strathclyde to simulate the effects of\u000a      imposing fishing moratoria in various configurations of spatial regions\u000a      (marine protected areas) in the North Sea as part of the evidence base for\u000a      a policy consultation by the EU Commission.\u000a    The development of this modelling technique [1], combining mathematical\u000a      representations of key biological processes with spatial resolution, was a\u000a      major technical achievement and a significant advance in the field. A key\u000a      finding of the research on Calanus [2] was that sub-regions of\u000a      high population abundance around the North Atlantic are interconnected by\u000a      passive transport of Calanus life stages. Key findings from the\u000a      research on cod were that transport of eggs and larvae by water currents,\u000a      and active migrations of adults to spawning sites were major factors in\u000a      maintaining the spatial structure of the stock [4]. Displacement of\u000a      fishing effort from small scale closed areas into the remaining open\u000a      spaces negated any beneficial effect of the closure as a conservation\u000a      measure. The models predicted that the most effective action for stock\u000a      conservation was to reduce the overall regional fishing effort.\u000a    The research was published in leading marine science journals,\u000a      specifically those with top ten impact factors in the \"Fisheries\" category\u000a      such as the Canadian Journal of Fisheries and Aquatic Science and\u000a      Fisheries Oceanography or the second top journal in the \"Zoology\"\u000a      category, the Journal of Animal Ecology. The novel technical\u000a      aspects of these models have also been published in leading mathematical\u000a      and statistical journals (Bulletin of Mathematical Biology, Journal\u000a        of the Royal Statistical Society: Series C.).\u000a    Key researchers at the University of Strathclyde:\u000a    The research was originally conceived and led by W.S.C Gurney (Professor\u000a      in Department of Mathematics in 2000; retired 2011, then part-time\u000a      contract until 2014) and continues under the leadership of Professor M.\u000a      Heath, who was involved in the project during previous employment at\u000a      Marine Scotland Science, joining the University of Strathclyde in 2010.\u000a    Dr D. Speirs (postdoctoral researcher) worked on the spatial modelling of\u000a      Calanus in the North Atlantic during 2000-2006. Dr J. Bridson nee Andrews\u000a      (postdoctoral researcher 2001-2006) worked on developing the model to\u000a      represent cod in the North Sea. E. McKenzie (Professor of Mathematics\u000a      2001; retired 2011) and Dr R. Hedger (postdoctoral researcher 1999-2002)\u000a      contributed the statistical analysis of cod data.\u000a    Key collaborators at other institutions:\u000a    Calanus modelling &#8212; University of St Andrews (S. Woods and E.\u000a      Clarke)\u000a    Cod modelling &#8212; Centre for Environment, Fisheries and Aquaculture\u000a      Science, Lowestoft (C. Darby and C. O'Brien); Marine Scotland Science,\u000a      Aberdeen (M. Heath, now University of Strathclyde member of staff, and P.\u000a      Wright)\u000a    "},{"CaseStudyId":"42316","Continent":[],"Country":[],"Funders":["Research Councils UK","Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000d\u000a    The main academic collaborators, Grindrod (Reading) Estrada and Higham\u000d\u000a      (both Strathclyde) made equal contributions in the co-authored\u000d\u000a      publications that led to this impact.\u000d\u000a    Process from research to impact\u000d\u000a    Bloom is a Leeds-based digital marketing and advertising company. The\u000d\u000a      Bloom Insight Team, led by Peter Laflin, came across the SIAM News article\u000d\u000a      [3], which cites the Physical Review E paper [2]. After implementing the\u000d\u000a      algorithms in [2] with some success, Bloom contacted Grindrod and Higham\u000d\u000a      and have since engaged in a range of on-going discussions about the best\u000d\u000a      way to apply the algorithms to real social media data and to deal with\u000d\u000a      various practical challenges. In particular, Bloom obtained Technology\u000d\u000a      Strategy Board funding in 2011 to investigate whether the concept of\u000d\u000a      dynamic communicability, which was introduced in [2], can be exploited by\u000d\u000a      their data analytics team. This 12 month project, entitled Digital\u000d\u000a      Business Analytics for Decision Makers, had costs shared between Bloom\u000d\u000a      (&#163;64K) and TSB (&#163;94K). The project led directly to the development of\u000d\u000a      Bloom's planning tool Whisper, which includes a facility to quantify and\u000d\u000a      monitor a client's current visibility and influence in the on-line digital\u000d\u000a      arena.\u000d\u000a    Collaboration and knowledge exchange with Bloom has also led to a\u000d\u000a      co-authored case study [5] illustrating the success of the approach, and\u000d\u000a      further academic research [4] motivated by issues raised by Bloom. Higham\u000d\u000a      has visited Bloom's offices on three occasions, and members of the Bloom\u000d\u000a      team have visited Strathclyde a further three times. The two groups have\u000d\u000a      co-presented work at two conferences (Digital Economy Annual All Hands\u000d\u000a      Meeting, Aberdeen 2012 and Socinfo 2012, Lausanne). Dr Alex Mantzaris, an\u000d\u000a      EPSRC-funded Research Assistant at Strathclyde from 2011-2013 is now\u000d\u000a      based at Bloom, working on a 12 month joint EPSRC\/Bloom funded knowledge\u000d\u000a      exchange project, where further insights from the Mathematics of Large\u000d\u000a      Technological Evolving Networks (MOLTEN) project are being shared. Bloom's\u000d\u000a      policy is to publicize their achievements and position themselves as a\u000d\u000a      leading player in digital media, working alongside cutting edge academic\u000d\u000a      partners. Hence, in addition to running confidential client-driven\u000d\u000a      projects, they have published a number of public domain blogs that\u000d\u000a      showcase this network methodology in analysing, for example, social media\u000d\u000a      activity around television and sports events.\u000d\u000a    Types of Impact\u000d\u000a    Adoption of new data analytics tool:\u000d\u000a    The Bloom website for Whisper [Source 1] recognizes the Strathclyde\u000d\u000a      research: \"In 2011, we set about building Whisper with the support of a\u000d\u000a      grant from the Technology Strategy Board. Working with the Universities of\u000d\u000a      Reading and Strathclyde, we have been able to implement innovative\u000d\u000a      mathematical algorithms to measure how influence in social networks\u000d\u000a      changes over time.\" The website also presents a range of topical case\u000d\u000a      studies. Their website states that \"Whisper uses social media data to\u000d\u000a      provide a deeper understanding of your customer profile-their brand\u000d\u000a      affinity, their mood, what device they're using and where they are.\u000d\u000a      Whisper can track changes in your customer profile in real-time...(and)\u000d\u000a      pinpoints exactly which social media profiles your brand should be\u000d\u000a      following, monitoring and engaging with during a campaign, helping your\u000d\u000a      brand's message to transfer to new networks as the conversation grows. ..\u000d\u000a      Whisper matches your CRM database with their social media accounts. This\u000d\u000a      means you can use these customers on social media to find new customers\u000d\u000a      and track which accounts went on to purchase. Whisper's ability to show\u000d\u000a      how, when and why a message both starts and stops having impact despite\u000d\u000a      continuing tweets and retweets can help your brand to understand how\u000d\u000a      messages become viral. This information can be used to accurately plan\u000d\u000a      creative campaigns that achieve virality.\"\u000d\u000a    In a letter to Higham [Source 2], Peter Laflin, Head of Data Insight at\u000d\u000a      Bloom, makes a range of points that indicate the advances that the company\u000d\u000a      has achieved through the implementation of the research of Higham et al.\u000d\u000a      He confirms that Whisper is the first data analytics tool that can\u000d\u000a      accurately measure impact and Return on Investment from social media. He\u000d\u000a      states \"At the heart of Whisper is a specific implementation of your work\u000d\u000a      and the measure of `influence' is a proxy for your communicability ideas.\"\u000d\u000a      He estimates \"by the end of 2013 Bloom expects to have invested close to\u000d\u000a      &#163;200K in the research and development project around Whisper.\"\u000d\u000a    Laflin explains that \"A major use of Whisper is to speed up the market\u000d\u000a      research cycle. By mining social data at scale, and then filtering noise\u000d\u000a      through the use of influence metrics, we are able to quickly and\u000d\u000a      accurately assess the \"Mood, Mode and Context\" of a conversation, which\u000d\u000a      provides more specific and targeting information than a traditional\u000d\u000a      marketing research would provide. This is saving our clients' money,\u000d\u000a      cutting the costs of conducting research to assess market conditions and\u000d\u000a      dynamics.\"\u000d\u000a    Improvement in business plans and performance: In terms of Bloom\u000d\u000a      and their clients, Laflin states that \"Our brand is heavily focused around\u000d\u000a      having unique insight for our clients, and Whisper allows us to deliver\u000d\u000a      this; in fact we changed our brand direction to include \"unique insight\"\u000d\u000a      as a direct consequence of the success of implementing your ideas. This\u000d\u000a      brand direction has been nominated for a 2013 Drum Award for the\u000d\u000a        Digital Industries (DADI). Whisper has opened doors for Bloom;\u000d\u000a      meetings with Adidas, SKY, Nestle, Perform, Peugeot, Emnos, QVC, Centre\u000d\u000a      Parcs, BAT, Virgin Atlantic and ITV have all resulted from Bloom's ability\u000d\u000a      to use Whisper to uncover unique insight. These brands are becoming\u000d\u000a      clients of Bloom and the reason they are converting is because the\u000d\u000a      technology and insight we can provide cannot be found anywhere else in the\u000d\u000a      market place. Without Whisper, Bloom would not have had the opportunity to\u000d\u000a      excel in these meetings and provide a platform for delivering work on\u000d\u000a      behalf of these clients. Whisper is also providing us with an opportunity\u000d\u000a      to white label analytics to other marketing agencies.\" Laflin summarizes\u000d\u000a      the impact by saying \"Without your support, or the support of the TSB,\u000d\u000a      this work would never have got off the ground and Bloom would not be\u000d\u000a      speaking to some of the world's largest brands.\"\u000d\u000a    The Drum, an Advertising\/Marketing Trade Magazine, reported on 2 April\u000d\u000a      2013 that [Source 3] \"Leeds-based digital marketing firm Bloom has grown\u000d\u000a      its income to &#163;2.4 million it has revealed, as a result of working\u000d\u000a      alongside brands such as LA Fitness, Anglian Home Improvements and\u000d\u000a      Infosys. The agency, which has doubled its staff numbers to around 60 in\u000d\u000a      recent months, has grown its income by half, it claimed, while also\u000d\u000a      released a real-time social planning tool for brands, Whisper.\"\u000d\u000a    Alex Craven, chief executive of Bloom explained how placing data driven\u000d\u000a      insight at the heart of the agency's model had impacted, saying: \"This\u000d\u000a      change in direction has driven some significant wins for us as we become\u000d\u000a      the agency of record for well-known brands. As real time, content driven\u000d\u000a      campaigns continue to grow in importance for brands, we believe we are\u000d\u000a      well positioned for growth over the next two years.\"\u000d\u000a    Skilled employment: Since October 2011, Bloom has hired two\u000d\u000a      mathematics graduates from the University of Leeds, in order to work on\u000d\u000a      embedding the algorithms from the underpinning research [2] into Whisper.\u000d\u000a      Bloom is providing &#163;26K (matching the EPSRC\/Strathclyde Impact\u000d\u000a      Acceleration Account funding) to second Dr Alex Mantzaris for 12 months,\u000d\u000a      and has also sponsored a CASE studentship at the University of Leeds that\u000d\u000a      will develop ideas in this area.\u000d\u000a    Wider recognition for the new technology:\u000d\u000a    Bloom's use of Whisper for Anglian Home Improvements was shortlisted at\u000d\u000a      both the 2012 Some Comms Awards and the 2012 Social Buzz Awards.\u000d\u000a    Bloom staff Laflin, Ainsley and Otley also co-authored with Grindrod,\u000d\u000a      Higham and Mantzaris an entry in the Royal Society 2013 Picturing Science\u000d\u000a      Competition [Source 4]. Their image, titled `Twitter activity: a\u000d\u000a        snaphot of tweeter-follower interactions as a conversation grows',\u000d\u000a      created with the Whisper software, was runner up in the Infographics\u000d\u000a      category, and therefore went on public display at the Summer Science\u000d\u000a        Exhibition 2013 on 2-7 July in Carlton House garden, attended by\u000d\u000a      Bloom representatives.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research by Higham, Estrada and Grindrod into new, computable measures\u000d\u000a      for large, dynamically evolving communication networks has allowed the\u000d\u000a      automatic identification of individuals who act as influencers, or\u000d\u000a      efficient listeners. This research insight has been taken up by Bloom\u000d\u000a      Agency (Leeds), a digital marketing and media agency. Bloom has used these\u000d\u000a      ideas to strengthen their Data Insights Team, leading to investment in new\u000d\u000a      jobs, generation of new business and delivery of better results for their\u000d\u000a      clients. Bloom's commercially available real time social planning software\u000d\u000a      product, Whisper, builds directly on the published research, and is at the\u000d\u000a      heart of the agency's success in doubling staff numbers to 60 in recent\u000d\u000a      months, having grown its annual income by 50% to &#163;2.4Million through the\u000d\u000a      use of these new tools.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Strathclyde\u000d\u000a    ","Institutions":[{"AlternativeName":"Strathclyde (University of)","InstitutionName":"University of Strathclyde","PeerGroup":"B","Region":"Scotland","UKPRN":10007805}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    References 1, 2 and 4 best exemplify the quality of the underpinning\u000d\u000a      research\u000d\u000a    \u000a[1] Evolving graphs: Dynamical models, inverse problems and propagation,\u000d\u000a      P. Grindrod and D. J. Higham, Proceedings of the Royal Society, Series A,\u000d\u000a      466, 2010, 753-770. Included in REF2\u000d\u000a    \u000a\u000a[2] Communicability across evolving networks, P. Grindrod, D. J. Higham,\u000d\u000a      M. C. Parsons and E. Estrada, Physical Review E, 83, 2011, 046120.\u000d\u000a    \u000a\u000a[3] People who read this article also read.... , P. Grindrod, D. J.\u000d\u000a      Higham and E. Estrada, SIAM News, 2011, Part I: January, Part II: March.\u000d\u000a    \u000a\u000a[4] A matrix iteration for summarizing dynamic networks, P. Grindrod and\u000d\u000a      D. J. Higham, SIAM Review, 2013, 55, 118-128.\u000d\u000a    \u000a\u000a[5] Dynamic targeting in an online social medium, P. Laflin, A. V.\u000d\u000a      Mantzaris, F. Ainley, A. Otley, P. Grindrod and D. J. Higham, Social\u000d\u000a      Informatics (Proceedings of Socinfo 2012), Lecture Notes in Computer\u000d\u000a      Science Volume 7710, 2012, 82-95.\u000d\u000a    \u000a\u000a[6] Demonstration of dynamic targeting in an online social medium, P.\u000d\u000a      Laflin, A. V. Mantzaris, P. Grindrod, F. Ainley, A. Otley and D. J.\u000d\u000a      Higham, real time demonstration at Social Informatics 2012, Lausanne.\u000d\u000a    \u000aEvidence for quality of research\u000d\u000a    In this field Professors Estrada and Higham are world leading. Evidence\u000d\u000a      for the quality of the research outputs stemming from these projects is\u000d\u000a      given by the journal publications (Proc. Roy. Soc. A, Physical Review E,\u000d\u000a      SIAM Review, featured article in SIAM News). Further, Higham has given\u000d\u000a      invited, plenary, talks on this work at a diverse range of recent research\u000d\u000a      events, including the SIAM Conference on Applied Linear Algebra (Valencia,\u000d\u000a      2012), the Heilbronn Institute\/Royal Statistical Society meeting on Large\u000d\u000a      Evolving Networks (Bristol 2013), the Big Data and Optimization workshop\u000d\u000a      (Edinburgh, 2013), the NetSci Satellite Meeting on Temporal and Dynamic\u000d\u000a      Networks (Copenhagen, 2013), Bifurcation Theory, Numerical Linear Algebra\u000d\u000a      &amp; Applications (Bath) and the Computational Linear Algebra and\u000d\u000a      Optimization for the Digital Economy workshop (ICMS Edinburgh, 2013).\u000d\u000a    The work in [2, 3, 4, 5, and 6] arose from a collaborative EPSRC\/RCUK\u000d\u000a      Digital Economy funded project \"Mathematics of Large Technological\u000d\u000a      Evolving Networks (MOLTEN)\", 2011-2013. This involved Higham and Estrada\u000d\u000a      (Strathclyde), Grindrod (Reading) and Mascolo (Cambridge), with\u000d\u000a      Strathclyde receiving &#163;180K.\u000d\u000a    An EPSRC\/Strathclyde Impact Acceleration Account\/Bloom Agency funded\u000d\u000a      secondment for the MOLTEN Research Assistant, Dr Alexander Mantzaris, is\u000d\u000a      currently running (2013-2014). This competitively awarded &#163;50K project\u000d\u000a      involves knowledge exchange and follow-on research, based on [1-6].\u000d\u000a    Higham was awarded a Royal Society Wolfson Research Merit Award\u000d\u000a      (2012-2017) on the basis of the project \"Stochastic Modelling and\u000d\u000a      Simulation for Interaction Networks\" which builds on the track record in\u000d\u000a      [1-6]. He was also awarded a 12 month Royal Society\/Leverhulme Trust\u000d\u000a      Fellowship for 2013-2104 to cover teaching and administration as he\u000d\u000a      extends these ideas into continuous-time models and algorithms for\u000d\u000a      evolving networks.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000d\u000a    \u000d\u000a      \u000ahttp:\/\/www.bloomagency.co.uk\/whisper\/\u000d\u000a        supports link to Strathclyde research.\u000d\u000a      Letter from Peter Laflin, Head of Data Insight at Bloom dated August\u000d\u000a        28, 2013 supports various claims of impact on Bloom\u000d\u000a      \u000ahttp:\/\/www.thedrum.com\/news\/2013\/04\/02\/digital-agency-bloom-sees-income-grow-24m-it-doubles-staff-numbers\u000d\u000a        supports claim of economic benefits and growth of Bloom\u000d\u000a      \u000ahttp:\/\/blogs.royalsociety.org\/inside-science\/2013\/07\/01\/picturing-science-competition-winners\/\u000d\u000a        shows wider recognition for the Whisper software.\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Commercial advantage through computational discovery of dynamic\u000d\u000a        communicators in large digital networks\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2657832","Name":"Aberdeen"}],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Context: In 2009, collaborative research began between Desmond\u000d\u000a      Higham (Strathclyde) and Peter Grindrod (Reading) on mathematical\u000d\u000a      modelling and analysis of evolving networks, leading to [1]. Ernesto\u000d\u000a      Estrada (Strathclyde) also became involved at an early stage [2]. The work\u000d\u000a      studied a common set of players, usually representing people, whose\u000d\u000a      interactions appear and disappear over time. This scenario covers\u000d\u000a      time-stamped peer-to-peer communications (who phoned whom, who tweeted\u000d\u000a      whom, etc.). Prior to this, published work on \"evolving networks\" focussed\u000d\u000a      on aggregation over time, for example, where links represent friendship,\u000d\u000a      and new links and nodes accumulate. However, online and digital\u000d\u000a      communication is more accurately represented as a network over a fixed\u000d\u000a      population where links appear and disappear over short time scales.\u000d\u000a      Grindrod and Higham [1] began by developing first-principles discrete time\u000d\u000a      dynamical models for these transient digital communications. This research\u000d\u000a      covered stochastic evolving networks, putting forward a new theoretical\u000d\u000a      framework for describing and analysing time-varying connectivity.\u000d\u000a    Key Findings: The research in [1] led naturally to algorithmic\u000d\u000a      approaches for identifying those people who act as especially strong\u000d\u000a      influencers, or as efficient listeners, within peer-to-peer networks. This\u000d\u000a      issue is critical for the new media sector, where the rise of advertising\u000d\u000a      spend on buzz marketing requires new ways to categorize individuals'\u000d\u000a      social roles and to target influences. Similar questions also arise in\u000d\u000a      security, where there is interest in agents who \"punch above their\u000d\u000a      weight,\" generating a level of influence that belies their apparent\u000d\u000a      low-key status. The work co-authored by Estrada, Grindrod, Higham and\u000d\u000a      Parsons (a PhD student at Reading) [2] used the ideas in [1] to propose a\u000d\u000a      novel and mathematically consistent generalization of Katz centrality-a\u000d\u000a      standard tool in social network analysis that identifies key players in a\u000d\u000a      complex network. Katz centrality applies only to a static network, the\u000d\u000a      work in [2] deals with the case of evolving networks and opens up the\u000d\u000a      possibility of real-time monitoring and prediction. Time's arrow induces\u000d\u000a      asymmetries for dynamical paths through evolving networks and the paper\u000d\u000a      [2] defined a \"communicability matrix\" that summarizes the activity in\u000d\u000a      order to support analysis of influence and strategic targeting. The\u000d\u000a      computational building block in [2] is the solution of a sparse linear\u000d\u000a      system, with sparsity determined by the underlying connectivity pattern.\u000d\u000a      The methods, therefore, scale up to the Big Data setting of tens of\u000d\u000a      millions of vertices. Dealing with such large scale networks is a\u000d\u000a      necessary step for convincing commercial exploiters, and allowing them to\u000d\u000a      evaluate the concepts and methods for their own purposes on realistic data\u000d\u000a      sets.\u000d\u000a    This work was further publicized through a two-part expository article in\u000d\u000a      SIAM News [3] and has been extended by Grindrod and Higham in [4] to deal\u000d\u000a      with \"topicality\" of information. A proof-of-principle case study\u000d\u000a      co-authored with colleagues at Bloom Agency (Leeds) was refereed and\u000d\u000a      accepted as a full paper for the Proceedings of Social Informatics 2012 in\u000d\u000a      Lausanne (acceptance rate 35%). In this work [5] we showed that the\u000d\u000a      computational techniques from [2] and [3] produce influence rankings on\u000d\u000a      very large scale Twitter data that correlate strongly with the views of\u000d\u000a      social media experts. We also presented a hands-on demo, [6], where\u000d\u000a      Twitter activity around the conference hashtag was analysed in real time.\u000d\u000a    "},{"CaseStudyId":"42317","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Process from research to impact:\u000a      C Robertson has a joint appointment with Health Protection Scotland and\u000a      this close working relationship between the researchers based in\u000a      Mathematics and Statistics at Strathclyde and those at Health Protection\u000a      Scotland enables the timely identification and investigation of important\u000a      health research areas. Implementation of the research outputs comes\u000a      through the contribution of university staff to core HPS strategy and\u000a      planning groups, such as HPV Surveillance in Scotland, the MRSA Screening\u000a      Programme Board, Pandemic Influenza, and the Olympic and Commonwealth\u000a      Games Surveillance Group (Source1). Wider dissemination takes place\u000a      through reports to Government and HPS study reports (Source 2).\u000a      Consequently, the results of Strathclyde's research are often transferred\u000a      to and implemented by HPS before publication in journals.\u000a    Preparatory research at the university led to the establishment of a\u000a      national surveillance system for the weekly \"organism report\" around the\u000a      time of the G8 summit in Gleneagles in July 2006. At this time, HPS had no\u000a      facility for processing large amounts of varied, daily produced data\u000a      within a unified surveillance system and the university research led\u000a      directly to the establishment of such systems. A review of the\u000a      surveillance strategy by HPS in the aftermath of the G8 conference\u000a      concluded that statistical methods for the automatic scanning of a large\u000a      amount of daily data were crucial to its operational effectiveness. In the\u000a      period 2008-13, at least 5 separate data analysis systems have been\u000a      developed and tested, based on adoption of Strathclyde's research.\u000a    Types of Impact\u000a        Monitoring Influenza Pandemic:\u000a     During the H1N1v Influenza Pandemic, HPS systems based on Strathclyde's\u000a      research were instrumental in successfully monitoring the evolution of the\u000a      pandemic, estimation of the transmission dynamics of the virus, monitoring\u000a      the numbers of people dying in relation to those expected to succumb; and\u000a      development of a national system for surveillance of GP consultations,\u000a      including the use of NHS24. HPS placed, and continues to place, a huge\u000a      reliance on these systems so that it can function effectively (Source 3).\u000a    Outputs derived from the university research, were regularly used by\u000a      government, health officials, and health protection agencies. Reports were\u000a      prepared for the both the UK Government's Scientific Advisory Group for\u000a      Emergencies (SAGE) and the Scientific Pandemic Influenza Subgroup on\u000a      Modelling (SPI-M),which monitored the course of the pandemic(Source 4).\u000a      Without the university research, HPS would not have had advanced syndromic\u000a      surveillance systems, nor would it have had as sophisticated a response to\u000a      pandemic influenza surveillance (Source 5). The benefits to HPS, and\u000a      consequently the general population, have been apparent throughout the\u000a      period 2008-13, with greatest impact in the period April 2009-June 2010.\u000a      The Clinical Director at HPS during this period (Source 1) states \"This\u000a      research had a great impact at HPS in helping to mitigate the increased\u000a      workload and pressures on the organisation associated with managing a\u000a      national outbreak and reporting, each day, on a wide range of issues\".\u000a    Vaccination programmes:\u000a      Vaccination is the main way of protecting the population against some\u000a      infectious diseases and HPS monitors the impact of the national\u000a      vaccination programmes. The university's research on vaccine effectiveness\u000a      is carried out in conjunction with colleagues at HPS (Dr Jim McMenamin) ,\u000a      Edinburgh University (Dr Colin Simpson, Dr Nazir Lone, Prof Aziz Shiekh)\u000a      and Aberdeen University (Prof Lewis Ritchie). A successful HTA grant\u000a      during the H1N1v pandemic yielded one of the first publications on the\u000a      effectiveness of the pandemic vaccine, demonstrating that the vaccination\u000a      programme had prevented infection, hospitalisations and death. This group\u000a      has also worked on Seasonal Influenza Vaccine Effectiveness (HTA grant\u000a      2011-12) and preparatory work to develop data systems for pandemic\u000a      preparedness (EAVE, HTA 2013-2015). As a result of this research HPS has\u000a      published mid-season and end-of-season estimates of the effect of the\u000a      influenza vaccines since 2009. These results are communicated to (a)\u000a      Scottish Government through reports and meetings, (b) Public Health\u000a      England and, hence, the UK Government, and (c) the I-Move network through\u000a      meetings and reports, where the results have assisted in guiding public\u000a      vaccination policy at national and European level.\u000a    Improved effectiveness of Health Protection Scotland:\u000a      Strathclyde's research working in partnership with HPS, has permitted the\u000a      demonstration of a positive health impact of the influenza vaccination\u000a      programme on the health of the UK population, with the added benefits of\u000a      improving the effectiveness of HPS in carrying out its responsibilities.\u000a      The surveillance systems provide HPS with the ability to monitor disease\u000a      levels in relation to expected levels and to detect outbreaks of\u000a      infectious diseases in a timely fashion. When combined with appropriate\u000a      interventions, a reduction in the magnitude of the outbreaks can be\u000a      achieved, with a reduction in the time in which they can be controlled.\u000a      Furthermore, there is a workforce impact as epidemiologists are freed from\u000a      routine surveillance, leading to a more efficient deployment of staff\u000a      within HPS. A conservative estimate of the savings is one staff per year\u000a      since 2009 at &#163;50,000 per annum (Source 6).\u000a    Reach and Significance: The results of the research on influenza\u000a      vaccine effectiveness have been presented annually at a European network\u000a      on vaccine effect, I-Move, which reports to ECDC and WHO (Source 7). This\u000a      regular reporting has influenced other national programmes demonstrating a\u000a      global reach. Early estimates of the effect of the current seasonal flu\u000a      vaccine are crucial for the planning of the next year's flu vaccine. As a\u000a      result of the university research, HPS, in conjunction with Public Health\u000a      England (PHE), are better able to provide timely estimates for the UK\u000a      (Source 8) placing HPS\/Strathclyde University amongst the leaders in the\u000a      early estimation of influenza vaccine effect in Europe.\u000a    A consultant epidemiologist at HPS has confirmed \"This research is\u000a      carried out at Strathclyde University but has a national, European and\u000a      international impact. Since 2009 we have developed a system of linked\u000a      datasets which, with complex statistical modelling, can be used to provide\u000a      timely estimates of the effectiveness of the seasonal influenza vaccine.\u000a      The results of this research have an impact at national level, where they\u000a      are communicated to Scottish Government, through reports and meetings and\u000a      to Public Health England and thence the UK Department of Health.\u000a      Furthermore, the impact is wider through the contribution of HPS in a\u000a      European network of researchers and public health officials from European\u000a      Centre for Disease Control. This data allows in-season estimation of the\u000a      severity of influenza as it impacts the public health and the success or\u000a      otherwise of public health measures to limit its impact. The network\u000a      regularly contributes to the World Health Organisation influenza vaccine\u000a      strain selection meeting where the composition of the seasonal influenza\u000a      vaccine for the next season is decided\" (Source 6).\u000a    ","ImpactSummary":"\u000a    Research on novel statistical methods for disease surveillance and\u000a      influenza vaccine effectiveness has led to the development of a suite of\u000a      automatic systems for detecting outbreaks of infectious diseases at Health\u000a      Protection Scotland (HPS). This work has improved the public health\u000a      response and helped to reduce costs in Scotland and also in the wider UK\u000a      and EU by providing real-time early warning of disease outbreaks and\u000a      timely estimates of the effectiveness of the influenza vaccine. This\u000a      research, commissioned by the Scottish Government, through HPS, and also\u000a      the UK National Institute for Health Research (NIHR) and the European\u000a      Centres for Disease Control (ECDC), but used in a wider context by many\u000a      others, formed the basis for the HPS response to the H1N1 Influenza\u000a      Pandemic and monitoring of the effects of Influenza Vaccines.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Strathclyde\u000a    ","Institutions":[{"AlternativeName":"Strathclyde (University of)","InstitutionName":"University of Strathclyde","PeerGroup":"B","Region":"Scotland","UKPRN":10007805}],"Panel":"B         ","PlaceName":[],"References":"\u000a    References 1, 3 and 5 best exemplify the quality of the body of\u000a        research. References1, 4 and 5 are part of the REF2014 submission\u000a    \u000a1. Kavanagh. K., Robertson, C., Murdoch, H., Crooks, G., McMenamin, J.\u000a      Syndromic surveillance of influenza-like illness in Scotland during the\u000a      Influenza A H1N1v pandemic and beyond. Journal of the Royal Statistical\u000a      Society Series A: Statistics in Society, 2012, 175 (4) pp.939-958\u000a    \u000a\u000a2. Unkel, S., Farrington, C., Garthwaite, P. H., Robertson, C., &amp;\u000a      Andrews, N. Statistical methods for the prospective detection of\u000a      infectious disease outbreaks: a review. Journal of the Royal Statistical\u000a      Society: Series A (Statistics in Society), 2012, 175(1), 49-82.\u000a    \u000a\u000a3. Wagner A.P., McKenzie E., Robertson C., McMenamin J., Reynolds A.,\u000a      Murdoch H. Automated mortality monitoring in Scotland from 2009. Euro\u000a      Surveillance, 2013, 18(15),pii=20451.Available online:: http:\/\/www.eurosurveillance.org\/ViewArticle.aspx?ArticleId=20451\u000a    \u000a\u000a4. Kavanagh K., Robertson C., McMenamin J. Assessment of the Variability\u000a      in Influenza A(H1N1) Vaccine Effectiveness Estimates Dependent on Outcome\u000a      and Methodological Approach. PLoS ONE, 2011, 6(12): e28743.\u000a      doi:10.1371\/journal.pone.0028743\u000a    \u000a\u000a5. Simpson C.R., Ritchie L.D., Robertson C., Sheikh A., McMenamin J.\u000a      Effectiveness of H1N1 vaccine for the prevention of pandemic influenza in\u000a      Scotland, UK: a retrospective observational cohort study. Lancet Infect.\u000a      Dis., 2012, Sept, 12(9),696-702. Epub 2012 Jun 26.\u000a    \u000aOther evidence for quality of research:\u000a      18 grants with a total value of over &#163;2.4M were awarded by HPS, HPA, ECDC,\u000a      CSO, NIHR, SEHD, EPSRC since 2003. Examples are\u000a    1. Simpson C.R., Lone N., Ritchie L.D., Robertson C., Sheikh A.,\u000a      McMenamin J. Early estimation of pandemic influenza Antiviral and Vaccine\u000a      Effectiveness (EAVE) - use of a unique community and laboratory national\u000a      linked dataset. 2012-15, NIHR, &#163;215,000.\u000a    2. Simpson C.R., Lone N., Ritchie L.D., Robertson C., Sheikh A.,\u000a      McMenamin J. Seasonal Influenza Vaccine Effectiveness (SIVE), 2010-12,\u000a      NIHR, &#163;104,000.\u000a    3. Cruickshank M., Campbell C., Choi Y., Cubie H., Cushieri K., Donaghy\u000a      M., Imrie J., Robertson C., Sullivan F., Weller D. The Scottish Cervical\u000a      Cancer Prevention Programme: Assessing And Modelling The Impact Of HPV\u000a      16\/18 Immunisation, 2010-15, CSO, &#163;450,000.\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    \u000a      Statement from Clinical Director, HPS will corroborate that the\u000a        university research is crucial to workings of HPS and that university\u000a        researchers are a vital part of HPS working groups.\u000a      \u000ahttp:\/\/www.documents.hps.scot.nhs.uk\/respiratory\/swine-influenza\/outbreak-report\/flu-a-h1n1-hp-response-2010-12.pdf\u000a        will corroborate that university research is published in reports by\u000a        Health Protection Scotland and which are then submitted to Scottish\u000a        Government\u000a      \u000ahttp:\/\/www.documents.hps.scot.nhs.uk\/respiratory\/seasonal-influenza\/flu-update\/2012-11-29.pdf\u000a        and Elliott, A. J., N. Singh, P. Loveridge, S. Harcourt, S. Smith, R.\u000a        Pnaiser, K. Kavanagh et al. \"Syndromic surveillance to assess the\u000a        potential public health impact of the Icelandic volcanic ash plume\u000a        across the United Kingdom, April 2010.\" Eurosurveillance 15, no. 23\u000a        (2010).will support the claim that the results of the university\u000a        research continue to have an impact\u000a      HPS Report to SAGE Committee. HPS_School_Closures_Scotland 23 July\u000a        2009 and HPS Report to SAGE Committee. SAGE paper impact of schools in\u000a        Scotland Opening after summer break Sept 2009 will support the claim\u000a        that University research contributed to SAGE committee during pandemic\u000a      \u000ahttp:\/\/www.straightstatistics.org\/article\/monitoring-hidden-deaths-swine-flu\u000a        will support the claim that the Scottish response to H1N1v was\u000a        sophisticated\u000a      Statement from Consultant Epidemiologist at HPS will affirm that\u000a        university research is crucial for influenza surveillance in Scotland\u000a        and improves efficiency at HPS\u000a      Valenciano M, Ciancio BC. I-MOVE: a European network to measure the\u000a        effectiveness of influenza vaccines. Euro Surveill.\u000a        2012;17(39):pii=20281. Available online: http:\/\/www.eurosurveillance.org\/ViewArticle.aspx?ArticleId=20281\u000a        demonstrates influenza surveillance in Scotland and vaccine\u000a        effectiveness estimates from UK are timely and are used to inform the\u000a        development of next season's vaccine.\u000a      Pebody, Richard, P. Hardelid, D. Fleming, J. McMenamin, N. Andrews, C\u000a        Robertson, D. Thomas et al. \"Effectiveness of seasonal 2010\/11 and\u000a        pandemic influenza A (H1N1) 2009 vaccines in preventing influenza\u000a        infection in the United Kingdom: mid-season analysis 2010\/11.\" Euro\u000a        Surveill 16, no. 6 (2011): 19791. Supports the claim that university\u000a        research is crucial for influenza surveillance in Scotland and that this\u000a        research is timely and is used to inform the development of next\u000a        season's vaccine.\u000a    \u000a    ","Title":"\u000a    Health and cost benefits of monitoring infectious diseases using novel\u000a        statistical methods.\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"},{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Context: Health Protection Scotland is the NHS agency concerned\u000a      with protecting the population of Scotland from the effects of infectious\u000a      diseases and outbreaks that are a major public health problem. It is also\u000a      responsible for assessing the impact of vaccination programmes on health.\u000a      The seasonal influenza vaccination programme is estimated to cost in\u000a      excess of &#163;15 million annually (the cost was &#163;14M in 2008) and it is\u000a      necessary to establish that the vaccine used is effective. Diminishing\u000a      resources and a large number of diseases means that little time can be\u000a      allocated to any one disease. Consequently, research into the use of\u000a      sophisticated statistical techniques to process automatically large\u000a      volumes of data was carried out in close collaboration with consultants\u000a      and epidemiologists at HPS.\u000a    Key Findings: Statistical models based upon Poisson regression\u000a      techniques were developed for statistical exception reporting [1]. These\u000a      models were extensions of research critically reviewed in [2] and led to\u000a      the development of a syndromic surveillance system for Scotland based upon\u000a      daily data provided by NHS24, the out of hours telephone service for\u000a      medical advice [1]. Methods for modelling these data were developed and\u000a      existing algorithms for signalling an exception were modified and compared\u000a      using real and simulated data. The initial research was carried out in\u000a      2006- 07, based upon earlier work in the PhD thesis of G. McCabe. Further\u000a      research has considered both spatial and temporal effects [3], which\u000a      permits HPS to provide epidemiological guidance at sub national (Health\u000a      Board) level and thus increase the specificity of the HPS response. The\u000a      research demonstrated that these surveillance systems detect outbreaks\u000a      early and are important in monitoring disease activity.\u000a    Competitive research grants funded by ECDC (EpiConcept), the Chief\u000a      Scientist's Office (CSO), NIHR, and HPS have permitted the utilisation of\u000a      HPS data to develop statistical methods for the estimation of vaccine\u000a      effect, particularly for seasonal and pandemic influenza, and the Human\u000a      Papilloma Virus (HPV) vaccine. Information on cohorts of patients was\u000a      assembled and linked with other health related details available in\u000a      Scotland, such as laboratory and hospital data and cause of death\u000a      statements. Statistical models for the estimation of vaccine effectiveness\u000a      were produced and compared. Over and above the novelty of the\u000a      epidemiological research, the novel statistical aspects of this research\u000a      are in the application of state-of-the-art statistical models and methods\u000a      for the control of confounding, such as instrumental variables, propensity\u000a      scores and negative confounders imbedded within time dependent Cox models,\u000a      with a correlated error structure to account for the clustering of\u000a      patients within GP practices in the cohorts. The research provided timely\u000a      estimates of the effect of the influenza vaccine in Scotland, as well as\u000a      an understanding of the effect of the influence of the methodology. [4, 5]\u000a    Key Researchers: Research was, carried out by the Epidemiology and\u000a      Population Modelling group at Strathclyde:\u000a    Chris Robertson (Professor, 2002- ) supported by D Greenhalgh (Reader,\u000a      1990- ) and G Gettinby (Professor, 1978 - ), undertook the initial\u000a      research into the surveillance systems.\u000a    C Robertson undertook the research on statistical methods for estimating\u000a      vaccine effectiveness in association with Dr J McMenamin (Health\u000a      Protection Scotland), Dr R Pebody and Dr N Andrews at Public Health\u000a      England, Dr D Fleming at Royal College of General Practitioners,\u000a      Birmingham, Dr C Simpson, Dr N Lone and Prof A Sheik (Edinburgh\u000a      University), and Prof L Ritchie (Aberdeen University). The key researchers\u000a      in this project were Prof C Robertson, Dr McMenamin and Dr Lone who\u000a      provided the medical background, and Dr Simpson who was PI on 2 NIHR\u000a      grants and is an expert on data systems.\u000a    "},{"CaseStudyId":"43011","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000a    Unique simulation tools created at the University of Southampton have\u000a      allowed Ford to achieve\u000a      significant efficiency gains through changes to the design of its engine\u000a      assembly lines across its\u000a      European operation. In effect every one of the engines produced by Ford\u000a      Europe each year has\u000a      benefitted from Currie and Cheng's modelling research, translating into\u000a      substantial cost savings\u000a      [5.1].\u000a    The academics' analysis of the machine breakdown data, and the subsequent\u000a      use of the\u000a      automated tool by the Process and Simulation Teams at Ford, has enabled\u000a      Ford engineers to\u000a      identify machines that are particularly vulnerable to failure. The Process\u000a      Team works with suppliers\u000a      to reduce future machine failures on the production lines and reduce the\u000a      repair time, so limiting the\u000a      downtime of the production lines. The main measure of throughput on a line\u000a      is the number of jobs\u000a      completed per hour (JPH). Ford estimate that, as a result of a greater\u000a      understanding of machine\u000a      breakdowns, JPH has increased by 1-2%, which equates to a significant\u000a      amount of revenue [exact\u000a      figure removed]. On average, there are three production lines running\u000a      simultaneously, meaning\u000a      that there have been significant direct savings [exact figure removed]\u000a      generated by this tool since\u000a      2010 &#8212; a strong return on the original &#163;15,000 research grant over three\u000a      years [5.1].\u000a    John Ladbrook [5.1], head of the Simulation Team at Ford's\u000a      Technical Centre at Dunton, UK\u000a      confirms: \"We have worked with the University of Southampton for more\u000a        than ten years on a\u000a        number of simulation projects. Thanks to their expertise, this project\u000a        was particularly successful\u000a        and the tool developed has helped with making significant direct savings\u000a        [exact figure removed]\u000a        since 2010.\"\u000a    Ford engineers use Southampton's automated tool to read raw data\u000a      detailing the downtime\u000a      recorded for each of the machines on the production line; fit a\u000a      distribution to describe the downtime\u000a      of each of these machines; and generate the specification files that\u000a      describe the machine\u000a      downtime on the line, which can then be entered into the simulation model.\u000a      The tool has a user-friendly\u000a\u0009  interface, allowing the full analysis of the data required for a\u000a      simulation project to be\u000a      completed in only two hours. Previously the engineers were unable to\u000a      distinguish between different\u000a      machines when recording the downtime of the production lines. Furthermore,\u000a      they would have\u000a      spent around five days carrying out the individual analysis of each\u000a      machine along the production\u000a      line. The resulting increase in staff productivity means that two\u000a      additional production line\u000a      simulations can be developed each year. All current Ford Europe engine\u000a      production lines have\u000a      been designed using this tool. For the cost of each simulation [exact\u000a      figure removed], Ford\u000a      confirms this has delivered a six-fold return [exact figure removed] since\u000a      its completion in 2010 and\u000a      continues to benefit Ford at a significant rate per year [exact amount\u000a      removed] [5.1].\u000a    The greater accuracy of the simulation models has contributed to further\u000a      economic benefit, albeit\u000a      one that is harder to quantify. By automating the generation of machine\u000a      downtimes, thus delivering\u000a      a more standardised approach to model development, capacity for human\u000a      error is reduced. Any\u000a      miscalculation can give rise to expensive consequences, as up to &#163;1m in\u000a      investment can ride on\u000a      the correct simulation of a new engine production line. Southampton's\u000a      input has made these kinds\u000a      of costly errors less likely [5.1].\u000a    The implementation of a new simulation tool requires specialist training\u000a      and Southampton\u000a      researchers have been responsible for training Ford engineers in its use [5.1].\u000a      Much of the\u000a      computational work for the initial project was carried out by PhD student\u000a      Lanting Lu (under the\u000a      close direction of Currie and Cheng), who then went on to spend three\u000a      months working within\u000a      Ford's Simulation Team to train employees and embed the analytical tool in\u000a      their wider approach to\u000a      simulation. Southampton has continued to work closely with the Ford team\u000a      in the development of\u000a      the tool through subsequent supervised MSc and PhD projects.\u000a    The techniques developed by Southampton to fit the data, including the\u000a      Arrows Classification\u000a      Method which is freely available for public download [5.2], have\u000a      applications beyond\u000a      manufacturing, particularly healthcare. Currie's work influenced the\u000a      strategic thinking of healthcare\u000a      provider BUPA Hospitals (in 2009) as it trialled the simulation tool to\u000a      optimise the scheduling of\u000a      operations in order to maximise the use of hospital beds [5.3].\u000a      Due to a major restructuring within\u000a      BUPA, the methods were never implemented on a permanent basis.\u000a    ","ImpactSummary":"\u000a    Through a close collaboration with Ford Motor Company, simulation\u000a      modelling software developed\u000a      at the University of Southampton has streamlined the design of the car\u000a      giant's engine production\u000a      lines, increasing efficiency and delivering significant economic benefits\u000a      in three key areas. Greater\u000a      productivity across Ford Europe's assembly operations has generated a\u000a      significant amount [exact\u000a      figure removed] in direct cost savings since 2010. Automatic analysis of\u000a      machine data has resulted\u000a      in both a 20-fold reduction in development time, saving a large sum per\u000a      year [exact figure\u000a      removed], and fewer opportunities for human error that could disrupt the\u000a      performance of production\u000a      lines costing a large sum [exact amount removed] each to program.\u000a    ","ImpactType":"Economic","Institution":"\u000a    University of Southampton\u000a    ","Institutions":[{"AlternativeName":"Southampton (University of)","InstitutionName":"University of Southampton","PeerGroup":"A","Region":"South East","UKPRN":10007158}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Publications:\u000a    \u000a3.1 Lu, L., Currie, C.S.M., Cheng, R.C.H. and Ladbrook. J. (2007)\u000a      \"Classification analysis for\u000a      simulation of machine breakdowns\" in Proceedings of the 2007 Winter\u000a      Simulation Conference,\u000a      pp 480 - 487.\u000a    \u000a\u000a3.2 (*) Lu, L., Currie, C.S.M., Cheng, R.C.H. and Ladbrook. J.\u000a      (2010) \"Classification analysis for\u000a      simulation of the duration of machine breakdowns\", Journal of the\u000a      Operational Research\u000a      Society, Vol. 62 pp760-767\u000a    \u000a\u000a3.3 Lu, L. and Currie, C.S.M. (2010) \"Evaluation of the Arrows\u000a      Method for Classification of Data\",\u000a      Asia-Pacific Journal of Operational Research, Vol. 27, Issue 1, pp\u000a      121-141.\u000a    \u000a(*) These references best indicate the quality of the underpinning\u000a      research.\u000a    Grants:\u000a    G1. October 2005-October 2008: &#163;15,000 support from Ford motor\u000a      company \"Modelling\u000a      breakdown durations in simulation models of engine assembly lines\".\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    5.1 Head of the Simulation Team at Ford's Technical Centre,\u000a      Dunton, UK.\u000a    5.2 http:\/\/www.southampton.ac.uk\/~ccurrie\/\u000a    5.3 Currie, Christine S.M. and Lu, Lanting (2009) Optimal\u000a      scheduling using length-of-stay data for\u000a      diverse routine procedures. In, McClean, Sally, Millard, Peter, El-Darzi,\u000a      Elia and Nugent, C.D.\u000a      (eds.) Intelligent Patient Management. Berlin, Springer, 193-205. (Studies\u000a      in Computational\u000a      Intelligence 189)\u000a    ","Title":"\u000a    10-02 Transforming the efficiency of Ford's engine production line\u000a    ","UKLocation":[{"GeoNamesId":"2637487","Name":"Southampton"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Ford Motor Company's UK production lines produce a quarter of the engines\u000a      used by Ford\u000a      worldwide, amounting to a throughput of around &#163;60 million each year. The\u000a      multinational carmaker\u000a      relies on simulation modelling to ensure that production lines are\u000a      designed to maximise efficiency\u000a      and minimise the use of factory space. Each assembly line comprises 70\u000a      machines connected in\u000a      series; one machine failure can result in the costly suspension of the\u000a      whole line, denting\u000a      productivity.\u000a    Building upon a long tradition of simulation modelling for industry, our\u000a      researchers began\u000a      collaborating with the Process and Simulation Teams at Ford in 2005.\u000a      Professor Russell Cheng,\u000a      (retired 2007), and Dr Christine Currie (Lecturer 2004-present), analysed\u000a      breakdown data for every\u000a      machine across three engine assembly lines. They created an automated tool\u000a      that Ford engineers\u000a      could employ to identify the machines that are particularly vulnerable to\u000a      failure and to accurately\u000a      forecast when a particular configuration of the production line would\u000a      break down [3.1]. This work\u000a      followed on from earlier research projects led by Cheng that revolved\u000a      around parameter estimation\u000a      in non-regular statistical problems.\u000a    In non-regular problems maximum likelihood estimation, the classical\u000a      fitting method, is no longer\u000a      valid due to the presence of irregularities in the likelihood surface.\u000a      Cheng and Currie developed a\u000a      practical methodology for fitting finite mixture models using Bayesian\u000a      statistics [3.1, 3.2], which\u000a      could combine a variety of factors into a single statistical distribution.\u000a      Finite mixture models &#8212; weighted\u000a      sums of standard statistical distributions &#8212; are ideal for describing\u000a      multimodal data such\u000a      as those encountered in machine breakdowns along the Ford assembly line.\u000a      However, as the\u000a      number of components in the finite mixture model is unknown, the problem\u000a      is statistically non-regular.\u000a      While the theoretical understanding of fitting finite mixture models is\u000a      well developed, prior\u000a      to this work little effort had gone into developing robust and efficient\u000a      fitting methods that enable a\u000a      good fit to be obtained in a reasonable length of time. The academics were\u000a      able to apply these\u000a      sophisticated mathematical models to translate complex, messy data sets\u000a      into useful inputs for\u000a      manufacturing simulation models for use by engineers with little\u000a      specialist mathematical expertise.\u000a    Every one of the machines along Ford's assembly lines had its own set of\u000a      data pertaining to the\u000a      time it would take to repair them in the event of a breakdown. The\u000a      Southampton researchers\u000a      recognised that in order to obtain more accurate parameter estimation and\u000a      reduce the number of\u000a      models to be fitted to data, it was necessary to group the machines\u000a      together, based on the\u000a      similarity of the data. Currie and PhD student Lanting Lu (2005-2009)\u000a      developed a new method\u000a      where each machine is characterised by a dataset of machine breakdown\u000a      durations: the Arrows\u000a      Classification Method (ACM) [3.2, 3.3]. The method measures the\u000a      similarity of two sets of data and\u000a      only groups machines together if their similarity is greater than a\u000a      user-defined threshold. ACM\u000a      works as an alternative to better known methods such as cluster analysis,\u000a      and is particularly\u000a      appropriate to the situation at Ford because it makes no assumption about\u000a      the underlying\u000a      distribution of the data and allows for the comparison of datasets.\u000a    This methodology provides a flexible, robust fitting procedure for\u000a      multimodal data that can be\u000a      applied to simulation input and output modelling in other sectors,\u000a      including healthcare. In a project\u000a      for BUPA Hospitals, the method was used to group medical procedures based\u000a      on length-of-stay in\u000a      hospitals following operations in order to devise more efficient\u000a      timetabling procedures. The\u000a      techniques used in the automated fitting tool are useful to any business\u000a      wishing to extract value\u000a      from their data, particularly for complex systems where relatively\u000a      detailed simulation models are\u000a      being implemented that require extensive and repetitive data analysis.\u000a    "},{"CaseStudyId":"43012","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"3996063","Name":"Mexico"},{"GeoNamesId":"2750405","Name":"Netherlands"}],"Funders":["Economic and Social Research Council"],"ImpactDetails":"\u000d\u000a    Decision makers tasked with devising and implementing effective and\u000d\u000a      inclusive socio-economic policies need as much information as possible.\u000d\u000a      Novel methodologies of small area estimation developed by researchers at\u000d\u000a      Southampton have provided policymakers, both in the UK and\u000d\u000a      internationally, with reliable data ranging from average household income\u000d\u000a      to unemployment figures right down to neighbourhood level. In particular,\u000d\u000a      the Southampton research has allowed small-area estimates of employment\u000d\u000a      indicators and income indicators to be derived from the Labour Force\u000d\u000a      Survey, the largest household survey in the UK, and the Family Resources\u000d\u000a      Survey, respectively.\u000d\u000a    Southampton's long-time collaboration with the ONS has proved essential\u000d\u000a      in providing this range of disaggregated data to the Department for Work\u000d\u000a      and Pensions (DWP) and local authorities around the UK. Dr Alan Taylor,\u000d\u000a      Head of the ONS' Small Area Team, has described Southampton's work in this\u000d\u000a      field as `a recognised breakthrough' [5.1]. He said: `In my\u000d\u000a        experience great research can often lead to academic papers but not get\u000d\u000a        translated into outputs. Our collaboration [with Southampton] has led to\u000d\u000a        significant improvements in ONS small area estimates of income and\u000d\u000a        unemployment and [has] led the world in model-based estimation.'\u000d\u000a    The ONS' own National Accounts Team and Labour Market Division have\u000d\u000a      relied upon the estimates to answer enquiries from local authorities,\u000d\u000a      policy advisers, government departments and academics. The ONS has\u000d\u000a      reported that demand for small area estimates has been `strong' [5.1]\u000d\u000a      and that users have drawn on them for a variety of socio-economic\u000d\u000a      purposes. An analysis conducted by the ONS in 2012 concluded: `Without\u000d\u000a        these estimates valuable insights into the differences between small\u000d\u000a        area geographies would be lost... Users would need to look around for an\u000d\u000a        alternative and less suitable source. In particular, a number of\u000d\u000a        parliamentary questions would have to be answered at higher geographies.'\u000d\u000a      [5.1]\u000d\u000a    Southampton's success in producing sets of local authority-level\u000d\u000a      estimates of labour force activity has proved invaluable for MPs in\u000d\u000a      serving the needs of their constituents. A letter [5.2] written to\u000d\u000a      the ONS in 2005 by the Chief Librarian for the House of Commons, John\u000d\u000a      Pullinger, reflects the significance of these data, obtained using\u000d\u000a      Southampton-designed methodologies. Pullinger stressed the importance of\u000d\u000a      making a common set of key labour force indicators &#8212; for example\u000d\u000a      Jobseekers Allowance claimants as a proportion of the constituency's\u000d\u000a      working age population &#8212; available to serving MPs, something which is only\u000d\u000a      possible as a result of this research. The model-based estimates of mean\u000d\u000a      income developed by Southampton researchers helped the ONS plug a gap in\u000d\u000a      its national statistics provision enabling the ONS to supply the DWP with\u000d\u000a      small area income estimates.\u000d\u000a    Local authorities are the main users of these estimates. The Greater\u000d\u000a      London Authority used small area estimates in its 2010 Focus on London\u000d\u000a      report, Income and Spending at Home [5.3], which compared\u000d\u000a      income at both individual and household level across the capital. This was\u000d\u000a      a key input in the Mayor of London's Outer London Commission Report\u000d\u000a      [5.4], which analysed the challenges and opportunities facing the\u000d\u000a      outer London economy. This in turn was an important source of information\u000d\u000a      for developing new policies in housing and transport proposed in the new\u000d\u000a      London plan (2011), the strategic development plan [5.5] for the\u000d\u000a      capital up to 2031.\u000d\u000a    Another local authority, the London Borough of Newham, used the estimates\u000d\u000a      in their Information Management System allowing members of the public to\u000d\u000a      summarise and map data via data interrogation tools on the borough's\u000d\u000a      website. Newham further used the data to answer queries from members of\u000d\u000a      the public and from other local authority officers. Users, including the\u000d\u000a      DWP, stated that without these estimates valuable insights into the\u000d\u000a      differences between small area geographies would be lost [5.1].\u000d\u000a    The impact of Southampton's research has extended overseas. In 2009, the\u000d\u000a      Mexican government agency, CONEVAL, sought the University's help in\u000d\u000a      producing estimates of income and social rights deprivation in Mexican\u000d\u000a      municipalities. Methodologies developed by the Southampton team since 2001\u000d\u000a      have allowed CONEVAL to measure access to health, education and food at\u000d\u000a      neighbourhood level as part of its brief to define, identify and measure\u000d\u000a      poverty across the country. Dr Aparicio Jimenez, CONEVAL's Poverty\u000d\u000a      Analysis Director, said: `With this research, the poorest\u000d\u000a        municipalities in Mexico can be identified. With this valuable\u000d\u000a        information, the federal government, the National Congress, federal\u000d\u000a        states and municipalities will be able to design more effective social\u000d\u000a        programmes.' CONEVAL described Southampton's research as `the most\u000d\u000a      prestigious' of its kind [5.6].\u000d\u000a    A year later Southampton's modelling methods enabled the Netherlands'\u000d\u000a      Centraal Bureau voor de Statistiek (CBS) to deliver more accurate\u000d\u000a      estimates of business turnover, which have guided policy changes at a\u000d\u000a      national level [5.7]. CBS reported that Southampton's\u000d\u000a      methodologies proved superior to other small area estimation approaches\u000d\u000a      `in several situations' [5.8].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Southampton statisticians have made a valuable contribution to government\u000d\u000a      policy formulation across the UK and further afield to areas of North\u000d\u000a      America and Europe. Novel methods for delivering more accurate estimates\u000d\u000a      of socio-economic indicators at neighbourhood level have given local\u000d\u000a      authorities, national government agencies and MPs the tools to implement\u000d\u000a      more effective policies designed to assist the poorest communities and\u000d\u000a      strengthen community cohesion. The UK's Office for National Statistics\u000d\u000a      (ONS) has described Southampton's contribution as `a breakthrough', while\u000d\u000a      the Mexican government agency, CONEVAL, regards this work as `the most\u000d\u000a      prestigious' of its kind.\u000d\u000a    ","ImpactType":"Economic","Institution":"\u000d\u000a    University of Southampton\u000d\u000a    ","Institutions":[{"AlternativeName":"Southampton (University of)","InstitutionName":"University of Southampton","PeerGroup":"A","Region":"South East","UKPRN":10007158}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    Publications:\u000d\u000a    \u000a3.1 (*) Molina, I, Saei, A, and Lombardia, MJ (2007): Small Area\u000d\u000a      Estimates of Labour Force Participation Under a Multinomial Logit Mixed\u000d\u000a      Model, Journal of the Royal Statistical Society, Series A, 170, 975-1000\u000d\u000a    \u000a\u000a3.2 (*) Chambers, R, and Tzavidis, N (2006): M-Quantile Models for\u000d\u000a      Small Area Estimation, Biometrika, 93, 255-268\u000d\u000a    \u000a\u000a3.3 (*) Chambers, R, Chandra, H, Salvati, N, and Tzavidis, N\u000d\u000a      (2013): Outlier Robust Small Area Estimation, Journal of the Royal\u000d\u000a      Statistical Society: Series B (DOI: 10.1111\/rssb.12019)\u000d\u000a    \u000a\u000a3.4 Tzavidis, N, Marchetti, S, and Chambers, R (2010): Robust\u000d\u000a      Prediction of Small Area Means and Distributions, Australian &amp; New\u000d\u000a      Zealand Journal of Statistics, 52, 167-186\u000d\u000a    \u000a\u000a3.5 Chambers, R, Chandra, H, and Tzavidis, N (2011): On\u000d\u000a      Bias-Robust Mean Squared Error Estimation for Pseudo-Linear Small Area\u000d\u000a      Estimators, Survey Methodology, 37 (2), 153-170\u000d\u000a    \u000a\u000a3.6 Marchetti, S, Tzavidis, N, and Pratesi, M (2011):\u000d\u000a      Non-Parametric Bootstrap Mean Squared Error Estimation for M-Quantile\u000d\u000a      Estimators of Small Area Averages, Quantiles and Poverty Indicators,\u000d\u000a      Computational Statistics and Data Analysis, 56, (10), 2889-2902\u000d\u000a    \u000a(*) These references best indicate the quality of the underpinning\u000d\u000a      research.\u000d\u000a    Grants:\u000d\u000a    3.G1 EURAREA (Enhancing Small Area Estimation Techniques to meet\u000d\u000a      European Needs), Framework Programme 5, 2001-2004, Professor Ray Chambers\u000d\u000a      [&#8364;1,833,781]\u000d\u000a    3.G2 Multi-Quantile Models for Small Area Estimation, ESRC,\u000d\u000a      2003-2006, Professor Ray Chambers [&#163;164,465.93]\u000d\u000a    3.G3 SAMPLE (Small Area Methodologies for Poverty and Living\u000d\u000a      Condition Estimates), Framework Programme 7, 2008-2011, Dr Nikos Tzavidis\u000d\u000a      (PI for University of Southampton) [&#8364;778,000]\u000d\u000a    3.G4 Office for National Statistics, Methodology Contract\u000d\u000a      (contract number PU-10\/0141), 2010-2015, University of Southampton\u000d\u000a      [minimum amount: &#163;135,000 x 5 years]\u000d\u000a    3.G5 Netherlands Central Bureau of Statistics, Methodology\u000d\u000a      Contract, 2010-2011 [&#8364;99,900]\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a    5.1 Head of Small Area Team, Office for National Statistics.\u000d\u000a    5.2 Letter by Librarian of House of Commons to Director, Labour\u000d\u000a      Market Division, Office for National Statistics).\u000d\u000a    5.3 http:\/\/data.london.gov.uk\/documents\/FocusOnLondon2010-income-and-spending.pdf\u000d\u000a      (The Head of the Small Area Team at ONS can confirm the use of the\u000d\u000a        Southampton research in producing this data)\u000d\u000a    5.4 http:\/\/static.london.gov.uk\/olc\/docs\/final-report.pdf\u000d\u000a      (This document makes substantial use of the data in 5.3)\u000d\u000a    5.5 http:\/\/www.london.gov.uk\/priorities\/planning\/london-plan\u000d\u000a      (This document builds on 5.4 and also uses the data in 5.3)\u000d\u000a    5.6 Letter\/Email about the value of Southampton\/CONEVAL\u000d\u000a      collaboration (2012) by the Director of Poverty Analysis, CONEVAL, Mexico.\u000d\u000a    5.7 CBS discussion paper: Small Area Estimation of Turnover of the\u000d\u000a      Structural Business Survey (2012): http:\/\/www.cbs.nl\/NR\/rdonlyres\/2FC2BE74-1AF0-462A-9C76-9C836EA07655\/0\/201203x10pub.pdf\u000d\u000a    5.8 The Researcher of Netherland Centraal Bureau voor de\u000d\u000a      Statistiek (CBS), Division of Methodology and Quality can corroborate the\u000d\u000a      use of the code developed by Southampton for implementing the methods.\u000d\u000a    ","Title":"\u000d\u000a    10-03 Small Area Estimation: Data Provision for Smarter Local\u000d\u000a      Policymaking\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2637487","Name":"Southampton"},{"GeoNamesId":"2643741","Name":"City of London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Surveys are often designed to provide estimates of social and economic\u000d\u000a      indicators at a population level. For example the Labour Force Survey,\u000d\u000a      conducted by ONS, is a large household survey which provides the official\u000d\u000a      measures of employment and unemployment in the UK. Policymakers at a local\u000d\u000a      level, for example local authorities or MPs serving their constituents,\u000d\u000a      require a much more detailed picture than is provided by national\u000d\u000a      estimates. However, obtaining accurate values for small geographical areas\u000d\u000a      &#8212; small area estimation &#8212; is problematic as the survey sample size is\u000d\u000a      usually too small to yield reliable direct estimates. Statistics are\u000d\u000a      instead based on `indirect' estimates that use information from other\u000d\u000a      areas with similar characteristics to the region under the microscope.\u000d\u000a    Statisticians at the University of Southampton have been developing novel\u000d\u000a      statistical methodologies of small area estimation since 2001 to produce\u000d\u000a      more accurate estimates of key social and economic indicators for more\u000d\u000a      effective local policymaking. Indicators that have been a particular focus\u000d\u000a      of the research are: (a) average household income and average business\u000d\u000a      turnover; (b) complex poverty indicators (head count ratio, poverty gap\u000d\u000a      and poverty severity); (c) proportions of unemployed, employed and\u000d\u000a      inactive individuals and corresponding measures of precision quantified by\u000d\u000a      mean squared error (MSE) estimators.\u000d\u000a    The first phase (2001-2004) of this research was carried out as part of\u000d\u000a      the EU's EURAREA (Enhancing Small Area Estimation Techniques to meet\u000d\u000a      European Needs) project. The aim of the project was to provide National\u000d\u000a      Statistical Institutes across Europe with a basis for deciding whether,\u000d\u000a      and how, to apply small area estimation techniques in the production of\u000d\u000a      official statistics. Professor Ray Chambers (1995-2006) and Dr Ayoub Saei\u000d\u000a      (Research Fellow, 2002-2006), led the development of techniques for\u000d\u000a      deriving point and MSE estimates of labour force activity at small area\u000d\u000a      level [3.1]. Local authorities in the UK had previously found\u000d\u000a      accurate unemployment information almost impossible to obtain; the\u000d\u000a      unreliability of estimates resulted in only about a quarter of the annual\u000d\u000a      estimates of unemployment in 1999\/2000 qualifying for publication.\u000d\u000a    From 2003 to 2006, funded by an ESRC grant, Chambers and Dr Nikos\u000d\u000a      Tzavidis (Research Fellow, 2003-2005; Senior Lecturer, 2010-present)\u000d\u000a      developed multi-quantile (M-quantile) models for small area estimation of\u000d\u000a      averages and totals [3.2, 3.3]. Tzavidis was subsequently PI on a\u000d\u000a      project that extended this work to methodologies for estimating poverty\u000d\u000a      indicators and distribution functions at small area level. This latter\u000d\u000a      strand of research, from 2008 to 2011, was part of the EU's SAMPLE (Small\u000d\u000a      Area Methodologies for Poverty and Living Condition Estimates) project,\u000d\u000a      which identified new indicators and models for inequality and poverty [3.4-3.6].\u000d\u000a    The research carried out in the course of the ESRC-funded work and the\u000d\u000a      SAMPLE project resulted in the development of innovative statistical\u000d\u000a      methodologies in small area estimation. Chambers and Tzavidis' 2006 paper\u000d\u000a      [3.2], published in Biometrika, was key to the development of this\u000d\u000a      field and is still regularly cited (117 citations to 31\/10\/13 &#8212; Google\u000d\u000a      Scholar). The Office for National Statistics (ONS) was able to build on\u000d\u000a      the methodology to publish improved unemployment figures at local\u000d\u000a      authority level, first as experimental statistics and then as official\u000d\u000a      national statistics.\u000d\u000a    All new methodologies were developed with users' needs in mind and\u000d\u000a      supported by easy-to-use software. This work led to significant\u000d\u000a      international interest in Europe (e.g. Eurostat EURAREA Project, Portugal,\u000d\u000a      the Ukraine) and outside (e.g. Australian Bureau of Statistics, Statistics\u000d\u000a      New Zealand, Korea). In particular, this approach, coupled with contracts\u000d\u000a      with key non-academic organisations such as the ONS, Mexico's National\u000d\u000a      Council for the Evaluation of Social Development Policy (CONEVAL) and the\u000d\u000a      Netherlands' Centraal Bureau voor de Statistiek, has widened the influence\u000d\u000a      of Southampton's pioneering techniques.\u000d\u000a    "},{"CaseStudyId":"43013","Continent":[{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d\u000a    The development of new medicines is a lengthy and costly procedure, with\u000d\u000a      some estimates putting\u000d\u000a      the average research and development spend as high as US$2 billion for\u000d\u000a      each new drug [5.C1].\u000d\u000a      Speeding up this process not only saves on costs but frees up personnel to\u000d\u000a      work on life-enhancing\u000d\u000a      interventions elsewhere. Helping pharmaceutical companies find\u000d\u000a      cost-effective ways of developing\u000d\u000a      the chemical processes for new drugs has been a key driver of research\u000d\u000a      into new statistical design\u000d\u000a      and modelling methods carried out at Southampton. The resulting research\u000d\u000a      has been pivotal in\u000d\u000a      helping industry professionals fine tune their experiments at both the\u000d\u000a      screening and robustness\u000d\u000a      stages to maximise yield and minimise impurities.\u000d\u000a    Chemical development in the pharmaceutical industry is an inherently\u000d\u000a      multi-factor problem, which\u000d\u000a      requires the study of many chemical properties and process features.\u000d\u000a      Hence, an important part of\u000d\u000a      gaining understanding and knowledge of the system as economically as\u000d\u000a      possible is the ability to\u000d\u000a      investigate a large number of factors using only a small experiment.\u000d\u000a    Southampton's expertise in the design of experiments has led to a\u000d\u000a      long-running relationship with\u000d\u000a      GSK's Product Development team. As a direct result of the underpinning\u000d\u000a      research, Southampton's\u000d\u000a      supersaturated designs were successfully applied in a GSK-funded pilot\u000d\u000a      project, which encouraged\u000d\u000a      the GSK collaborators to adopt our methodology in the development of a\u000d\u000a      number of new drugs.\u000d\u000a      Since 2011 GSK has been performing approximately six experiments per year\u000d\u000a      using\u000d\u000a      Southampton's methodology, resulting in indicative savings of more than\u000d\u000a      &#163;25,000 and three weeks\u000d\u000a      of scientists' time per experiment.\u000d\u000a    GSK scientists have applied Southampton's supersaturated experiments in\u000d\u000a      the robustness step\u000d\u000a      necessary to verify a drug's `robust operating region' &#8212; a US Food and\u000d\u000a      Drug Administration (FDA)\u000d\u000a      regulatory requirement for the registration of a new drug [5.C2].\u000d\u000a      A probabilistic, risk-based\u000d\u000a      approach is taken to establishing this region through a sequence of\u000d\u000a      experiments and associated\u000d\u000a      statistical modelling. It is then verified through further\u000d\u000a      experimentation, in which a large number of\u000d\u000a      factors (input variables or process parameters) are varied to investigate\u000d\u000a      whether or not they have a\u000d\u000a      negligible impact on the response.\u000d\u000a    GSK drew on the research to verify the robust operating regions for two\u000d\u000a      new drugs for metastatic\u000d\u000a      melanoma, which were approved in 2013 by the FDA. Melanoma is the most\u000d\u000a      serious form of skin\u000d\u000a      cancer. The US National Cancer Institute has predicted that melanoma will\u000d\u000a      cause more than 9000\u000d\u000a      deaths in the USA in 2013. Worldwide, there is a 50% one-year survival\u000d\u000a      rate, with only 16% of\u000d\u000a      patients in the USA surviving for five years [5.1]. For metastatic\u000d\u000a      melanoma, the median age of a\u000d\u000a      newly diagnosed patient is almost a decade younger than for other cancers.\u000d\u000a    The first application of Southampton's research was in the development of\u000d\u000a      trametinib, a drug that\u000d\u000a      inhibits a protein pathway involved in tumour growth which is activated in\u000d\u000a      around 50% of\u000d\u000a      melanoma cases [5.1, 5.2]. GSK's submission for FDA registration\u000d\u000a      for trametinib included results\u000d\u000a      from a supersaturated design developed by Southampton, which allowed the\u000d\u000a      investigation of 16\u000d\u000a      factors in only 10 runs to verify a key stage in the development process.\u000d\u000a      This design achieved a\u000d\u000a      two-thirds saving in costs and time compared with GSK's previous standard\u000d\u000a      approach of using a\u000d\u000a      regular fractional factorial design, which would have required 32 runs to\u000d\u000a      produce the data to ensure\u000d\u000a      a quality product for the patient.\u000d\u000a    It is difficult to quantify the increased scientific understanding gained\u000d\u000a      from the ability to perform\u000d\u000a      experimentation involving a large number of factors that would otherwise\u000d\u000a      be infeasible. This\u000d\u000a      increased understanding is considered a key benefit of the new methods. `The\u000d\u000a        real importance to\u000d\u000a        GSK of the successful application of these methods is the deeper\u000d\u000a        scientific understanding gained\u000d\u000a        through the ability to experiment simultaneously on larger numbers of\u000d\u000a        variables than previously\u000d\u000a        possible. This case study demonstrates the value of a long term\u000d\u000a        relationship in enabling mutual\u000d\u000a        understanding of scientific context and areas of opportunity.' (John\u000d\u000a      Whittaker, Vice President,\u000d\u000a      Statistical Platforms and Technologies). Typically, a supersaturated\u000d\u000a      design allows between a 1.5\u000d\u000a      and two-fold increase in the number of factors that scientists are able to\u000d\u000a      investigate for a given\u000d\u000a      resource. Thus, risk management can be significantly improved from the use\u000d\u000a      of supersaturated\u000d\u000a      designs.\u000d\u000a    To empower the scientists in GSK product development worldwide to use\u000d\u000a      these methods, in 2011\u000d\u000a      the Southampton research team produced a protocol for finding and\u000d\u000a      assessing suitable\u000d\u000a      supersaturated designs using industry-leading statistical software (SAS\u000d\u000a      JMP). They worked with\u000d\u000a      SAS to produce a new piece of code to implement methods for best practice\u000d\u000a      statistical modelling of\u000d\u000a      the data obtained from using such designs. Southampton researchers advised\u000d\u000a      on a tailored\u000d\u000a      training programme that facilitated the use of these new methods in GSK's\u000d\u000a      chemical development.\u000d\u000a        `Key to the successful use of the methods was the proactivity and\u000d\u000a        engagement of UoS in\u000d\u000a        translating high level research to application on GSK projects through\u000d\u000a        collaborative work. This has\u000d\u000a        provided case-studies to demonstrate value and enabled the development\u000d\u000a        of training and\u000d\u000a        mentoring approaches with key GSK staff' (Martin Owen,\u000d\u000a      Quality-by-Design Innovation Leader,\u000d\u000a      GSK) [5.3]. GSK scientists have presented results from their\u000d\u000a      application of supersaturated designs\u000d\u000a      at a number of academic and industrial meetings, including at two\u000d\u000a      Southampton-organised\u000d\u000a      academic-industry events at the Isaac Newton Institute for the\u000d\u000a      Mathematical Sciences in\u000d\u000a      Cambridge.\u000d\u000a    As a measure of the value of the research to GSK, the pharmaceutical\u000d\u000a      company awarded\u000d\u000a      Southampton nearly &#163;200,000 in 2012 to fund further fundamental\u000d\u000a      statistical research from which\u000d\u000a      future pharma benefits may be obtained [3.G3].\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Collaboration between the University of Southampton and scientists at\u000d\u000a      GlaxoSmithKline (GSK) has\u000d\u000a      resulted in the adoption of new statistical design of experiments and\u000d\u000a      modelling methods for the\u000d\u000a      confirmation of a robust operating region for the industrial production of\u000d\u000a      new drugs. These methods\u000d\u000a      have enabled larger numbers of factors to be investigated simultaneously\u000d\u000a      than previously possible,\u000d\u000a      improving scientific understanding of the chemical processes and producing\u000d\u000a      savings of time,\u000d\u000a      money and effort. Southampton's new methods were used in a key process\u000d\u000a      required for the\u000d\u000a      registration of a new skin cancer drug with the US Food and Drug\u000d\u000a      Administration, where the\u000d\u000a      research enabled the verification of a robust operating region to be\u000d\u000a      completed in a third of the\u000d\u000a      previous time.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Southampton\u000d\u000a    ","Institutions":[{"AlternativeName":"Southampton (University of)","InstitutionName":"University of Southampton","PeerGroup":"A","Region":"South East","UKPRN":10007158}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    Publications:\u000d\u000a    \u000a3.1 (*) Lewis, S.M. and Dean, A.M. (2001). Detection of\u000d\u000a      interactions in experiments with large\u000d\u000a      numbers of factors (with discussion). Journal of the Royal Statistical\u000d\u000a      Society Series B, 63,\u000d\u000a      633-672.\u000d\u000a    \u000a\u000a3.2 Marley, C.J. and Woods, D.C. (2010). A comparison of design\u000d\u000a      and model selection methods\u000d\u000a      for supersaturated designs. Computational Statistics and Data Analysis,\u000d\u000a      54, 3158-3167.\u000d\u000a    \u000a\u000a3.3 Marley, C.J. (2011). Screening experiments using\u000d\u000a      supersaturated designs with application to\u000d\u000a      industry. PhD thesis, University of Southampton (supervised by D.C.\u000d\u000a      Woods).\u000d\u000a    \u000a\u000a3.4 (*) Draguljic, D., Woods, D.C., Dean, A.M., Lewis, S.M. and\u000d\u000a      Vine, A.E. (2013). Screening\u000d\u000a      strategies in the presence of interactions. Accepted for Technometrics as\u000d\u000a      a discussion paper.\u000d\u000a    \u000a(*) These references best indicate the quality of the underpinning\u000d\u000a      research.\u000d\u000a    Grants:\u000d\u000a    3.G1 Lewis, S.M. (PI), Please, C.P. and Keane, A.J. Improved\u000d\u000a      product design and\u000d\u000a      manufacturing through economical experimentation. EPSRC, 2000-2003,\u000d\u000a      &#163;276,252.\u000d\u000a    3.G2 Woods, D.C. Supersaturated designs in pharmaceutical\u000d\u000a      development. GlaxoSmithKline,\u000d\u000a      2010-2011, &#163;5,000.\u000d\u000a    3.G3 Woods D.C. Efficient experimentation for effective\u000d\u000a      identification of reliable design spaces,\u000d\u000a      GlaxoSmithKline, 2012-2014, &#163;195,189.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000d\u000a    Contextual References:\u000d\u000a    5.C1 Adams and Brantner (2006). Health Affairs, 25,\u000d\u000a      420-428\u000d\u000a      (http:\/\/content.healthaffairs.org\/content\/25\/2\/420.long)\u000d\u000a    5.C2 `Guidance for Industry Q8 (R2) Pharmaceutical Development' US\u000d\u000a      Department of Health and\u000d\u000a      Human Services, Food and Drug Administration, Centre for Drug Evaluation\u000d\u000a      and Research,\u000d\u000a      Centre for Biologics Evaluation and Research.\u000d\u000a      (http:\/\/www.fda.gov\/downloads\/Drugs\/...\/Guidances\/ucm073507.pdf)\u000d\u000a    Sources to corroborate Impact:\u000d\u000a    5.1 http:\/\/www.gsk.com\/media\/press-releases\/2013\/two-new-gsk-oral-oncology-treatments--braf-inhibitor-tafinlar---.html\u000d\u000a      and references therein.\u000d\u000a    5.2 Ascierto et al. (2012). Journal of Translational Medicine,\u000d\u000a      10:85 (http:\/\/www.translational-medicine.com\/content\/10\/1\/85).\u000d\u000a    5.3 Senior Scientific Investigator and Quality-by-Design\u000d\u000a      Innovation Leader, GlaxoSmithKline.\u000d\u000a    ","Title":"\u000d\u000a    10-05 Improved Drug Development Using Supersaturated Experiments\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2637487","Name":"Southampton"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Southampton has an established strong research programme in the\u000d\u000a      development and\u000d\u000a      implementation of methods for the statistical design and analysis of\u000d\u000a      experiments that\u000d\u000a      simultaneously vary a large number of factors. Such experiments are\u000d\u000a      routinely used for screening\u000d\u000a      (early stage experimentation to identify active factors having a\u000d\u000a      substantial impact on a product or\u000d\u000a      process) and, increasingly, for robustness studies (late stage\u000d\u000a      experimentation to verify empirically\u000d\u000a      derived models and system optimisation).\u000d\u000a    These experiments are regularly conducted by industry for the development\u000d\u000a      of more efficient,\u000d\u000a      economically viable products and processes, from drug development by\u000d\u000a      pharmaceutical\u000d\u000a      companies to performance improvements by car manufacturers. Successful\u000d\u000a      screening and\u000d\u000a      robustness studies both depend on factor sparsity, that is, when\u000d\u000a      the performance of the system is\u000d\u000a      dominated by a small subset of the factors; the quick identification of\u000d\u000a      these few active factors can\u000d\u000a      save considerable time and money.\u000d\u000a    From 2000 to 2006, research [3.1] by Professor Susan Lewis,\u000d\u000a      Professor Angela Dean (at the Ohio\u000d\u000a      State University; moved to Southampton in 2011) and PhD student Anna Vine\u000d\u000a      (2001-2006)\u000d\u000a      developed methodology for experimentation to investigate both individual\u000d\u000a      factor effects and the\u000d\u000a      joint effects of two factors. In particular, the research produced methods\u000d\u000a      for exploiting information\u000d\u000a      obtained from subject experts to provide a definition of a substantively\u000d\u000a      important, or active, effect\u000d\u000a      that can help to improve the effectiveness of data modelling from designed\u000d\u000a      experiments.\u000d\u000a      Assessments of the resulting design and modelling methods also\u000d\u000a      incorporated realistic\u000d\u000a      assumptions on the impact of inactive effects. This strand of research was\u000d\u000a      funded by the EPSRC\u000d\u000a      with project partners Jaguar Cars, Hosiden Besson and Goodrich, who were\u000d\u000a      looking to solve\u000d\u000a      complex engineering problems using Southampton's novel methodology.\u000d\u000a    Later research from 2007 onwards, by Professor David Woods, Lewis, Dean\u000d\u000a      and PhD student\u000d\u000a      Christopher Marley (2007 - 2011), focussed on supersaturated designs. It\u000d\u000a      was supported by the\u000d\u000a      EPSRC and industrial funding from both GSK and Lubrizol. These designs are\u000d\u000a      used in\u000d\u000a      experiments where there are fewer runs than factors in the experiment and\u000d\u000a      are particularly useful\u000d\u000a      when experiments are expensive to perform. This economy of resource makes\u000d\u000a      supersaturated\u000d\u000a      designs beneficial to industry but also presents challenges in the\u000d\u000a      analysis of the resulting data. In\u000d\u000a      fact, so controversial have these methods been that although they were\u000d\u000a      first proposed in the\u000d\u000a      1960s, they have only very recently started to be applied in industry.\u000d\u000a      Research at Southampton\u000d\u000a      has played a significant role in this adoption through the provision by\u000d\u000a      Marley and Woods [3.2] of\u000d\u000a      (i) an assessment of the performance of supersaturated designs, and the\u000d\u000a      associated data\u000d\u000a      modelling methods, for different experimental scenarios, and (ii) the\u000d\u000a      first recommendations for the\u000d\u000a      successful application of supersaturated designs.\u000d\u000a    Further research [3.3] developed new methods of selecting\u000d\u000a      effective designs through minimising\u000d\u000a      multi-factor dependencies, and optimal designs for robust product\u000d\u000a      experiments. Most recently, the\u000d\u000a      Southampton team provided the first evidence for the effectiveness of\u000d\u000a      these novel methods for\u000d\u000a      experiments with interacting factors and a new methodology for Bayesian\u000d\u000a      modelling of data from\u000d\u000a      supersaturated experiments [3.4].\u000d\u000a    "},{"CaseStudyId":"43014","Continent":[{"GeoNamesId":"6255149","Name":"North America"},{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2077456","Name":"Australia"},{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Some 107 million Americans live in areas that violate health standards\u000a      for ozone, the USEPA reports. Like the weather, air quality can change\u000a      from day to day, hour to hour, and national and local media provide daily\u000a      air quality reports as part of weather forecasts. It is the responsibility\u000a      of government agencies, principally the USEPA, to continuously monitor and\u000a      provide air quality information to the public and media. According to the\u000a      2009 USEPA report on Air Quality Index (AQI): `In large US cities (more\u000a        than 350,000 people), state and local agencies are required to report\u000a        the AQI to the public daily. When the AQI is above 100, agencies must\u000a        also report which groups, such as children or people with asthma or\u000a        heart disease, may be sensitive to that pollutant and would alert groups\u000a        about how to protect their health. These forecasts help local residents\u000a        protect their health by alerting them to plan their strenuous outdoor\u000a        activities for a time when air quality is better.' [5.C1]\u000a    From 2010, the USEPA has used Southampton's forecasting methods and its\u000a      associated software to continuously increase the quality of its air\u000a      pollution data in its short-term forecasts for the whole of the USA. More\u000a      accurate and reliable forecasts have had an immediate positive impact by\u000a      enabling those at risk to plan their outdoor activities to minimise any\u000a      negative effect on their health. The current USEPA training document aimed\u000a      at doctors `Ozone and your Patients Health' [5.C2] states: `Healthcare\u000a        providers should recommend that patients reduce their ozone exposure on\u000a        days when air quality is bad, especially people with asthma, who are\u000a        more susceptible to the effects of exposure.' It gives detailed\u000a      advice on how those at risk should modify their behaviour when air quality\u000a      (and ozone in particular) is predicted to be unhealthy and provides\u000a      evidence that such advice has a positive impact on health. Commenting on\u000a      the impact of Southampton's research, the USEPA said: `The forecast\u000a        methods of Sahu et al (2010) were adapted to forecast spatial patterns\u000a        of current 8 hour average ozone concentrations in real-time. This fusion\u000a        model combines both real-time ozone monitoring data with numerical model\u000a        output to achieve precise and accurate forecasts' [5.1].\u000a    The general public, including schools and local authorities, are among\u000a      the direct beneficiaries. According to the Center for Disease Control and\u000a      Prevention, the existence of accurate and reliable air pollution forecasts\u000a      bring significant health benefits to 7 million children and 18.7 million\u000a      adult Americans who suffer from asthma and millions of others who use\u000a      healthcare facilities to alleviate their respiratory illnesses [5.C3].\u000a      The WHO advises that limiting exposure to high levels of air pollution\u000a      impacts positively on long-term health [5.C4]. A reduction in GP\u000a      and hospital visits also delivers substantial economic impact.\u000a    The improved forecasts benefit the US agricultural economy. Prolonged\u000a      exposure to high levels of ozone is harmful to crops. It damages materials\u000a      such as rubber, paint and textiles. Further, it reduces growth and\u000a      survivability of tree seedlings, and increases susceptibility to diseases,\u000a      pests and harsh weather conditions. In the USA, ground-level ozone is\u000a      responsible for an estimated $500 million in reduced crop production each\u000a      year. This emphasises the need for a long-term dimension to the modelling\u000a      of air pollution. Southampton's models have provided accurate estimates of\u000a      trends in air pollution for the eastern US on which local authorities and\u000a      regulators, including the USEPA, can base long-term emission control\u000a      strategies and assess compliance to the regulatory standards of areas that\u000a      fall outside the sparse ozone-monitoring network in the US. If national\u000a      standards are not met, local authorities can introduce more stringent\u000a      emission measures.\u000a    Southampton's research has fed into the policy debate in the US over\u000a      regulations to limit air pollution. It contributed directly to the\u000a      formation of USEPA's influential `NOx Budget Trading Program: 2005 Program\u000a      Compliance and Environmental Results' [5.2], which set out new\u000a      measures that it said, by 2015, would secure `$85 to $100bn in annual\u000a        health benefits, annually preventing 17,000 premature deaths, millions\u000a        of lost work and school days and ten of thousands of non-fatal heart\u000a        attacks and hospital admissions.' The 2005 report &#8212; combined with\u000a      the use of Sahu's statistical spatio-temporal modelling to provide air\u000a      quality information at unmonitored sites &#8212; formed the foundations of the\u000a      current environmental regulatory policies in the USA, notably the 2011\u000a      Integrated Review Plan for the Ozone National Ambient Air Quality\u000a      Standards [5.3]. These reports help develop programs aimed at\u000a      reducing pollution by reformulating fuels and consumer\/commercial products\u000a      that contain harmful chemicals, and voluntary programs that encourage\u000a      communities to adopt practices, such as carpooling, to reduce harmful\u000a      emissions.\u000a    The impact of Southampton's work in the USA is being felt further afield.\u000a      It prompted the EPSRC to provide a grant of &#163;365,643 in 2013 for Sahu to\u000a      produce a new statistical framework for estimating the long-term health\u000a      effects of air pollution in the UK [3.G3]. The Australia's\u000a      national science agency, the Commonwealth Scientific and Industrial\u000a      Research Organisation (CSIRO), has used the spTimer software developed by\u000a      Sahu [5.4] to model large scale environmental data [5.5].\u000a    ","ImpactSummary":"\u000a    Working closely with scientists at the United States Environmental\u000a      Protection Agency (USEPA), the University of Southampton has developed new\u000a      methods for space-time modelling that have trebled the accuracy of air\u000a      pollution forecasts. The USEPA has adopted the research as its official\u000a      forecasting method to protect the American public and agriculture. More\u000a      than 19 million children and 16 million adult Americans suffering from\u000a      respiratory conditions such as asthma now benefit by being able to adjust\u000a      their outdoor activities based on the forecasts, and improved data has fed\u000a      into policy debates on carbon emission regulations. Success in the USA has\u000a      led the EPSRC to fund a similar project in the UK and Australia's national\u000a      science agency is using Southampton-developed software for its air\u000a      pollution forecasts.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Southampton\u000a    ","Institutions":[{"AlternativeName":"Southampton (University of)","InstitutionName":"University of Southampton","PeerGroup":"A","Region":"South East","UKPRN":10007158}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Publications:\u000a    \u000a3.1 (*) Sahu, S. K., Gelfand, A. E. and Holland, D. M. (2010)\u000a      Fusing point and areal level space-time data with application to wet\u000a      deposition. Journal of the Royal Statistical Society, Series C, 59,\u000a      77-103.\u000a    \u000a\u000a3.2 (*) Sahu, S. K., Yip, S., and Holland, D. M. (2009) A\u000a      fast Bayesian method for updating and forecasting hourly ozone levels. Environmental\u000a        and Ecological Statistics, 18, 185-207.\u000a    \u000a\u000a3.3 Sahu, S. K. and Bakar, K. S. (2012) Hierarchical Bayesian\u000a      auto-regressive models for large space time data with applications to\u000a      ozone concentration modelling, with discussion. Applied Stochastic\u000a        Models in Business and Industry, 28, 395-415.\u000a    \u000a(*) These references best indicate the quality of the underpinning\u000a      research.\u000a    Grants:\u000a    3.G1 Sahu, S. Modelling large space-time data sets, The National\u000a      Academies, USA, August 2007-January 2008, US$45,000.\u000a    3.G2 A grant in the form a Vice Chancellor's scholarship won from\u000a      a university wide competition in Southampton. Sahu. Forecasting Air\u000a      pollution Levels, University of Southampton, October 2012 &#8212; September\u000a      2015.\u000a    3.G3 Sahu S. A rigorous statistical framework for estimating the\u000a      long-term health effects of air pollution. EPSRC, 2013-16, &#163;356,643.\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"11","Level2":"2","Subject":"Cardiorespiratory Medicine and Haematology"}],"Sources":"\u000a    Contextual References:\u000a    5.C1 `Air Quality: A Guide to Air Quality and Your Health' US\u000a      Environment Protection Agency Report (2009) EPA-456\/F-09-002 [This\u000a      demonstrates the importance US EPA attaches to predicting AQI]\u000a    5.C2 `Ozone and your Patients Health: Patient Exposure and the Air\u000a      Quality Index': http:\/\/www.epa.gov\/o3healthtraining\/aqi.html\u000a      [This demonstrates that clinicians actively promote the use of AQI\u000a      forecasts and this has a significant positive impact on health]\u000a    5.C3 `Asthma: Centers for Disease Control and Prevention' http:\/\/www.cdc.gov\/asthma\/default.htm\u000a      [This demonstrates the scale of the problem and the impact of reliable\u000a      forecasts of air quality]\u000a    5.C4 `WHO Report: Health Aspects of Air Pollution with Particulate\u000a      Matter, Ozone and Nitrogen Dioxide (2003) [This details the long-term\u000a      health impacts of exposure to air pollution]\u000a    Sources to corroborate impact:\u000a    5.1 Senior Statistician, National Exposure Research Laboratory,\u000a      United States Environmental Protection Agency (USEPA).\u000a    5.2 `NOx Budget Trading Program 2005 Program Compliance and\u000a      Environmental Results', US Environment Protection Agency Report (2006)\u000a      EPA430-R-06-013 [Senior Statistician, USEPA in 5.1 will confirm the\u000a        input of the work of Sahu into this document]\u000a    5.3 `Integrated Review Plan for the Ozone National Ambient Air\u000a      Quality Standards', US Environment Protection Agency Report (2011) EPA\u000a      452\/R-11-006 [This drew directly on the work of 5.2 above]\u000a    5.4 spTimer: available through Comprehensive R Archive\u000a      Network\u000a      http:\/\/cran.r-project.org\/web\/packages\/spTimer\/index.htm\u000a    5.5 http:\/\/www.eric-lehmann.com\/Presentations\/MODSIM_pres_ChiuLehmann.pdf\u000a      [For example this work on water resources done in conjunction with the\u000a        Australian Bureau of Meteorology makes direct use of the spTimer\u000a        software developed by Sahu]\u000a    ","Title":"\u000a    10-08 Using Novel Statistical Modelling Techniques to Deliver More\u000a      Accurate Air Pollution Forecasts\u000a    ","UKLocation":[{"GeoNamesId":"2637487","Name":"Southampton"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Even only moderate levels of air pollution can damage the health of\u000a      vulnerable people with lung disease &#8212; in particular sufferers of asthma,\u000a      chronic bronchitis and emphysema &#8212; and heart disease. Air pollutants can\u000a      also adversely affect healthy people who regularly exercise outdoors.\u000a      Children are at greater risk because they often play outdoors in warmer\u000a      weather when ozone levels are higher, their lungs are still developing,\u000a      and they are more likely to have asthma which is aggravated by ozone\u000a      exposure. The resulting healthcare costs are high; aggravations of lung\u000a      diseases lead to increased medication use, GP and A&amp;E visits, and\u000a      hospital admissions.\u000a    Professor Sujit Sahu (1999-present) has spent the last decade conducting\u000a      research in spatio-temporal statistical modelling and applications to\u000a      atmospheric processes that are harmful to human health, publishing more\u000a      than 25 peer-reviewed papers in this period. Having established an ongoing\u000a      collaboration with Dr David Holland, a senior scientist at the United\u000a      States Environmental Protection Agency (USEPA), and Professor Alan Gelfand\u000a      at Duke University, North Carolina, in 2003, Sahu has focused on\u000a      developing accurate methods for forecasting air pollution levels at any\u000a      given location, even where there is no air pollution monitoring station\u000a      nearby.\u000a    This need to spatially interpolate and forecast at unmonitored sites over\u000a      vast regions spanning thousands of square kilometres, led to the\u000a      development of Bayesian hierarchical models for spatio-temporal processes\u000a      that accurately describe air pollution levels over space and time. This\u000a      research revealed that statistical models that combine past\u000a      observations obtained from monitoring sites with the output of numerical\u000a      air-quality models are far superior to either standalone empirical\u000a      observations or numerical models [3.1, 3.2]. This insight led to\u000a      the development of a coherent Bayesian forecasting framework, which was\u000a      shown to be substantially faster and three times more accurate than\u000a      previous models. The framework was able to instantly update the air\u000a      pollution map for the current hour as soon as the monitor data was\u000a      received for that hour, and forecast the map for several hours ahead.\u000a    Once the key modelling and forecasting techniques proved superior in\u000a      principle, the natural next step was to develop a software package that\u000a      would be capable of delivering the forecasts in production mode in real\u000a      time for use by forecasters and environmental agencies. Sahu worked with\u000a      the USEPA to develop this software in 2010-2011. The result was a robust\u000a      software system that is able to produce the forecasts and their\u000a      statistical uncertainties for the entire United States. An educational and\u000a      research version of the software package, spTimer, which can be used under\u000a      a freely available GNU public license, is available through the online\u000a      Comprehensive R Archive Network, opening up the improved forecasting\u000a      techniques for use by any agency anywhere in the world.\u000a    Additional research has developed methods to estimate location-specific,\u000a      long-term trends in air pollution [3.3]. As in the case of\u000a      forecasting, air pollution is often only monitored at a handful of\u000a      locations. Hence, accurate spatial interpolations along with their\u000a      uncertainties must be obtained from data through statistical modelling. A\u000a      high-resolution, space-time model was developed for this purpose that has\u000a      been further adapted to model several million sets of space-time data.\u000a    "},{"CaseStudyId":"43015","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"2017370","Name":"Russia"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Europe is one of the world's leading players in space engineering,\u000d\u000a      competing in the commercial\u000d\u000a      markets for telecommunications and launchers and forming a key partner for\u000d\u000a      the United States,\u000d\u000a      Russia and other countries. The ESA employs more than 2,000 people and has\u000d\u000a      an annual budget\u000d\u000a      of &#8364;4 billion. The European Space Research and Technology Centre (ESTEC)\u000d\u000a      which makes use of\u000d\u000a      the Southampton research is ESA's technical heart with a workforce of\u000d\u000a      around 2,700 and an\u000d\u000a      annual budget of &#8364;2.4 billion [5.C1]. ESTEC guarantees investors a\u000d\u000a      return rate of 96%; thus, the\u000d\u000a      annual value it adds to the European economy exceeds &#8364;2 billion, excluding\u000d\u000a      spin-off companies.\u000d\u000a    Each year, well over 100 payloads worth $100 billion are launched into\u000d\u000a      space [5.C2]. Arianespace,\u000d\u000a      the ESA's major launch provider and Europe's foremost commercial space\u000d\u000a      transportation company\u000d\u000a      with an annual revenue of &#8364;1 billion, accounts for about half the\u000d\u000a      commercial market. Each of their\u000d\u000a      satellites contains at least one antenna system, several heat pumps and\u000d\u000a      various other items whose\u000d\u000a      efficiency and cost require careful optimisation. Currently, the ESA has\u000d\u000a      20 spacecraft and missions\u000d\u000a      in the planning and design phases with launch dates between 2013 and 2022\u000d\u000a      and costs for each\u000d\u000a      mission ranging from several hundred million to one billion euros. Each\u000d\u000a      contains various\u000d\u000a      subsystems with conflicting design objectives. With costs of up to $40,000\u000d\u000a      to bring just one\u000d\u000a      kilogramme of payload into orbit, the design of efficient but lightweight\u000d\u000a      system components plays a\u000d\u000a      key role in spacecraft engineering.\u000d\u000a    Southampton's research has led to the development of a software tool\u000d\u000a      capable of efficiently solving\u000d\u000a      multi-objective optimisation problems that occur in space engineering and\u000d\u000a      spacecraft design. The\u000d\u000a      toolbox has, since April 2011, been a standard part of the ESA's design\u000d\u000a      technology. It has been\u000d\u000a      applied to the thermal design of spacecraft subsystems and in optimising\u000d\u000a      the shape of antenna\u000d\u000a      systems. The use of these tools has halved the computational time for\u000d\u000a      optimising the thermal\u000d\u000a      design of spacecraft sub-systems and the shape of antennae systems,\u000d\u000a      leading to an increased\u000d\u000a      efficiency equivalent to savings of 20 person-years (approximately &#8364;1\u000d\u000a      million). As the Head of the\u000d\u000a      ESA Guidance Navigation and Control Section will confirm, Southampton's\u000d\u000a      research has\u000d\u000a      significantly lowered the hurdles to new spacecraft designs and allowed\u000d\u000a      the ESA to maintain a\u000d\u000a      competitive edge in the manufacture of these products [5.1].\u000d\u000a    As a specific example, in the design of a heat pipe an engineer needs to\u000d\u000a      vary the design\u000d\u000a      parameters that specify the basic geometry and the properties of the\u000d\u000a      materials used in construction\u000d\u000a      to maximise the heat throughput, while minimising the weight and the cost.\u000d\u000a      These criteria are\u000d\u000a      combined to produce a trade-off curve, which reveals the optimum\u000d\u000a      performance. Previously this\u000d\u000a      involved a large number of one-off calculations and there was no certainty\u000d\u000a      that the optimum\u000d\u000a      scenario was indeed achieved, often resulting in a large waste of\u000d\u000a      computational effort and\u000d\u000a      preventing a comprehensive exploration of the design space. The new\u000d\u000a      process developed by\u000d\u000a      Southampton automatically generates the design curve along with the\u000d\u000a      associated parameters and\u000d\u000a      cost. This enables designers to explore the parameter space in an\u000d\u000a      efficient and user-friendly way,\u000d\u000a      while the GUI interface allows them to develop an intuitive insight into\u000d\u000a      the problem. In particular,\u000d\u000a      the toolbox has enabled designers to find previously undiscovered regions\u000d\u000a      of the design space and\u000d\u000a      rapidly find the limits to a particular design, which aids the development\u000d\u000a      of new and innovative\u000d\u000a      solutions to the problem [5.C3].\u000d\u000a    According to the official report of Dr G Ortega Head of the ESA Guidance,\u000d\u000a      Navigation, and Control\u000d\u000a      Section and responsible for evaluating the project `ESA has concluded\u000d\u000a        with high success the\u000d\u000a        development of a Nonlinear Programming Solver (NLP) for numerical\u000d\u000a        optimization. The\u000d\u000a        Development Team has been deeming to be classified as fully outstanding.\u000d\u000a        The newly developed\u000d\u000a        NLP solver increases capacity and capability of European industry in the\u000d\u000a        area of numerical\u000d\u000a        optimization for applications such as launcher design optimization and\u000d\u000a        material optimization.' In a\u000d\u000a      measure of the value the ESA places on our research, it gave &#8364;200,000 in\u000d\u000a      funding in 2010 to\u000d\u000a      enable the German software company Astos Solutions to develop the\u000d\u000a      Southampton co-designed\u000d\u000a      software tool further [5.2], as part of the ESA's own General\u000d\u000a      Support Technology Programme\u000d\u000a      (GSTP). These GSTP activities, now in their fifth five-year cycle, are\u000d\u000a      designed `to ensure the right\u000d\u000a      technologies at the right maturity are available at the right time' [5.C4]\u000d\u000a      and to convert the best\u000d\u000a      concepts into mature products.\u000d\u000a    The success of the software tool inspired &#8364;480,000 in EU funding to\u000d\u000a      Southampton to adapt the tool\u000d\u000a      to meet design challenges in the avionics industry under the EUP FP7\u000d\u000a      project AWACS &#8212; Adaption\u000d\u000a        of WORHP to Avionics Constraints, coordinated by Thales, the world's\u000d\u000a      third-largest avionics\u000d\u000a      service provider [3.G3]. AWACS is funded under the EU Clean Sky\u000d\u000a      Initiative, which aims to\u000d\u000a      develop technologies that will achieve a 75% reduction in CO2\u000d\u000a      emissions per passenger kilometre,\u000d\u000a      a 90% reduction in nitrogen oxide emissions and a 65% reduction in\u000d\u000a      perceived aircraft noise.\u000d\u000a      These targets pose significant challenges for aircraft trajectory\u000d\u000a      optimisation and AWACs is one of\u000d\u000a      the first projects to attempt to develop a corresponding software solution\u000d\u000a      to optimise aircraft\u000d\u000a      trajectory.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Through close collaboration with scientists at the European Space Agency\u000d\u000a      (ESA), research at the\u000d\u000a      University of Southampton has developed new algorithms and an associated\u000d\u000a      software tool that\u000d\u000a      have contributed to more efficient spacecraft design. Now a standard\u000d\u000a      component of the ESA's\u000d\u000a      design technology, the tools have doubled the speed in which crucial\u000d\u000a      design processes can be\u000d\u000a      completed, resulting in increased efficiency over the REF period of 20\u000d\u000a      person-years &#8212; equivalent to\u000d\u000a      &#8364;1 million in monetary terms &#8212; and maintaining the ESA's manufacturing\u000d\u000a      competitiveness. The\u000d\u000a      success of this work led to a &#8364;480,000 EU grant to adapt the tools for the\u000d\u000a      avionics industry as part\u000d\u000a      of efforts to meet ambitious environmental targets under the EU Clean Sky\u000d\u000a      Initiative.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of Southampton\u000d\u000a    ","Institutions":[{"AlternativeName":"Southampton (University of)","InstitutionName":"University of Southampton","PeerGroup":"A","Region":"South East","UKPRN":10007158}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    Publications:\u000d\u000a    \u000a3.1 (*) Fliege, J. (2007): The\u000d\u000a        effects of adding objectives to an optimisation problem on the solution\u000d\u000a        set. Operations Research Letters, 35, (6), 782-790.\u000d\u000a    \u000a\u000a3.2 (*) Fliege, Drummond, M.G. and Svaiter, B.F. (2009):\u000d\u000a      Newton's method for multicriteria\u000d\u000a      optimisation. SIAM Journal on Optimisation, 20, (2), 602-626.\u000d\u000a    \u000a\u000a3.3 Gerdts, Fliege, Vicente, L.N. (2009): FGS Toolbox Software\u000d\u000a      Manual. European Space\u000d\u000a      Agency Study Contract Report 22170\/08\/NL\/ST WP3000. September 2009.\u000d\u000a    \u000a\u000a3.4 Fliege, J. and Xu H. (2011): Stochastic Multiobjective\u000d\u000a      Optimisation: Sample Average\u000d\u000a      Approximation and Applications. Journal of Optimisation Theory and\u000d\u000a        Applications, 151 (2). 135-162.\u000d\u000a    \u000a(*) These references best indicate the quality of the underpinning\u000d\u000a      research.\u000d\u000a    Grants:\u000d\u000a    3.G1 European Space Agency Study Contract Report 22170\/08\/NL\/ST\u000d\u000a      `Versatility of Filtering\u000d\u000a      Techniques in Non-Linear Programming Optimisation'. Awarded to the\u000d\u000a      Universities of\u000d\u000a      Birmingham, Southampton, and Coimbra. November 2008 &#8212; January 2010, EURO\u000d\u000a      180,000.\u000d\u000a    3.G2 European Space Agency Contract t 5001003472, FGS-ECM-RfQ-1001\u000d\u000a      `MCO Add-On for\u000d\u000a      FGS Toolbox'. Awarded to the University of Southampton. December 2010 &#8212;\u000d\u000a      April 2011,\u000d\u000a      EURO 20,000.\u000d\u000a    3.G3 Fliege, J. EU FP7 CLEANSKY Programme, call ID\u000d\u000a      SP1-JTI-CS-2012-03, `AWACS &#8212;\u000d\u000a      Adaption of WORHP to Avionics Constraints'. Awarded to the University of\u000d\u000a      Southampton, April\u000d\u000a      2013 &#8212; April 2015, EURO 474,000. http:\/\/www.cleansky.eu\/\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000d\u000a    Contextual References:\u000d\u000a    5.C1 http:\/\/www.esa.int\/About_Us\/ESTEC\/ESTEC\u000d\u000a        European Space Research and Technology\u000d\u000a        Centre\u000d\u000a    5.C2 Governmental budgets for space activities in The Space\u000d\u000a        Economy at a Glance 2011,\u000d\u000a      OECD Publishing (2011)\u000d\u000a    5.C3 The design features of the toolbox and GUI interface are\u000d\u000a      described in detail in ESA Study\u000d\u000a      Contract Report, 5001003472, `MCO-FGS Software Toolbox Manual'; ESA Study\u000d\u000a      Contract\u000d\u000a      Report, 5001003472, `MCO-FGS Architectural Design Document' and `ESA Study\u000d\u000a      Contract\u000d\u000a      Report, 5001003472, `Multicriteria Optimisation'.\u000d\u000a    5.C4 http:\/\/www.esa.int\/Our\u000d\u000a        Activities\/Technology\/About the General Support Technology Programme GST\u000d\u000a    Sources to corroborate the impact:\u000d\u000a    5.1 Head of the Guidance, Navigation and Control Section, European\u000d\u000a      Space Agency (ESA) [He\u000d\u000a        can confirm the increased efficiency saving resulting from the use of\u000d\u000a        the computational\u000d\u000a        toolbox]\u000d\u000a    5.2 http:\/\/www.worhp.de\/content\/history.\u000d\u000a      [This specifically mentions University of\u000d\u000a        Southampton's involvement in the WORHP development].\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    10-10 Optimising Spacecraft Design for A World-leading Space Agency\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2637487","Name":"Southampton"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Spacecraft are used for a variety of purposes: communications,\u000d\u000a      meteorology, navigation,\u000d\u000a      exploration and transportation. Designing spacecraft is an incredibly\u000d\u000a      complex process; a vast\u000d\u000a      number of parameters, including component geometry and material\u000d\u000a      properties, must be taken into\u000d\u000a      account. Rather than looking to optimise a single objective, engineers\u000d\u000a      simultaneously weigh up\u000d\u000a      several competing criteria such as cost, weight and durability in a\u000d\u000a      process known as multi-objective\u000d\u000a      optimisation. The design of heat pumps in spacecraft is a case in point;\u000d\u000a      the cost and weight of the\u000d\u000a      pumps need to be minimised, while their mechanical efficiency must be\u000d\u000a      maximised. Engineers rely\u000d\u000a      on mathematical algorithms to solve these multi-objective problems as\u000d\u000a      efficiently as possible.\u000d\u000a    Standard techniques for multi-objective optimisation &#8212; for example\u000d\u000a      weighting objectives and adding\u000d\u000a      them up &#8212; involve the transformation of the given problem into an infinite\u000d\u000a      family of parameterised\u000d\u000a      standard optimisation problems. Solving all such problems for all\u000d\u000a      possible parameters amounts to\u000d\u000a      solving the original problem, but requires the solution of an infinite\u000d\u000a      number of sub-problems. Thus\u000d\u000a      in the real world one usually resorts to solving a finite number of these\u000d\u000a      problems to obtain an\u000d\u000a      approximate answer to the original question. This approach leaves various\u000d\u000a      important numerical\u000d\u000a      questions unanswered that need to be considered before practical\u000d\u000a      algorithms can be designed, in\u000d\u000a      particular:\u000d\u000a    \u000d\u000a      What are the numerical properties of transformations from a\u000d\u000a        multi-objective problem to a\u000d\u000a        family of parameterised single-objective problems?\u000d\u000a      Is there an efficient way to discretise the parameter space of such a\u000d\u000a        family of parameterised\u000d\u000a        single-objective problems?\u000d\u000a      Is it possible to use information from one parameterised problem to\u000d\u000a        draw conclusions about\u000d\u000a        the solutions of another parameterised problem?\u000d\u000a    \u000d\u000a    The University of Southampton's Joerg Fliege (Professor, 2007-present) is\u000d\u000a      an expert in\u000d\u000a      mathematical and multi-objective optimisation and has contributed to a\u000d\u000a      greater understanding of all\u000d\u000a      aspects of these problems, from the underpinning mathematics to the design\u000d\u000a      of efficient\u000d\u000a      algorithms. He showed that a certain class of transformations have\u000d\u000a      favourable numerical properties\u000d\u000a      that can be exploited to reduce computation times [3.1]. Together\u000d\u000a      with colleagues in Brazil, he\u000d\u000a      developed, for the first time, a highly efficient algorithmic scheme for\u000d\u000a      discretizing parameter spaces\u000d\u000a      in an efficient manner [3.2] and designed a sensitivity analysis\u000d\u000a      that found it was indeed possible to\u000d\u000a      provide a solution to the third question outlined above.\u000d\u000a    This highly efficient numerical scheme makes use of recent algorithm\u000d\u000a      developments, notably an\u000d\u000a      adaptive scheme for the parameter space of a multi-objective programming\u000d\u000a      problem. From 2008-2010,\u000d\u000a      a team comprising Fliege, Gerdts (University of the Federal Armed Forces\u000d\u000a      Munich), Vicente\u000d\u000a      (University of Coimbra) and scientists at the European Space Agency (ESA)\u000d\u000a      employed these ideas\u000d\u000a      to develop new algorithms and produce a computational toolbox to solve\u000d\u000a      multi-objective problems\u000d\u000a      in spacecraft design [3.3].\u000d\u000a    The multi-objective optimisation algorithm was implemented and\u000d\u000a      incorporated into the toolbox in a\u000d\u000a      follow-up project [3.4]. The Nonlinear Programme Solver code that\u000d\u000a      was developed included a\u000d\u000a      Graphical User Interface that allows an efficient exploration of the\u000d\u000a      design space for a given multi-objective\u000d\u000a      problem and consideration of trade-offs between the different objectives.\u000d\u000a      ESA engineers\u000d\u000a      were then able to explore the full parameter space in an efficient way to\u000d\u000a      significantly improve the\u000d\u000a      design process and enhance the quality of the final product.\u000d\u000a    "},{"CaseStudyId":"43016","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000a    According to the Helping to Shape Tomorrow White Paper [5.C1]:\u000a      `Government, local authorities,\u000a      the health service, the education and academic community, commercial\u000a      business, professional\u000a      organisations and the public at large all need reliable information on the\u000a      number and\u000a      characteristics of people and households if they are to conduct many of\u000a      their activities effectively.'\u000a      However, in order for all these end users to reap maximum benefit from the\u000a      Census, `the\u000a      information must be authoritative, accurate and comparable for all parts\u000a      of the country' and\u000a      `...should be in such a form, and at varying levels of statistical and\u000a      geographical detail, to meet the\u000a      changing requirements of users, subject to the overriding requirement to\u000a      protect statistical\u000a      confidentiality.'\u000a    Southampton's research into survey design, estimation and statistical\u000a      disclosure control (SDC) has\u000a      been integral to major improvements in the accuracy with which the Census\u000a      is able to estimate the\u000a      UK population's size and characteristics, and in allowing that information\u000a      to be released while\u000a      preserving the rights to privacy of all UK citizens. The immediate\u000a      beneficiary of Southampton's\u000a      research was the ONS, who were able to improve the accuracy, reliability\u000a      and usefulness of the\u000a      2011 Census and also save money by limiting the need for extensive\u000a      additional surveys. Dr Marie\u000a      Cruddas, Head of Census Methodology at the ONS, said: `Southampton's\u000a        research played a key\u000a        role in the statistical design, estimation and dissemination strategies\u000a        for the 2011 Census, enabling\u000a        the production of the high-quality data that the government needs to\u000a        develop policies, and to plan\u000a        and run public services.' [5.1]\u000a    The application of Southampton's dual-system estimation allowed basic\u000a      demographic data of those\u000a      missed in the Census to be estimated and the adjustments made. For the\u000a      2011 Census, the ONS\u000a      used the CCS in which Southampton's research was the key component of the\u000a      overall coverage\u000a      assessment and adjustment strategy. Brown's involvement was cited\u000a      prominently in the summary\u000a      of the 2011 CCS [5.2], published on the ONS website. The\u000a      objectives for the CCS sample design\u000a      were to select a robust sample that avoided bias, achieved maximum\u000a      precision and targeted\u000a      resources in the most important areas. Brown was a central member of the\u000a      team working on (a) a\u000a      re-design of the CCS for the 2011 Census, (b) a development of the main\u000a      estimation strategy and\u000a      (c) a reassessment of the adjustment system [5.3, see also 3.1].\u000a      An independent review team of\u000a      the coverage assessment and adjustment methodology [5.4] stated:\u000a      `The methods give confidence\u000a      that the resulting final census population estimates will be better than\u000a      any other method and will be\u000a      suitable for use in resource allocation and planning.' Further ONS quality\u000a      assurance work following\u000a      the 2011 census also used the CCS, and `added to ONS' confidence in the\u000a      2011 Census' [5.5].\u000a    Apart from the ONS, the main beneficiaries of Southampton's research are\u000a      the users of the\u000a      Census who now have access to data that is both more accurate and\u000a      available at narrower local\u000a      geographies than previously possible. The Census drives the allocation of\u000a      local services such as\u000a      schools and transport systems, and thus accurate estimates ensure that\u000a      these services are placed\u000a      in the right locations. The provision of local health services is also a\u000a      key factor; the NHS allocates\u000a      about &#163;120bn a year to trusts largely based on a range of census-derived\u000a      population estimates,\u000a      according to the White Paper [5.C1]. Public Health Wales NHS Trust\u000a      stated: `It is crucial that we\u000a        continue to access this gold standard information at small area level.'\u000a    Southampton's work on statistical disclosure control allowed the release\u000a      of fine-grained local data\u000a      while preventing the disclosure of sensitive information about individual\u000a      respondents. This was of\u000a      particular importance to local authorities, who were relying on the\u000a      provision of social indicators right\u000a      down to neighbourhood level &#8212; to fulfil their obligations laid out in the\u000a      Localism Act 2011. The\u000a      Localism Act marked the beginning of a shift in power from the central\u000a      government to local\u000a      authorities that required decisions on issues like planning and housing to\u000a      be taken locally. Without\u000a      the local data provided by the 2011 Census, local authorities would have\u000a      less quantitative evidence\u000a      to support their decision-making. Of equal significance is the pressure on\u000a      local authorities to\u000a      respond to the demands of the Equality Act 2010 for which they need\u000a      localised data to minimise\u000a      discrimination among local populations. The importance to local\u000a      authorities of more localised\u000a      Census data was fully documented in the ONS 2012 Report: Beyond 2011\u000a        Public Consultation on\u000a        User Requirements [5.C2].\u000a    The release of microdata to a vast range of end users, including central\u000a      and local government\u000a      departments, and the health service, was facilitated by Southampton's SDC\u000a      research [5.6]. The\u000a      ONS assessed three SDC methods through a disclosure framework and software\u000a      package\u000a      developed by Southampton researchers, Shlomo and Ph.D. student Young. The\u000a      dependence of\u000a      the ONS approach to disclosure risk assessment on Southampton's research\u000a      is described in their\u000a      2010 report on Statistical Disclosure [5.7, see page 6].\u000a      Based on this work, the ONS decided on\u000a      the pre-tabular method of record swapping for protecting the 2011 UK\u000a      Census tables. According to\u000a      Jane Naylor of the ONS SDC team `the research undertaken by Natalie Shlomo\u000a      was central to the\u000a      final choice of Statistical Disclosure Control (SDC) method used for the\u000a      2011 Census. Her\u000a      research and the software that she developed (Infoloss) around measuring\u000a      disclosure risk and\u000a      utility were used to produce quantitative evidence for the performance of\u000a      different SDC methods in\u000a      the Census context' [5.8]. Later development work at Southampton\u000a      further enhanced statistical\u000a      disclosure control. This led to its use by the ONS for downstream\u000a      processing of census data,\u000a      enabling the release of more useful microdata to a range of end users\u000a      including central and local\u000a      government and the health service.\u000a    ","ImpactSummary":"\u000a    Statistical techniques developed at the University of Southampton have\u000a      transformed the accuracy\u000a      with which Census data can estimate the UK population's size and\u000a      characteristics, delivering far-reaching socio-economic impact. The methodologies developed by Southampton\u000a      have increased\u000a      the accuracy and availability of the 2011 UK Census data, not only for the\u000a      Office for National\u000a      Statistics but for central government, local authorities, the NHS and the\u000a      private sector, who all use\u000a      the data as a basis for policy decisions. Preserving the privacy of the UK\u000a      population,\u000a      Southampton's work allowed, for the first time, the release of highly\u000a      localised data, which is used\u000a      by local authorities to target resources efficiently and meet the demands\u000a      imposed by the Localism\u000a      and Equality Acts.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Southampton\u000a    ","Institutions":[{"AlternativeName":"Southampton (University of)","InstitutionName":"University of Southampton","PeerGroup":"A","Region":"South East","UKPRN":10007158}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Publications:\u000a    \u000a3.1 (*) Brown J., Abbott O. and Smith P. (2011). Design of the\u000a      2001 and 2011 census coverage\u000a      surveys for England and Wales. Journal of the Royal Statistical\u000a        Society, Series A, 174(4), 881-906.\u000a    \u000a\u000a3.2 Brown J., Abbott O. and Diamond I. (2006). Dependence in the\u000a      2001 one-number census\u000a      project. Journal of the Royal Statistical Society, Series A,\u000a      169, 883-902.\u000a    \u000a\u000a3.3 Brown J., Buckner L., Diamond I., Chambers R. and Teague A\u000a      (1999). A methodological\u000a      strategy for a one number census in the UK. Journal of the Royal\u000a        Statistical Society, Series A,\u000a      162, 247-267.\u000a    \u000a\u000a3.4 (*) Steele F., Brown J., and Chambers R. (2002). A controlled\u000a      donor imputation system for a\u000a      one-number census. Journal of the Royal Statistical Society, Series A,\u000a      165, 495-522.\u000a    \u000a\u000a3.5 Shlomo, N. (2007) Statistical disclosure control methods for\u000a      census frequency tables.\u000a      International Statistical Review, 75(2), pp. 199-217.\u000a    \u000a\u000a3.6 Shlomo, N. and Young, C. (2006). Statistical Disclosure\u000a      Control Methods Through a Risk-Utility\u000a      Framework. Privacy in Statistical Databases. Springer Lecture Notes in\u000a        Computer Science,\u000a      4302, 68-81.\u000a    \u000a\u000a3.7 (*) Shlomo, N. and Skinner, C.J. (2010) Assessing the\u000a      protection provided by misclassification-based disclosure limitation methods for survey microdata. Annals of\u000a        Applied Statistics, 4(3),\u000a      1291-1310.\u000a    \u000a(*) These references best indicate the quality of the underpinning\u000a      research.\u000a    Grants:\u000a    3.G1 University of Southampton-ONS research contract: Provision of\u000a      Research Services in\u000a      Statistical Methodology, 2005-2010 (&#163;850,000) and 2010-2015 (&#163;700,000).\u000a    3.G2 EU 5th Framework Programme: Computational Aspects of\u000a      Statistical Confidentiality\u000a      (CASC), 2000-2003, Chris Skinner, Principal Investigator at the University\u000a      of Southampton.\u000a    3.G3 EU 7th Framework Programme: Data Without Borders, 2010-2014,\u000a      Natalie Shlomo,\u000a      Principal Investigator at the University of Southampton (&#163;185,000).\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000a    Contextual References:\u000a    5.C1 Government White Paper: Helping to Shape Tomorrow,\u000a      http:\/\/www.official-documents.gov.uk\/document\/cm75\/7513\/7513.asp\u000a    5.C2 ONS 2012 Report: Beyond 2011 Public Consultation on User\u000a      Requirements\u000a      http:\/\/www.ons.gov.uk\/ons\/about-ons\/who-ons-are\/programmes-and-projects\/beyond-2011\/reports-and-publications\/beyond-2011-user-requirements-consultation-report.pdf\u000a    Sources to corroborate the impact:\u000a    5.1 Head of Census Methodology, ONS.\u000a    5.2 ONS 2011: 2011 Census Coverage Survey Summary. http:\/\/www.ons.gov.uk\/ons\/guide-method\/census\/2011\/census-data\/2011-census-data\/2011-first-release\/first-release--quality-assurance-and-methodology-papers\/census-coverage-survey-summary.pdf\u000a    5.3 ONS One Number Census Steering Committee.\u000a    5.4 Census 2011: Independent Review of Coverage Assessment,\u000a      Adjustment and Quality\u000a      Assurance. http:\/\/www.ons.gov.uk\/ons\/guide-method\/census\/2011\/the-2011-census\/the-2011-census-project\/independent-assessments\/independent-review-of-coverage-assessment--adjustment-and-quality-assurance\/independent-review-final-report.pdf\u000a    5.5 ONS 2013 report: Results from using routinely-collected\u000a      government information for 2011\u000a      Census quality assurance.\u000a      http:\/\/www.ons.gov.uk\/ons\/guide-method\/census\/2011\/census-data\/2011-census-user-guide\/quality-and-methods\/quality\/quality-assurance\/cqs-report.pdf\u000a    5.6 Head of SDC Research Team, Methodology Directorate, ONS.\u000a    5.7 ONS 2010 Report: Statistical Disclosure Control for the 2011\u000a      UK Census.\u000a      http:\/\/www.ons.gov.uk\/ons\/guide-method\/census\/2011\/the-2011-census\/producing-and-delivering-data\/confidentiality\/statistical-disclosure-control-for-the-2011-uk-census.pdf\u000a    5.8 Member of SDC Research Team, Methodology Directorate, ONS.\u000a    ","Title":"\u000a    10-12 Making It Count: Improving the Census\u000a    ","UKLocation":[{"GeoNamesId":"2637487","Name":"Southampton"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Government agencies and local authorities rely on accurate census data to\u000a      make effective\u000a      planning decisions. The government white paper, `Helping to shape\u000a        tomorrow: the 2011 Census of\u000a        Population and Housing in England and Wales' says that: `The Census\u000a      underpins the allocation of\u000a      billions of pounds in funding for public services, and is the foundation\u000a      of many economic and social\u000a      statistics. These in turn influence policy across government and\u000a      investment decisions in the\u000a      commercial sector.'\u000a    In the 1991 Census, an estimated 3.8% of the UK population was missed,\u000a      causing a significant\u000a      underestimation of local population sizes and provoking considerable\u000a      frustration among users. This\u000a      prompted the Office for National Statistics (ONS) to instigate the One\u000a      Number Census project in\u000a      1997. It asked the University of Southampton to develop new methodologies\u000a      for survey design and\u000a      estimation to assess the coverage of the population census and adjust the\u000a      results accordingly.\u000a    Professor Ian Diamond (1980-2010) led the initial work with Professor Ray\u000a      Chambers (1995-2006)\u000a      and Dr James Brown (PhD student 1996-2000; Lecturer 1999 to 2008;\u000a      Reader\/Professor 2010-2013). Brown examined the design of the 2001 Census Coverage Survey (CCS)\u000a      and the main\u000a      estimation strategy for calculating key demographic characteristics from\u000a      the data. The CCS was an\u000a      independent survey of 320,000 households conducted after the 2001 Census.\u000a      The results of the\u000a      Census were matched against those of the CCS, identifying the number and\u000a      characteristics of\u000a      those missed in the Census. It led to the development of a dual-estimation\u000a      system, which was\u000a      used to amend the census results to reflect those omitted in the original\u000a      count. For areas not\u000a      included in the CCS, Southampton's statistical techniques were applied to\u000a      estimate the undercount\u000a      [3.1, 3.2].\u000a    Chambers led research into producing Local Authority estimates of basic\u000a      demographic data from\u000a      the main CCS estimates. He developed a system where missing records were\u000a      imputed based on\u000a      recorded Census data to produce a complete set of responses for each\u000a      household and person\u000a      rather than just basic demographic characteristics. Chambers' idea was\u000a      tested by Brown, who led\u000a      the modelling with the LSE's Fiona Steele [3.3, 3.4]. The UK is\u000a      unique in having such a database,\u000a      representing the best estimate of what would have been collected had the\u000a      Census not been\u000a      subject to under-enumeration. Brown has gained an international reputation\u000a      for his research in this\u000a      area and has been invited to review methods being developed for censuses\u000a      in Canada, Australia\u000a      and America.\u000a    Equally important as the accuracy of Census data is the ability to make\u000a      available detailed census\u000a      outputs to users, while at the same time protecting the privacy of\u000a      individual citizens. Southampton's\u000a      history of research on statistical disclosure control (SDC) dates back to\u000a      the 1990s with work led by\u000a      Professor Chris Skinner (at Southampton until 2011) to assess disclosure\u000a      risk in Samples of\u000a      Anonymised Records. This resulted in an EU project led by Skinner, which\u000a      developed\u000a      methodologies and software for the assessment of disclosure risk and\u000a      application of SDC methods\u000a      to both microdata and tabular data. Between 2005 and 2007 Dr Natalie\u000a      Shlomo (2007-2012)\u000a      worked on protecting Census tabular data without compromising its\u000a      usability. She developed a\u000a      software package for the ONS for comparing original with protected Census\u000a      tables in order to\u000a      assess optimal SDC methods and parameters [3.5, 3.6]. Further\u000a      development by Shlomo and\u000a      Skinner [3.7] in response to ONS requirements resulted in modified\u000a      SDC assessment methods to\u000a      ensure that statistical outputs provided maximum value to users while\u000a      protecting the confidentiality\u000a      of information concerning individuals.\u000a    "},{"CaseStudyId":"43216","Continent":[{"GeoNamesId":"6255149","Name":"North America"}],"Country":[{"GeoNamesId":"6252001","Name":"United States"}],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Data Assimilation is an essential part of the Met Office forecasting\u000a      procedures [C]. However, a significant problem faced by the Met Office is\u000a      that of assimilating data in the presence of atmospheric inversion layers\u000a      or other fine structures. Misrepresenting these layers in the\u000a      computations, leads to spurious correlations between observed data and the\u000a      underlying physical structures. This has a negative effect on the\u000a      assimilation of data (for example from radiosondes) into the forecast,\u000a      degrading the forecast performance. The nature, and societal impact, of\u000a      this problem is described in the following two quotes from publications\u000a      authored by Met Office personnel.\u000a    A common problem in forecast case-studies is the misrepresentation of\u000a        inversions and stratocumulus layers in the assimilation due to\u000a        inappropriate background error covariances, e.g. smooth and broad\u000a        vertical correlation functions which do not allow accurate fitting of\u000a        high resolution radiosonde soundings. This inhibits the ability to\u000a        diagnose realistic stratocumulus layers and boundary-layer structures\u000a      which then results in poor forecasts. An example of the\u000a        impact of this problem in the Met Office NWP system happened in December\u000a        2006 when poor visibility at Heathrow led to significant\u000a          travel disruption during the Christmas period. In this\u000a        instance radiosonde observations were not able to improve the analysis\u000a        of the inversion and so the fog was not accurately forecast.\u000a      Met Office Publication [B] (Emphasis added by case study author.) Accurate\u000a        representation of the boundary layer in NWP models is important for\u000a        instance in the forecasting of fog or icy roads. Met Office\u000a      Publication [B]\u000a    To address this problem, Budd and his team at Bath have been working with\u000a      the Manager of Data Assimilation R&amp;D at the Met Office and his\u000a      colleagues, to develop cheap and reliable methods to better resolve the\u000a      atmospheric features and increase the accuracy of the data assimilation\u000a      methods. In a series of collaborative (and partly Met Office funded)\u000a      projects [D,E,F], they combined the PMA algorithm, developed at Bath and\u000a      described in [2-6], with the Met Office data assimilation software. This\u000a      work used the adaptive mesh transformations generated by the PMA\u000a      algorithm, to rescale the spatial coordinates used in the data\u000a      assimilation calculation. This rescaling meant that the vertical\u000a      correlations of the background error covariance matrix in the inversion or\u000a      ground boundary layers were much better resolved. In particular this has\u000a      improved the ability of the assimilation system to accurately use\u000a      high-resolution information like radiosonde soundings. A key breakthrough\u000a      in this work was the incorporation of an appropriate monitor function of\u000a      the potential vorticity atmospheric state (as described in [4,5]) into the\u000a      rescaling algorithm. The PMA algorithm has proved especially appropriate,\u000a      flexible and robust for this procedure, and has been particularly suitable\u000a      when dealing with real meteorological data, which can be very noisy.\u000a    The development of adaptive grids based on the parabolic Monge-Ampere\u000a        equation at the University of Bath provides an affordable technique\u000a        which can be tuned to meet the challenges of real data. It works through\u000a        a monitor function, which can be chosen to meet a variety of user\u000a        requirements, and can be smoothed to ensure the resulting grid can be\u000a        safely used in an operational environment. [G] Letter from the\u000a      Manager of Data Assimilation R&amp;D at the Met Office\u000a    Adaptive data assimilation software, based directly on the research at\u000a      Bath described in Section 2, was first incorporated into the operational\u000a      data assimilation code for the Met Office 4km grid UK models in November\u000a      2010 [A,B]. Operational codes make forecasts every six hours [H] and the\u000a      operational codes, incorporating the PMA algorithm, have been used to\u000a      forecast the UK weather for the last three years. More advanced Met Office\u000a      codes based on the results in [6] are being developed and are forming part\u000a      of the ongoing Data Assimilation research at the Met Office.\u000a    The application to data assimilation is particularly suitable for this\u000a      technique, as the resulting grid is only used to define spatial\u000a      correlation structures, not to apply a flow solver. As a result\u000a        this technique has been used in our operational UK analysis for\u000a        the last 3 years and an extended formulation will be\u000a      incorporated for trialling within the next 12 months. [G] (Emphasis\u000a    added by case study author.)\u000a    A direct consequence of this work is an improvement of the Met Office\u000a      forecasting skill in terms of the so-called UK Index (which is a measure\u000a      of the forecasting skill of limited-area NWP models over the UK and is\u000a      based on forecasts of selected parameters and for a selected set of\u000a      positions verified by comparison with available station observations\u000a      across the UK at 3- to 6-hourly intervals).\u000a    ... the adaptive mesh transformation led to positive impact in the\u000a        forecast skill of UK models both in winter and summer. Analysis RMS\u000a        errors are reduced with respect to radiosonde, aircraft, SEVIRI and\u000a        ground GPS observations for both periods. Background RMS errors are\u000a        reduced with respect to aircraft, surface and ground GPS observations\u000a        for both periods and also with respect to radiosonde observations for\u000a        relative humidity in the lower part of the troposphere and for potential\u000a        temperature around the inversions. These results are consistent with the\u000a        change in the monitor function structures coming from the updated\u000a        normalization procedure and recalculation of the adaptive mesh within\u000a        the nonlinear minimization procedure. These led also to improvement of\u000a        the background state in the full cycled analysis\/forecast system and\u000a        therefore to better representation of the vertical structure of the\u000a        boundary layer. For these reasons this new version of the adaptive mesh\u000a        transformation was implemented operationally in the Met Office data\u000a        assimilation system in July 2011 for UKV and UK4 models. Met Office\u000a      Publication [A]\u000a    Obviously, the improvement of the Met Office forecasting skill has\u000a      economic and societal impacts. In particular, the enhanced resolution of\u000a      the ground boundary layer, provided by the PMA algorithm, has led to an\u000a      improvement in the accuracy of the prediction of fog hazards and road\u000a      temperatures. These temperature predictions are used, for example, to\u000a      provide input for the Met Office OpenRoad software [J] that is\u000a      employed to advise local councils on ice hazards and the need (or not) for\u000a      road gritting.\u000a    The improvement of 2m temperature forecasts is relevant for the Met\u000a        Office OpenRoad system which provides 24 hour forecasts of road state to\u000a        companies and local authorities to help maintain essential road\u000a        services, mainly in winter. For this reason, the adaptive mesh transform\u000a        was implemented operationally in two Met Office high-resolution\u000a        limited-area models (UK4 and UK1.5) on 2 November 2010. Met Office\u000a      Publication [B]\u000a    A new method of adapting computational grids to the expected solution\u000a        is now being exploited in the high resolution analyses used to drive the\u000a        short-range forecasts for the UK. Particular benefit is found in\u000a        predicting low-level temperatures, which is very important for\u000a        maintaining the road network in a safe condition and for predicting fog.\u000a      [K] Email from the Manager of Data Assimilation R&amp;D at the Met Office\u000a    In the winter of 2011\/12, the Met Office provided OpenRoad based\u000a      forecasts for over 350 routes in the UK. The use of OpenRoad reduces the\u000a      impact of cold weather on road networks, in particular on road safety,\u000a      and, via more accurate forecasting of road temperatures, leads to a more\u000a      cost-effective use of grit supplies (gritting can cost a council in the\u000a      order of &#163;10k to &#163;15k per day). Moreover, since salt is a corrosive\u000a      substance, avoidance of gritting when it is not necessary, leads to\u000a      savings for road users in general and to a reduction of damage to the\u000a      transport infrastructure in particular. In the USA, it has been estimated\u000a      that the total costs, including these indirect ones, of using salt are\u000a      three times greater than the direct costs. But even this is likely to\u000a      underestimate the total costs, as environmental damage caused by salt run\u000a      off into the ecosystem is not taken into account.\u000a    ","ImpactSummary":"\u000a    Weather impacts all of our lives and we all take a close interest in it,\u000a      with every news report finishing with a weather forecast watched by\u000a      millions. Accurate weather forecasting is essential for the transport,\u000a      agricultural and energy industries and the emergency and defence services.\u000a      The Met Office plays a vital role by making 5-day forecasts, using\u000a      advanced computer algorithms which combine numerical weather predictions\u000a      (NWP) with carefully measured data (a process known as data assimilation).\u000a      However, a major limitation on the accuracy of these forecasts is the sub-\u000a      optimal use of this data. Adaptive methods, developed in a partnership\u000a      between Bath and the Met Office have been employed to make better use of\u000a      the data, thus improving the Met Office operational data assimilation\u000a      system. This has lead to a significant improvement in forecast accuracy as\u000a      measured by the `UK Index' [A] with great societal and economic impact.\u000a      These forecasts, in particular of surface temperatures, are pivotal for\u000a      the OpenRoad forecasting system used by local authorities to plan road\u000a      clearing and gritting when snow or ice are predicted [B].\u000a    ","ImpactType":"Environmental","Institution":"\u000a    University of Bath\u000a    ","Institutions":[{"AlternativeName":"Bath (University of)","InstitutionName":"University of Bath","PeerGroup":"B","Region":"South West","UKPRN":10007850}],"Panel":"B         ","PlaceName":[],"References":"\u000a    References that best indicate the quality of the underpinning research\u000a      are starred.\u000a    \u000a[1]* C J Budd, W Huang &amp; R Russell, Moving mesh methods for problems\u000a      with blow-up, SIAM J. Sci. Comp., 17, (1996), 305-327.\u000a      (This paper is highly cited and was specially noted in the RAE 2001\u000a      assessment report.) http:\/\/dx.doi.org\/10.1137\/S1064827594272025\u000a    \u000a\u000a[2] C J Budd and J F Williams, Parabolic Monge-Ampere methods for blow-up\u000a      problems in several spatial dimensions, J. of Physics A, 39,\u000a      (2006), 5425-5463, doi:10.1088\/0305-4470\/39\/19\/S06\u000a    \u000a\u000a[3] C J Budd and J F Williams, Moving mesh generation using the parabolic\u000a      Monge-Ampere equation, SIAM J. Sci. Comput., 31, (2009),\u000a      3438-3465. http:\/\/dx.doi.org\/10.1137\/080716773\u000a    \u000a\u000a[4]* C J Budd, W-Z Huang and R Russell, Adaptivity with moving grids, Acta\u000a        Numerica, 18, (2009), 1-131.\u000a        http:\/\/dx.doi.org\/10.1017\/S0962492906400015\u000a    \u000a\u000a[5]* C J Budd, M Cullen and E Walsh, Monge-Ampere based moving mesh\u000a      methods for numerical weather prediction, with applications to the Eady\u000a      problem, J. Comp. Phys, 236, (2013), 247-270.\u000a      http:\/\/dx.doi.org\/10.1016\/j.jcp.2012.11.014\u000a    \u000a\u000a[6] P A Browne, C J Budd, C Piccolo and M Cullen, Fast three dimensional\u000a      r-adaptive mesh redistribution, submitted 2013, http:\/\/people.bath.ac.uk\/mascjb\/Papers09\/paperdraft.pdf\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"3","Subject":"Numerical and Computational Mathematics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"},{"Level1":"1","Level2":"1","Subject":"Pure Mathematics"}],"Sources":"\u000a    [A] C Piccolo and M Cullen, A new implementation of the adaptive mesh\u000a      transform in the Met Office 3D-Var System, Q. J. R. Meteorol. Soc,\u000a      138, (2012), pp. 1560-1570. DOI:10.1002\/qj.1880\u000a    [B] C Piccolo and M Cullen, Adaptive mesh method in the Met Office\u000a      variational data assimilation system, Q. J. R. Meteorol. Soc., 137,\u000a      (2011), pp. 631-640. DOI:10.1002\/qj.801\u000a    [C] The importance of data assimilation to the work of the Met Office is\u000a      described in\u000a\u0009  http:\/\/www.metoffice.gov.uk\/research\/news\/ndp-data-assimilation\u000a    [D] 2006-2010 EPSRC CASE Award with the Met Office, &#163;65,000, for Prof C\u000a      Budd to support the work of E Walsh on `Monge-Ampere methods for adaptive\u000a      grid generation'.\u000a    [E] 2010-2011 EPSRC Knowledge Transfer Grant KTA Grant KTA008_Budd for\u000a      Prof C Budd &#163;14,748 (to support E Walsh) on `Adaptive numerical methods\u000a      for weather forecasting'.\u000a    [F] 2012 EPSRC Knowledge Transfer Grant KTA092_Budd with the Met Office\u000a      for &#163;60k, Adaptive mesh data assimilation methods for problems in three\u000a      dimensions (to support P Browne).\u000a    [G] Letter of support from Manager Data Assimilation Research and\u000a      Development at the Met Office, describing the use and importance of the\u000a      Bath PMA algorithm.\u000a    [H] The nature of an operational forecast is described in\u000a\u0009http:\/\/research.metoffice.gov.uk\/research\/nwp\/numerical\/operational\/\u000a    [J] The OpenRoad software is described in: http:\/\/www.metoffice.gov.uk\/roads\/openroad\u000a    [K] Email from Manager Data Assimilation Research and Development at the\u000a      Met Office, describing the impact of adaptive methods on forecasting low\u000a      temperatures and fog.\u000a    ","Title":"\u000a    Improving Met Office weather forecasting accuracy\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research at Bath started as a systematic study of cheap,\u000a      flexible, and robust, adaptive mesh redistribution methods with evolving\u000a      mesh density. These meshes are used in numerical algorithms to compute the\u000a      solutions of evolutionary partial differential equations (PDEs) in several\u000a      spatial dimensions. Such PDEs are typically discretised on a mesh and the\u000a      discrete equations solved numerically. If features of the solution evolve\u000a      on small time or length scales, conventional methods (based on nearly\u000a      uniform meshes) may fail, whereas the adaptively redistributed meshes\u000a      provide accurate robust solutions in a wide range of applications.\u000a    The research conducted by Budd (a Professor at Bath since 1995) has been\u000a      centred on devising methods for moving the mesh so that mesh points are\u000a      concentrated where they are most needed to resolve fine structures, such\u000a      as atmospheric inversion layers, without additional computational cost.\u000a      The advantages of this approach over other approaches are that it is\u000a      computationally simpler and can be readily inserted into legacy software,\u000a      the mesh regularity can be controlled a-priori and it can explicitly\u000a      exploit the structures of the underlying PDE.\u000a    The underpinning research has evolved from theoretical ideas on mesh\u000a      redistribution for PDE solutions, to methods which are now directly used\u000a      in Met Office data assimilation codes. Bath, in collaboration with Simon\u000a      Fraser University (Canada), developed a procedure for moving mesh methods\u000a      in one-dimension that could cope with specific singular PDEs [1]. A major\u000a      advance in the programme at Bath was the extension of mesh redistribution\u000a      methods to two and three dimensions by using ideas from geometry and fluid\u000a      mechanics, facilitating the development of the Parabolic Monge-Ampere\u000a      (PMA) algorithm [2,3]. This method for mesh redistribution combined the\u000a      equidistribution of an appropriate monitor of the solution with optimal\u000a      transport methods and the solution of an associated Monge-Ampere equation.\u000a      The PMA algorithm was first implemented by a PhD student, Williams\u000a      (2000-2004), and proved to be effective on model problems. In a more\u000a      developed form, it was the basis of an invited paper [4], which described\u000a      in detail how the PMA method could either be used to solve PDEs or to\u000a      derive meshes to better represent the fine structure in their solutions.\u000a      This paper was of significant interest to the Met Office as many\u000a      meteorological phenomena occur on small length scales relative to the\u000a      overall scale of the Earth.\u000a    In 2006, an EPSRC\/Met Office CASE student at Bath (Walsh), started a\u000a      programme of research in close collaboration with the Met Office,\u000a      developing the PMA algorithm specifically for meteorological problems.\u000a      The PMA algorithm was applied to improve the numerical prediction of\u000a      severe storms associated with rapid variations in wind speed and wind\u000a      pressure. Intensive research in this context led to the identification of\u000a      appropriate monitors of the atmospheric state which in turn were invoked\u000a      to obtain effective computational meshes [5]. The PMA algorithm, in\u000a      combination with these monitors, was then used to generate new meshes\u000a      which increased the resolution of the atmospheric state close to inversion\u000a      layers, and ground boundary layers, where there are rapid changes in\u000a      temperature.\u000a    Resolving these temperature changes is important for accurate data\u000a      assimilation calculations. The successful use of PMA in this context led\u000a      to its incorporation into Met Office data assimilation software, giving\u000a      much improved resolution of the vertical atmospheric state. This research\u000a      was followed up by work in 2011-2012 by Walsh and a further Bath PDRA\u000a      Browne, funded by EPSRC\/Met Office funded Knowledge Transfer awards,\u000a      leading to the development of PMA for fully three dimensional data\u000a      assimilation calculations. Budd and Browne, in collaboration with the Met\u000a      Office, have in this process developed a fast, general purpose adaptive 3D\u000a      adaptive mesh redistribution algorithm based on PMA [6] which is usable\u000a      for the UK Area weather forecast.\u000a    "},{"CaseStudyId":"43217","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2921044","Name":"Germany"}],"Funders":["Royal Society"],"ImpactDetails":"\u000d\u000a    The main impact-related insights from the research are as\u000d\u000a      follows.\u000d\u000a    \u000d\u000a      The estimated trends provide evidence for increased defoliation of the\u000d\u000a        main species, spruce,\u000d\u000a        between 2004 and 2009, compared to previous years since 1985 (Fig.),\u000d\u000a        with a similar trend for\u000d\u000a        beech, the second most common species [E].\u000d\u000a      Results also suggest that there has been a switch in the primary\u000d\u000a        drivers of damage: initial\u000d\u000a        damage can be associated solely with pollution, while more recent damage\u000d\u000a        is also associated with\u000d\u000a        drought years due to climate change. In addition to pollution, climate\u000d\u000a        change has become the most\u000d\u000a        severe threat to forest health in Europe, as some of the main species no\u000d\u000a        longer thrive in the\u000d\u000a        changed climatic conditions.\u000d\u000a      The spatial trends highlight areas where intervention to improve\u000d\u000a        forest health is required, such as\u000d\u000a        liming to control acidity or planting species which are better suited to\u000d\u000a        the new climate conditions.\u000d\u000a    \u000d\u000a    The research has led to improved forest health monitoring and\u000d\u000a          management, and we give four\u000d\u000a      examples below.\u000d\u000a    (a) Spatio-temporal models developed in Bath are used for\u000d\u000a          monitoring and management. The\u000d\u000a      methodology has been adopted for the yearly official report on forest\u000d\u000a      health of the federal state BW,\u000d\u000a      the 'Waldzustandsbericht\/Report on Forest condition in BW' [A-E], which\u000d\u000a      drives the policy of the\u000d\u000a      Ministry of Rural Affairs and Consumer Protection, BW, Germany. In the\u000d\u000a      foreword of the 2012\u000d\u000a      report, the BW Minister for Rural Affairs and Consumer Protection,\u000d\u000a      Alexander Bonde, refers to the\u000d\u000a      Bath-developed methodology and results [E, pp 3-4].\u000d\u000a    \"The Waldzustandsbericht makes it possible ... [for us] to make\u000d\u000a        management recommendations. ...\u000d\u000a        This is how trends can be recognised and strategies for sustainability\u000d\u000a        of our multi-functional forests\u000d\u000a        can be developed. ... [It] is the basis for counteractive measures\u000d\u000a        including policy on air pollution\u000d\u000a        control, ground liming and the creation of robust mixed forest stands.\u000d\u000a        ... We use modern and\u000d\u000a        scientifically backed methods and instruments of forest ecosystem\u000d\u000a        monitoring to secure\u000d\u000a        sustainability of species diversity [and] wood production as well as the\u000d\u000a        protective and recreational\u000d\u000a        function of our forests, so that we can benefit from it in future.\"\u000d\u000a        (translation by Augustin)\u000d\u000a    (b) Improved sampling scheme in terms of cost-efficiency and\u000d\u000a          precision. Funded by [2, 3],\u000d\u000a      Augustin carried out a simulation study to optimise the sampling scheme\u000d\u000a      based on the spatio-\u000d\u000a      temporal model developed in Bath [1, A, B]. In particular, this work\u000d\u000a      results in a substantial cost\u000d\u000a      saving.\u000d\u000a    \"The spatio-temporal model developed by Dr Augustin in collaboration\u000d\u000a        with the FRI BW and others\u000d\u000a        (see [1]) was crucial in assessing the cost-efficiency of the sampling\u000d\u000a        scheme for monitoring tree\u000d\u000a        health in BW. After about 20 years of changing sampling grids between\u000d\u000a        resolutions 4x4km, 8x8km\u000d\u000a        and 16x16km depending on available survey funds, a sampling grid of\u000d\u000a        8x8km is now used in BW.\u000d\u000a        This change in policy was a direct consequence of a simulation study\u000d\u000a        carried out by Dr Augustin\u000d\u000a        which in turn was based on [1]. This study showed that the much cheaper\u000d\u000a        and EU-wide used\u000d\u000a        16x16km grid resolution was not adequate in terms of detecting changes\u000d\u000a        in defoliation at a relevant\u000d\u000a        magnitude. The results also showed that the 4x4km grid resolution was\u000d\u000a        unnecessarily low for\u000d\u000a        achieving the required precision, and hence wasteful in terms of\u000d\u000a        resources. This resulted in a\u000d\u000a          yearly cost-saving of 100,000 Euros. [Our emphasis.] The\u000d\u000a        simulation study also showed up\u000d\u000a        problem areas with very high prediction errors which could then be dealt\u000d\u000a        with by inserting additional\u000d\u000a        sampling locations.\" [G]\u000d\u000a    (c) Improvement of forest growth conditions by reversion and\u000d\u000a          mitigation of pollution effects\u000d\u000a          on the soil through a forest liming programme. Spatial trend\u000d\u000a      estimates of defoliation produced\u000d\u000a      in Bath [A, B] were used by forest health managers to establish the need\u000d\u000a      and planning of a forest\u000d\u000a      liming programme in BW [C, F p.23].\u000d\u000a    \"Moreover the reduction of sampling density in the crown condition\u000d\u000a        survey enabled direct linking of\u000d\u000a        the survey schemes on crown condition, forest nutrition and soil\u000d\u000a        condition. Thus synoptic\u000d\u000a        evaluation of data from these diverse monitoring schemes have been\u000d\u000a        enabled [H] which are a\u000d\u000a        sound basis for practical countermeasures of harmful effects of\u000d\u000a        environmental change, like e.g.\u000d\u000a        increasing deficiency in forest nutrition caused by acid rain and\u000d\u000a        subsequent soil acidification. A\u000d\u000a        central strategy for mitigating the effects of acidification is soil\u000d\u000a        protective liming.\u000d\u000a    \u000d\u000a      Since 2006 Dr Augustin has produced spatio-temporal trend estimates of\u000d\u000a        defoliation based\u000d\u000a        on [1] for the yearly Report on Forest Condition \/ Waldzustandsbericht\u000d\u000a        published by the Forstliche\u000d\u000a        Versuchs- und Forschungsanstalt BW (FVA), Freiburg available at\u000d\u000a        http:\/\/www.fva-bw.de\/ ). The\u000d\u000a        spatial trends she produced in 2008 and 2009 [A, B] established the need\u000d\u000a        for a forest liming\u000d\u000a        programme, and were used by the FVA in its plans for such a programme in\u000d\u000a        Baden-W&#252;rttemberg\u000d\u000a        [C, F].\u000d\u000a    \u000d\u000a    The aim of the forest liming programme is in the long term not only to\u000d\u000a        reverse the acidification\u000d\u000a        of the soil, but also to reduce the acid stored in the soil. The liming\u000d\u000a        programme entails on average\u000d\u000a        a yearly liming of 15,000 ha with a dosage of 3t\/ha. This is altogether\u000d\u000a        45,000 t of lime which is\u000d\u000a        applied by turbine blowers from tractors or helicopters. In order to\u000d\u000a        reverse the effects of\u000d\u000a        acidification in the soil caused by pollution and improve growth\u000d\u000a        condition a long-term yearly liming\u000d\u000a        programme over the next 30 years is envisaged. Without\u000d\u000a          Augustin's work [1] it would have\u000d\u000a          been more difficult to convince the state government to fund the\u000d\u000a          programme, and we would\u000d\u000a          have much less confidence in a positive outcome.\" [G]. [Our\u000d\u000a      emphasis.]\u000d\u000a    (d) Adaptation to climate change. \"The spatio-temporal\u000d\u000a        trends of tree health produced in Bath\u000d\u000a        highlighted particular areas where tree health is deteriorating due to a\u000d\u000a        combination of climate\u000d\u000a        change and soil condition. This contributed to a change in forest\u000d\u000a        management to plant better suited\u000d\u000a        trees and robust mixed stands. The spatio-temporal model of Augustin et\u000d\u000a        al. [1] on crown condition\u000d\u000a        allowed for proper identification of the spatial \"hotspots\" of crown\u000d\u000a        damages and revealed that they\u000d\u000a        changed from areas with soils, most susceptible to acidification (e.g.\u000d\u000a        Black Forest) to lowland areas\u000d\u000a        which are most susceptible for drought. This new spatial pattern of\u000d\u000a        severe crown damages is\u000d\u000a        observed since the year 2000 and is a strong indication for new, climate\u000d\u000a        change-related drought\u000d\u000a        stress.\" [G]\u000d\u000a    The research contributes to the conservation of the natural forest\u000d\u000a      environment in BW. Forest\u000d\u000a      managers, policy makers and members of the public will therefore benefit\u000d\u000a      from securing the\u000d\u000a      economic production, recreational opportunities, ecological features and\u000d\u000a      ecosystem services of the\u000d\u000a      forest, including its biodiversity and its role as a sink for atmospheric\u000d\u000a      carbon dioxide.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Forests are economically, recreationally and ecologically important,\u000d\u000a      providing timber and wildlife\u000d\u000a      habitat and acting as a carbon sink, among many ecosystem services. They\u000d\u000a      are therefore\u000d\u000a      extremely valuable to society, and it is crucial to ensure that they\u000d\u000a      remain healthy. A statistical\u000d\u000a      model has been developed in Bath to estimate spatio-temporal trends of\u000d\u000a      forest health from\u000d\u000a      monitoring data. This work has led directly to more efficient data\u000d\u000a      collection, and to new and\u000d\u000a      improved interventions to mitigate the impact of pollution and climate\u000d\u000a      change (such as soil liming to\u000d\u000a      control acidity and reforestation regimes). The methodology has been\u000d\u000a      adopted for official reporting\u000d\u000a      in the yearly 'Waldzustandsbericht (Report on Forest Condition)'\u000d\u000a      [A-E] of the German state\u000d\u000a      Baden-Wuerttemberg (BW), which is 39% forested, an area of 14,000km2\u000d\u000a      with an estimated stock\u000d\u000a      value of 17 billion US$.\u000d\u000a    ","ImpactType":"Environmental","Institution":"\u000d\u000a    University of Bath\u000d\u000a    ","Institutions":[{"AlternativeName":"Bath (University of)","InstitutionName":"University of Bath","PeerGroup":"B","Region":"South West","UKPRN":10007850}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2925177","Name":"Freiburg"}],"References":"\u000d\u000a    References that best indicate the quality of the underpinning research\u000d\u000a      are starred.\u000d\u000a    \u000a[1]* Augustin, N.H., Musio, M., von Wilpert, K., Kublin, E., Wood,\u000d\u000a      S. and Schumacher, M. 2009.\u000d\u000a      Modelling spatio-temporal trends of forest health monitoring data. The\u000d\u000a      Journal of the American\u000d\u000a      Statistical Association. 104(487): 899-911. DOI:\u000d\u000a      10.1198\/jasa.2009.ap07058.\u000d\u000a    \u000a[2] Augustin (PI). 'Modelling Forest Inventory Data' , Forestry\u000d\u000a      Research Institute Baden-\u000d\u000a      Wuerttemberg. 01\/09\/05 - 31\/07\/08 &#163;52 754. Grant.\u000d\u000a    [3] Augustin (PI). 'Spatio-temporal modelling of tree defoliation\u000d\u000a      monitoring data', The Royal\u000d\u000a      Society. 01\/04\/06 - 31\/05\/09. &#163;61 237. Grant.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000d\u000a    [A-E] Report on Forest Condition \/ Waldzustandsbericht, Forstliche\u000d\u000a      Versuchs- und\u000d\u000a      Forschungsanstalt Baden-Wuerttemberg, Freiburg. 2008-2012.\u000d\u000a      http:\/\/www.fva-bw.de\/publikationen\/index3.html\u000d\u000a        .\u000d\u000a    [F] von Wilpert, K. Schaeffer, J. Holzmann, S., Hug, R. Meining,\u000d\u000a      S, Zirlewagen, D. and Augustin,\u000d\u000a        N. 2010. Ableitung eines langfristigen Kalkungsprogramms. Was\u000d\u000a      Waldzustandserfassung und\u000d\u000a      Forstliche Umweltueberwachung bewirkt haben. AFZ- Der Wald. pp. 20-25.\u000d\u000a      Heft 3\/2010.\u000d\u000a      http:\/\/ww.afz-derwald.de.\u000d\u000a    [G] Head of Department for Soil and Environment. FRI, BW.\u000d\u000a      Supporting Letter.\u000d\u000a    [H] Musio, M., v. Wilpert, K., Augustin, N. (2007): Crown\u000d\u000a      condition as a function of soil, site and tree\u000d\u000a      characteristics. European Journal of Forest Research. 126\/1, 91-100,\u000d\u000a      doi:10.1007\/s10342-006-\u000d\u000a      0132-8.\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Monitoring and management of German forest\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    The statistical model [1] is a result of an on-going collaboration\u000d\u000a      between Augustin (Senior Lecturer\u000d\u000a      at Bath, appointed as a Lecturer in 2005) and the Forest Research\u000d\u000a      Institute (FRI), BW, Germany,\u000d\u000a      part of the Ministry of Rural Affairs and Consumer Protection of BW,\u000d\u000a      together with Wood (Professor at\u000d\u000a      Bath since 2006). Augustin had previously collaborated on research with\u000d\u000a      the FRI while working in\u000d\u000a      Germany, but the Bath grants and consultancy agreements from the FRI [2]\u000d\u000a      and the Royal Society\u000d\u000a      [3] led directly to her new method for the spatio-temporal estimation of\u000d\u000a      tree health [1]. Data on\u000d\u000a      forest health from other German states are collected differently; the BW\u000d\u000a      data are the most dense in\u000d\u000a      terms of spatio-temporal resolution, and are therefore the most suitable\u000d\u000a      for method development.\u000d\u000a    \u000d\u000a    \u000d\u000a    \u000d\u000a   Figure Temporal (left) and spatial (right) estimates of\u000d\u000a      spruce tree health, indicated by defoliation in\u000d\u000a      the crown. Left: temporal trend estimates from the traditional\u000d\u000a        method are the red triangles with\u000d\u000a      95% confidence intervals; temporal trend estimates from the new method\u000d\u000a      with age as observed and\u000d\u000a      standardised for age are the black circles and the blue crosses\u000d\u000a      respectively, both with Bayesian\u000d\u000a      95% confidence bands. Right: spatial maps for spruce of median\u000d\u000a      age, unobtainable with the\u000d\u000a      traditional method. The black dots indicate the sampling locations, the\u000d\u000a      red isolines indicate the level\u000d\u000a      of defoliation in the crown (yellow is high defoliation of at least 30%,\u000d\u000a      green is medium defoliation at\u000d\u000a      25%).\u000d\u000a   \u000d\u000a   The traditional method (Fig. left, red triangles) estimates mean\u000d\u000a      defoliation separately for each\u000d\u000a      year using summary statistics from the monitoring surveys. The method\u000d\u000a      delivers temporal but not\u000d\u000a      spatial trend estimates, and the attached confidence intervals do not take\u000d\u000a      the spatio-temporal\u000d\u000a      correlation and age structure of the data into account. It cannot be used\u000d\u000a      for inference, since\u000d\u000a      ignoring correlation in the data may result in biased trend and variance\u000d\u000a      estimates.\u000d\u000a    The new method uses the framework of generalized additive mixed\u000d\u000a      models (GAMMs). Augustin,\u000d\u000a      Wood and collaborators developed a new modelling approach that accounts\u000d\u000a      for possible spatial\u000d\u000a      and temporal correlation and incorporates important predictors [1] while\u000d\u000a      being computationally\u000d\u000a      efficient in parameter estimation. The model for the response mean tree\u000d\u000a      defoliation was developed\u000d\u000a      using the moderately large annual BW monitoring data, which were sampled\u000d\u000a      on a grid with yearly\u000d\u000a      varying resolutions of 4X4 km, 8X8 km and 16X16 km. Hence each year from\u000d\u000a      1983 to 2012, a\u000d\u000a      subset of the 1910 unique grid locations were chosen, and 24 trees were\u000d\u000a      sampled at each, giving in\u000d\u000a      all 180,000 observations of individual tree crown defoliation. The model\u000d\u000a      included space-time\u000d\u000a      interactions, as the temporal trend of defoliation differs between areas\u000d\u000a      with different characteristics\u000d\u000a      and pollution levels. It also included a non-linear effect of mean tree\u000d\u000a      age, the most important\u000d\u000a      predictor variable, allowing the separation of trends in time, which may\u000d\u000a      be pollution-related, from\u000d\u000a      trends that relate purely to the ageing of the survey population, as shown\u000d\u000a      in the left panel of the\u000d\u000a      Figure.\u000d\u000a    GAMMs allow for non-linear effects of explanatory variables, random\u000d\u000a      effects, general correlation\u000d\u000a      structures for cases where the data are not independent and any response\u000d\u000a      with a distribution from\u000d\u000a      the exponential family. The novel aspect of [1] is a scale-invariant\u000d\u000a      tensor-product smooth of the\u000d\u000a      space-time dimension, which improves on existing methodology for\u000d\u000a      estimating spatio-temporal\u000d\u000a      trends. These tensor-product smooths allow combinations of different basis\u000d\u000a      functions most suitable\u000d\u000a      for the dimensions of space and time as well as time-varying spatial\u000d\u000a      estimates. Hence the\u000d\u000a      smoothness parameters and penalties can be separate for time and space,\u000d\u000a      avoiding the need to\u000d\u000a      make arbitrary choices about the relative scaling of space and time. In\u000d\u000a      addition to a temporal trend\u000d\u000a      due to site characteristics and other conditions modelled with the\u000d\u000a      space-time smooth, random\u000d\u000a      temporal correlation at site level is accounted for by an auto-regressive\u000d\u000a      moving average (ARMA)\u000d\u000a      process. The method does not rely on a regular grid and allows\u000d\u000a      incorporation of a wide range of\u000d\u000a      correlation structures. The model can incorporate non-linear effects, e.g.\u000d\u000a      for mean tree age, the\u000d\u000a      most important predictor. It provides predicted spatial maps of\u000d\u000a      defoliation and marginal estimates of\u000d\u000a      average defoliation over time with Bayesian confidence bands (Fig.), hence\u000d\u000a      allowing estimation of\u000d\u000a      trends in forest health and identifying possible causes of health\u000d\u000a      deterioration, so that rapid or\u000d\u000a      unusual change, in particular, can be detected as early as possible.\u000d\u000a    Communication of results and future development.\u000d\u000a      Spatio-temporal trends based on [1] are\u000d\u000a      communicated to policy makers and forest managers via the\u000d\u000a      Waldzustandsbericht BW. Since 2011\u000d\u000a      Augustin has been named as a coauthor of these yearly reports [A-E], but\u000d\u000a      she has made\u000d\u000a      substantial contributions since 2006. She has also co-authored an article\u000d\u000a      in a forest trade journal\u000d\u000a      (AFZ &#8212; Der Wald) on the impact of forest ecosystem monitoring,\u000d\u000a      recommending the introduction of a\u000d\u000a      permanent liming campaign as a tool to mitigate effects of pollution [F].\u000d\u000a      The main output [1] has\u000d\u000a      been cited in several articles of AFZ-Der Wald. The citations of [1] show\u000d\u000a      that the methods are\u000d\u000a      widely used, with applications in fisheries, epidemiology and ecosystem\u000d\u000a      monitoring through satellite\u000d\u000a      images. Software implementing the methods of [1] has been made available\u000d\u000a      to scientists at other\u000d\u000a      forestry institutes in Germany on request. Augustin has a PhD student\u000d\u000a      working on spatio-temporal\u000d\u000a      modelling of the entire European forest health monitoring data from the\u000d\u000a      UNECE International\u000d\u000a      Cooperative Programme on Assessment and Monitoring of Air Pollution\u000d\u000a      Effects on Forests\u000d\u000a      (http:\/\/icp-forests.net).\u000d\u000a    "},{"CaseStudyId":"43218","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3017382","Name":"France"}],"Funders":[],"ImpactDetails":"\u000a    The main contribution of the research is to provide numerical-statistical\u000a      methods that make the practical use of generalized additive models as\u000a      reliable and routine as the use of generalized linear models has long\u000a      been, ensuring wide uptake of the methods beyond academic research. The\u000a      quality of the methods produced by the research has been recognised by the\u000a      R core team's inclusion of the Bath produced software \"mgcv\" as one of\u000a      only a dozen recommended packages (out of thousands) supplied with all\u000a      versions of the R statistical computing environment.\u000a    R is an open source statistical package\/environment that is widely used\u000a      both in academia and beyond [A]. While the wide academic uptake of R is\u000a      doubtless driven partly by the fact that it is free, this is less likely\u000a      as a primary driver of business uptake, where reliability and flexibility\u000a      are overriding concerns. mgcv and its underpinning research are part of\u000a      providing this reliability and flexibility.\u000a    Example 1. The electricity company EDF produces 22% of Europe's\u000a      electricity consumption, generating 652 TWh per year (at a wholesale price\u000a      of around &#163;40 per MWh). It is the dominant electricity producer in France,\u000a      where it is also the monopoly distributor. French grid load varies between\u000a      30 and 80 GW, and with daily energy flows of around 1 Million MWh,\u000a      accurate load prediction is critical to the success of EDF as a company.\u000a      Moreover, as the dominant producer, substantial wider social benefits\u000a      accrue to client countries through the provision of a reliable electricity\u000a      supply [C].\u000a    Load prediction is particularly important for EDF because it generates\u000a      77% of its electricity from nuclear power plants, which cannot respond\u000a      rapidly to unforeseen demand. Under-prediction of load leads either to\u000a      supply failure, or to EDF having to buy in energy at premium prices.\u000a      Over-prediction leads to unnecessary production and business\u000a      inefficiencies. The cost to EDF of over-production by 1% for a single day\u000a      is around &#163;0.5M [C].\u000a    EDF have developed methods for electricity grid load forecasting based on\u000a      the mgcv software and its underlying methods. EDF's use of GAMs has been\u000a      built directly on collaboration with Bath on large dataset and\u000a      autocorrelation issues [D]. The EDF work is, in particular, reliant on the\u000a      high degree of numerical reliability provided by the methods developed in\u000a      Bath [1, 2], on the ability to handle large datasets [D], and on the\u000a      handling of interaction terms [3].\u000a    An EDF representative [E] states that the Bath mgcv work \"has had a\u000a        number of concrete and important impacts on our work at EDF... These are\u000a        commercially important for EDF, both in terms of complying with the\u000a        requirements of the national grid management bodies, and of matching\u000a        supply to demand in an economically and environmentally efficient\u000a        manner\". He goes on to list several specific areas:\u000a    \u000a      \"The methods encoded in mgcv are used to discover and investigate\u000a          new effects... A number of such effects have subsequently been\u000a          incorporated in the parametric models currently used for operational\u000a          forecasting. \u000a\u000a      The methods have been successfully employed in pilot studies on EDF\u000a          subsidiary companies, and are currently being further developed for\u000a          operational forecasting purposes for these companies.\u000a      The methods have been used operationally on the French national grid\u000a          as a tool to help operators when special meteorological events happen\u000a          (extreme absolute temperatures or rapid temperature variations, for\u000a          example). In these cases the GAM based models capture the electricity\u000a          grid load dynamics better than the current operational models, and are\u000a        used to correct the operational models.\u000a        EDF uses the methods for forecasting of heat demand for cogeneration\u000a          plants where it achieves a 20% gain over the existing methods.\u000a        \u000aEDF leads some important research projects around [the methods].\u000a          Among them ... collaborations with IBM to implement GAM for massive\u000a          simulation and online forecasting.\" [E]\u000a    \u000a    Example 2. The methods are widely used in fisheries where they\u000a      contribute to policy decisions about quota setting, as the following\u000a      examples illustrate. The enhanced reliability offered by the methods\u000a      allows CSIRO Tasmania to use GAMs to analyse and design their fisheries\u000a      independent survey programme, which helps to improve management of the\u000a      south east of Australian fisheries (estimated annual value AU$700M,\u000a      2005\/6) [F]. Similarly, models based on [2] and [5] above have been used\u000a      as part of IFREMER's (French Research Institute for Exploration of the\u000a      Sea, wwz.ifremer.fr) input to quota setting for the Blue Ling Fishery [G].\u000a      The methods have also been used to develop models for catch per unit\u000a      effort standardization in deep sea fisheries which in turn inform the\u000a      policy and management advice of ICES (International Council for the\u000a      Exploration of the Sea), which is used by the EU for quota setting and\u000a      other management [H]. A separate use of the methods developed models for\u000a      fish stock indicator indices used by the EU for stock management\u000a      assessment [J]. To illustrate the breadth of extra academic impact within\u000a      fisheries, of the 689 publications citing Bath mgcv related papers on\u000a      Google Scholar from fisheries, 77% had at least one author with an address\u000a      outside higher education and on average each publication had 1.3 such\u000a      addresses [K]. The project has been sufficiently successful in making GAM\u000a      use statistically routine, that many fisheries uses result in no citation\u000a      [L]. It is the numerical reliability combined with sound smoothness\u000a      selection methods that has changed practice among many fisheries\u000a      statisticians involved in assessment so that they now use the Bath\/mgcv\u000a      based methods.\u000a    The success of the methods means that they have become part of the\u000a      `statistical infrastructure', and in combination with their availability\u000a      as free software, this complicates the process of gathering direct\u000a      evidence of extra-academic reach. However, indirect evidence is obtainable\u000a      [K]. By September 2013, there were over 3200 citations to Bath authored\u000a      mgcv related publications (i.e. to publications from 2006 onwards) on\u000a      Google Scholar. Approximately 55% of these have at least one author with a\u000a      non-academic address and the average number of non-academic author\u000a      addresses per paper is about 0.9. A substantial proportion of these were\u000a      government institutes charged with natural resource management (fisheries,\u000a      forestry, agriculture), but there were also private companies, health\u000a      charities and bodies, Government bodies (e.g. Deutsch Bundesbank) and\u000a      international bodies (e.g. WHO and UNESCO). Notable topics were fisheries\u000a      (689), air pollution (425), medicine (730) and energy (391). Further\u000a      evidence of the extra academic impact of the work is that SAS, the\u000a      major commercial provider of statistical software to industry are\u000a      currently implementing GAM functionality into their software, based on the\u000a      Bath work [M], while private statistics companies run courses in which\u000a      mgcv is a major component [N].\u000a    ","ImpactSummary":"\u000a    A generalized additive model (GAM) explores the extent to which a single\u000a      output variable of a complex system in a noisy environment can be\u000a      described by a sum of smooth functions of several input variables.\u000a    Bath research has substantially improved the estimation and formulation\u000a      of GAMs and hence\u000a    \u000a      driven the wide uptake, outside academia, of generalized additive\u000a        models,\u000a      increased the scope of applicability of these models.\u000a    \u000a    This improved statistical infrastructure has resulted in improved data\u000a      analysis by practitioners in fields such as natural resource management,\u000a      energy load prediction, environmental impact assessment, climate policy,\u000a      epidemiology, finance and economics. In REF impact terms, such changes in\u000a      practice by practitioners leads ultimately to direct economic and societal\u000a      benefits, health benefits and policy changes. Below, these impacts are\u000a      illustrated via two specific examples: (1) use of the methods by the\u000a      energy company EDF for electricity load forecasting and (2) their use in\u000a      environmental management. The statistical methods are implemented in R\u000a      via the software package mgcv, largely written at Bath. As a `recommended'\u000a      R package mgcv has also contributed to the global growth of R,\u000a      which currently has an estimated 1.2M business users worldwide [A].\u000a    ","ImpactType":"Economic","Institution":"\u000a    University of Bath\u000a    ","Institutions":[{"AlternativeName":"Bath (University of)","InstitutionName":"University of Bath","PeerGroup":"B","Region":"South West","UKPRN":10007850}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2147291","Name":"State of Tasmania"}],"References":"\u000a    References that best indicate the quality of the underpinning research\u000a      are starred.\u000a    \u000a[1]* S.N. Wood 2008 Fast stable direct fitting and smoothness selection\u000a      for Generalized Additive Models. J. Roy. Stat. Soc. B 70(3),\u000a      495-518. http:\/\/dx.doi.org\/10.1007\/s11222-012-9314-z\u000a    \u000a\u000a[2]* S.N. Wood 2011 Fast stable restricted maximum likelihood and\u000a      marginal likelihood estimation of semiparametric generalized linear\u000a      models. J. Roy. Stat. Soc. B, 73(1), 3-36.\u000a      http:\/\/dx.doi.org\/10.1007\/s11222-012-9314-z\u000a    \u000a\u000a[3] S.N. Wood 2006 Low-rank scale-invariant tensor product smooths for\u000a      generalized additive mixed models. Biometrics 62, 1025-1036. http:\/\/dx.doi.org\/10.1007\/s11222-012-9314-z\u000a    \u000a\u000a[4] S.N. Wood, F. Scheipl and J.J. Faraway 2013 Straightforward\u000a      intermediate rank tensor product smoothing in mixed models. Statistics\u000a        and Computing. 23:341-360.\u000a      http:\/\/dx.doi.org\/10.1007\/s11222-012-9314-z\u000a    \u000a\u000a[5] S.N. Wood, M.V. Bravington and S.L. Hedley 2008 Soap film smoothing.\u000a      J. Roy. Stat. Soc. B, 70(5), 931-955. http:\/\/dx.doi.org\/10.1007\/s11222-012-9314-z\u000a    \u000a\u000a[6]* S.N. Wood 2006 Generalized Additive Models: An introduction with R.\u000a      CRC Press. e.g. http:\/\/reseau-mexico.fr\/sites\/reseau-mexico.fr\/files\/igam.pdf\u000a    \u000a\u000a[7] http:\/\/cran.r-project.org\/web\/packages\/mgcv\/index.html\u000a      (software package, also available in any version of R, by typing\u000a      library(mgcv)).\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"},{"Level1":"1","Level2":"2","Subject":"Applied Mathematics"}],"Sources":"\u000a    [A] http:\/\/prezi.com\/s1qrgfm9ko4i\/the-r-ecosystem\/\u000a      based on data from Revolution Analytics (http:\/\/www.revolutionanalytics.com\/)\u000a      puts the number of R business users at &gt;1.2M (c.f. ~1M academic).\u000a      However the figures are very uncertain. A Jan 2009 New York Times online\u000a      article (http:\/\/bits.blogs.nytimes.com\/2009\/01\/08\/r-you-ready-for-r\/)\u000a      puts the user figure at 250,000 - 1M. Various other data are at: http:\/\/r4stats.com\/articles\/popularity\/\u000a    [B] In 2002, problems with an earlier GAM function in S-plus, in the\u000a      context of air pollution modelling, were reported in the New York Times\u000a      http:\/\/www.nytimes.com\/2002\/06\/05\/us\/data-revised-on-soot-in-air-and-deaths.html\u000a    [C] http:\/\/about-us.edf.com\/about-us-43666.html\u000a      provides EDF information.\u000a    [D] Wood, Goude and Shaw \"Generalized Additive Models for Large\u000a      Datasets\", accepted subject to minor corrections for Applied\u000a        Statistics (JRSSC). Describes collaboration with EDF on grid load\u000a      problem\u000a    [E] Letter from EDF Research Engineer on the use of mgcv methods at EDF.\u000a    [F] Peel, Bravington, Kelly, Wood and Knuckey (2012) A Model-Based\u000a      Approach to Designing a Fishery-Independent Survey. Journal of\u000a        Agricultural, Biological and Environmental Statistics 18(1):1-21\u000a      describes application of GAM modelling in design of a fisheries survey for\u000a      management. http:\/\/dx.doi.org\/10.1007\/s13253-012-0114-x\u000a    [G] Email from scientist at IFREMER, describing use of Blue Ling work and\u000a      referring to official ICES blue ling assessment document (section 3.6 Data\u000a      analysis: space-time modelling).\u000a    [H] Email from same scientist at IFREMER describing CPUE work, with\u000a      supporting documents.\u000a    [J] Email from another IFREMER scientist describing stock indicator work,\u000a      with supporting documents.\u000a    [K] Report on impact of mgcv project beyond the higher education\u000a        sector. University of Bath, internal report.\u000a    [L] Example: the FAO Tuna working group papers include a paper by Haritz\u000a      Arrizabalaga (2009) on \"Estimation of Tuna fishing capacity from stock\u000a      assessment related information\" (http:\/\/www.fao.org\/docrep\/012\/i1212e\/i1212e.pdf).\u000a      This makes considerable use of mgcv based GAMS, but you can only tell this\u000a      by looking up the citation to Retrepos, V.R. (2007) \"Estimates of large\u000a      scale purse seine...\", (FAO Fish. Proceedings 8:51-62), which also\u000a      contains no citation, but contains figures clearly plotted from mgcv,\u000a      making it clear that it is the basis of the GAM analysis.\u000a    [M] Email from SAS.\u000a    [N] For example http:\/\/www.highstat.com\/.\u000a    ","Title":"\u000a    Improving data analysis via better statistical infrastructure\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The underpinning research was undertaken by Simon Wood (Professor at Bath\u000a      since January 2006). The aim of the research programme is to make the use\u000a      of generalized additive models as reliable and routine as the use\u000a      of generalized linear models has long been, in order that these\u000a      flexible statistical models can routinely be used beyond academic\u000a      statistics.\u000a    A generalized additive model is a regression model that relates a\u000a      univariate random response variable to one or more predictor variables. A\u000a      key feature is that the response depends on a sum of smooth functions of\u000a      the predictor variables. These functions must be estimated from the data.\u000a      The flexibility to specify models in terms of unknown functions is useful\u000a      in fields as diverse as fisheries science and finance, but the additional\u000a      flexibility comes at the cost of decreased numerical stability and the\u000a      need to estimate the degree of smoothness of the functions.\u000a    The primary contributions of the research programme undertaken at Bath\u000a      are:\u000a    \u000a      \u000aReliable and efficient computational methods. The major problem\u000a        is simultaneously to estimate several smoothing parameters in a\u000a        computationally efficient and robust way [1, 2]. We have developed a\u000a        numerical scheme for this for which convergence is guaranteed, provided\u000a        that the GAM penalized likelihood has a well-defined optimum. Before the\u000a        development of this method, GAM estimation methods did not always\u000a        converge [B]. Before mgcv, the only software that estimated GAM\u000a        smoothing parameters had O(n3)\u000a        computational cost, limiting its usefulness. With mgcv the cost is about\u000a        O(n13\/9).\u000a      \u000aImproved means of smoothing with respect to several variables.\u000a        Smooth interactions are best represented using tensor product smooth\u000a        constructions. [3] shows how this can be done in general, while\u000a        maintaining the important property of scale invariance (the results\u000a        should not depend on the units of measurement, for example). The\u000a        generality was important as it allowed, for the first time, the routine\u000a        construction of space-time smoothers for large datasets. [4] provides an\u000a        alternative general construction which has the advantage of being usable\u000a        as a component of any generalized linear mixed model, and also of being\u000a        quite natural when ANOVA decompositions of functions are of interest.\u000a        Moving to spatial smoothing, [5] uses the physical analogy of a\u000a        distorted soap film, and some PDE theory, to construct a novel method\u000a        for smoothing within finite geographic areas, without smoothing across\u000a        boundary features. The resulting smoothers have a form that allows their\u000a        full integration into GAMs.\u000a      \u000aA monograph on GAMs. This helps statisticians beyond HE to use\u000a        the methods [6].\u000a      \u000aHigh quality software implementing the methods. The Bath\u000a        written mgcv package [7] is supplied with the R statistical\u000a        programme as the default method for generalized additive modelling.\u000a    \u000a    "},{"CaseStudyId":"43219","Continent":[],"Country":[],"Funders":[],"ImpactDetails":"\u000a    (i) Impact of core theory and methodology on clinical trial\u000a          practice\u000a    As outlined in Section 2, the theory in [1] and its application in [2]\u000a      are fundamental to a unified treatment of stopping rules for the\u000a      termination of a clinical trial. Early stopping in favour of a new therapy\u000a      benefits patients beyond the trial by making an effective new\u000a      treatment available sooner and increases the financial return to the\u000a        manufacturer. Halting a trial early when a new treatment performs\u000a      poorly releases resources for studies of other promising\u000a      therapies.\u000a    The unified theory in [1] and [2] has found wide applicability and met\u000a      practical needs. The book [2] also synthesises a large volume of research\u000a      into a form accessible to statisticians responsible for the design of\u000a      clinical trials in the pharmaceutical industry. It has sold 3,669 copies,\u000a      had over 2,100 chapter downloads (A), been translated into Japanese [2],\u000a      and become the standard reference work for the industry. Google Scholar\u000a      shows that since 2008, over 40 medical journal publications have cited [2]\u000a      as the source of methodology used in specific clinical trials: the\u000a      outcomes of these trials have affected treatment of patients in the REF\u000a      period (with more rapid effect when early stopping occurred); in later\u000a      examples, the trials were conducted wholly in the REF period.\u000a    We now give some specific examples of clinical trials. Publications (B)\u000a      and (C) report trials that relied on [1] to design a group sequential\u000a      trial with special types of response. In (B), analysis of covariance is\u000a      used to adjust for baseline variables while (C) has a survival response.\u000a      The report in publication (D) is of a trial using the methodology in\u000a      Chapter 7 of [2] and the article states explicitly that \"Jennison and\u000a        Turnbull's Rho stopping boundary (03c1 = 3.0) was used\".\u000a    Economic benefit accrues also to producers of statistical software.\u000a      Cytel's East software for the design and analysis of sequential\u000a      trials draws on Jennison's work in its implementation of group sequential\u000a      designs (E). In his letter (F), the president of Cytel states\u000a    \"We have annual revenues of about $27,000,000. Our flagship software\u000a        package East&#169; is used by almost all major\u000a        pharmaceutical companies (e.g., GSK, Novartis, Pfizer, Merck, Amgen,\u000a        Lilly, Genentech), numerous smaller pharma and biotech companies ... and\u000a        governmental agencies (e.g., FDA, NIH). A heavily used module in East is\u000a        the \"Survival Module\" for the design and interim monitoring of trials\u000a        with mortality endpoints ... The statistical methodology that we have\u000a        implemented in East for such trials relies on the theory that was\u000a        published by Jennison and Turnbull (JASA, 1997). This seminal paper has\u000a        had a huge impact on clinical trials and has facilitated the use of\u000a        group sequential and adaptive methods that can save patient resources\u000a        and bring new drugs to market faster. It is fair to say that many\u000a        companies have purchased East almost entirely because of its Survival\u000a        Module. The reason that the methodology developed by Jennison and\u000a        Turnbull (JASA, 1997) has been so influential is that it provides a\u000a        unified group sequential theory that covers normal, binomial and\u000a        survival distributions, with or without covariates.\"\u000a    (ii) Impact of research into optimal group sequential and adaptive\u000a          design on FDA policy Research at Bath has shaped the policy of the\u000a        US Food and Drug Administration (FDA). Jennison's results on the\u000a      optimisation of clinical trial designs for particular objectives have\u000a      informed regulators who make policy for pharmaceutical companies to\u000a      follow. The \"Guidance for Industry\" of February 2010 (G) describes FDA\u000a        policy on adaptive designs in Phase III trials. This document cites\u000a      [2], [4] and two further articles by Jennison which draw on results in [3]\u000a      and [5]. The conclusions in [3] and [5] are re-iterated in the statement\u000a      (G, Section VI C):\u000a    \" ... one adaptive design approach is to allow an increase in the\u000a        initially planned study sample size based on knowledge of the unblinded\u000a        treatment-effect sizes at an interim stage ... In general, using this\u000a        approach late in the study is not advisable ... The potential to\u000a        decrease the sample size is best achieved through group sequential\u000a        designs with well-understood alpha spending rules ... .\"\u000a    The Associate Director for Adaptive Design and Pharmacogenomics in the\u000a      FDA's Office of Biostatistics, writes (H)\u000a    \"Jennison's work has been invaluable in providing benchmarks by which\u000a        to judge group sequential designs, in appraising the benefits of novel\u000a        proposals for adaptive designs, and in extending adaptive methods to\u000a        overcome impediments to their application.\"\u000a    (iii) Use of a new combination test for survival data in an\u000a        adaptive clinical trial\u000a    The new form of combination test defined in [6] provides a sound basis\u000a      for adaptive designs with survival endpoints and guarantees the crucial\u000a      property, insisted upon by regulators, that the type I error rate is\u000a      controlled unequivocally. Previous proposals failed to do this. The impact\u000a      of this research output has already been seen in Hoffman-La Roche's GATSBY\u000a      trial (study BO27952), a multi-national trial of treatments for advanced\u000a      gastric cancer. The adaptive design of this Phase III trial relies\u000a      fundamentally on results in [6]. The trial starts by comparing two\u000a      treatment formulations against a control then, at an interim analysis,\u000a      just one of these formulations is chosen for comparison with the control\u000a      in the remainder of the study. Combining treatment selection and testing\u000a      in a single trial achieves the statistical requirements with fewer\u000a        patients. Our new form of combination test is used to analyse data\u000a      from the two phases of the trial in a way which guarantees full control of\u000a      the type I error rate. The letter from Hoffman-La Roche's Associate\u000a      Director of Biostatistics (J) quotes the reference to [6] in the trial\u000a      protocol:\u000a    \"As discussed in Jenkins et al. (2011), as the extent of follow up of\u000a        Stage 1 patients remains unchanged, the final testing procedure\u000a        described within the Roche study BO27952 guarantees full control of the\u000a        Type I error rate.\"\u000a    Communication of research findings\u000a    Jennison has enhanced the impact of his research by communicating results\u000a      to practitioners. Since 2008, he has presented 10 short courses on group\u000a      sequential and adaptive methods, from half a day to 2 days in length, at\u000a      conferences and to companies. He speaks frequently at conferences with a\u000a      high proportion of industrial participants; see the listing of around 10\u000a      talks per year at http:\/\/people.bath.ac.uk\/mascj.\u000a      He also provides consultancy to companies on the design of individual\u000a      trials with innovative group sequential or adaptive features.\u000a    ","ImpactSummary":"\u000a    Clinical trials form a crucial step in translating fundamental medical\u000a      research into improved healthcare. Many hundreds of trials are conducted\u000a      every year, each involving hundreds, sometimes thousands, of patients.\u000a      These trials are expensive, with costs as high as 20 or 30 thousand pounds\u000a      per patient. Research in Bath on group sequential monitoring and\u000a      the adaptive design of clinical trials has improved the conduct of\u000a      clinical trials, leading to:\u000a    \u000a      faster results: making effective new treatments available sooner;\u000a        also, stopping negative trials early avoids exposing patients to\u000a        ineffective treatments and releases resources for new studies;\u000a      smaller sample sizes: average reductions of 20-30% are possible in\u000a        sequential trials;\u000a      the ability to modify trial conditions while retaining statistical\u000a        validity: this flexibility can accelerate the drug development process\u000a        by months or even years.\u000a    \u000a    The impact of this research is economic (the business performance of\u000a      pharmaceutical companies and businesses that support them), societal (by\u000a      enhancing public health and by changing the policies adopted by\u000a      regulators) and ethical (ensuring clinical trials remain safe, while\u000a      bringing life-saving treatments into clinical use as rapidly as possible).\u000a    ","ImpactType":"Health","Institution":"\u000a    University of Bath\u000a    ","Institutions":[{"AlternativeName":"Bath (University of)","InstitutionName":"University of Bath","PeerGroup":"B","Region":"South West","UKPRN":10007850}],"Panel":"B         ","PlaceName":[],"References":"\u000a    References that best indicate the quality of the underpinning research\u000a      are starred.\u000a    \u000a[1]* Jennison, C. and Turnbull, B. W. (1997) Group sequential analysis\u000a      incorporating covariate information. Journal of the American\u000a        Statistical Association, 92, 1330-1341. [doi:\u000a      10.1080\/01621459.1997.10473654]\u000a    \u000a\u000a[2] Jennison, C. and Turnbull, B.W. (2000) Group Sequential Tests\u000a        with Applications to Clinical Trials, Chapman and Hall\/CRC, 390\u000a      pages. [ISBN 0-8493-0316-8] This book has been translated into Japanese by\u000a      Professors M. Toshihiko and T. Yamanaka. [ISBN 978-4-9902097-4-2]\u000a    \u000a\u000a[3] Barber, S. and Jennison, C. (2002) Optimal asymmetric one-sided group\u000a      sequential tests. Biometrika, 89, 49-60. [doi:\u000a      10.1093\/biomet\/89.1.49]\u000a    \u000a\u000a[4]* Jennison, C. and Turnbull, B. W. (2003) Mid-course sample size\u000a      modification in clinical trials based on the observed treatment effect. Statistics\u000a        in Medicine, 22, 971-993. [doi: 10.1002\/sim.1457]\u000a    \u000a\u000a[5]* Jennison, C. and Turnbull, B. W. (2006). Adaptive and non-adaptive\u000a      group sequential tests. Biometrika, 93, 1-21. [doi:\u000a      10.1093\/biomet\/93.1.1]\u000a    \u000a\u000a[6] Jenkins, M., Stone, A. and Jennison, C. (2011) An adaptive seamless\u000a      phase II\/III design for oncology trials with subpopulation selection using\u000a      correlated survival endpoints. Pharmaceutical Statistics, 10,\u000a      347-356. [doi: 10.1002\/pst.472]\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000a    (A) Letter from Senior Acquisitions Editor for Statistics, Chapman &amp;\u000a      Hall\/CRC, Taylor &amp; Francis.\u000a    (B) Lo, A. C. et al, (2010) Robot-assisted therapy for long-term\u000a      upper-limb impairment after stroke, New England Journal of Medicine,\u000a      362, 1772-1783. [doi: 10.1056\/NEJMoa0911341]\u000a    (C) Loehrer, P.J. et al (2011) Gemcitabine alone versus gemcitabine plus\u000a      radiotherapy in patients with locally advanced pancreatic cancer: An\u000a      Eastern Cooperative Oncology Group trial, Journal of Clinical Oncology,\u000a      31, 4105-4112. [doi: 10.1200\/JCO.2011.34.8904]\u000a    (D) Barrios, C.H. et al (2010) Phase III randomized trial of sunitinib\u000a      versus capecitabine in patients with previously treated HER2-negative\u000a      advanced breast cancer, Breast Cancer Research and Treatment, 121,\u000a      121-131. [doi: 10.1007\/s10549-010-0788-0]\u000a    (E) East5 manual, p.1255: refers to \"Rho spending function ... Jennison\u000a      and Turnbull (2000)\" [available from the HEI].\u000a    (F) Letter from President of Cytel Inc. (producers of the East\u000a      software package).\u000a    (G) U.S. FDA, \"Guidance for Industry Adaptive Design Clinical Trials for\u000a      Drugs and Biologics\", February 2010 http:\/\/www.fda.gov\/downloads\/Drugs\/...\/Guidances\/ucm201790.pdf\u000a      [also available from the HEI].\u000a    (H) Letter from Associate Director for Adaptive Design and\u000a      Pharmacogenomics, U.S. FDA.\u000a    (J) Letter from Associate Director Biostatistics, F. Hoffman-La Roche\u000a      Ltd.\u000a    ","Title":"\u000a    Improving Clinical Trials by Innovative Statistical Design\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Randomised clinical trials originated in the 1940s. Group sequential\u000a      methods, which involve monitoring results on a small number of occasions\u000a      during the course of a trial, were proposed in the 1970s. The group\u000a      sequential approach has steadily gained favour as:\u000a    \u000a      designs for trials have been developed to meet practical needs more\u000a        closely;\u000a      dedicated software has become available;\u000a      clear expositions of the statistical methods and their validity have\u000a        been provided.\u000a    \u000a    While research groups worldwide have contributed to this field, the\u000a      series of contributions by Jennison in Bath has been distinctive,\u000a      influential and widely applied. The following description of the\u000a      underpinning research is organised into three parts for clarity.\u000a    (i) Core theory and methodology for group sequential tests\u000a    The research article [1], cited over 40 times, derives theory for the\u000a      joint probability distributions that arise when accumulating data are\u000a      analysed repeatedly. This allows a common treatment of sequential\u000a      hypothesis tests for different types of clinical response data and\u000a      provides the foundation for a unified approach to a wide variety of trial\u000a      settings. Results in [1] widen the applicability of methods for simple,\u000a      normally distributed responses to general parametric models incorporating\u000a      baseline covariates, to survival endpoints, and to longitudinal data. This\u000a      unified approach is further developed and expanded in the book [2], which\u000a      applies the theory in [1] to a series of important examples. Section 7.3\u000a      of [2] extends error-spending designs to one-sided, group sequential\u000a      hypothesis tests, which are now a standard requirement in many Phase III\u000a      clinical trials. The stopping rule for an error spending design is\u000a      produced by allocating a portion of the overall type I error probability\u000a      to each analysis, depending on the observed statistical information at\u000a      that point, so dealing flexibly and efficiently with the common problem of\u000a      unpredictable increments in sample size and information between analyses.\u000a    (ii) Optimal group sequential designs and adaptive sample size\u000a        modification\u000a    Sequential monitoring aims to reduce the number of patients needed in a\u000a      study. Results in [3] quantify the maximum savings that group sequential\u000a      testing can achieve and knowledge of these optimal values allows the\u000a      efficiency of other designs to be assessed. In particular, the Rho family\u000a      of error spending tests proposed in [2] is seen to offer a highly\u000a      efficient set of designs.\u000a    The last decade has seen intense interest in adaptive trial designs in\u000a      which aspects of a study are altered as data are observed. An important\u000a      question is whether it is actually advantageous to modify a trial's sample\u000a      size adaptively in response to interim estimates of the treatment effect.\u000a      Case studies and the general theory of optimal design reported in [3], [4]\u000a      and [5] give a clear answer to this question: traditional, non-adaptive\u000a      group sequential tests capture almost all of the efficiency gains that can\u000a      be achieved by more complex adaptive designs; moreover, a poor choice of\u000a      adaptive design can be substantially inferior to a good group sequential\u000a      design.\u000a    (iii) Combination tests for survival response data in adaptive\u000a        designs\u000a    Adaptive designs enable innovation in clinical trials, such as the\u000a      data-driven selection of one of several dose levels during the course of a\u000a      study, or deciding whether to focus attention on a patient subgroup in\u000a      which the new treatment shows a more substantial effect. The combination\u000a      test of Bauer and K&#246;hne (Biometrics, 1994) controls the type I\u000a      error rate in a great many situations and is a key element of adaptive\u000a      designs. However, it is well known that application of the combination\u000a      test to survival endpoints can inflate the type I error rate. The new form\u000a      of combination test for survival data defined in [6] solves this important\u000a      and longstanding problem &#8212; moreover, this work has been taken up\u000a      immediately by practitioners.\u000a    The above research was carried out by Jennison at Bath, where he has been\u000a      Professor of Statistics since 1993. Items [1], [2], [4] and [5] were\u000a      written in collaboration with Professor Bruce Turnbull (Cornell\u000a      University); [3] is joint work with Stuart Barber, a Bath PhD student from\u000a      1996-99; [6] is joint work with Martin Jenkins and Andrew Stone of\u000a      AstraZeneca, Macclesfield.\u000a    "},{"CaseStudyId":"43220","Continent":[],"Country":[],"Funders":["Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000a    Regulation of air pollution has important effects on human health and it\u000a      has recently been estimated that the 1990 Clean Air Act in the US\u000a      prevented 160,000 premature deaths, 1.7 million asthma attacks and 13\u000a      million lost work days per year &#8212; with these figures set to increase over\u000a      time [A].\u000a    The Clean Air Act requires the Environmental Protection Agency (EPA) to\u000a      set national ambient air quality standards (NAAQS) for ozone and five\u000a      other pollutants considered harmful to public health and the environment\u000a      (the other pollutants are particulate matter, nitrogen oxides, carbon\u000a      monoxide, sulfur dioxide and lead). As mandated by the Clean Air Act, the\u000a      EPA must periodically review the scientific bases (or criteria) for the\u000a      various NAAQS by assessing newly available scientific information for each\u000a      of the pollutants listed above. This process occurs every ten years for\u000a      each pollutant.\u000a    Models for estimating personal exposures based on [1] and [2] were used\u000a      extensively as part of the scientific basis for the most recent changes in\u000a      the NAAQS for ozone. The goal was to determine the effect of ambient\u000a      regulatory strategies on human exposure by observing how different ambient\u000a      concentration scenarios changed the predicted personal exposure for\u000a      different demographic groups.\u000a    The review of the standards for ozone was announced in 2008 and resulted\u000a      in a reduction of the primary standard level (8-hour average) for ozone\u000a      from 0.08 to 0.075 parts per billion [B, C, D]. This reduction\u000a      \"significantly strengthened its national ambient air quality standards\u000a      (NAAQS) for ground-level ozone, the primary component of smog. These\u000a      changes will improve both public health protection and the protection of\u000a      sensitive trees and plants\" [C]. After a series of petitions and\u000a      legislation, the final stage in the implementation of the new limits,\u000a      which refers to methods for designating areas as either in accordance or\u000a      in non-attainment was passed in 2012 [D, E].\u000a    During the process of formulating the new limits for ozone, three volumes\u000a      of scientific background information were released for consultation, \"Air\u000a      Quality Criteria for Ozone and Related Photochemical Oxidants\" [F]. These\u000a      linked the underpinning research to the subsequent formulation of the\u000a      legislation. Over 20 pages of the summary (Volume 1) are related to\u000a      estimating personal exposures together with a 70 page technical appendix\u000a      (Volume 2). The methods derived in [1] provide \"the underpinning\u000a      theoretical probabilistic framework underlying these exposure simulators\"\u000a      as referenced in [G] (in which 3rd and 4th authors\u000a      are members of the EPA). Our work [1,2], in particular pCNEM, is\u000a      extensively discussed in Section 3, Vol.1 and Chapter 3, Vol. 2 of [F],\u000a      with its advantages noted specifically in terms of its ability, in\u000a      contrast to other methods, to provide a coherent approach to incorporating\u000a      \"uncertainties in the predicted distributions\" and \"its ability to\u000a      estimate the effects of reductions in ambient levels of pollutants\"\u000a      (Section 3, Vol.1, pages 3-61 and 3-62).\u000a    To summarise:\u000a    \u000a      Detailed information is required on the potential exposures\u000a        experienced by individual members of the population to provide\u000a        scientific support for potential changes in air quality standards. The\u000a        research developed in [1,2] provides a convenient framework that can be\u000a        used to generate this information.\u000a      Our work had impact on public policy and changes to legislation and\u000a        regulations. Models for estimating personal exposures based on [1] and\u000a        [2] were used extensively as part of the scientific basis for the most\u000a        recent changes in air quality standards for ozone.\u000a      Our research has impact on the management of environmental risk. The\u000a        result of the 2008 EPA review was to make the standards for ozone more\u000a        rigorous. Areas classified \"nonattainment\" may have to impose more\u000a        stringent emission controls.\u000a      According to the EPA these changes \"will improve both public health\u000a        protection and the protection of sensitive trees and plants.\" [C]\u000a    \u000a    ","ImpactSummary":"\u000a    Air pollution poses significant threats to both the environment and to\u000a      human health and the World Health Organization estimates that 800,000\u000a      deaths per year could be related to ambient air pollution. Formulating air\u000a      quality legislation and understanding its effect on human health requires\u000a      accurate information on ambient concentrations of air pollution and how\u000a      these translate into exposures actually experienced by individuals\u000a      (personal exposures).\u000a    Our research provides a framework for estimating personal exposures for\u000a      specific susceptible sub-populations, such as the elderly and those\u000a      suffering from respiratory diseases. This framework also provides novel\u000a      means of assessing uncertainty associated with the estimates of exposures.\u000a      Furthermore, it allows changes in exposures to be assessed under\u000a      hypothetical scenarios reflecting potential regulatory changes.\u000a    These models were used in the US Environmental Protection Agency's (EPA)\u000a      recent review of ozone standards that resulted in a reduction in the\u000a      statutory limits of ozone in the United States. The EPA stated that \"These\u000a      changes will improve both public health protection and the protection of\u000a      sensitive trees and plants\" [C].\u000a    ","ImpactType":"Health","Institution":"\u000a    University of Bath\u000a    ","Institutions":[{"AlternativeName":"Bath (University of)","InstitutionName":"University of Bath","PeerGroup":"B","Region":"South West","UKPRN":10007850}],"Panel":"B         ","PlaceName":[],"References":"\u000a    References that best indicate the quality of the underpinning research\u000a      are starred.\u000a    \u000a[1]* Zidek, J. V., Shaddick, G., Meloche, J., Chatfield, C. and White,\u000a      R., 2007. A framework for predicting personal exposures to environmental\u000a      hazards. Environmental and Ecological Statistics, 14 (4), pp.\u000a      411-431. DOI 10.1007\/s10651-007-0028-x\u000a    \u000a\u000a[2]* Zidek, J. V., Shaddick, G., White, R., Meloche, J. and Chatfield,\u000a      C., 2005. Using a probabilistic model (pCNEM) to estimate personal\u000a      exposure to air pollution. Environmetrics, 16 (5), pp. 481-493.\u000a      DOI: 10.1002\/env.716\u000a    \u000aBoth papers contain developments of work which first appeared in a\u000a      technical report: Zidek, J.V., Meloche, J., Shaddick, G., Chatfield, C.\u000a      and White, R.A., 2003. A Computational Model for Estimating Personal\u000a      Exposure to Air Pollutants with Application to London's PM10 in 1997,\u000a      Technical Report TR#2003-3, Statistical and Applied Mathematical\u000a        Sciences Institute, Research Triangle Park, NC.\u000a    ","ResearchSubjectAreas":[{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    [A] U.S. EPA. The Benefits and Costs of the Clean Air Act from 1990 to\u000a      2020. (http:\/\/www.epa.gov\/oar\/sect812\/feb11\/summaryreport.pdf).\u000a    [B] Federal Register. http:\/\/www.gpo.gov\/fdsys\/pkg\/FR-2008-03-27\/pdf\/E8-5645.pdf\u000a    [C] FACT SHEET FINAL REVISIONS TO THE NATIONAL AMBIENT AIR QUALITY\u000a      STANDARDS FOR OZONE (http:\/\/www.epa.gov\/glo\/pdfs\/2008_03_factsheet.pdf)\u000a    [D] March 2008 Final National Ambient Air Quality Standards for\u000a      Ground-level Ozone. http:\/\/www.epa.gov\/glo\/pdfs\/2008_03_text_slides.pdf\u000a    [E] EPA Final Area Designations for 2008 Ground-level Ozone Standards\u000a      (issued May 2012).\u000a      \u000a        http:\/\/www.epa.gov\/glo\/designations\/2008standards\/final\/qandafinal.htm\u000a    [F] U.S. EPA Report \"Air Quality Criteria for Ozone and Related\u000a      Photochemical Oxidants, Volumes 1 and 2, EPA\/600\/R-05\/004aF and\u000a      EPA\/600\/R-05\/004bF, 2006. http:\/\/www.epa.gov\/ttn\/naaqs\/standards\/ozone\/s_o3_cr_cd.html\u000a    [G] Berrocal, V. J., Gelfand, A. E., Holland, D. M., Burke, J. and\u000a      Miranda, M. L., 2011. On the use of a PM2.5\u000a      exposure simulator to explain birthweight. Environmetrics, 22, pp.\u000a      553-571. DOI 10.1002\/env.1086\u000a    ","Title":"\u000a    Strengthening air pollution standards in the USA\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Direct measurement of personal exposures requires an exposure monitor to\u000a      be worn. Whilst accurate, this is extremely costly and time-consuming. As\u000a      a consequence, in studies using this approach, sample sizes are small and\u000a      the information provided may therefore be limited. In order to provide\u000a      accurate data for larger samples, an indirect method has been developed in\u000a      which concentrations of pollutants in specific micro-environments, such as\u000a      the home, workplace or car, are modelled. When combined with models of\u000a      human behaviour that estimate the time spent in the different\u000a      microenvironments, these provide an integrated framework for estimating\u000a      personal exposures.\u000a    Early approaches to combining micro-environments and time activity were\u000a      deterministic and did not have means of assessing the uncertainty\u000a      associated with the resulting estimated exposures. In [1], we provide a\u000a      theoretical model framework for estimating personal exposures\u000a      stochastically with full integration of the uncertainties inherent in the\u000a      process.\u000a    Based on this framework, stochastic models, known as `exposure\u000a      simulators', have been developed which predict the exposures to a\u000a      pollutant experienced by individuals together with quantification of the\u000a      associated uncertainties. These individual exposures may then be\u000a      aggregated over demographic groups. Estimating individual personal\u000a      exposures is performed by sampling individuals from each demographic group\u000a      and randomly associating to each individual a time activity pattern that\u000a      matches the subject in terms of personal characteristics, day of the week,\u000a      temperature, season, etc.\u000a    In [2] we describe pCNEM (Personal Computer National Exposure Model), a\u000a      specific computational implementation of the framework developed in [1].\u000a      pCNEM comprises a large-scale computer simulation model that provides a\u000a      flexible platform for developing a wide variety of models and produces\u000a      distributions of predicted exposures. It can be formulated to produce\u000a      estimates for a range of pollutants, and has been used for both\u000a      particulate matter and ozone. For registered users, pCNEM can be accessed\u000a      via the internet [2]. This allows users to define their own models for the\u000a      levels of pollution in individual micro-environments and incorporate data\u000a      on ambient levels of pollution from specific areas.\u000a    The US Environmental Protection Agency (EPA) has commissioned two\u000a      implementations based on the framework in [1, 2]: SHEDS (Stochastic Human\u000a      Exposure and Dose Simulation) and APEX (Air Pollution EXposure model).\u000a    An important application of the personal exposure simulation framework is\u000a      to help quantify the possible effects of abatement strategies, e.g.,\u000a      regulations and mandatory surveillance, by running them before and after\u000a      the hypothetical change. For example, the effects of a potential decrease\u000a      in ambient concentrations of a pollutant due to a new law, known as\u000a      `rollbacks', can be assessed in terms of the changes in exposures actually\u000a      experienced by individuals. In [2] it is shown how the effects of such\u000a      `rollbacks' can be assessed.\u000a    The underpinning research was carried out by Shaddick at Bath where he\u000a      has been Lecturer, Senior Lecturer and Reader in Statistics since 2001.\u000a      The work was produced in collaboration with Professor Jim Zidek of the\u000a      University of British Columbia (UBC) and colleagues within the departments\u000a      of Mathematical Sciences at Bath (Chatfield) and Statistics at UBC. Large\u000a      parts of the work have been performed during long-term visits by Prof.\u000a      Zidek to Bath which have been funded by an EPSRC travel grant (2002\/3),\u000a      the EPSRC funded Bath Institute for Complex Systems (2005, 2007) and the\u000a      Canadian National Science and Engineering Research Council (2004-2013).\u000a      Work has also been performed during visits by Shaddick to UBC with funding\u000a      including a Peter Wall Institute for Advanced Studies Personal Fellowship\u000a      (2004).\u000a    "},{"CaseStudyId":"43313","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"2963597","Name":"Ireland"},{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000a    The Groenemeyer Institute for Microtherapy in Bochum, Germany is a high\u000a      profile, privately run\u000a      medical research and treatment institute. Its main aim is to develop and\u000a      apply products and\u000a      processes for minimally invasive therapy, including those in the field of\u000a      prenatal diagnosis. One of\u000a      their aims is to find ways to identify and classify foetal pathological\u000a      conditions very early in the\u000a      pregnancy, such as growth retardation and foetal stress. The Department of\u000a      Biomagnetism at the\u000a      Groenemeyer Institute has taken up research in this direction supported by\u000a      the evidence [5] of\u000a      prenatal interaction between mother and foetus on the basis of foetal\u000a      heart rate. However, as\u000a      explained above, their research faced the fundamental problem of dealing\u000a      with passive\u000a      experiments. This means that it was impossible to obtain, from the data\u000a      they collected, statistically\u000a      robust evidence to support their conclusions. A better understanding of\u000a      this interaction and ways to\u000a      evaluate its statistical significance were needed for foetal surveillance\u000a      and the detection of\u000a      pathological conditions during pregnancy. The breakthrough came from our\u000a      research at IPAM,\u000a      which equipped the clinicians at the Groenemeyer Institute with the\u000a      mathematical and statistical\u000a      tools to develop the new diagnostic algorithms, as well as computer\u000a      software which implemented\u000a      them.\u000a    The research team at IPAM in Aberdeen started working on the general\u000a      problem of passive\u000a      experiments in 2007, inspired by the mother-foetus heartbeat\u000a      synchronisation problem which was\u000a      presented at a conference by members of the Groenemeyer Institute. Our\u000a      work on the twin\u000a      surrogate method was then published [3] in 2008. After that, contact was\u000a      established between the\u000a      research group led by Professor van Leeuwen at the Groenemeyer Institute\u000a      and the IPAM\u000a      researchers. A collaboration between the two groups followed. As a result\u000a      we made the twin\u000a      surrogate method available to clinicians at the Groenemeyer Institute by\u000a      means of a computer\u000a      algorithm that was developed at IPAM in 2008. This software made it\u000a      possible for clinicians at the\u000a      Groenemeyer Institute to take up their clinical research by analysing and\u000a      interpreting\u000a      magnetocardiographic data in a statistically meaningful way [1]. As a\u000a      result they successfully\u000a      established the physiological significance of heartbeat synchronization.\u000a      Beyond a contribution to\u000a      pure clinical research, our software has also been fundamental to the\u000a      development of diagnostic\u000a      methods, since data from pregnant women can now be processed by\u000a      practitioners in their clinics.\u000a      Our algorithms have been implemented in a widely used time series analysis\u000a      software package\u000a      (CROSS RECURRENCE PLOT TOOLBOX v5.17, http:\/\/tocsy.pik-potsdam.de\/CRPtoolbox\/),\u000a      with\u000a      more than 2,000 downloads by researchers from a wide distribution of\u000a      scientific disciplines beyond\u000a      mathematics, ranging from cognitive neurosciences and cardiology, to\u000a      medical research and\u000a      systems biology.\u000a    The Head of the Groenemeyer Institute for Microtherapy stated in [c1]\u000a      that \"the physiological\u000a        significance (of the research) lies in the demonstration of the physical\u000a        synchronisation of biological\u000a        organ systems of separate individuals. The fact that it occurs\u000a        prenatally is a sign that it may have\u000a        developmental importance.\" In addition, with respect to IPAM's work,\u000a      he mentioned that \"the\u000a        methodological significance lies in the fact that procedures have been\u000a        developed which permit the\u000a        identification and quantification of such interaction under the\u000a        conditions of pregnancy.\" The\u000a      Department of Biomagnetism at the Groenemeyer Institute has since taken up\u000a      this work further\u000a      through collaboration with other partners, e.g. Humboldt University in\u000a      Berlin, in order to develop\u000a      protocols that will enable them to better understand the phenomenon of\u000a      heartbeat synchronisation\u000a      between mother and child.\u000a    To summarise, our fundamental research in dynamical systems provided the\u000a      turning point in a new\u000a      clinical study carried out in Germany. Primarily, our work enabled the\u000a      researchers in the\u000a      Groenemeyer Institute to carry out their medical research. Secondly, the\u000a      software derived from our\u000a      mathematical results has made it possible to apply the research in actual\u000a      clinical trials of pregnant\u000a      women in order to develop diagnostic protocols that identify foetal\u000a      pathological conditions.\u000a    This impact achieved additional reach among the non-specialist public\u000a      through media coverage in\u000a      the UK and Germany, raising awareness and understanding of the\u000a      significance of the findings\u000a      among non-specialists and the general public. For example, in January\u000a      2010, the German popular\u000a      science magazine Spektrum der Wissenschaft (circulation 110,000)\u000a      published an article on the\u000a      findings, explaining the circumstances in which maternal and foetal heart\u000a      rates synchronise [c2].\u000a      BBC News for North-East Scotland, Orkney &amp; Shetland reported on\u000a      the diagnostic significance of\u000a      the findings in July 2010 [c3], as did the written media such as The\u000a        Scotsman [c4], Aberdeen\u000a        Evening Express, Aberdeen Press and Journal, Daily Record, and the Irish\u000a        Examiner [c5]\u000a      (combined circulation: over 440,000).\u000a    ","ImpactSummary":"\u000a    This Impact Case Study illustrates the impact of our research on\u000a      clinicians and medical\u000a      researchers. Research conducted by the Institute of Pure and Applied\u000a      Mathematics (IPAM) at the\u000a      University of Aberdeen has developed software enabling clinical trials to\u000a      be carried out in Germany\u000a      aimed at creating new diagnostic tools for unborn children in order to\u000a      identify foetal developmental\u000a      issues. The research, focusing on time series analysis and dynamical\u000a      systems, derived clinical\u000a      benefits in that the Groenemeyer Institute, a privately-run research and\u000a      treatment organisation,\u000a      used it in the development of pioneering non-invasive methods for the\u000a      diagnosis of foetal\u000a      pathological conditions. The research also achieved considerable reach\u000a      among the non-specialist\u000a      public through media coverage in the UK and Germany.\u000a    ","ImpactType":"Health","Institution":"\u000a    University of Aberdeen\u000a    ","Institutions":[{"AlternativeName":"Aberdeen (University of)","InstitutionName":"University of Aberdeen","PeerGroup":"B","Region":"Scotland","UKPRN":10007783}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2947416","Name":"Bochum"}],"References":"\u000a    \u000a[1] Van Leeuwen, P., Geue, D., Thiel, M., Cysarz, D., Lange, S.,\u000a      Romano, M.C., Wessel, N.,\u000a      Kurths, J., and Groenemayer, D.H. \"Influence of paced maternal\u000a        breathing on foetal-maternal\u000a        heart rate coordination,\" PNAS, 106, 13661-13666 (2009). In this\u000a        paper, we showed that paced\u000a        breathing-induced maternal respiratory arrhythmia increases the degree\u000a        of synchronisation\u000a        between the heartbeats of mother and foetus, and we applied the twin\u000a        surrogate method to\u000a        show the statistical significance of the results.\u000a    \u000a\u000a[2] Robinson G. and Thiel, M. \"Recurrences determine the\u000a        dynamics,\" Chaos, 19, 023104 (2009).\u000a      In this paper we proved that dynamical systems with the same\u000a        recurrences are dynamically\u000a        equivalent. The main theorem states that the recurrence matrix\u000a        determines the dynamical\u000a      system.\u000a    \u000a\u000a[3] Thiel, M. Romano, M.C., Kurths, J., Rolfs, M.\u000a      and Kliegl, R., \"Generating Surrogates from\u000a        Recurrences\", Phil. Trans. Roy. Soc. A, 366 (1864), 545-557\u000a      (2008). In this paper we presented\u000a        the approach to recover the dynamics from recurrences of a system and\u000a        then generate\u000a        (multivariate) twin surrogate trajectories. We showed that, in contrast\u000a        to other approaches such\u000a        as the linear-like surrogates, this technique produces surrogates that\u000a        correspond to independent\u000a        realisations of the underlying system's trajectories. We showed that\u000a        these surrogates are well\u000a        suited to test for complex synchronisation.\u000a    \u000a\u000a[4] Romano, M.C., Thiel, M., Kurths, J.,\u000a      Mergenthaler, K., and Engbert, R. \"Hypothesis test for\u000a      synchronisation: twin surrogates revisited\", Chaos, 19, 015108\u000a      (2009). In this paper we derived\u000a        new analytical expressions for the number of twins depending on the size\u000a        of the neighbourhood,\u000a        as well as on the length of the trajectory. This allowed us to determine\u000a        optimal parameters for\u000a        the generation of twin surrogates.\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"11","Level2":"9","Subject":"Neurosciences"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"11","Level2":"3","Subject":"Clinical Sciences"}],"Sources":"\u000a    [c1] The Head of Department of Biomagnetism, Groenemeyer Institute for\u000a      Microtherapy can\u000a      corroborate the applicability of the twin surrogate method to the\u000a      development of non-invasive\u000a      diagnosis of foetal pathological conditions.\u000a    [c2] http:\/\/www.spektrum.de\/alias\/physiologie\/herzen-von-mutter-und-foetus-im-gleichtakt\/1019580\u000a      Article from Spektrum der Wissenschaft, 22nd January\u000a      2010. This source confirms the impact\u000a        achieved in non-specialist public in Germany.\u000a    [c3] http:\/\/www.bbc.co.uk\/news\/uk-scotland-north-east-orkney-shetland-10696611\u000a      Article from BBC\u000a      News online for NE Scotland, Orkney and Shetland, 20th July\u000a      2011.\u000a    [c4] http:\/\/www.irishexaminer.com\/ireland\/foetal-heartbeat-can-adjust-to-mothers-125697.html,\u000a      Article from the Irish Examiner, 21st July 2010.\u000a    These last two sources verify the impact on public understanding\u000a        achieved through media\u000a        coverage in Scotland and Ireland.\u000a    \u000a    ","Title":"\u000a    Foetal-maternal heart rate coordination\u000a    ","UKLocation":[{"GeoNamesId":"8299623","Name":"Orkney"},{"GeoNamesId":"8299621","Name":"Shetland"}],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    It was shown in [1] that respiratory arrhythmia induced by paced\u000a      breathing results in\u000a      synchronisation between maternal and foetal heart beats. Absence of\u000a      synchronisation may indicate\u000a      problems in the foetal development. This prompted researchers in the\u000a      Groenemeyer Institute\u000a      (Germany) to develop non-invasive methods to diagnose abnormal foetus\u000a      development.\u000a    A healthy foetus interacts physiologically with his mother. For example,\u000a      the heartbeat frequencies of\u000a      mother and a healthy foetus are expected to synchronise. It is a\u000a      remarkable fact that even if the\u000a      frequencies of the heartbeats are different, a minute interaction will\u000a      lead to synchronisation. The\u000a      problem is, however, that even in the event of complete lack of\u000a      interaction, the heartbeats might\u000a      falsely appear to be synchronised because of their intrinsic regularity.\u000a      This can be illustrated by two\u000a      wristwatches which may appear to be perfectly synchronised because they\u000a      are both designed to\u000a      exhibit the same frequency, but they are, in effect, two independent\u000a      systems. Another issue is the\u000a      fact that synchronisation will not occur instantaneously but with a time\u000a      delay, as is the problem that\u000a      synchronisation can be lost for certain periods of time and then\u000a      re-established.\u000a    The challenge becomes to mathematically detect minuscule changes in the\u000a      synchronisation pattern\u000a      due to the interaction of the heartbeats. To statistically corroborate the\u000a      higher level of\u000a      synchronisation, one needs to establish a \"base level\" of synchronisation,\u000a      i.e. how synchronised\u000a      heartbeats may appear to be, due to their regular nature, in the absence\u000a      of any interaction at all.\u000a      This would be easy if one could \"switch off\" the interaction between\u000a      mother and foetus - an active\u000a      experiment - and then quantify and compare the observed level of\u000a      synchrony. This is, of course,\u000a      impossible to do.\u000a    Looking at the wider context, this is just one example of a problem\u000a      prevalent in so-called \"passive\u000a      experiments\". In an \"active\" experiment in which the strength of\u000a      interaction between two\u000a      subsystems is measured, the coupling strength between them can be\u000a      systematically changed,\u000a      thereby allowing one to measure change in synchronisation, and thus\u000a      establish a reference (i.e. low\u000a      versus high degree of synchronisation). In contrast, in a \"passive\u000a      experiment\", it is not possible to\u000a      change the coupling strength of the subsystems in a controlled manner.\u000a      This is the case in the\u000a      mother and foetus example, since the coupling strength between their heart\u000a      beats cannot be tuned\u000a      at wish; it is determined by the physiology of the mother and child.\u000a    The synchronisation between the magnetocardiographic (MCG) signals of a\u000a      mother-and-foetus pair\u000a      can be quantified by a method called synchrogram. This method identifies\u000a      time-windows during\u000a      which the mother and foetus heartbeat frequencies exhibit the same\u000a      pattern, namely are\u000a      synchronised. A \"synchronisation index\" can be calculated based on the\u000a      proportion of time during\u000a      which signal synchronisation occurs. In theory, the statistical\u000a      significance of the result can then be\u000a      assessed, using standard statistical methods, by comparison to\u000a      synchronisation indices obtained\u000a      from the heartbeat signals of the foetus under study and signals from a\u000a      large sample of different\u000a      pregnant women (\"surrogate mothers\"). Since interaction between the\u000a      \"surrogate mothers\" and the\u000a      foetus can certainly be ruled out, we may establish in this way a base\u000a      level of spurious\u000a      synchronisation, and then test whether the mother and foetus have a higher\u000a      level of\u000a      synchronisation or not. Low synchronisation index may suggest abnormal\u000a      foetal development. This\u000a      was the approach initially taken by the researchers in the Groenemeyer\u000a      Institute.\u000a    However, due to physiological differences among different women, the\u000a      statistical significance of the\u000a      results remained questionable. The problem facing the clinicians was that\u000a      only \"surrogate mother-foetus\" pairs with identical physiological parameters to the actual mother\u000a      are suitable for the\u000a      calculation of synchronisation indices. The issue is that these parameters\u000a      inevitably change from\u000a      one mother to mother, a fact that makes a statistical hypothesis test\u000a      impossible.\u000a    Research carried out by Romano, Thiel and Kurths,\u000a      at the University of Aberdeen between 2007\u000a      and 2009, in collaboration with the Groenemeyer Institute for Microtherapy\u000a      in Germany, resulted in\u000a      the development of an approach to mathematically generate MCG time series\u000a      of surrogate mothers\u000a      that are physiologically identical. This allows us to statistically test\u000a      for synchronisation in passive\u000a      experiments. This method, called the \"twin surrogates method\",\u000a      applies to all passive experiments.\u000a    The approach is to view the problem in terms of dynamical systems. In\u000a      short, we mathematically\u000a      create an ensemble of independent trajectories (or \"realisations\") of an\u000a      unknown, highly complex\u000a      dynamical system from a single observation of the given system, i.e. a\u000a      single trajectory of that\u000a      system. In the language of the mother and foetus problem which motivated\u000a      this research, these\u000a      trajectories represent heartbeat measurements from ideal surrogate\u000a        mothers, with exactly the same\u000a      physiological characteristics of the mother. The remarkable fact is that\u000a      the entire ensemble is\u000a      generated from a single measurement of the pregnant woman carrying the\u000a      foetus.\u000a    The twin surrogate method is a special application of a \"bootstrap\u000a      procedure\". In order to generate\u000a      new trajectories we repeatedly splice randomly selected segments of the\u000a      original trajectory in the\u000a      phase space of the mother-and-foetus dynamical system, such that the\u000a      consecutive segments are\u000a      dynamically consistent [2,3]; the notion of dynamical consistency is\u000a      related to Poincar&#233; recurrence.\u000a      In [2] we showed that recurrences determine the dynamical system.\u000a      Therefore, the new trajectories\u000a      we generate belong to the same dynamical system. Importantly they are\u000a      independent of the original\u000a      trajectory in the sense that they don't interact with it (in the same way\u000a      that the \"surrogate mothers\"\u000a      do not interact among each other and with the foetus and mother being\u000a      examined). Hence, this\u000a      method generates independent perfect surrogate mothers, with exactly the\u000a      same physiological\u000a      characteristics. This allows reliable assessment of the statistical\u000a      significance of the synchronisation\u000a      indices. More specifically, the twin surrogate data allows us to establish\u000a      the significance level by\u000a      which we could reject the null hypothesis that the mother-foetus systems\u000a      do not interact [3].\u000a    "},{"CaseStudyId":"43314","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3175395","Name":"Italy"},{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"2264397","Name":"Portugal"}],"Funders":[],"ImpactDetails":"\u000a    The research undertaken resulted in a threefold impact: (i) a software\u000a      platform EPILAB, which has mainly been developed for academic purposes,\u000a      nevertheless has strong potential for non- academic impact, (ii) a\u000a      commercial device called LTM-EU for epilepsy monitoring, and (iii) the\u000a      world's largest epilepsy database of its type with commercial and\u000a      scientific impact.\u000a    EPILAB: Implementation of the methods we developed described in\u000a      issues (i) and (iii) in the underpinning research, together with other\u000a      multivariate algorithms and various pre-processing steps, led to the\u000a      compilation of the open source software package EPILAB [3] between 2008\u000a      and 2012 under the co-supervision of Schelter (IPAM, Aberdeen) and\u000a      collaborator Dr Teixeira (Coimbra, Portugal). It is available from http:\/\/www.epilepsiae.eu\/project_outputs\/epilab_software.\u000a    The main contribution and impact of EPILAB is that researchers can run\u000a      their own seizure prediction studies interactively without spending time\u000a      working on data and programming data analysis algorithms. EPILAB's\u000a      potential to non-academic impact is evident since it can be used as a\u000a      toolkit to implement seizure prediction protocols. As of July 2013, at\u000a      least 262 groups worldwide [c1] use this software platform. As we can only\u000a      track the number of downloads and not its field of usage (academic or\u000a      non-academic), we cannot investigate if EPILAB has a particular impact\u000a      beyond academia already, although its' progression beyond mathematics is\u000a      clear. The software was designed to facilitate research in seizure\u000a      prediction but it has also been designed to be ready for use in a\u000a      commercial device.\u000a    LTM-EU (also called Brainatics): The research in IPAM has\u000a      provided major scientific contributions that have made it possible to\u000a      define and significantly improve the requirements and specifications of a\u000a      long-term monitoring device (LTM), which monitors a patient with epilepsy\u000a      continuously over a long period of time. The device is a low energy\u000a      hardware acquisition system for measuring brain activity in epilepsy\u000a      patients using bluetooth roaming capabilities. It monitors the brain\u000a      activity via electroencephalography; the device is wearable and therefore\u000a      suitable for use in hospitals as well as ambulatory monitoring. It enables\u000a      long-term monitoring over a period of days without requiring the patients\u000a      to remain in their beds.\u000a    Knowledge gained through the analytical statistics and the seizure\u000a      prediction algorithms implemented in EPILAB, has been key to defining the\u000a      device specifications. Together with the company Micromed (Italy)\u000a      a partner in the EPILEPSIAE consortium since 2008, the minimal\u000a      requirements for such a device were identified [c2, c3]. Micromed phrases\u000a      this as follows [c3]: \"These research results demonstrated that in\u000a        particular in the field of EEG and seizure prediction (1) higher\u000a        sampling rates of up to 2048 Hz, (2) wireless low-energy bluetooth\u000a        coverage with roaming capabilities, and (3) high number of channels for\u000a        an extensive spatial coverage were needed.\" Micromed developed the\u000a      LTM-EU, also marketed since 2010 under the name \"Brainatics\", based on\u000a      their previous commercial product LTM-Express. Micromed also states [c3]:\u000a      \"We discussed these requirements within the EPILEPSIAE consortium; we\u000a        were able to meet all requirements and successfully developed the LTM-EU\u000a        prototype device.\" The device and its software benefited several\u000a      updates since then.\u000a    The LTM-EU is a medically certified device (CE 0051) [c3, c4]. Since its\u000a      development, these devices have successfully been tested in 2011 on\u000a      patients in hospital and ambulatory environments: \"After thorough in\u000a        house testing and testing by the partners in the consortium in a\u000a        clinical environment, we could prove the effectiveness of this\u000a        acquisition prototype\" [c3]. Micromed refers to the device as\u000a      follows [c5]: \"Though the EPILEPSIAE project was quite challenging, we\u000a        believe that this gave us also the opportunity to improve a lot the\u000a        hardware and software performances and allowed us to give our\u000a        contribution for a better `European Health.\" Micromed claims that: \"we\u000a        would predict the benefits will develop with time, ultimately leading to\u000a        improvements to early diagnosis and long term monitoring for patients\u000a        suffering from various brain related diseases such as epilepsy. We\u000a        anticipate that a fully automatic wearable seizure prediction and\u000a        alarming device might then change the life for epilepsy patients\"\u000a      [c3].\u000a    The EPILEPISAE Database: The EPILEPSIAE project defined the need\u000a      for a comprehensive epilepsy database that contains well-annotated\u000a      long-term continuous EEG recordings and the corresponding meta-data\u000a      including information about medication, duration of the disease, and\u000a      information about the seizures. The EPILAB software was pivotal to this,\u000a      especially related to the statistical evaluation framework for seizure\u000a      prediction developed in IPAM, which provided information about the minimum\u000a      number of seizures, the minimum time between seizures, etc. that the\u000a      database must contain. As part of his role in the EPILEPSIAE project, Schelter\u000a      supervised the design and implementation of what has become the world's\u000a      largest relational database for seizure prediction in epilepsy [5, 6, c6].\u000a      With considerable influence of Schelter, a working party was\u000a      established to determine what information should be included in the\u000a      database, in particular with respect to meta information and the minimum\u000a      requirements of sampling rates, number of channels and other data types.\u000a      Consequently, the necessary structure and tables in the database were\u000a      created [5, 6] under guidance and supervision of Schelter and\u000a      clinical partners in the EPILEPSIAE consortium then populated this\u000a      database with datasets from 275 patients including highly annotated brain\u000a      signals and meta data [c2]. The team led by Schelter (IPAM,\u000a      Aberdeen) supervised the population of the database in Freiburg; including\u000a      all necessary quality checks.\u000a    The database is marketed by and directly available through the University\u000a      of Freiburg, Germany [see c7]. Although commercial exploitation of the\u000a      database has only recently begun (September 2012), it has already\u000a      generated revenue of approximately &#8364;24,000. The European Union projects\u000a      the sales figures of the database to be in the order of &#8364;1 Million [c2].\u000a      Susan Arthurs, chair of the patient organisation Alliance for Epilepsy\u000a      Research, phrases the importance and impact of the database in a\u000a      letter of support as follows [c6]: \"Dr Schelter used advancements in\u000a        technology to study detecting and treating seizures in very different,\u000a        non-pharmaceutical ways. The keystone of much of this research is the\u000a        European Epilepsy Database. Without standardized data it would be\u000a        impossible to compare let alone replicate the many studies being\u000a        conducted worldwide. In addition, having this data already collected\u000a        reduces the workload in individual research laboratories and allows for\u000a        exceptionally large research studies. This enables many researchers from\u000a        around the world and from seemingly disparate fields such as physics,\u000a        computer science, mathematics and engineering, as well as medicine, to\u000a        run and validate their seizure prediction algorithms.\"\u000a    ","ImpactSummary":"\u000a    Epilepsy is one of the most common neurological diseases. It is\u000a      characterised by apparently unpredictable seizures that severely affect\u000a      the quality of patients' life. In this case study we demonstrate how our\u000a      research has derived commercial impact within the medical technology\u000a      industry, as well as impact on researchers and practitioners in\u000a      neuroscience and medical science. Mathematical research carried out at the\u000a      Institute of Pure and Applied Mathematics (IPAM) at the University of\u000a      Aberdeen has led to a threefold impact. First, our research shaped the\u000a      development, implementation and validation of a new software platform,\u000a      called EPILAB, containing a vast number of sophisticated algorithms\u000a      targeting seizure prediction together with novel statistical tools to\u000a      evaluate prediction performance. Second, our research resulted in\u000a      commercial impact through the development of a new automatic long term\u000a      monitoring device, called LTM-EU, by one of our industrial collaborators,\u000a      Micromed (Italy). Third, a direct consequence of our research is the\u000a      compilation and commercial exploitation of the world's largest epilepsy\u000a      database of its type, which enables novel studies into seizure prediction\u000a      in epilepsy.\u000a    ","ImpactType":"Health","Institution":"\u000a    University of Aberdeen\u000a    ","Institutions":[{"AlternativeName":"Aberdeen (University of)","InstitutionName":"University of Aberdeen","PeerGroup":"B","Region":"Scotland","UKPRN":10007783}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2925177","Name":"Freiburg"}],"References":"\u000a    \u000a1. Schelter, B., Thiel, M., Mader, W., and Mader, M.\u000a      Signal Processing of the EEG: Approaches Tailored to Epilepsy. World\u000a      Scientific, eds. Tetzlaff, R., Elger, C.E., Lehnertz, K., Oct. 2013, ISBN:\u000a      978-981-4525-34-3. In this publication, we investigate and discuss the\u000a        pre-processing of EEG signals, focussing on the time-resolved estimation\u000a        autoregressive models, which provide a robust means to model the\u000a        dynamics that underlies EEG signals.\u000a    \u000a\u000a2. Nawrath, J., Romano, M.C., Thiel, M., Kiss, I.Z.,\u000a      Wickramasinghe, M., Timmer, J., Kurths, J., Schelter, B. Distinguishing\u000a        direct and indirect interactions in oscillatory networks with multiple\u000a        time scales. Phys. Rev. Lett. 104, 2010, 038701. In this\u000a        publication, we developed a new technique to investigate interactions\u000a        between signals exhibiting various time scales in a multivariate\u000a        analysis.\u000a    \u000a\u000a3. Teixeira, C.A., Direito, B., Feldwisch, H., Valderrama, M., Costa,\u000a      R.P., Alvarado-Rojas, C., Nikolopoulos, S., Le Van Quyen, M., Timmer, J.,\u000a      Schelter, B., Dourado, A.. EPILAB: A software package for\u000a        studies on the prediction of epileptic seizures. J. Neurosci. Meth.\u000a      200, 2011, 257- 271. In this publication, we introduce EPILAB,\u000a        describing its features and content. In a small study we show how EPILAB\u000a        advances seizure prediction studies.\u000a    \u000a\u000a4. Feldwisch, H., Schulze-Bonhage, A., Timmer, J., Schelter, B.\u000a        Statistical validation of event predictors: A comparative study based on\u000a        the field of seizure prediction. Phys. Rev. E 83, 2011,\u000a      066704, and introduces the statistics that played a key role for the\u000a        database and evaluation of prediction performance. We demonstrate the\u000a        superiority of the approach followed in Aberdeen.\u000a    \u000a\u000a5. Klatt, J., Feldwisch, H., Ihle, M., Navarro, V., Neufang, M.,\u000a      Teixeira, C., Adam, C., Valderrama, M., Alvarado-Rojas, C., Witon, A., Le\u000a      Van Quyen, M., Sales, F., Dourado, A., Timmer, J., Schulze-Bonhage, A., Schelter,\u000a      B. The EPILEPSIAE database: An extensive EEG database of epilepsy\u000a        patients. Epilepsia 9, 2012, 1669-1676. In this\u000a        publication, we demonstrate the content of the database and show its\u000a        uniqueness.\u000a    \u000a\u000a6. Ihle, M., Feldwisch, H., Teixeira, C.A., Witon, A., Schelter,\u000a      B., Timmer, J., Schulze-Bonhage, A.. EPILEPSIAE - A European\u000a        epilepsy database. Comp. Meth. Prog. Biomed. 106, 2012,\u000a      127-138. In this publication, we introduce the database. This paper in\u000a        particular focuses on the structure and design of the database.\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"11","Level2":"9","Subject":"Neurosciences"},{"Level1":"1","Level2":"4","Subject":"Statistics"}],"Sources":"\u000a    [c1]: A contact at the Centre for Informatics and Systems (CISUC),\u000a      University of Coimbra, Portugal verifies the number of users of EPILAB.\u000a    [c2]: A source at the Dept. of Physics, University of Freiburg, Germany,\u000a      confirms the importance of the database, and verifies the economic impact\u000a      to the consortium as well as the importance of Brainatics.\u000a    [c3]: The Technical Director, Micromed S.p.a., Italy, confirms the\u000a      involvement of Dr. Schelter and the EPIELSPIAE consortium in the\u000a      development of the LTM-EU, its testing in clinical environments and its\u000a      impact for the company including the CE certification.\u000a    [c4]: http:\/\/www.micromed.eu\/pdf\/11-1-BRAIN_QUICK_LTM_ENG_3.01_web.pdf.\u000a      This source documents the specifications of the LTM devices.\u000a    [c5]: http:\/\/www.epilepsiae.eu\/project_outputs\/brainatics.\u000a      This source describes the Brainatics device from the EPILEPSIAE\u000a        consortium's perspective.\u000a    [c6] A source at the Alliance for Epilepsy Research, USA, documents the\u000a      importance of the database from a patient organisation's perspective.\u000a    [c7]: http:\/\/epilepsy-database.eu\u000a      This source corroborates the commercial availability of the database.\u000a    \u000a    ","Title":"\u000a    Brainatics: A Unique Approach to Seizure Prediction in Epilepsy\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Seizure prediction in epilepsy is presently at the forefront of research\u000a      in neuroscience. In a series of biannual workshops on seizure prediction\u000a      running since 2000, four open key problems have been identified: (i)\u000a      improvement of advanced signal (pre-) processing of electroencephalography\u000a      (EEG) signals, i.e. measurements of brain activity; (ii) as epilepsy is\u000a      believed to be a network phenomenon, quantification of the interactions\u000a      between brain regions by means of interdependence measures; (iii) rigorous\u000a      evaluation of the performance of seizure prediction algorithms based on\u000a      large comprehensive data sets, and (iv) the need for a comprehensive\u000a      database as a requirement to address issue (iii), and with obvious\u000a      benefits to other issues in research on epilepsy. Researchers (Thiel\u000a      and co-workers) at IPAM have investigated issues (i)- (iii) since 2008. Schelter\u000a      became a member of staff in 2010, taking over the lead in nonlinear data\u000a      analysis.\u000a    Schelter being part of the consortium EPILEPSIAE consisting of\u000a      academic and clinical partners from Paris (France), Coimbra (Portugal),\u000a      and Freiburg (Germany), and the company Micromed (Italy) took over the\u000a      lead of the IPAM research in epilepsy in 2009\/10. The research in pure and\u000a      applied mathematics of Thiel and Schelter has resulted in\u000a      the development of a long-term patient monitoring device called LTM-EU in\u000a      2010, in addition to the creation of the world's largest epilepsy\u000a      database, thereby addressing issue (iv).\u000a    Between 2008 and 2012, IPAM carried out research to address issue (i),\u000a      namely the improvement of the EEG signal pre-processing. In epilepsy\u000a      monitoring, continuous acquisition of data is necessary as seizures can\u000a      occur at any time. A seizure predictor that is too sensitive to artefacts,\u000a      or which is hampered by natural changes in brain activity (e.g. during\u000a      sleep), would be of little value, practically. One of our crucial\u000a      contributions was the development, between 2010 and 2012, of a\u000a      time-resolved estimation of so-called autoregressive processes that are\u000a      used as an effective model for a signal. Our contribution [1] serves three\u000a      purposes. First, by using so-called state space models in the estimation\u000a      process, we are able to separate the signals of interest from the\u000a      observational noise. Second, related to the removal of observational noise\u000a      in our estimation procedure, signal outliers and artefacts that occur in\u000a      long-term EEG recordings are inherently removed to a large extent. Third,\u000a      in the estimation process the parameters of the autoregressive models are\u000a      allowed to change over time. This enabled us to track the changes in brain\u000a      activity, e.g. sleep transitions, etc. It is currently under investigation\u000a      by neuroscientists to assess to what extent our approach advances seizure\u000a      prediction performance. It outperforms standard approaches based on, for\u000a      example, ordinary filtering [1].\u000a    There is growing evidence that epilepsy is a network phenomenon. Issue\u000a      (ii) addresses this through the quantification of interactions between\u000a      components of dynamical systems, in this case different regions in the\u000a      brain. This presents a challenge directly related to the research of\u000a      several members of IPAM [1, 2]. Of particular interest for epilepsy\u000a      research are measures for synchronisation and measures for the directed\u000a      information flow between signals. During an epileptic seizure, various\u000a      brain regions synchronise their activity, often elicited by a small brain\u000a      region called the epileptic focus. The location of the focus changes from\u000a      patient to patient. Research during 2009\/2010 at IPAM has led to the\u000a      development of a new measure for synchronisation. In contrast to previous\u000a      measures, which assume a single dominant oscillation in the signal, our\u000a      new multivariate approach allows us to cope with multiple such\u000a      oscillations in the signal as typically expected in brain activity [2]. In\u000a      particular during a seizure, but also during sleep and transitions between\u000a      the states, multiple oscillations characterise the signal. Only IPAM's new\u000a      technique enables a reliable estimation of the network synchronisation.\u000a      The state space modelling that enables the pre-processing of the data,\u000a      developed in the IPAM between 2010 and 2012, also enables to infer\u000a      directed information flow between brain regions [1]. The synchronisation\u000a      analysis and the state space modelling and a combination of these\u000a      approaches provided by IPAM turned out to be very successful at predicting\u000a      epileptic seizures from signal analysis when tested 2012. They show a\u000a      statistical significant prediction performance, although not yet good\u000a      enough to be clinically relevant [unpublished results, in preparation].\u000a    In the 1990s, several groups made overoptimistic claims about seizure\u000a      prediction performance. This was due to the fact that the statistical\u000a      evaluation of seizure prediction performance poses certain challenges,\u000a      which renders standard statistical tools useless. For seizure prediction a\u000a      statistical evaluation should:\u000a    a) evaluate not only the sensitivity but also the specificity of the\u000a      prediction performance - seizures, and only seizures, should be predicted;\u000a    b) account for a time window between the prediction itself and the\u000a      seizure onset to render an intervention possible &#8212; ultimately the goal of\u000a      seizure prediction is to provide novel therapeutic interventions such as\u000a      the application of brain stimulation or the application of highly\u000a      effective drugs locally into the brain which need some time to become\u000a      effective; and\u000a    c) include a second time window that constrains the time interval during\u000a      which the seizure needs to start &#8212; interventions are only effective for\u000a      limited time.\u000a    Points (a) &#8212; (c) actually depend on the type of intervention. A locally\u000a      applied drug needs a different intervention time than a warning via a\u000a      mobile phone for instance. Early approaches to seizure prediction did not\u000a      take all this into account. Only in the 2000's, first approaches for the\u000a      evaluation of seizure prediction performance were suggested accounting for\u000a      some of the features (a) to (c); they were based on Monte-Carlo\u000a      algorithms, a numerically rather demanding approach. Research at IPAM\u000a      between 2008 and 2011 [3, 4] has investigated a newly developed analytical\u000a      approach. This analytic approach is based on a so-called random predictor,\u000a      a predictor that predicts seizures at random without using the actual EEG\u000a      measurements by raising alarms at a fixed mean rate. A Poisson process\u000a      results as each point in time has the same probability for an `alarm'.\u000a      Accounting for (a)-(c), it can be compared to any seizure prediction\u000a      algorithm. The statistical characteristics of the random predictor are\u000a      known analytically; thus it has become possible to evaluate the prediction\u000a      performance in a numerically efficient way. In terms of size and coverage,\u000a      our analytic algorithm outperforms the Monte-Carlo based ones [4] and is\u000a      numerically much more efficient.\u000a    "},{"CaseStudyId":"43315","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"2921044","Name":"Germany"},{"GeoNamesId":"2802361","Name":"Belgium"},{"GeoNamesId":"1814991","Name":"China"}],"Funders":[],"ImpactDetails":"\u000a    By 2012, the algorithms developed in IPAM based on the fundamental\u000a      research described above,\u000a      provided a time-resolved estimation of the directed interactions between\u000a      brain structures [2, 3]. In a\u000a      preclinical application, analysing data that had been obtained by\u000a      Professor Bettina Platt at the\u000a      Institute for Medical Sciences (IMS) in Aberdeen, we could clearly\u000a      demonstrate in [2, 3 and further\u000a      unpublished results] the implications of our novel approach to dementia\u000a      research and, potentially to\u000a      early diagnosis. Transitions between sleep stages (see above), which are\u000a      critical in dementia,\u000a      could be defined with a much higher temporal precision than before.\u000a      Standard approaches had\u000a      been allowing a resolution of around 4 seconds, whereas our new data-based\u000a      modelling approach\u000a      yields a resolution of typically 5ms. Additionally, we demonstrated\u000a      through a pilot study that our\u000a      theory and the corresponding algorithms outperform standard EEG based\u000a      spectral analyses\u000a      (unpublished results, [1]).\u000a    Between 2012 and early 2013, the promising findings of our pilot study\u000a      led IPAM, under the lead of\u000a      Thiel and Schelter, to build the mathematical algorithm\u000a      into a software package which was\u000a      integrated as part of in an online platform marketed commercially by\u000a      BrainMarker BV in May 2013.\u000a    BrainMarker is a Dutch company whose vision is to \"provide the gold\u000a        standard in mental healthcare\u000a        by implementing an easy to use decision support and quality management\u000a        system in clinical\u000a        practice\" [c1]. They aim to bridge the gap between scientific\u000a      knowledge and clinical practice \"for all\u000a        lines of mental healthcare by implementing quantitative EEG (qEEG)\u000a        measurements in a very\u000a        user-friendly way into practices. This allows a swift knowledge transfer\u000a        from the scientific\u000a        community into the clinical practice\" [c1].\u000a    The software package developed by IPAM fits well into the vision and\u000a      mission of BrainMarker. Via\u000a      our joint collaboration partner Professor Platt, we began to discuss the\u000a      potential exploitation of our\u000a      algorithm through the BrainMarker Platform in 2012. This resulted in a\u000a      sustained and fruitful\u000a      collaboration between researchers of the IPAM and IMS on the one side, and\u000a      the management\u000a      and computer engineers of the company BrainMarker on the other.\u000a    Parallel to the development of the software package, extensive\u000a      beta-testing of the software was\u000a      carried out in applications to clinical data obtained by groups at the IMS\u000a      at Aberdeen [2, 3], the\u000a      Xi'an Jiaotong University (China), and a Neurologist Practitioner in\u000a      Freiburg, Germany. In the beta-testing,\u000a      our algorithm was applied by the team of medical specialists at X'ian\u000a      Jiaotong to study\u000a      various cognitive deficits [c2]. The Director of the Institute of\u000a      Biomedical Engineering, Xi'an\u000a      Jiaotong University, states \"We can confirm that this novel technique\u000a        presents a milestone in data-based\u000a        modelling and model-based data analysis\" and \"The new insights we\u000a        gained by this\u000a        technique that would have been impossible before enabled us to prepare a\u000a        manuscript to publish\u000a        our results in a high ranked international journal\" [c2]. The\u000a      Practitioner in Freiburgh has applied it to\u000a      research in Parkinson's disease [c3], and has stated: \"Based on my\u000a        experience as a neurologist, I\u000a        can confirm that the results I have obtained using this unique technique\u000a        and looked at so far are\u000a        very promising; I truly believe that this technique has the potential to\u000a        provide the means for an\u000a        early diagnosis of diseases like dementia\" [c3].\u000a    Given these facts, BrainMarker decided in early 2013 to offer\u000a      practitioners access to the algorithm\u000a      developed by IPAM, via their subscription-based online platform programmed\u000a      in LabVIEW\u000a      (National Instruments, http:\/\/uk.ni.com).\u000a      According to the Managing Director of BrainMarker, \"the\u000a        system has already been used in over 35 practices in the Netherlands,\u000a        Germany, and Belgium and\u000a        continues to grow. Among their users are hospitals and research\u000a        institutions that have enabled\u000a        them to expand their database of human EEGs in various pathologies\"\u000a      [c4]. The revenue created\u000a      through the IPAM research is substantial. Access to the pay-per-click\u000a      software platform of\u000a      BrainMarker, of which Thiel and Schelter's algorithm is\u000a      one part, \"costs &#8364;250 per month\u000a      (hardware, software, technical support) and &#8364;5 per measurement for a\u000a      clinic\" [c4].\u000a    The service provides different environments for researchers and for\u000a      practitioners who upload their\u000a      EEG files and receive an analysis report of the data. The advantage of\u000a      this approach is that it\u000a      delivers new and sophisticated health-care methods to practitioners\u000a      without requiring them to buy\u000a      expensive hardware. Also any updates to the software, including\u000a      improvement of algorithms, are\u000a      immediately available to customers. It is an ideal platform for an\u000a      optimised knowledge transfer\u000a      between academia and the health industry.\u000a    The Managing Director of BrainMarker BV considers Aberdeen's contribution\u000a      to have been \"a very\u000a        valuable component of [their] portfolio\" and that it adds \"considerably\u000a        to its functionality\" [c4]. In\u000a      summary, the software framework developed by IPAM became in 2013 one of\u000a      the key components\u000a      in BrainMarker's platform to tackle dementia in Europe. Its impact is both\u000a      commercial, and will\u000a      potentially change procedures carried out by clinical practitioners. There\u000a      is also strong potential for\u000a      growth globally as the relationship with Brainmarker continues.\u000a      BrainMarker anticipates that \"[this]\u000a        highly competitive online system provides a novel approach to health\u000a        care not only in Europe\" [c4].\u000a    Other companies have also become aware of the software tool. Companies\u000a      such as AbbVie\u000a      expressed their interest in continuing \"the previously started common\u000a        efforts on pharmaco-EEG\u000a        analysis together with Aberdeen University\". This particularly\u000a      includes our novel data-based\u000a      modelling approaches [c5].\u000a    ","ImpactSummary":"\u000a    Alzheimer's disease is the most common form of dementia, with a cost to\u000a      society estimated at\u000a      &#8364;177 billion per annum across Europe, according to the European\u000a      Collaboration on Dementia\u000a      (EuroCoDe) project funded by Alzheimer Europe. Data-based modelling of\u000a      network structures is a\u000a      modern approach to study and understand many diseases including dementia.\u000a      Research carried\u000a      out at the Institute of Pure and Applied Mathematics (IPAM) at the\u000a      University of Aberdeen has led\u000a      to the development, implementation, and testing of novel mathematical\u000a      algorithms to infer network\u000a      structures by means of observations of their dynamics. The results of our\u000a      research have been\u000a      implemented as part of a software package now offered by the\u000a      Netherlands-based company\u000a      BrainMarker to researchers and practitioners across Europe in an online\u000a      `pay-per-click' platform\u000a      (section 5.c1 and 5.c4). As such our research generated impact on clinical\u000a      practitioners in addition\u000a      to commercial impact.\u000a    ","ImpactType":"Technological","Institution":"\u000a    University of Aberdeen\u000a    ","Institutions":[{"AlternativeName":"Aberdeen (University of)","InstitutionName":"University of Aberdeen","PeerGroup":"B","Region":"Scotland","UKPRN":10007783}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2925177","Name":"Freiburg"}],"References":"\u000a    Researchers from IPAM in bold, lead author named last.\u000a    \u000a[1] Ramb, R., Eichler, M., Ing, A., Thiel, M., Grebogi,\u000a      C., Schwarzbauer, Ch., Timmer, J.,\u000a      Schelter, B. The impact of latent confounders in directed\u000a        network analysis in neuroscience.\u000a      Phil. Trans. A 371, 2013, 20110612 (submitted 2012).\u000a      In this publication, we investigated the role of latent confounders and\u000a        developed an algorithm to\u000a        identify these. This is vital for investigating the \"true\" underlying\u000a        network structure to avoid false\u000a        positive conclusions.\u000a    \u000a\u000a[2] Sommerlade. L., Thiel, M., Grebogi, C.,\u000a      Platt, B., Plano, A., Riedel, G., Timmer, J., Schelter,\u000a      B. Time-Variant Estimation of Connectivity and Kalman Filter,\u000a      Taylor and Francis, 2013, eds.\u000a      L.A. Baccala and K. Sameshima (submitted in 2011).\u000a      Here, we particularly investigated the superiority of the presented\u000a        approach to more\u000a        conventional approaches. We clearly show that the developed novel\u000a        technique outperforms\u000a        conventional approaches.\u000a    \u000a\u000a[3] Sommerlade, L., Thiel, M., Platt, B., Plano, A.,\u000a      Riedel, G., Grebogi, C., Timmer, J., Schelter,\u000a      B. Inference of Granger causal time-dependent influences in noisy\u000a        multivariate time series. J.\u000a      Neuroscience Methods 203, 2012, 173-185\u000a      This is the key publication for the Impact Case Study. It presents for\u000a        the first time the novel\u000a        mathematical algorithm and its implementation that is key for\u000a        investigating the complete 3\u000a        dimensional picture of time, frequency and strength of an interaction.\u000a    \u000a\u000a[4] Lenz, M., Musso, M., Linke, Y., T&#252;scher, O., Timmer, J., Weiller, C.,\u000a      Schelter, B. Joint\u000a        EEG\/fMRI state space model for the detection of directed\u000a        interactions in human brains.\u000a      Physiological Measurements 32, 2011, 1725-1736\u000a      This publication investigates the role of the state space model. The\u000a        superior information of EEG\u000a        data over other common approaches to observe brain dynamics was\u000a        investigated and shown.\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"11","Level2":"9","Subject":"Neurosciences"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000a    [c1] http:\/\/www.brainmarker.com\/en\/about-us\/94-vision-and-mission\u000a      This source confirms the approach to cutting-edge health care followed\u000a        by BrainMarker.\u000a    [c2] The Director of the Institute of Biomedical Engineering, Xi'an\u000a      Jiaotong University, Xi'an,\u000a      Shaanxi, P.R. China, can corroborate the successful beta-testing of the\u000a      software tool.\u000a    [c3] A Neurologist (practitioner), Freiburg, Germany who can confirm the\u000a      importance of the\u000a      software tool for various diseases and its importance for dementia\u000a      patients from a\u000a      practitioner's (neurologist) perspective.\u000a    [c4] The Managing Director of BrainMarker BV, Netherlands can confirm the\u000a      role of the software\u000a      tool in BrainMarker's online platform; and corroborates the economic\u000a      impact to the company.\u000a    [c5] The Associate Director of AbbVie Deutschland GmbH &amp; Co. KG,\u000a      Germany can demonstrate\u000a      the interest of other global companies in IPAM's software tools.\u000a    \u000a    ","Title":"\u000a    A Mathematical Algorithm to Improve Diagnosis of Dementia\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The concept of networks has become pivotal in many fields of research\u000a      including the biological\u000a      sciences. When analysing networks in neuroscience, a notoriously difficult\u000a      task known as the\u000a      \"inverse problem\" is faced. From measured time series of processes in the\u000a      network, conclusions\u000a      on the underlying system are sought. Of particular interest is the\u000a      estimation of the interrelations\u000a      between the processes and their directions.\u000a    The human brain is a prototypical example of a highly complex network. Of\u000a      particular interest for\u000a      the human brain is the study of neuronal oscillators reflecting brain\u000a      activity. Measurement of\u000a      electrical activities in the brain by means of electroencephalography\u000a      (EEG) recordings is routinely\u000a      used in the diagnosis, the prediction of progression and the effectiveness\u000a      of potential therapeutic\u000a      interventions for various diseases. As an illustration, relevant to this\u000a      case study, diseases that\u000a      affect the human brain and the central nervous system are typically\u000a      related to pathological changes\u000a      in the brain network structure. In dementia, brain regions such as the\u000a      hippocampus and the\u000a      prefrontal cortex and their interactions are believed to have an important\u000a      role in our understanding\u000a      of the underlying mechanisms. One of the goals in the analysis of the EEG\u000a      signals is to obtain\u000a      information on the direction and strength of the interactions between\u000a      different parts of the brain and\u000a      to obtain statistically significant results. Monitored over time,\u000a      inference about these interactions\u000a      may lead to novel biomarkers for an early detection of dementia.\u000a    In practice, the presence of noise in measurements is one obstacle to the\u000a      achievement of this goal.\u000a      Another is the presence of rapid changes in the system's parameters, which\u000a      are extremely\u000a      important in the analysis of the network but are computationally a major\u000a      challenge as we explain\u000a      below. As an example, consider human sleep with its different stages\u000a      including Rapid Eye\u000a      Movement (REM) sleep (the dreaming phase), which is important for\u000a      dementia. The transition into\u000a      REM sleep occurs in the order of seconds, and is marked by a significant\u000a      change in the electric\u000a      activity in the brain.\u000a    Several time series analysis techniques have been suggested to analyse\u000a      interactions between\u000a      processes. However none of which was able to provide a complete picture.\u000a      For example, the\u000a      directed transfer function (DTF) has been widely applied in neuroscience\u000a      research since the early\u000a      90s; it provides the direction of information flow, but it cannot\u000a      distinguish direct from indirect\u000a      interactions. This inevitably leads to detection of spurious directed\u000a      interactions in the network,\u000a      which hampers the interpretability of the results. Other multivariate\u000a      approaches such as \"partial\u000a      correlation\" and \"partial (cross-)spectral analysis\" and \"partial phase\u000a      synchronisation analysis\"\u000a      promise to distinguish between direct and indirect interactions in\u000a      networks but do not provide\u000a      information about the direction of the interactions. The \"renormalised\u000a      partial direct coherence\u000a      method\" previously developed by Schelter (and collaborators) gives\u000a      information on the strength of\u000a      interactions, their direction, and can distinguish direct from indirect\u000a      interactions, however is\u000a      inherently incapable of coping with observational noise. Basically\u000a      applied, it cannot detect rapid\u000a      changes in the interaction parameters, which is a serious deficiency,\u000a      especially in light of the\u000a      importance of the transition between sleeping stages described above to\u000a      the study of dementia.\u000a      None of these techniques, or others that were available prior to the\u000a      research described below\u000a      carried out in IPAM, could provide the complete picture about the\u000a      frequency, strength, and the\u000a      direction of the direct interactions in a network of potentially\u000a      non-stationary, noisy, non-linear\u000a      systems.\u000a    Between 2010 and 2012 Thiel, Schelter and Grebogi\u000a      developed a new mathematical and\u000a      statistical theory to infer the connection topology of coupled complex\u000a      systems from observations of\u000a      their node dynamics alone [1]. No prior information or assumptions on the\u000a      interactions is needed in\u000a      contrast to many other approaches. The mathematical framework that has\u000a      been developed by\u000a      IPAM provides the complete picture in connection to the parameters\u000a      mentioned above and was\u000a      demonstrated to be superior to previously existing methods [2, 3]. The\u000a      mathematical theory is\u000a      based on nonlinear state space models. The estimation of the parameters\u000a      that characterise the\u000a      network's structure is based on the expectation-maximisation algorithm,\u000a      which maximises the\u000a      incomplete data likelihood in an iterative procedure, and requires\u000a      application of an improved\u000a      version of the dual Kalman filter that we developed [2, 3]. The new\u000a      technique called the \"time-\u000a      resolved renormalised partial directed coherence\", gives a time-resolved\u000a      estimation of the direct,\u000a      directed interaction structure for stochastic linear as well as non-linear\u000a      systems in the presence of\u000a      observational noise. Thus, our novel approach enables the inference of the\u000a      `true' network structure\u000a      underlying noisy data, making little assumption about the type of signals\u000a      or signal quality. Its ability\u000a      to infer the network structure with temporal resolution only limited by\u000a      the sampling rate promises to\u000a      gain new insights into potentially rapidly changing signals or interaction\u000a      structures.\u000a    "},{"CaseStudyId":"43438","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3017382","Name":"France"}],"Funders":["Research Councils UK","Biotechnology and Biological Sciences Research Council","Engineering and Physical Sciences Research Council","Royal Society"],"ImpactDetails":"\u000d\u000a    The purpose of the Food and Environment Research Agency (Fera) is \"to\u000d\u000a      support and develop a sustainable food chain, a healthy natural\u000d\u000a      environment, and to protect the global community from biological and\u000d\u000a      chemical risks\" [5]. It has over 7,500 government and commercial customers\u000d\u000a      and provides services to customers in over 100 countries. As a government\u000d\u000a      agency dealing with food safety and environmental issues, Fera is\u000d\u000a      immediately involved in disease outbreaks, such as foot and mouth disease\u000d\u000a      in cattle, and food contamination threats around the world. As a result of\u000d\u000a      Wilson's work on chemometric methods, Fera scientists are now able to\u000d\u000a      apply these underpinning mathematical techniques to a wide range of\u000d\u000a      applications, allowing them to offer a more effective service to their\u000d\u000a      customers and respond more rapidly to such outbreaks and threats.\u000d\u000a    Wilson's work with Fera began at the initiation of their\u000d\u000a      metabolomics programme, giving them access to state of the art chemometric\u000d\u000a      algorithms. This has resulted in Fera securing projects totalling a value\u000d\u000a      of &#163;8M to date from Defra, the Food Standards Agency, the European\u000d\u000a      Commission and BBSRC [6]. Applications are wide ranging with examples\u000d\u000a      including the determination of disease-related biomarkers, contaminant\u000d\u000a      detection, food traceability and the development of drought and disease\u000d\u000a      resistant crop varieties. Many applications require a non-targeted\u000d\u000a      approach, which relies on the ability to identify consistent differences\u000d\u000a      between groups (for example between diseased and healthy animals). The new\u000d\u000a      feature extraction methods significantly reduce the within-class variance\u000d\u000a      that can mask these differences, thus revealing biochemical signals that\u000d\u000a      might otherwise have been missed.\u000d\u000a    As part of their programme Fera have invested in the development of Metabolab,\u000d\u000a      a bespoke, modular Matlab based software package [7], which incorporates Wilson's\u000d\u000a      algorithms and also allows them to quickly and flexibly implement new\u000d\u000a      algorithms as they emerge from research. Metabolab allows the techniques\u000d\u000a      to be used by non-experts, and the software is now used routinely in the\u000d\u000a      Chemical and Biochemical Profiling section at Fera for the processing of\u000d\u000a      metabolomic datasets. Designed to efficiently process the extremely large\u000d\u000a      data sets typically required to analyse two-dimensional spectra, this\u000d\u000a      software gives Fera the competitive advantage of being able to use the new\u000d\u000a      algorithms make use of highly resolved, and therefore more informative,\u000d\u000a      2-D NMR experiments for routine metabolomics studies.\u000d\u000a    Disease-related Biomarkers:\u000d\u000a    Using a simple blood test the chemometric techniques can be used to\u000d\u000a      identify biomarkers to detect diseased, and therefore infectious, animals\u000d\u000a      before physical signs are apparent. This is used to identify and\u000d\u000a      distinguish many high profile diseases such as BSE in cattle, TB in\u000d\u000a      badgers, foot and mouth disease, and various plant diseases. Dr Adrian\u000d\u000a      Charlton, Head of Chemical and Biochemical Profiling at Fera, leads a\u000d\u000a      research team that provides novel solutions to problems of food\u000d\u000a      contamination and authentication. He says \"Of particular note was the\u000d\u000a      contribution that the adaptive binning and 2 stage GP algorithms made to\u000d\u000a      the delivery of a &#163;1.7M project for the Food Standards Agency (FSA),\u000d\u000a      investigating the determination of novel biomarkers of BSE and scrapie\"\u000d\u000a      [6]. Bovine spongiform encephalopathy (BSE), commonly known as mad cow\u000d\u000a      disease, is characterized by spongy degeneration of the brain in cattle\u000d\u000a      with a variant in humans called Creutzfeldt-Jakob disease (CJD). As part\u000d\u000a      of this project, a workshop was hosted by the University of York to advise\u000d\u000a      the project team, with scientists from Fera, the Veterinary Laboratories\u000d\u000a      Agency (VLA) and the Institute for Grasslands and Environmental Research\u000d\u000a      (IGER), on the correct implementation of Wilson's novel approaches\u000d\u000a      as well as other multivariate analysis techniques for application into\u000d\u000a      other areas.\u000d\u000a    Food Traceability:\u000d\u000a    At European level, the genetic programming approaches developed by Wilson\u000d\u000a      were used to underpin a &#8364;15M FP6 project (TRACE) to `provide consumers\u000d\u000a      with added confidence in the authenticity of European food through\u000d\u000a      complete traceability along entire fork-to-farm food chains' [8]. The\u000d\u000a      project was coordinated by Fera and utilized a range of analytic tools\u000d\u000a      that make use of the computational techniques developed by Wilson's\u000d\u000a      team at the University of York [9]. The methods enable molecular\u000d\u000a      fingerprinting to be used to determine the origin of food. Products to\u000d\u000a      which TRACE's methods have been applied include European mineral water,\u000d\u000a      cereals, honey, meat and chicken [5,8,10]. For example, Corsican honey is\u000d\u000a      the only one produced in France that carries the prestigious Appellation\u000d\u000a      of Controlled Origin designation (AOC label). As a result of the new\u000d\u000a      methods, it is now possible to use a number of chemical markers to make\u000d\u000a      fine geographical distinctions between different origins and content of\u000d\u000a      honey, with widely differing prices [10].\u000d\u000a    The research has been featured repeatedly in New Scientist [11]\u000d\u000a      and the results from the TRACE project have been disseminated in over 200\u000d\u000a      presentations and workshops worldwide to an enormous range of participants\u000d\u000a      from industry [8]. In 2012 Wilson was an invited speaker at New\u000a        developments in food science: realising the potential of 'omics'\u000d\u000a        technologies, the 13th annual joint symposium in 2012 of\u000d\u000a      FERA and the US Joint Institute for Food Safety and Applied Nutrition. The\u000d\u000a      meeting's sponsors included Agilent, Thermo Scientific, AB Sciex and\u000d\u000a      Waters.\u000d\u000a    Contaminant Detection:\u000d\u000a    The methods have also enabled significant improvements in procedures for\u000d\u000a      the detection of contaminants. In this case the differences from what is\u000d\u000a      considered normal need to be recognized, as any extraneous variance could\u000d\u000a      result in false negatives. Some toxins can be lethal at extremely low\u000d\u000a      concentrations. The new techniques allow compounds that may only occur at\u000d\u000a      lower concentration, and which may have been obscured in variance-based\u000d\u000a      multivariate analyses, to be identified (e.g. melamine in milk and\u000d\u000a      infant formula). Furthermore, the variables relate to peaks in the spectra\u000d\u000a      rather than individual data points, thereby making it easier to interpret\u000d\u000a      the results and thus identify the chemical compounds responsible.\u000d\u000a    Disease Resistant Crops:\u000d\u000a    Most recently Fera have won a &#8364;3M, 5 year project from the European\u000d\u000a      Commission (ABSTRESS) [12], which is further exploiting and continuing to\u000d\u000a      develop the technologies arising from the collaboration with Wilson.\u000d\u000a      The project aims to identify the processes in plant biochemistry\u000d\u000a      associated with the way drought and disease combine to make matters much\u000d\u000a      worse than either alone. Building on the information available from\u000d\u000a      chemometric techniques researchers are developing novel principles and\u000d\u000a      techniques that can be used to significantly reduce the time taken to\u000d\u000a      produce new crop varieties in support of commercial plant breeding. This\u000d\u000a      should produce new crop varieties that are more able to withstand the\u000d\u000a      challenges commonly associated with climate change, such as extreme\u000d\u000a      weather and changing incidence of pests and diseases. Although the\u000d\u000a      University of York is not a partner in ABSTRESS, Fera have sponsored an\u000d\u000a      EngD studentship, co-supervised by Wilson, on the integration of\u000d\u000a      data from the different &#8212; omics technologies being used in the project.\u000d\u000a    In addition to the EngD, the collaboration with Fera has led to funding\u000d\u000a      for two PhD students: Richard Davis held an EPSRC CASE studentship with\u000d\u000a      Fera (then CSL) and James McKenzie had Fera seedcorn funding.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Recent food crises show the importance of having effective means of food\u000d\u000a      identification and analysis. Many tests have been developed to monitor\u000d\u000a      food, but analysis of the resulting data is highly problematic.\u000d\u000a      Mathematical techniques developed by Dr Julie Wilson at the University of\u000d\u000a      York allow complex mixtures to be analysed and interpreted. They have\u000d\u000a      enabled the Food and Environment Research Agency (Fera) to maximize the\u000d\u000a      information available from food testing, resulting in improved food safety\u000d\u000a      and authentication worldwide, and underpin the analytical testing services\u000d\u000a      delivered by Fera. The techniques have been incorporated into a bespoke\u000d\u000a      Matlab based solution which is now routinely used by Fera's Chemical\u000d\u000a        and Biochemical Profiling section in the specialist testing services\u000d\u000a      which Fera provides across the food storage and retail, agri-environment\u000d\u000a      and veterinary sectors to over 7,500 customers in over 100 countries. In\u000d\u000a      addition, the techniques are used in Fera's research, supporting around\u000d\u000a      &#163;8M worth of work to develop a wide range of global applications including\u000d\u000a      the determination of disease-related biomarkers, contaminant detection,\u000d\u000a      food traceability and the development of drought- and disease-resistant\u000d\u000a      crop varieties.\u000d\u000a    ","ImpactType":"Technological","Institution":"\u000d\u000a    University of York\u000d\u000a    ","Institutions":[{"AlternativeName":"York (University of)","InstitutionName":"University of York","PeerGroup":"B","Region":"Yorkshire And Humberside","UKPRN":10007167}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"3023519","Name":"Corse"}],"References":"\u000d\u000a    \u000a[1] R. Davis, A. Charlton, S. Oehlschlager and J. C. Wilson.\u000d\u000a      Novel feature selection method for genetic programming using 1H NMR data.\u000d\u000a      Chemom. Intell. Lab. Syst. 81 (2006) 50-59. doi:10.1016\/j.chemolab.2005.09.006\u000d\u000a    \u000a\u000a*[2] R. A. Davis, A. J. Charlton, J. Godward, S. A. Jones, M. Harrison\u000d\u000a      and J. C. Wilson. Adaptive Binning: An Improved Binning Method for\u000d\u000a      Metabolomics Data Using the Undecimated Wavelet Transform. Chemom.\u000d\u000a        Intell. Lab. Sys. 85 (2007) 144-154. doi:10.1016\/j.chemolab.2006.08.014\u000d\u000a    \u000a\u000a*[3] J. S. McKenzie, A. J. Charlton, J. Donarski, J. C. Wilson.\u000d\u000a      Peak Fitting in 2D 1H-13C HSQC NMR Spectra for Metabolomic Studies. Metabolomics,\u000d\u000a      6 (2010) 574-582. doi:\u000a        10.1007\/s11306-010-0226-7\u000d\u000a    \u000a\u000a*[4] S. Poulding, A. J Charlton, J. Donarski and J. C Wilson.\u000d\u000a      Removal of t1 Noise from 2D 1H-13C HSQC NMR Spectra by Correlated Trace\u000d\u000a      Denoising, J. Mag. Res. 189 (2007) 190-199.\u000d\u000a      doi:10.1016\/j.jmr.2007.09.004\u000d\u000a    \u000aChemometrics and Intelligent Laboratory Systems publishes `novel\u000d\u000a      developments in techniques ... characterized by ... statistical and\u000d\u000a      computer methods'. Metabolomics is the official journal of the\u000d\u000a      Metabolomics Society, and `publishes ... the most significant current\u000d\u000a      research'. The Journal of Magnetic Resonance publishes\u000d\u000a      `significant theoretical and experimental results' in `all aspects of\u000d\u000a      magnetic resonance'. All three are respected international peer-reviewed\u000d\u000a      journals.\u000d\u000a    All research and algorithm development was carried out at the University\u000d\u000a      of York by Wilson and her students Richard Davis, James McKenzie\u000d\u000a      and Simon Poulding; other authors above are Fera scientists, who provided\u000d\u000a      the data and integrated the techniques into Matlab software.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"11","Level2":"9","Subject":"Neurosciences"},{"Level1":"6","Level2":"1","Subject":"Biochemistry and Cell Biology"}],"Sources":"\u000d\u000a    [5] http:\/\/www.fera.defra.gov.uk\/\u000d\u000a      (accessed 15\/10\/2012). Corroborates claim of Fera's purpose.\u000d\u000a    [6] E-mail provided by Head of Chemical and Biochemical Profiling at\u000d\u000a      FERA. Corroborates the value to Fera of project funding and the\u000d\u000a      contribution of the methods developed by Wilson in all projects\u000d\u000a      mentioned.\u000d\u000a    [7] Metabolab software. Corroborates the claim that adaptive\u000d\u000a      binning and the two-stage GP have been incorporated into the software.\u000d\u000a    [8] TRACE events archive: http:\/\/trace.eu.org\/archive\/events.php\u000d\u000a      (accessed 24\/09\/2013). Corroborates the extent to which TRACE results have\u000d\u000a      been disseminated and the level of industrial contacts.\u000d\u000a    [9] Fera Annual Review 2011-12, p10 http:\/\/fera.co.uk\/news\/documents\/feraAnnualReview1112.pdf\u000d\u000a      Corroborates Fera's involvement in food safety and traceability studies.\u000d\u000a    [10] J. A. Donarski, S. A. Jones, A. J. Charlton. Application of\u000d\u000a      Cryoprobe 1H Nuclear Magnetic Resonance Spectroscopy and Multivariate\u000d\u000a      Analysis for the Verification of Corsican Honey. J. Agr. Food\u000d\u000a        Chemistry 56 (2008) 5451; J. A. Donarski, S. A. Jones, M. Harrison,\u000d\u000a      M. Driffield, A. J. Charlton. Identification of botanical biomarkers found\u000d\u000a      in Corsican honey. Food Chemistry 118 (2010) 987-994. Corroborates\u000d\u000a      use of methods in published research, and application to Corsican honey.\u000d\u000a    [11] K. Ravilious, \"Buyer beware; When you shell out for a premium food\u000d\u000a      how do you know you're getting what you pay for?\", New Scientist,\u000d\u000a      11th November 2006, p40-43; M. Inman, \"Fifty ways to\u000d\u000a      interrogate your dinner; To check the credentials of the food you would\u000d\u000a      like to eat, just take your cellphone to the supermarket and snap the\u000d\u000a      barcode\", New Scientist, 13th June 2009, p18-19.\u000d\u000a      Corroborates the reporting of food safety and traceability issues.\u000d\u000a    [12] http:\/\/www.abstress.eu\/\u000d\u000a      (accessed 15\/10\/2012). Corroborates Fera's coordination of the ABSTRESS\u000d\u000a      project.\u000d\u000a    ","Title":"\u000d\u000a    Mathematical methods to improve food safety and traceability\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Julie Wilson is a mathematician who began her career in number\u000d\u000a      theory and, after a Royal Society University Research Fellowship 1999-2007\u000d\u000a      and an RCUK fellowship in chemoinformatics 2007-2010, has held a joint\u000d\u000a      lectureship between the Departments of Mathematics and of Chemistry at the\u000d\u000a      University of York since 2010. Wilson applies a wide range of\u000d\u000a      mathematical and statistical techniques to a variety of scientific and\u000d\u000a      technological problems, primarily in chemometrics. Wilson has been\u000d\u000a      collaborating with Fera and its precursor the CSL (Central Science\u000d\u000a      Laboratory) for around ten years, developing NMR data processing and\u000d\u000a      chemometric techniques to analyse data from food safety and environmental\u000d\u000a      studies. The research described was carried out at the University of York\u000d\u000a      with data provided by Fera.\u000d\u000a    The use of Nuclear Magnetic Resonance (NMR) methods allows the\u000d\u000a      simultaneous identification of a wide range of small molecules, or\u000d\u000a      metabolites, which provide characteristic ``fingerprints'' that detail the\u000d\u000a      relative concentrations of compounds present in a sample. Each sample may\u000d\u000a      produce thousands of data points, requiring peak modelling and other data\u000d\u000a      reduction techniques. Chemometrics applies mathematical methods from\u000d\u000a      statistics and pattern recognition to these metabolomic fingerprints,\u000d\u000a      extracting relevant features that enable samples to be classified,\u000d\u000a      anomalies recognized and markers for different biological states\u000d\u000a      identified.\u000d\u000a    Changes in experimental parameters such as temperature, pH and ionic\u000d\u000a      strength result in unwanted shifts in peak position. It is common practice\u000d\u000a      to accommodate small spectral shift changes by integrating the spectral\u000d\u000a      data over regions of equal length. Uniform binning can dissect NMR\u000d\u000a      resonances or assign multiple peaks to the same bin, adding to the\u000d\u000a      variance and making data interpretation difficult. Wilson designed\u000d\u000a      the adaptive binning algorithm [1] to allow variable-length bins, which\u000d\u000a      correspond directly to peaks in the spectra and thus facilitate\u000d\u000a      interpretation. As noise regions are excluded, the method significantly\u000d\u000a      reduces variation within a biological class (for example, disease state)\u000d\u000a      in comparison to fixed-width binning.\u000d\u000a    Although the use of integrated peaks rather than individual data points\u000d\u000a      reduces the number of variables, the search space in metabolomics studies\u000d\u000a      is still prohibitively large for evolutionary computing methods such as\u000d\u000a      Genetic Programming (GP). However, the advantage of GPs over standard\u000d\u000a      multivariate analyses is that they do not involve a transformation of the\u000d\u000a      variables, and thus produce results that are easier to interpret in terms\u000d\u000a      of the underlying chemistry. Wilson therefore developed a\u000d\u000a      two-stage GP algorithm [2] designed specifically for use with (the\u000d\u000a      one-dimensional) 1H NMR datasets. Computational efficiency is\u000d\u000a      significantly improved by limiting the number of generations in the first\u000d\u000a      stage and only submitting the most discriminatory variables to the second\u000d\u000a      stage, in which the optimal classification solution is sought.\u000d\u000a    Efficient feature extraction is also required to allow two-dimensional\u000d\u000a      NMR techniques, such as Heteronuclear Single Quantum Coherence (HSQC) and\u000d\u000a      Heteronuclear Multiple Bond Correlation (HMBC), to be used in the analysis\u000d\u000a      of complex mixtures. Wilson's feature extraction method uses a\u000d\u000a      modified Lorentzian function to model peaks in 1H-13C\u000d\u000a      HSQC spectra [3] and provides elliptical footprints corresponding to peaks\u000d\u000a      in the spectra. Integrating over these footprints for each spectrum\u000d\u000a      provides a dramatically reduced set of variables that now allows\u000d\u000a      metabolomic analyses to be performed with 2-D spectra.\u000d\u000a    In the specific case of phase-cycled HSQC, systematic noise needs to be\u000d\u000a      removed before feature extraction. Despite its superior sensitivity, this\u000d\u000a      technique has been limited by the presence of noise ridges, which can mask\u000d\u000a      genuine peaks of low-concentration compounds. Wilson's Correlated\u000d\u000a      Trace Denoising (CTD) algorithm [4] takes advantage of the systematic\u000d\u000a      nature of this so-called t1 noise and, unlike other methods for t1 noise\u000d\u000a      removal that have specific pre-requisites, CTD can be used regardless of\u000d\u000a      complexity and the number of peaks in a spectrum, making it suitable for\u000d\u000a      metabolomic studies.\u000d\u000a    "},{"CaseStudyId":"43439","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3175395","Name":"Italy"}],"Funders":["Economic and Social Research Council","Engineering and Physical Sciences Research Council"],"ImpactDetails":"\u000d    4.1 Impact through transport planning software\u000d    Smith has a longstanding relationship with the developers of the\u000d      SATURN (Simulation and Assignment of Traffic in Urban Road Networks)\u000d      software suite, which is now developed and marketed by Atkins Global\u000d      (\"one of the world's leading design, engineering and project management\u000d      consultancies\"). Since 2008 SATURN \"has been utilised to evaluate 80\u000d        significant changes to transport networks in the UK and 20 changes to\u000d        transport networks overseas. It is currently used to some degree by 400\u000d        users in the UK and by 100 users overseas and generates an annual income\u000d        in excess of &#163;500K.\" [7] (95% of the user-base is estimated to be\u000d      non-academic [7]). Smith has a longstanding collaboration with Dr\u000d      Dirck van Vliet, the designer and developer of SATURN; Smith's\u000d      former research assistant Xiang was employed by Atkins\u000d      until recently.\u000d    The impact of papers [1,2] was to convince the SATURN team that Smith's\u000d      policy P0 and developments of \"Algorithm D\" should be incorporated in\u000d      their product. Van Vliet writes in [7] that \"The SATURN model has made\u000d        extensive use of the concept of Social Pressure developed from the\u000d        `Algorithm D' originally proposed in the mid 1980s by Mike\u000d          Smith. This is used in solving the algorithmic problems\u000d        that arise in traffic assignment models where different streams of\u000d        traffic interact with one another (e.g. traffic on a minor arm at a\u000d        T-junction giving way to traffic on a major arm). In addition, SATURN\u000d        contains an implementation of the responsive control policy called P0\u000d        previously proposed by Smith... [The SATURN implementations of\u000d        the late 1990's] were informed and supported by the stability results\u000d        obtained by Smith and van Vuren in [1] and by Smith et\u000d        al in [2]. More generally, without the ideas of Mike the\u000d        convergence of SATURN and its ability to provide a single\u000d        self-consistent answer to traffic problems would be extremely\u000d        restricted. Moreover, his results on the stability of these algorithms\u000d        (going beyond the previously standard work of Frank-Wolfe) allow us to\u000d        market SATURN with confidence. SATURN is under continual development and\u000d        Mike Smith and I have had many conversations about the details of\u000d        the proportional method and about the SATURN Social Pressure\u000d        implementation. His research continues to be a significant influence on\u000d        our thinking.\" Van Vliet's published work acknowledges \"ideas\u000d      pinched from\" Smith.\u000d    A significant application of the SATURN suite, demonstrating its\u000d      flexibility and utility, was in the preparation for the London 2012\u000d      Olympic and Paralympic Games in both \"helping to define [traffic] plans\u000d      around venues\" and \"to provide a strategic overview of the impact of the\u000d      ORN [Olympic Route Network], road events\/management and the venues\u000d      themselves on roads in London\", which was applied in particular to inform\u000d      the \"Get Ahead of the Games\" advertising campaign on likely road impact\u000d      [8]. More generally, Transport for London \"has recently developed a\u000d      comprehensive set of new sub-regional highway assignment models, based on\u000d      the SATURN suite of software\" [9].\u000d    Very recently, the Italian company SISTeMA has stated \"We\u000d        propose to investigate a revised iterative process (using the P0 policy)\u000d        and we are likely to implement it within our software if that\u000d        investigation shows benefits.\" [13] Crucially, their interest arises\u000d      because \"Smith's mathematical analysis suggests that with this policy\u000d        the control\/routeing iterations may be expected to converge both more\u000d        quickly and more reliably.\" Smith's mathematical analysis is\u000d      reported in [1, 4, 5] above; [4] is cited explicitly by SISTeMA\u000d      [13]. SISTeMA's portfolio includes traffic projects in Brussels,\u000d      Bogot&#224;, London, Moscow, New Delhi and Rome [13].\u000d    4.2 Impact on practitioners and external collaboration\u000d    Smith has stimulated and informed the international traffic\u000d      planning debate and influenced traffic planners and signal engineers by\u000d      communicating his results in a wide range of forums. Examples include: (1)\u000d      a lecture at the 17th JCT Traffic Signal Symposium (September\u000d      2012), the main forum for signal engineers in the UK, with an emphasis on\u000d      papers and presentations from working signal engineers and manufacturers\u000d      [10], where his paper Traffic control and route choice: modelling and\u000d        optimisation received a prize for \"the most thought-provoking paper\"\u000d      and (2) a lecture at the Modelling World conference in June 2013\u000d      (a central forum for modelling practitioners).\u000d    Smith also works to develop and test new ideas via on-road pilot\u000d      schemes locally, which he considers essential to generate confidence and\u000d      so enable on-street impact. He has a long-standing relationship with the\u000d      City of York Council and the York-based specialist traffic control\u000d      consultants Ian Routledge Consultancy. Smith has been\u000d      involved with various innovative modelling projects aimed at reducing\u000d      congestion, and in Ian Routledge's words \"was the catalyst in bringing\u000d        together a team that led to the FREEFLOW project (2008 - 2011; funded by\u000d        DfT, TSB, EPSRC, ESRC and the project partners, including five\u000d        industrial partners, to the value of &#163;6.4M)\" [11], an exemplar of\u000d      the combined \"local academia\/local authority\/local consultancy\u000d        platform Mike had pioneered\". Routledge continues \"Mike has over\u000d        the last decade shown how academic research can be combined with local\u000d        authority and local industry partners to demonstrate in major national\u000d        and international projects the potential of integrating new academic\u000d        thinking into fields that have tended in many ways to be reluctant to\u000d        try new approaches and techniques.\" [11]\u000d    The FREEFLOW project (2008-2011) enabled proof-of-concept of a new\u000d      responsive gating system on the main arterial York to Hull A1079. It\u000d      achieved significant reductions in mean and variance of bus journey times\u000d      [6]. A long-term goal is to implement it nationally and internationally\u000d      through its incorporation into standard modelling programs and bus\u000d      routeing and timetabling software.\u000d    4.3 Impact through pattern recognition software\u000d    The novel pattern matching metric of [6] has been taken up for more\u000d      general applications by Cybula, a successful company with 10\u000d      employees set up in 2000 by Prof. Jim Austin, who leads a research group\u000d      in computer science at the University of York. Cybula licences\u000d      technology from the University and others to undertake its business;\u000d      however the University has no equity holding in the company. The risk\u000d      capital behind Cybula comes from private investors, who evaluate\u000d      technology carefully before investing their funds and the company's\u000d      profits in the methods. Cybula's Business Director John McAvoy\u000d      writes [12] \"Cybula's main business is in high performance pattern\u000d        matching... The cumulative encoding method described in [6] is being\u000d        used by the company within a number of projects. It's ideal for encoding\u000d        the data in many problems prior to recognition with [the existing match\u000d        engine]. We are using it in a project with Simulation Software Ltd on\u000d        data from BP for possible detection of leaks in oil pipelines and it\u000d        also forms a part of a project with Sheffield University and Thames\u000d        Water. We are also using it in a project with EDF Energy... Thus Cybula\u000d        is using the technology regularly, furthermore, if the projects' outputs\u000d        are taken up by the companies, the method will also be used on a daily\u000d        basis with these companies... [this technology] is critical in making\u000d        the company competitive in the world market.\" The projects in which\u000d      Cybula uses the methods of [6] are collectively worth &#163;200k per\u000d      annum [12].\u000d    ","ImpactSummary":"\u000d    Improvements in traffic flow on urban road networks have a direct daily\u000d      impact on citizens, business and tourism. To make improvements,\u000d      transportation planners and signal engineers rely on modelling and control\u000d      software that implements mathematical methods designed to optimize traffic\u000d      flows, signal timings or both. Research by Mike Smith's group at\u000d      York since 1993 has led to:\u000d    \u000d      the implementation of some of Smith's older ideas in the\u000d        SATURN (Simulation and Assignment of Traffic in Urban Road Networks)\u000d        software suite, which is routinely utilised to model proposed network\u000d        changes in over 100 cities, including London, and was used to help\u000d        design and assess traffic schemes for the 2012 Olympics;\u000d      the evaluation by Italian company SISTeMA of Smith's\u000d        ideas for possible inclusion in its Optima transportation software;\u000d      the incorporation and use of Smith's associated recent work on\u000d        pattern-matching in a number of commercial projects by York firm Cybula\u000d        in its Signal Data Explorer software.\u000d    \u000d    ","ImpactType":"Economic","Institution":"\u000d    University of York\u000d    ","Institutions":[{"AlternativeName":"York (University of)","InstitutionName":"University of York","PeerGroup":"B","Region":"Yorkshire And Humberside","UKPRN":10007167}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"3169070","Name":"Rome"},{"GeoNamesId":"2800867","Name":"Bruxelles-Capitale"},{"GeoNamesId":"1261481","Name":"New Delhi"},{"GeoNamesId":"1261481","Name":"New Dehli"},{"GeoNamesId":"524901","Name":"Moscow"}],"References":"\u000d    Transportation Science and Transportation Research Parts B\u000d      and C regularly publish mathematical innovations designed to address the\u000d      particular characteristics of both equilibrium and dynamical\u000d      transportation and traffic modelling. (Citations: Google Scholar\u000d      20\/9\/2013.)\u000d    \u000a*[1] Smith, M. J., van Vuren, T (1993). Traffic equilibrium with\u000d      responsive traffic control. Transportation Science, 27, 118-132 doi:\u000a        10.1287\/trsc.27.2.118. (Leading journal; 80 citations)\u000d    \u000a\u000a[2] Smith, M. J., Xiang, Y., Yarrow, R., Ghali, M. O.\u000d      (1996). Bilevel and Other Modelling Approaches to Urban Traffic Management\u000d      and Control. Proceedings, Equilibrium and Advanced Transportation\u000d        Modelling (1998); Editors: Patrice Marcotte and Sang Nguyen),\u000d      Kluwer, 283 - 325. DOI: 10.1007\/978-1-4615-5757-9_12 (Invited\u000d      presentation; high quality conference; 14 cites)\u000d    \u000a\u000a*[3] Clegg, J., Smith, M. J., Xiang, Y., Yarrow, R. (2001).\u000d      Bilevel Programming Applied to Optimising Urban Transportation, Transportation\u000a        Research B 35, 41-70. (Top journal; 58 cites) doi:10.1016\/S0191-2615(00)00018-7\u000d    \u000a[4] Smith, M. J. (2010). Intelligent Network Control: Using an\u000d      Assignment-Control Model to Design Fixed Time Signal Timings. New\u000d        Developments in Transport Planning &#8212; Advances in Dynamic Traffic\u000d        Assignment (Eds: Tampere, C., Viti, F. and Immers, L.), Edward\u000d      Elgar, 57 - 72. DOI: 10.4337\/9781781000809. (The first in a prestigious\u000d      series of conferences titled Models and Technologies for Intelligent\u000d        Transportation Systems organised with the specific aim of bringing\u000d      theorists and practitioners together; 6 cites)\u000d    \u000a*[5] Smith, M. J., Mounce, R. (2011) A splitting rate model of\u000d      traffic re-routeing and traffic control. Transportation Research\u000d      Part B, 45 (2011), 1389-1409. doi:10.1016\/j.trb.2011.05.013\u000d      (Also in the Proceedings of the Nineteenth International Symposium on\u000d        Transportation and Traffic Theory Procedia Social and Behavioral\u000d      Sciences 17 (2011) 316-340) (Top class journal; Very high quality,\u000d      peer-reviewed conference and conference proceedings; 9 cites)\u000d    [6] Mounce, R., Hollier, G., Smith, M. J., Hodge, V.J.,\u000d      Jackson, T., Austin, J., (2013) A Metric For Pattern-Matching Applications\u000d      To Traffic Management, Transportation Research C, 29, 148-155.\u000d      DOI: 10.1016\/j.trc.2012.04.019 (A top journal in the field; NB submitted\u000d      to journal in Nov 2010.)\u000d    \u000aFunding for this research 1993 - 2011:\u000d    Nine EPSRC grants on which Smith was PI: Congestion Pricing Via\u000d      the Urban Traffic Control System GR\/R33274\/01 &#163;58,546, Experimental\u000d      Investigation Of The Dynamics Of Driver Route Choice GR\/M80093\/01 &#163;83,626,\u000d      Bilevel Optimisation Of Signal Timings To Minimise Urban Traffic\u000d      Congestion GR\/M22482\/01 &#163;140,784, Assessment Of Road-User Charging Systems\u000d      Taking Account Of Public Transport And Equity GR\/K08901\/01 &#163;112,617,\u000d      RONETS: A New Microsimulation Model For Designing And Assessing Road\u000d      Pricing And Traffic Control Schemes GR\/J73919\/01 &#163;58,226, The Development\u000d      Of A New Theory Of Parallel Dynamic Traffic Assignment Using Splitting\u000d      Rates GR\/J71847\/01 &#163;57,550, Network Wide Redistributional Effects Of\u000d      Traffic Control And Road Pricing Together GR\/J97793\/01 &#163;69,320,\u000d      Distributed Computation Of Dynamic Traffic Equilibria On Large Networks\u000d      GR\/H43540\/01 &#163;53,482, Dynamic Traffic Assignment And Dynamic Traffic\u000d      Control in Congested Signal-Controlled Road Networks GR\/H39710\/01 &#163;45,349.\u000d      Plus two EPSRC grants on which Smith was CI: FREEFLOW EP\/F005156\/1\u000d      &#163;1,442,820, Development Of Combined Assignment &amp; Control Models &amp;\u000d      Their Applications To Traffic Management GR\/K78447\/01 &#163;81,606.\u000d    EU grant (PI: Smith) MUSIC (Management of traffic USIng Control)\u000d      project (&#163;2.5 million; 1996-1999; contract: UR-95-SC.173.). MUSIC applied\u000d      Smith's ideas in York, Thessaloniki &amp; Porto. Partners: Univ.\u000d      York (co-ordinator), TRIAS SA (Greece), Univ. Porto (Portugal), Hague\u000d      Consulting Group (Holland), City of York Council (UK), Univ. of Coimbra\u000d      (Portugal). Grant from City of York Council, JAH83\/UTMC (2001-2002)\u000d      &#163;50,000; Grant from the City of Dublin, (2002) &#163;23,750\u000d    ","ResearchSubjectAreas":[{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"9","Level2":"5","Subject":"Civil Engineering"}],"Sources":"\u000d    [7] Letter and emails from Dr Dirck van Vliet, developer of SATURN in\u000d      conjunction with the Institute for Transport Studies (University of Leeds)\u000d      and Atkins Global.\u000d    [8] 'Delivering transport for the London 2012 games', (London: Olympic\u000d      Delivery Authority, 2012), p39, at www.ice.org.uk\/Information-resources\/Document-Library\/Delivering-Transport-for-the-London-2012-Games\u000d    [9] `Travel in London Report 4' (London: Transport for London, 2011), p9,\u000d      at www.tfl.gov.uk\/assets\/downloads\/travel-in-london-report-4.pdf\u000d    [10] http:\/\/www.jctconsultancy.co.uk\/Symposium\/Symposium2012\/symposium2012.php\u000d    [11] Letter from Mr Ian Routledge, of Ian Routledge Consultancy,\u000d      York.\u000d    [12] Letter from Mr John McAvoy, Business Director, Cybula &amp;\u000d      information from Prof. J. Austin.\u000d    [13] Letter from Dott. Ing. Lorenzo Meschini (Sapienza Universita di Roma\u000d      and CEO of SISTeMA, a PTV Group Company and Sapienza spin-off). SISTeMA's\u000a      portfolio: www.sistemaits.com\/portfolio\/\u000d    ","Title":"\u000d    Mathematical Modelling to Improve Traffic Flow and Control\u000d    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"}],"UKRegion":[{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d    Professor Mike Smith has been a member of the Mathematics\u000d      Department at the University of York since 1964. On retirement in 2006, he\u000d      was appointed Emeritus Professor, and was re-employed by York on a 30%\u000d      basis as a Researcher Co-investigator on the FREEFLOW grant from 1 January\u000d      2008 - 30 June 2011. Smith has made influential contributions to\u000d      the field of traffic modelling over a long timescale, starting with the\u000d      introduction of the \"P0\" responsive traffic control policy (1979) and his\u000d      \"Algorithm D\" for solving network equilibrium problems (1984). Since 1993\u000d      Smith has worked on the interlocking problems of (1) the design of\u000d      signal control adjustments to achieve economical routeing; (2) the design\u000d      of traffic models that can evaluate these signal control adjustments and\u000d      (3) pattern matching techniques to recognise when signal timing\u000d      interventions should be implemented on-street. This post-1993 research has\u000d      established that P0 and Algorithm D have many desirable properties\u000d      (especially stability properties) and has led to their implementation (in\u000d      modified form) in SATURN, the on-going evaluation of P0 by SISTeMA\u000d      and also to a commercial application of the pattern matching technique by\u000d      Cybula.\u000d    Since 1993, Smith has worked with his RAs Dr Y. Xiang\u000d      (1995-2002), Dr M. Ghali (1996-1998), Dr J. Clegg\u000d      (1999-2000 and for various short periods thereafter until she became a\u000d      lecturer in the Department of Electronics in 2006) and Dr R. Mounce\u000d      (2004-2005 (Mathematics) and 2009 (Computer Science)). Smith's\u000d      work draws on variational inequality theory; indeed, he was the first to\u000d      apply this theory to traffic equilibria, and it is now the standard\u000d      framework. His contribution was recognized in 2007 through the award of\u000d      the Robert Herman Lifetime Achievement Award by INFORMS (\"the largest\u000d      professional society in the world for operations research, management\u000d      science, and business analytics\"; the award is made annually to an\u000d      individual who has \"made fundamental and sustained contributions to\u000d      transportation science and logistics\").\u000d    Key results since 1993 underpinning the Impact described here include the\u000d      following.\u000d    \u000d      Papers [1,4,5] show that Smith's P0 policy is more stable than\u000d        others (particularly the standard equisaturation and delay minimisation\u000d        policies) and can significantly reduce congestion by encouraging route\u000d        switches, especially in high congestion situations.\u000d      Wide area network modelling, involving the solution of large network\u000d        equilibrium models, has been shown to be much more feasible using Smith's\u000d        algorithm D and related algorithms than with the Frank-Wolfe methods\u000d        previously used. Ref [1] is central to establishing that P0, rather than\u000d        other control policies, is by far the most natural policy for\u000d        implementation in transportation models which permit signals to change\u000d        in response to flow changes following algorithm D.\u000d      A main theme in [1,4,5] was to show that signal timing policies such\u000d        as P0 are well-suited to implementation within network models (perhaps\u000d        using algorithm D as the basic solution method), to give both an\u000d        equilibrium flow solution and a set of good consistent signal timings.\u000d      Refs [2] and [3] showed how the best possible control may be\u000d        calculated. Ref [2] gave new results comparing standard control systems'\u000d        performance with P0 when route choices vary. Ref [3] developed the\u000d        method for calculating the optimal control and shows that in an example\u000d        P0 performance is very close to the best that can be achieved.\u000d      Ref [6] introduced a new (and more meaningful) metric to identify when\u000d        a current traffic pattern is \"similar\" to a previous traffic pattern,\u000d        enabling a previously proven intervention to be recovered and\u000d        re-utilised in the current situation. The metric is being used in a\u000d        number of industry development projects by Cybula in general\u000d        pattern matching applications. \u000d    \u000d    Accordingly, Smith's research since 1993 has provided very strong\u000d      evidence of the desirability of both P0 and Algorithm D, especially when\u000d      used in conjunction. As described below, Refs [1,2] were crucial in the\u000d      decision to implement P0 and a \"social pressure algorithm\" based on\u000d      Algorithm D in SATURN; the later work [3,4,5] provides further support;\u000d      Refs [4,5] motivated SISTeMA's current evaluation of the P0\u000d      policy; finally, Ref [6] underpins the Cybula impact.\u000d    "},{"CaseStudyId":"43440","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"},{"GeoNamesId":"1668284","Name":"Taiwan"}],"Funders":[],"ImpactDetails":"\u000a    MacKay's introduction to Lanchester combat models [1] has proven\u000a      useful to a number of military analysts, from the US Army, the (US) Air\u000a      Force Institute of Technology and Air Combat Command [6] and the\u000a      Australian Defence Science &amp; Technology Organization [7], and in MS or\u000a      PhD theses at the US Naval Postgraduate School [8], Canadian Forces\u000a      College [9] and by a USAF Operations Analyst [10]. It is also used in\u000a      teaching SM212 Differential Equations, taught 2008-13 by Prof. W.\u000a      D. Joyner and subsequently Prof. R. L. Jackson, at the US Naval Academy,\u000a      to a class of around 400 midshipmen, where its notation was adopted and\u000a      its text heavily quoted (Prof. Jackson commented [11] that [1] `said it\u000a      much better than I could'). The mixed-forces paper [2] has been used (as\u000a      `the latest Lanchester mixed forces model') by analysts from Taiwan [12].\u000a    The Battle of Britain work [3,4] has been presented to mixed audiences\u000a      from defence and industry, including serving ranking officers, at Mathematics\u000a        in Defence 2009 (by MacKay, at QinetiQ, Farnborough) and Historical\u000a        Analysis for Defence and Security (by Price, at DSTL Portsdown West,\u000a      2011). More significantly, it has been used at the US Naval Postgraduate\u000a      School (NPS), as follows.\u000a    The US Navy is one of the largest technological military organizations in\u000a      the world, larger than the next ten navies combined. The NPS operates more\u000a      widely, as `America's national security research university', and is the\u000a      world-leader in dynamic combat modelling. It has about 2500 students on\u000a      2-year master's programmes, mostly mid-career officers from the armed\u000a      forces of the USA and its allies taking sabbaticals before returning to\u000a      front-line service. Within the NPS, MOVES (MOdeling, Virtual Environments\u000a      and Simulation) `is the nation's institute for defense modeling and\u000a      simulation ... in support of all the services and our allies.'\u000a    Prof. T. W. Lucas teaches OA\/MV 4655, Joint Combat Modeling, for\u000a      which [3] has become compulsory reading. MacKay visited the NPS in\u000a      2011 for collaborative work with Profs. M. Kress, R. Szechtman and M.\u000a      Atkinson on the dynamics of insurgency and to give an invited guest\u000a      lecture on [3,4] entitled Safety in Numbers: Lanchester, Fuller and\u000a        the Big Wing (Thurs 28 July 2011), attendance at which was\u000a      compulsory for OA\/MV 4655 students. The total audience was about 100,\u000a      composed of mixed faculty and students. Thus MacKay's work is\u000a      informing and influencing the thinking of a wide range of mid-career\u000a      officers, the purpose of whose study at the NPS is precisely to develop\u000a      their operational thinking. The impact here is in this knowledge transfer,\u000a      via the development of officers' understanding of combat dynamics before\u000a      their return to operational practice.\u000a    Most recently the historical air combat analysis of [3,4,5] was\u000a      referenced in a talk [13] on Prediction, given by Wayne Hughes,\u000a      the NPS's Professor of OR Practice, whose Fleet Tactics is `said\u000a      to be in every wardroom afloat' (ORMS Today, August 2007). This\u000a      lecture was Keynote Speech at the Military Applications Society 2012\u000a      conference, the annual meeting of the military operational analysis\u000a      practitioner community. Hughes called [3-5] a `detailed recent analysis\u000a      [whose conclusions are] no theoretical matter'. Brian McCue, senior\u000a      analyst at the US Center for Naval Analyses and field representative at\u000a      the US Fleet Forces Command (which controls around 200 ships and 1000\u000a      aircraft), has described the results of [5] as `a severe strike against\u000a      the operational utility of [Lanchestrian] theory [for] the operational\u000a      planner' [14]. Countering the belief that Lanchester's square law applies\u000a      to air power will remain important for as long as planners continue to use\u000a      it in support of their theses, as for example in [10].\u000a    MacKay, Price and Wood have been formally invited [15] to visit\u000a      the NPS in 2013-14 to present and discuss their new work on naval\u000a      simulations, and have a standing invitation from Profs Kress and Lucas to\u000a      present their work in the Naval Postgraduate School Combat Modeling and\u000a      Advanced Combat Modeling courses.\u000a    Finally, MacKay has presented his warfare modelling work widely\u000a      in public engagement, giving presentations about the Battle of Britain\u000a      research for the Further Mathematics Support Programme, to the National\u000a      Mathematics Teachers' Summer School (of about 30 school mathematics\u000a      teachers from across the UK), and locally at various schools, societies\u000a      and open days. The work has also been featured in New Scientist\u000a      [16]. Most recently MacKay and Wood, together with\u000a      historians Chris Price and Ian Horwood, organized a half-day event\u000a      (25-6-2013), with lectures and wargames\/simulations, on mathematics in\u000a      First World War naval tactics and the Battle of Jutland as part of the York\u000a        Festival of Ideas [17].\u000a    ","ImpactSummary":"\u000a    What is the best way to organize firepower in war, and what weight should\u000a      be placed on mathematical models? The oldest and simplest approach is\u000a      dynamical-systems based and begins with Lanchester's models. Recent work\u000a      has exposed some of the subtleties and limitations of these, and the\u000a      dangers in the interplay between the models and organizational culture and\u000a      doctrine. Above all it has been demonstrated that Lanchester's `square\u000a      law' does not apply to the use of air power. The impact is in the form of\u000a      knowledge transfer: the research has been used in the professional\u000a      development of serving officers at the US Naval Postgraduate School\u000a      (`America's national security research university' and the world leader in\u000a      the subject) and in the community of military analysis practitioners.\u000a    ","ImpactType":"Societal","Institution":"\u000a    University of York\u000a    ","Institutions":[{"AlternativeName":"York (University of)","InstitutionName":"University of York","PeerGroup":"B","Region":"Yorkshire And Humberside","UKPRN":10007167}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] N. J. MacKay, `Lanchester combat models', Mathematics\u000a        Today: Bulletin of the Institute for Mathematics and its Applications\u000a      42 (2006) 170-173, arXiv:math.HO\/0606300\u000a    \u000a\u000a*[2] N. J. MacKay, `Lanchester models for mixed forces with\u000a      semi-dynamical target allocation', Journal of the Operational Research\u000a        Society 60 (2009) 1421-1427 [JORS is a respected\u000a      international peer-reviewed journal for Operational Research (OR).] DOI:10.1057\/jors.2008.97\u000a    \u000a\u000a*[3] I. R. Johnson and N. J. MacKay, `Lanchester models and the\u000a      Battle of Britain', Naval Research Logistics 58 (2011)\u000a      210-222. First published online December 2008; print publication delayed\u000a      for inclusion in memorial special volume for Richard E. Rosenthal. [NRL\u000a      is another international, high-quality, peer-reviewed general OR journal.]\u000a      DOI: 10.1002\/nav.20328\u000a    \u000a\u000a*[4] N. MacKay and C. Price, `Safety in Numbers: Ideas of\u000a      concentration in Royal Air Force fighter defence from Lanchester to the\u000a      Battle of Britain', History 96 (2011) 304-325. [History\u000a      is one of the top journals in the discipline.] DOI:10.1111\/j.1468-229X.2011.00521.x\u000a    \u000a\u000a[5] N. J. MacKay, `Is air combat Lanchestrian?', Phalanx: the\u000a        Bulletin of Military Operations Research 44 no.4 (2011)\u000a      12-14.\u000a      http:\/\/www.mors.org\/userfiles\/file\/phalanx\/mors_phalanx_dec2011_web.pdf\u000a      [Phalanx is the professional magazine of the military OR community,\u000a      and `presents a cross-section of important current research, meetings\u000a      reports, MORS news and informative oral histories'.]\u000a    \u000a[1] is a summary of old ideas with extra original material, intended for\u000a      a general mathematical readership. [2,3,4] are academic research\u000a      publications in peer-reviewed international journals. [5] is original\u000a      research, but is intended primarily for military analysis practitioners.\u000a    ","ResearchSubjectAreas":[{"Level1":"21","Level2":"3","Subject":"Historical Studies"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000a    [6] Maj M. J. Artelli, USAF, Modeling and analysis of resolve and\u000a        morale for the `long war', Air Force Institute Of Technology\u000a      dissertation AFIT\/DS\/ENS\/07-02; M. J. Artelli and R. F. Deckro, `Modeling\u000a      the Lanchester Laws with System Dynamics', Journal of Defense Modeling\u000a        and Simulations 5 (2008) 1 www.au.af.mil\/au\/awc\/awcgate\/afit\/artelli_modeling_resolve_morale.pdf\u000a    [7] V. Bui, L. Bui, H. Abbass, A. Bender and P. Ray, `On the role of\u000a      information networks in logistics: An evolutionary approach with military\u000a      scenarios', Evolutionary Computation 2009, 598-605 doi:10.1109\/CEC.2009.4983000\u000a    [8] Lt C. M. Mahon, USN, A littoral combat model for land-sea missile\u000a        engagements, NPS MS thesis, September 2007 www.dtic.mil\/cgi-bin\/GetTRDoc?AD=ADA473951\u000a    [9] LtCol R. Dundon, CAF, Coping with the complexity of conflict,\u000a      Canadian Forces College MDS thesis, 2009. www.cfc.forces.gc.ca\/259\/290\/295\/286\/dundon.pdf\u000a    [10] E. S. Gons, Access challenges and implications for airpower in\u000a        the Western Pacific, Pardee RAND doctoral thesis 2010 http:\/\/www.rand.org\/pubs\/rgs_dissertations\/RGSD267.html\u000a    [11] Prof. R. L. Jackson, USNA, email 3.8.2011\u000a    [12] P-L Liu, H-K Sun, Y-T You, `Combined arms system dynamics model for\u000a      modern land battle', CCIT Journal 41 (2012) 19-28\u000a    [13] Prof. Wayne P. Hughes (USN, retired), Professor of OR Practice, US\u000a      NPS, Monterey. Keynote Address on Prediction, Military\u000a      Applications Society 2012 conference, March 2012. Hughes is a MORS\u000a      [Military Operations Research Society] Fellow and ex-President, and a\u000a      winner of MORS' highest honor, the Vance R. Wanner Award for significant\u000a      contributions to US national security.\u000a      10thsymposium.com\/presentations\/Tues%20am\/0900-0930%20Hughes%20PREDICTION%20for%20publication.pdf\u000a    [14] Dr Brian McCue, USN CNA, email 16.5.2012\u000a    [15] Prof. R. Dell, Head of Dept of OR, NPS. Letter of 30.10.2012.\u000a    [16] Kate Ravilious, `Patterns of War', New Scientist, 31st\u000a      July 2010, 35-39\u000a    [17] Tales of South and North: Understanding the Battle of Jutland\u000a        and its preludes in World War I http:\/\/yorkfestivalofideas.com\/2013\/performances-and-films\/battle-of-jutland\/\u000a      , accessed 13-8-2013\u000a    ","Title":"\u000a    How far can mathematical models of war and combat be trusted?\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Niall MacKay is an academic mathematical physicist in the\u000a      Department of Mathematics, University of York, as Lecturer since 2000, and\u000a      Reader since 2009. Since 2006 he has developed broader interests in\u000a      applied mathematics, especially in combat modelling and its history.\u000a    The modeling of operational and larger scale warfare nowadays tends to\u000a      fall somewhere between two extremes. The first is of large, computerised\u000a      simulations, with many variables and parameters, which may be used for\u000a      wargaming and in procurement. The second is of much simpler models in\u000a      which the underlying processes can be laid bare and analyzed. This is of\u000a      importance in the development of control software, but also, at its\u000a      simplest, in stimulating thought about conditions of engagement and in the\u000a      understanding of these that can be achieved through mathematics. It is\u000a      therefore particularly important in the thinking of military analysts and\u000a      in the training and development of service personnel, and this is where\u000a      its impact can be found.\u000a    The oldest and simplest, `Lanchester' models have for their main\u000a      conclusion the maxim that in modern war numbers and concentration of\u000a      forces are disproportionately important, other things being equal, with\u000a      fighting strength varying as individual unit effectiveness multiplied by\u000a      the square of engaged numbers (the 'square law'). This stands in\u000a      contrast to the more intuitive `linear law', in which fighting strength is\u000a      simply the product of effectiveness and numbers.\u000a    The first of MacKay's papers on the subject was an introduction\u000a      [1]. Lanchester's model is essentially a very simple dynamical system, and\u000a      the term `Lanchestrian' is often used to describe more complex\u000a      dynamical-systems based warfare modelling. Such an approach was used by MacKay\u000a      to model combat between forces of mixed types [2], providing a neat\u000a      solution, based on conserved quantities, to the question `which of my\u000a      opponents should I attack first?'. This paper also resolved a Cold War-era\u000a      academic dispute about the correct use of Lanchester equations for mixed\u000a      forces (W. W. Kaufmann, Nonnuclear deterrence, in Alliance\u000a        Security: NATO and the no- first-use question, Brookings Inst.,\u000a      Washington, DC, 1983, and 1987 papers by J. W. R. Lepingwell and T. F.\u000a      Homer-Dixon in International Security).\u000a    Next came a Lanchestrian campaign analysis of the Battle of Britain [3],\u000a      which demonstrated that Lanchester's insights did not apply there. The\u000a      interplay of Lanchestrian thought with the development of organizational\u000a      culture in the RAF, and its implications for the `Big Wing' controversy in\u000a      the form of confusion and misconceived doctrine, were explored in a paper\u000a      with historian Chris Price (York St John U.) in the leading academic\u000a      journal History [4].\u000a    Most recently, MacKay greatly strengthened the conclusion that\u000a      Lanchester's square law does not describe air combat by conducting a\u000a      combined analysis of data from the Battle of Britain and other air\u000a      campaigns of WWII, the US-Japanese Pacific war and the Korean war [5]. An\u000a      earlier study of the WWII and Korean data had been used by leading US\u000a      airpower authority John Warden to support his Lanchestrian claim that the\u000a      casualty exchange ratio depends sensitively on the force ratio&#8212;that is,\u000a      that in air combat massed numbers are disproportionately effective (J. A.\u000a      Warden, The Air Campaign: planning for combat, Brassey's:\u000a      Washington, 1989). MacKay's analysis showed that the full data set\u000a      does not support this claim. Indeed, a claim that airpower is symmetric\u000a      (between attacker and defender) and square law would be precisely wrong.\u000a      Rather, to the extent that airpower departs from the linear law, it turns\u000a      out to be highly asymmetric. Further work on this is under way in\u000a      collaboration with airpower historian Ian Horwood, incorporating results\u000a      on the Falklands, Yom Kippur, Vietnam and other campaigns.\u000a    MacKay and Price are also now working with systems biologist A.\u000a        J. Wood (Maths\/Biology) in a significant extension of their earlier\u000a      work, using techniques of Markov chain Monte Carlo simulation to\u000a      investigate large-scale naval battles, in the context of the pre-WW1\u000a      incorporation of geometry and calculus into naval tactics. MacKay\u000a      is also collaborating with Profs M. Kress and K. Lin of the US Naval\u000a      Postgraduate School on, respectively, modelling counterinsurgent warfare\u000a      and in further work on optimal policies for the mixed-force Lanchester\u000a      problem. MacKay and Wood have supervised four summer\u000a      students on aspects of warfare simulation and modelling, and many\u000a      undergraduate project students.\u000a    "},{"CaseStudyId":"43441","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"3144096","Name":"Norway"},{"GeoNamesId":"2802361","Name":"Belgium"},{"GeoNamesId":"2658434","Name":"Switzerland"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"1861060","Name":"Japan"}],"Funders":["Engineering and Physical Sciences Research Council","Natural Environment Research Council"],"ImpactDetails":"\u000d\u000a    Fisheries policy has implications for ecology, conservation, food\u000d\u000a      security, and major consumer\u000d\u000a      markets (&#8364;55Billion p.a. in the EU). The results of [4], underpinned by\u000d\u000a      [1] and [2], show that a\u000d\u000a      balanced harvest policy offers the prospect of simultaneously increasing\u000d\u000a      yield while conserving the\u000d\u000a      structure of marine ecosystems and their resilience. They have influenced\u000d\u000a      the policy of\u000d\u000a      international NGOs with global reach and stimulated and informed thinking\u000d\u000a      and policy debate in the\u000d\u000a      EU Parliament and elsewhere, e.g., in pursuit of long-term reform of the\u000d\u000a      Common Fisheries Policy\u000d\u000a      and the delivery of the EU 2020 Biodiversity Strategy.\u000d\u000a    The York research has been adopted by members of the Fisheries Expert\u000d\u000a      Group (FEG) of the\u000d\u000a      International Union for Conservation of Nature (IUCN) and the European\u000d\u000a        Bureau for Conservation\u000d\u000a        and Development (EBCD). The FEG is an influential group of\u000d\u000a      international leaders in fisheries\u000d\u000a      science, including Law's co-author Kolding, and chaired by Dr\u000d\u000a      Serge Garcia, ex-Director of the\u000d\u000a      Fisheries Management Division of the United Nations Food and\u000d\u000a        Agriculture Organization (FAO).\u000d\u000a      Law was an invited speaker at a FEG workshop in Nagoya (2010),\u000d\u000a      presenting his work with Datta,\u000d\u000a        Delius and Plank [6]. Law's was one of the three theoretical\u000d\u000a      talks at the workshop, which aimed to\u000d\u000a      \"derive the practical consequences of the emerging science, to raise\u000d\u000a      decision makers' and\u000d\u000a      scientists' awareness [and] eventually, deliver relevant general\u000d\u000a      management advice\". The\u000d\u000a      associated workshop report [7] is a potentially paradigm-changing\u000d\u000a      contribution to global thinking on\u000d\u000a      sustainable fisheries policy that puts the case for a balanced harvest\u000d\u000a      approach in contrast to\u000d\u000a      traditional selective fishing. The consequent Science Policy Forum\u000d\u000a      paper [8], of which Law and\u000d\u000a      Kolding are co-authors, makes specific proposals for fisheries management,\u000d\u000a      endorsed by the IUCN\u000d\u000a      in a press release: \"The new approach proposed by IUCN, called `balanced\u000d\u000a      harvesting', involves\u000d\u000a      targeting all edible components of the marine environment, in proportion\u000d\u000a      to their productivity.\" [10]\u000d\u000a    The main report and the Policy Forum paper have received considerable\u000d\u000a      interest (over 40 news\u000d\u000a      stories\/press releases worldwide [11]). Policy debate on fisheries\u000d\u000a      management has been informed\u000d\u000a      and stimulated at a high level among policy makers:\u000d\u000a    \u000d\u000a      Garcia addressed the ALDE (Liberal\/Democrats) group of MEPs at a\u000d\u000a        seminar introduced by EU\u000d\u000a        Fisheries Commissioner Damanaki [12], and the Committee on Fisheries\u000d\u000a        (COFI) of the UN\u000d\u000a        FAO at their meeting 9-13 July 2012 [COFI is influential over the\u000d\u000a        direction of FAO work].\u000d\u000a      Kolding made invited presentations on Balanced Harvest to the European\u000d\u000a        Fisheries Advisors\u000d\u000a        (May 2012, Oslo), and the Nordic Council of Ministers meeting 23-24\u000d\u000a        October 2012.\u000d\u000a      \u000aLaw presented his joint work with Plank and Kolding at the 6th\u000d\u000a        World Fisheries Congress, a\u000d\u000a        major conference that \"draws leading international figures influential\u000d\u000a        in driving debate and\u000d\u000a        shaping global policy on fishing\" [9].\u000d\u000a      \u000aLaw, Garcia and Kolding were the main speakers at a workshop on\u000d\u000a        balanced harvest [13] at\u000d\u000a        the European Parliament involving the Director of Policy Development and\u000d\u000a        Coordination, EU\u000d\u000a        Directorate-General for Maritime Affairs and Fisheries (8 November\u000d\u000a        2012). Law focussed on\u000d\u000a        the implications of [4] for balanced harvesting, building on [1, 2].\u000d\u000a    \u000d\u000a    The success of these efforts can be gauged from a press release by Struan\u000d\u000a      Stevenson MEP, the\u000d\u000a      workshop Chair and Senior Vice President of the European Parliament\u000d\u000a      Fisheries Committee.\u000d\u000a      Entitled Scots Euro MP showcases revolutionary new management system\u000d\u000a        for fisheries,\u000d\u000a      Stevenson's press release quotes him as saying \"Balanced Harvest is a\u000d\u000a      hugely exciting\u000d\u000a      development for fisheries sustainability... ...If balanced harvesting was\u000d\u000a      introduced in the North\u000d\u000a      Sea, it would help end the constant battle between fishermen who use a\u000d\u000a      wide variety of fishing\u000d\u000a      gears and target a broad spectrum of species and sizes, and managers in\u000d\u000a      Brussels, Westminster\u000d\u000a      and Holyrood who try to impose size limitations and gear regulations. It\u000d\u000a      may be the answer to\u000d\u000a      achieving the 'Holy Grail' of fishing above Maximum Sustainable Yield\u000d\u000a      (MSY), so that fish stocks\u000d\u000a      are able readily to replenish themselves and catches and profits rise\u000d\u000a      accordingly\" [13].\u000d\u000a      Furthermore, balanced harvest ideas are incorporated into the vision\u000d\u000a      document for Norwegian\u000d\u000a      Fisheries in 2050 [14], which, citing [8], envisions that a much increased\u000d\u000a      part of the ecosystem will\u000d\u000a      be considered in determining quotas and management.\u000d\u000a    Further evidence of the influence of balanced harvesting comes from the\u000d\u000a      recent ALTER-Net\u000d\u000a      Conference on the EU 2020 Biodiversity Strategy, where balanced harvest\u000d\u000a      appears in the\u000d\u000a      preparatory statements to stimulate debate [15]. A discussion of balanced\u000d\u000a      harvesting has been\u000d\u000a      initiated in India by V. Vivekanandan, former Chief Executive of the South\u000d\u000a      Indian Federation of\u000d\u000a      Fishermen Societies [the largest NGO for small scale fishermen in India],\u000d\u000a      and currently a\u000d\u000a      consultant to the FAO, who wrote to a group of other Indian fisheries\u000d\u000a      experts saying: \"This is quite\u000d\u000a      revolutionary and the \"selective fishing\" vs. \"balanced harvesting\" debate\u000d\u000a      could very well turn out to\u000d\u000a      be the fisheries sector equivalent of the \"flat earth\" vs. \"round earth\"\u000d\u000a      debate in astronomy and\u000d\u000a      Newtonian principles vs. Relativity theory in physics\" [16].\u000d\u000a    Fisheries policy is intensely political; decisions are influenced not\u000d\u000a      only by science but by many\u000d\u000a      other powerful lobbies. However, in a relatively short time, Balanced\u000d\u000a      Harvest has been established\u000d\u000a      as a radical new approach on the policy scene, because it is\u000d\u000a      well-supported by the models.\u000d\u000a    Kolding writes: \"Balanced Harvest would never have had the impact it now\u000d\u000a      has without model\u000d\u000a      work. Firstly, because there are practicably no empirical observations to\u000d\u000a      study &#8212; and even if there\u000d\u000a      were, we could write volumes without getting anywhere... ...[Balanced\u000d\u000a      Harvest] has entered the\u000d\u000a      world `stage' with rocket speed AFTER the models started to look into it\"\u000d\u000a      [17]. In summary: not only\u000d\u000a      did the pioneering work of Datta, Delius and Law find its\u000d\u000a      impact through the Balanced Harvest\u000d\u000a      community; the Balanced Harvest paradigm itself owes its impact to their\u000d\u000a      work.\u000d\u000a    References relating to the dissemination phase, between the\u000d\u000a        mathematical research and impact:\u000d\u000a    [6] Ecological drivers of stability and instability in marine\u000d\u000a        ecosystems\u000d\u000a      R. Law, M.J. Plank, G.W. Delius and J.L. Blanchard, (2011)\u000d\u000a      in [7].\u000d\u000a    [7] Selective Fishing and Balanced Harvest in Relation to Fisheries\u000d\u000a        and Ecosystem Sustainability\u000d\u000a      Report of a scientific workshop organized by the IUCN-CEM Fisheries Expert\u000d\u000a      Group (FEG) and the\u000d\u000a      European Bureau for Conservation and Development (EBCD) in Nagoya (Japan),\u000d\u000a      14-16 October\u000d\u000a      2010. S. M. Garcia (Ed.), with 18 authors including R. Law and J.\u000d\u000a      Kolding (IUCN and EBCD,\u000d\u000a      Gland, Switzerland and Brussels, Belgium, 2011).\u000d\u000a    [8] Reconsidering the Consequences of Selective Fisheries, S. M.\u000d\u000a      Garcia et al. (including J.\u000d\u000a      Kolding and R. Law) Science 335 (2012) 1045-1047.\u000d\u000a      Policy Forum papers are highly prestigious and high profile contributions\u000d\u000a      to knowledge transfer in\u000d\u000a      one of the foremost international scientific journals. (38 citations in\u000d\u000a      Google scholar 19\/9\/2013).\u000d\u000a    [9] Evaluation Of Balanced Exploitation Of Marine Ecosystems: Results\u000d\u000a        From Dynamic Size\u000d\u000a        Spectra, R. Law, M. Plank, J. Kolding, talk PSA5.05 at the\u000d\u000a      6th World Fisheries Congress\u000d\u000a      (Edinburgh, 7th - 11th May 2012) [talks PSA3.04 and PSA5.04 also relate to\u000d\u000a      balanced harvests].\u000d\u000a      The description of the Congress appears in the press release World\u000d\u000a        Fisheries Congress Opens in\u000d\u000a        Edinburgh linked from www.6thwfc2012.com\/press\/.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Mathematical models recently developed in York have improved our\u000d\u000a      understanding of the\u000d\u000a      dynamics of marine ecosystems. They underpin paradigm-changing proposals\u000d\u000a      to orient fisheries\u000d\u000a      policy towards a \"balanced harvest\" and away from the traditional\u000d\u000a      selective harvesting of species\u000d\u000a      and sizes. These proposals have:\u000d\u000a    \u000d\u000a      influenced, and are now being actively pursued by, international NGOs\u000d\u000a        involved in shaping the\u000d\u000a        future direction of fisheries policy worldwide;\u000d\u000a      informed and stimulated debate among policy makers in the EU\u000d\u000a        Parliament and elsewhere;\u000d\u000a      been incorporated into long range planning for Norwegian fishery\u000d\u000a        management.\u000d\u000a    \u000d\u000a    ","ImpactType":"Environmental","Institution":"\u000d\u000a    University of York\u000d\u000a    ","Institutions":[{"AlternativeName":"York (University of)","InstitutionName":"University of York","PeerGroup":"B","Region":"Yorkshire And Humberside","UKPRN":10007167}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2618528","Name":"Kolding"},{"GeoNamesId":"3143244","Name":"Oslo"},{"GeoNamesId":"2800867","Name":"Bruxelles-Capitale"},{"GeoNamesId":"1856057","Name":"Nagoya"}],"References":"\u000d\u000a    All papers were submitted prior to Law's retirement. (Citations:\u000d\u000a      Google Scholar, 19\/9\/2013).\u000d\u000a    \u000a*[1] S. Datta, G.W. Delius, R. Law. A jump-growth model for\u000d\u000a        predator-prey dynamics: derivation\u000d\u000a        and application to marine ecosystems. Bull. Math. Biol. 72 (2010)\u000d\u000a      1361-1382. 15 citations. DOI:\u000d\u000a      10.1007\/s11538-009-9496-5\u000d\u000a    \u000a\u000a*[2] S. Datta, G.W. Delius, R. Law, M.J. Plank. A stability\u000d\u000a        analysis of the power-law steady state of\u000d\u000a        marine size spectra, J. Math. Biol. 63 (2011) 779-799. DOI:\u000d\u000a      10.1007\/s00285-010-0387-z.\u000d\u000a      J Math Biol and Bull Math Biol are among the top international journals in\u000d\u000a      their field. 18 citations.\u000d\u000a    \u000a\u000a*[3] J.A. Capitan, G.W. Delius. Scale-invariant Model of\u000d\u000a        Marine Population Dynamics. Phys. Rev.\u000d\u000a      E81 (2010) 061901. DOI: 10.1103\/PhysRevE.81.061901.\u000d\u000a      Capitan was a visiting PhD student from Madrid. Phys Rev E is a major\u000d\u000a      international journal in\u000d\u000a      many body\/statistical Physics covering applications in Biology and complex\u000d\u000a      systems. 11 citations.\u000d\u000a    \u000a\u000a[4] R. Law, M.J. Plank, J. Kolding. On balanced exploitation\u000d\u000a        of marine ecosystems: results from\u000d\u000a        dynamic size spectra, ICES J. Marine Science, 69 (2012) 602-614.\u000d\u000a      DOI:10.1093\/icesjms\/fss031.\u000d\u000a      13 citations. The International Council for the Exploration of the Sea\u000d\u000a      (ICES) is a major international\u000d\u000a      research network, with intergovernmental support, that coordinates and\u000d\u000a      promotes research and\u000d\u000a      advises governments and NGOs. ICES journals aim to form part of the\u000d\u000a      scientific basis for such\u000d\u000a      advice.\u000d\u000a    \u000a\u000a[5] R. Law. Fishing, selection and phenotypic evolution\u000d\u000a      ICES Journal of Marine Science, 57 (2000)\u000d\u000a      659-668. DOI: 10.1006\/jmsc.2000.0731. [Included for wider context:\u000d\u000a      relevant to the case study and\u000d\u000a      based on earlier mathematical modelling work, it has lower mathematical\u000d\u000a      content. 510 citations.]\u000d\u000a    \u000aPeer Reviewed Grants directly related to the work\u000d\u000a    i. Dynamics of size spectra in marine ecosystems, NERC CASE\u000d\u000a      studentship\u000d\u000a      supervised by R. Law and G.W. Delius December\u000d\u000a      2007-November 2010 c.&#163;63,000\u000d\u000a    ii. Aquatic ecosystem dynamics: size or species? Marsden Fund\u000d\u000a      (New Zealand) 2009-2013\u000d\u000a      NZ-$213,062 A. James, R. Law and M. Plank (PI), University of\u000d\u000a      Canterbury, New Zealand.\u000d\u000a    iii. CEFAS PhD scholarship supporting Celina Wong, co-supervisors: J.\u000d\u000a      Pitchford &amp; R. Law, S.\u000d\u000a      Mackinson (CEFAS) October 2010-September 2013 &#163;55,600\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"5","Level2":"2","Subject":"Environmental Science and Management"},{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"7","Level2":"4","Subject":"Fisheries Sciences"}],"Sources":"\u000d\u000a    [10] A balanced kettle of fish &#8212; IUCN suggests a novel approach to\u000d\u000a        fishing.\u000d\u000a      IUCN news story reporting the publication of [7, 8]. www.iucn.org\/?uNewsID=9313\u000d\u000a    [11] Linked list of news stories\/press releases concerning balanced\u000d\u000a      harvest between March 2012\u000d\u000a      and April 2013. Extended version of a list originally compiled by the\u000d\u000a      University of Bergen.\u000d\u000a    [12] Selective fishing, balanced harvesting and sustainability of\u000d\u000a        fisheries and ecosystems\u000d\u000a      Presentation given by S. Garcia based on the report [7], at the seminar\u000d\u000a      The Marine Food Chain:\u000d\u000a        Better Management for new Challenges organised by the Alliance of\u000d\u000a      Liberals and Democrats for\u000d\u000a      Europe (ALDE) group, EU Parliament, 8\/12\/2010 www.alde.eu\/event-seminar\/events-details\/article\/the-marine-food-chain-better-management-for-new-challenges-35651\/\u000d\u000a      Garcia's\u000d\u000a      presentation is at: http:\/\/www.ebcd.org\/pdf\/en\/338-2-2011-Balanced_harvest_and_food_chain_-Alde_seminar-_Brussels.pdf\u000d\u000a    [13] Increased Selectivity versus Balanced Harvest: How do we best\u000d\u000a        meet ecosystem objectives in\u000d\u000a        fisheries? Workshop at the EU Parliament, 8\/11\/2012. www.ebcd.org\/pdf\/en\/72-Report_Balanced_Harvesting_Seminar.pdf\u000d\u000a      The related press release by Struan Stevenson MEP is\u000d\u000a      at: www.struanstevenson.com\/media\/news-release\/scots_euro_mp_showcases_revolutionary_new_management_system_for_fisheries\/\u000d\u000a    [14] Verdiskaping basert p&#229; produktive hav i 2050, [Wealth creation\u000d\u000a        based on productive seas in\u000d\u000a        2050.] Report commissioned by the Royal Norwegian Society of\u000d\u000a      Sciences and Letters and the\u000d\u000a      Norwegian Academy of Technological Sciences, available at\u000d\u000a      www.regjeringen.no\/en\/dep\/fkd\/Documents\/reports-and-plans\/reports\/2012\/verdiskaping-basert-pa-produktive-hav-i-.html?id=697596\u000d\u000a      (in Norwegian)\u000d\u000a    [15] ALTER-Net Conference 2013: Science underpinning the EU 2020\u000d\u000a      Biodiversity Strategy.\u000d\u000a      www.alter-net.info\/outputs\/conf-2013\/biodiversity-strategy\/provocative-statements\/target4\u000d\u000a      ALTER-Net\u000d\u000a      comprises 26 leading environmental research institutes (primarily outside\u000d\u000a      HEIs) from 18\u000d\u000a      European countries to: \"assess changes in biodiversity, analyse the effect\u000d\u000a      of those changes on\u000d\u000a      ecosystem services and inform the public and policy makers about this at a\u000d\u000a      European scale.\"\u000d\u000a    [16] Email from former Chief Executive of the South Indian Federation of\u000d\u000a      Fishermen Societies to a\u000d\u000a      number of Indian fisheries experts (quoted with permission).\u000d\u000a    [17] Email from Professor, University of Bergen.\u000d\u000a    ","Title":"\u000d\u000a    Balanced Harvest: Mathematical underpinnings of a sustainable fisheries\u000d\u000a      policy\u000d\u000a    ","UKLocation":[{"GeoNamesId":"2650225","Name":"Edinburgh"}],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Traditional fisheries policy attempts to reduce ecological impact by\u000d\u000a      selective harvesting of species\u000d\u000a      and sizes, protecting juveniles and rare species. Evidence is accumulating\u000d\u000a      that selective\u000d\u000a      harvesting can destabilize ecosystem dynamics, and has led to evolutionary\u000d\u000a      change in fish (see\u000d\u000a      e.g. [5]). This case study concerns a mathematical model developed in York\u000d\u000a      by Datta, Delius and\u000d\u000a      Law, which provides a sound theoretical basis for an alternative\u000d\u000a      policy called balanced harvest.\u000d\u000a    Gustav Delius and Richard Law are members of the York\u000d\u000a      Centre for Complex Systems Analysis\u000d\u000a      (YCCSA). Delius is a Lecturer in Mathematics, who began his career\u000d\u000a      in Mathematical Physics and\u000d\u000a      came to York in 1999 as an EPSRC Advanced Fellow. He now focusses on the\u000d\u000a      stochastic\u000d\u000a      modelling of ecological dynamical systems. Law, now an Emeritus\u000d\u000a      Professor, was a member of the\u000d\u000a      Biology Department from 1983 until retirement in 2011. His research\u000d\u000a      interests span evolutionary\u000d\u000a      ecology and the dynamics of biological communities. Working at the\u000d\u000a      interface of Mathematics and\u000d\u000a      Biology, he has always maintained strong links with the Department of\u000d\u000a      Mathematics (e.g., via a\u000d\u000a      jointly run MRes Mathematics in the Living Environment), such\u000d\u000a      links now being facilitated by\u000d\u000a      YCCSA. Delius and Law were joint PhD supervisors of Samik\u000d\u000a        Datta on a NERC-funded CASE\u000d\u000a      project, in partnership with the Centre for Environment, Fisheries &amp;\u000d\u000a      Aquaculture Science (CEFAS,\u000d\u000a      an executive agency of DEFRA), 2007-10. Datta is now a PDRA at the\u000d\u000a      University of Warwick.\u000d\u000a    Marine ecosystem models treat organisms spanning several orders of\u000d\u000a      magnitude in body mass,\u000d\u000a      with predation of the larger on the smaller. Understanding the resilience\u000d\u000a      of an ecosystem to fishing\u000d\u000a      or other interventions requires both a model for the dynamics of biomass\u000d\u000a      flow and an analysis of\u000d\u000a      the stability of steady state solutions, such as the power-law\u000d\u000a      relationships between body mass and\u000d\u000a      abundance (size spectra) widely observed in many aquatic ecosystems. The\u000d\u000a      standard approach\u000d\u000a      appropriates the McKendrick-von Foerster partial differential equation for\u000d\u000a      age distributions and\u000d\u000a      assumes that it applies also to body size. By contrast, Datta, Delius\u000d\u000a      and Law [1] analysed a\u000d\u000a      stochastic model of biomass dynamics in which predation events change the\u000d\u000a      mass distribution\u000d\u000a      through the death of the prey and growth of the predator. Using a\u000d\u000a      master-equation approach, a\u000d\u000a      deterministic integro-differential \"jump-growth equation\" was derived as\u000d\u000a      the macroscopic\u000d\u000a      description of the dynamics, to which the McKendrick-von Foerster equation\u000d\u000a      emerges as a first\u000d\u000a      order approximation; at second order there is an additional diffusion\u000d\u000a      term. These results provide a\u000d\u000a      firm foundation for ecosystem models of this type. In conjunction with\u000d\u000a      Plank (University of\u000d\u000a      Canterbury, New Zealand), a stability analysis of the power-law steady\u000d\u000a      state solutions to the jump-growth\u000d\u000a      equations of [1] and its first- and second-order approximations was\u000d\u000a      conducted in [2]. Here,\u000d\u000a      the significance of the more detailed model became apparent, because the\u000d\u000a      steady state proved to\u000d\u000a      be unstable in the McKendrick-von Foerster model, while the inclusion of a\u000d\u000a      diffusion term yielded\u000d\u000a      results that (numerically) indicate good agreement with the full\u000d\u000a      jump-growth equation. The analytic\u000d\u000a      results were extended to include effects of reproduction, metabolic loss\u000d\u000a      and natural death in [3].\u000d\u000a      Further basic research is in progress to understand how dynamics of\u000d\u000a      plankton interact with the size\u000d\u000a      spectra (CEFAS-funded PhD project co-supervised in York by Law and\u000d\u000a      Jon Pitchford, a\u000d\u000a      Mathematics\/Biology jointly-appointed Senior Lecturer.)\u000d\u000a    Because the models obtained in [1] automatically take care of biomass\u000d\u000a      bookkeeping, they are\u000d\u000a      ideally suited to modelling marine exploitation, which Law and\u000d\u000a      Plank have investigated with\u000d\u000a      Kolding, a highly influential Norwegian fisheries biologist [4]. They give\u000d\u000a      a fundamentally new\u000d\u000a      perspective on fisheries management, supporting balanced rather than\u000d\u000a      selective harvesting as a\u000d\u000a      means of reducing the disruption of the natural (unfished) size spectrum,\u000d\u000a      improving ecosystem\u000d\u000a      resilience and, strikingly, substantially increasing biomass yield. This\u000d\u000a      provides the mathematical\u000d\u000a      foundation for intuitions Kolding had long held, but which, lacking proof,\u000d\u000a      had not gained currency\u000d\u000a      against standard approaches, which ignore the central truth that fish grow\u000d\u000a      by eating others.\u000d\u000a    In summary, the basic modelling and mathematical analysis of [1] and [2]\u000d\u000a      underpins radical new\u000d\u000a      proposals for managing marine ecosystems [4], which would not have been\u000d\u000a      possible otherwise.\u000d\u000a      Dynamic size spectra have only been investigated by a handful of groups\u000d\u000a      worldwide and [4] is the\u000d\u000a      first study to demonstrate quantitatively the range of benefits of\u000d\u000a      balanced harvesting.\u000d\u000a    "},{"CaseStudyId":"43598","Continent":[{"GeoNamesId":"6255151","Name":"Oceania"}],"Country":[{"GeoNamesId":"2186224","Name":"New Zealand"}],"Funders":[],"ImpactDetails":"\u000a    Background\u000a    Campylobacteriosis is the most common cause of human bacterial\u000a      gastroenteritis in the developed\u000a      world. In most cases the effect on humans is limited to fever, diarrhoea,\u000a      and abdominal pain, with\u000a      the illness lasting between 2 and 10 days. However more serious\u000a      complications, including\u000a      occasional deaths, can occur, particularly in young babies and the\u000a      elderly. The mortality rate has\u000a      been estimated as 0.1% of reported cases.\u000a    Introduction of Code of Practice for the New Zealand Poultry Industry\u000a        (Oct 2007)\u000a    The New Zealand Food Safety Authority (NZFSA) has close links with\u000a      French's group at Massey,\u000a      and both they and the poultry industry were made aware of the findings of\u000a      the joint Massey-Lancaster\u000a      research on source attribution as they were happening. The research led to\u000a      public\u000a      health professionals advocating more rigorous controls on foodborne\u000a      pathways of\u000a      campylobacteriosis, particularly for poultry, and in turn to the\u000a      introduction of a new code of practice\u000a      for the poultry industry.\u000a    Impact of the Code of Practice (Nov 2007 onwards)\u000a    This code of practice rapidly led to over a 50 percent reduction in the\u000a      number of reported cases of\u000a      campylobacter infection caused by food (from roughly 17,000 cases in 2006\u000a      to 8,000 in 2008 with\u000a      an initial reduction noted as early as November 2007). This has been\u000a      maintained to the present\u000a      data. With notification rates often estimated at around 1 in 10 (Duncan\u000a      2011), this corresponds to\u000a      around 90,000 fewer actual cases per year, and given estimated mortality\u000a      rates, to a saving of\u000a      about 50 lives during the census period.\u000a    The annual economic saving, including direct health costs and loss of\u000a      output, has been estimated\u000a      to be in the region of NZ$36M (http:\/\/www.foodsafety.govt.nz\/elibrary\/industry\/Zealand_Leads-Efforts_Drastically.htm) to NZ$50M (Duncan 2011). Over the REF\u000a      census period this corresponds\u000a      to a total saving in region of &#163;100M to &#163;150M.\u000a    The method for source attribution is still used in New Zealand to monitor\u000a      the effectiveness of the\u000a      change of policy in an ongoing surveillance programme (French 2013).\u000a    Evidence of the Role of the Underpinning Research on Impact\u000a    The role the underpinning research had on the introduction of the new\u000a      code of practice is\u000a      evidenced by a number of sources. Firstly, Mullner et al. (2009) state \"The\u000a        evidence provided by\u000a        our approach has supported national policy making by providing an\u000a        important contribution to the\u000a        NZFSA Campylobacter Risk Management Strategy\". The introduction to\u000a      the NZFSA\u000a      Campylobacter Risk Management Strategy states \"It has been\u000a        scientifically established that poultry\u000a        meat is a primary exposure pathway in New Zealand\", a conclusion\u000a      that comes from the Massey-\u000a      Lancaster research. And this fact is used to motivate the resulting\u000a      strategy for reducing\u000a      campylobacter levels in poultry.\u000a    Two editorials (Dixon 2009a,b) also highlight that the Massey-Lancaster\u000a      research led by French,\u000a      and in particular that the use of the modelling methodology of Wilson, was\u000a      central to change in\u000a      policy. For example, Dixon 2009b states that this \"modelling\u000a        methodology provided the clinching\u000a        evidence to influence an industry highly resistant to any suggestions\u000a        that chickens were the major\u000a        source of campylobacteriosis in the country.\" Dixon (2009a) adds \"..in\u000a        New Zealand .... cases of\u000a        campylobacteriosis have halved over the past year. This has been done by\u000a        the adoption of new\u000a        hygiene measures by a poultry industry initially hostile to the idea\u000a        that it was the major source of\u000a        the problem &#8212; after their necessity had been established by\u000a        sophisticated computer modelling of\u000a        the infection.\" A recent presentation at the NZAE Annual Conference\u000a      (Duncan 2011) also states \"It\u000a        was this study conducted by Massey University for NZFSA ... that\u000a        motivated the poultry industry to\u000a        begin investing to reduce the Campylobacter loading on product for sale\u000a        for human consumption.\"\u000a    The work of the group at Massey, to which Wilson contributed, is cited in\u000a      NZFSA reports on risk\u000a      management strategies for campylobacter. (For example, French 2009 is\u000a      cited by in the NZFSA's\u000a      Campylobacter Risk Management Strategies 2010-2013). French himself writes\u000a      that \"The\u000a        contribution to public health made by Dr Wilson and colleagues at\u000a        Lancaster University should not\u000a        be underestimated.\"\u000a    ","ImpactSummary":"\u000a    Research at Lancaster led to a novel approach to detect the source of\u000a      cases of campylobacteriosis\u000a      (a bacterial foodborne disease). The application of this method to data\u000a      from New Zealand pin-\u000a      pointed that New-Zealand's high rate of cases was linked to the eating of\u000a      contaminated poultry.\u000a      These results were a key part of the evidence used by New Zealand's Food\u000a      Safety Authority to\u000a      introduce a new code of practice for the poultry industry. The impact of\u000a      this code of practice has\u000a      been a halving of the number of reported cases of campylobacteriosis in\u000a      New Zealand (from\u000a      around 16,000 cases in 2006 to less than 7,000 in 2008). With notification\u000a      rates estimated as 1 in\u000a      10, this corresponds to around 90,000 fewer actual cases per year. The\u000a      saving for the New\u000a      Zealand economy during the REF census period has been independently\u000a      estimated as between\u000a      &#163;100M and &#163;150M.\u000a    ","ImpactType":"Political","Institution":"\u000a    Lancaster University\u000a    ","Institutions":[{"AlternativeName":"Lancaster University","InstitutionName":"Lancaster University","PeerGroup":"B","Region":"North West","UKPRN":10007768}],"Panel":"B         ","PlaceName":[],"References":"\u000a    Key References\u000a    \u000aMullner, Spencer, Wilson, Jones, Noble, Midwinter, Collins-Emerson,\u000a      Carter, Hathaway and\u000a      French (2009) Assigning the source of human campylobacteriosis in New\u000a      Zealand: A comparative\u000a      genetic and epidemiological approach. Infection, Genetics and\u000a        Evolution. 9(6) 1311-1319\u000a      doi:10.1016\/j.meegid.2009.09.0003\u000a    \u000a\u000aWilson, Gabriel, Leatherbarrow, Cheesbrough, Gee, Bolton, Fox, Fearnhead,\u000a      Hart and Diggle\u000a      (2008) Tracing the source of campylobacteriosis. PLoS Genetics\u000a      4(9):e1000203.\u000a      doi:10.1371\/journal.pgen.1000203\u000a    \u000aOther References\u000a    \u000aLi and Stephens (2003) Modeling Linkage Disequilibrium and Identifying\u000a      Recombination Hotspots\u000a      Using Single-Nucleotide Polymorphism Data. Genetics, 165(4), 2213-2233.\u000a    \u000a\u000aFearnhead and Donnelly (2001) Estimating recombination rates from\u000a      population genetic data.\u000a      Genetics 159: 1299-1318.\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"}],"Sources":"\u000a    Dixon (2009a) Editorial, The Lancet Infectious Diseases, 9, p.527.\u000a    Dixon (2009b) Editorial, Microbe, 4, p.394-395.\u000a    French (2009) Enhancing Surveillance of Potentially Foodborne Enteric\u000a      Diseases in New Zealand:\u000a      Human Campylobacteriosis in the Manawatu: Project extension incorporating\u000a      additional poultry\u000a      sources. Final Report FDI \/236\/2005. http:\/\/www.foodsafety.govt.nz\/elibrary\/industry\/enhancing-surveillance-potentially-research-projects\/finalreportducketc2009.pdf\u000a    Mullner P, Spencer S.E.F, Wilson D.J, Jones G, Noble A.D., Midwinter\u000a      A.C., Collins-Emerson J.M.,\u000a      Carter P., Hathaway S. and French N.P. (2009) Assigning the source of\u000a      human campylobacteriosis\u000a      in New Zealand: A comparative genetic and epidemiological approach.\u000a      Infection, Genetics and\u000a      Evolution. 9(6) p. 1311-1319.\u000a    Duncan (2011) Food safety in the poultry industry: An estimate of the\u000a      health benefits. 52nd NZAE\u000a      Annual Conference.\u000a    Letter of Support from the Director, mEpiLab, Institute of Veterinary,\u000a      Animal and Biomedical\u000a      Sciences, Massey University, New Zealand.\u000a    Campylobacter Risk Management Strategies 2008-2011, NZFSA.\u000a    Articles detailing the reduction in cases, and resulting economic\u000a      savings:\u000a      http:\/\/www.foodsafety.govt.nz\/elibrary\/industry\/Zealand_Leads-Efforts_Drastically.htm\u000a    Data on reduction of human campylobacter cases appears in:\u000a      Campylobacter Risk Management Strategies 2013-2014, NZFSA, (Figure 2, page\u000a      10)\u000a      http:\/\/www.foodsafety.govt.nz\/elibrary\/industry\/Campylobacter_Risk-Comprehensive_Aimed.pdf\u000a    The economic savings are also reported in\u000a      http:\/\/foodsafety.govt.nz\/elibrary\/industry\/economic-cost-foodborne-disease\/sis.pdf\u000a    \u000a    ","Title":"\u000a    Accurate statistical methods for detecting the source of human\u000a      campylobacteriosis cases in New\u000a      Zealand leads to an annual reduction of around 90,000 cases per year.\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    Research Team at Lancaster\u000a    The methodological research was led by Wilson, Fearnhead and Diggle as\u000a      part of a Veterinary\u000a      Training and Research Initiative grant to Liverpool and Lancaster, funded\u000a      by DEFRA and HEFCE.\u000a      Wilson was a PDRA funded by this grant. He was based in the Department of\u000a      Mathematics and\u000a      Statistics at Lancaster, and was supervised by Fearnhead.\u000a    Methodological Research (2005-2007)\u000a    The underpinning research involved developing a new method for detecting\u000a      the source of human\u000a      cases of campylobacteriosis, motivated by interest in source detection by\u000a      collaborators from the\u000a      Vet School at Liverpool. This method (Wilson et al. 2008) uses as input\u000a      genetic data of\u000a      Campylobacter isolates from both human cases and from different potential\u000a      animal and\u000a      environmental source populations. Comparing how similar the genetic type\u000a      of isolates from human\u000a      cases are to the genetic type of isolates from the different source\u000a      population enables one to\u000a      estimate the relative contribution of each source to the total number of\u000a      human cases. The key to\u000a      doing this accurately is obtaining good estimates of the population\u000a      frequency of different genetic\u000a      types in each of the animal and environmental sources, which requires a\u000a      form of density estimation\u000a      over the high-dimensional space of possible genetic types. The novelty of\u000a      our method was in\u000a      constructing a model-based approach to the density estimation, using\u000a      tractable approximations to\u000a      well-developed population genetic models. This builds on earlier work\u000a      developing similar\u000a      approximations for estimating recombination (Fearnhead and Donnelly 2001,\u000a      Li and Stephens\u000a      2003). The final method is substantially more accurate than cruder\u000a      alternatives, which often have\u000a      to throw away information in the data.\u000a    Application to New Zealand Data (2007)\u000a    During 2007, this method was applied to data from New Zealand, in\u000a      collaboration with Nigel\u000a      French's veterinary epidemiologist group at Massey University, New\u000a      Zealand. This collaboration\u000a      came out of close links between Lancaster and Massey, with French aware at\u000a      an early stage of our\u000a      research on detecting the source of human campylobacteriosis cases, a\u000a      problem his group were\u000a      also interested in. Lancaster supported this application of the research,\u000a      with Wilson collaborating\u000a      directly in the research of the group in New Zealand. Wilson was the sole\u000a      statistical geneticist\u000a      involved in this research, applying the method developed at Lancaster to\u000a      analyse the data. The\u000a      results of this analysis appeared later in Mullner et al (2009). This work\u000a      was carried out while\u000a      Wilson was at Lancaster, but the paper was published later, after Wilson\u000a      had moved to Chicago.\u000a    Results of Source Attribution in New Zealand\u000a    The results of the analysis showed that ~75% of human campylobacter cases\u000a      in New Zealand\u000a      were due to poultry sources. This was a much higher proportion than for\u000a      other developed\u000a      countries. In 2006 New Zealand had the highest rate of Campylobacteriosis\u000a      cases in the\u000a      developed world. The results from the source attribution were evidence\u000a      that poor standards in the\u000a      poultry industry were responsible for this high rate of Campylobacteriosis\u000a      cases in New Zealand.\u000a    The importance of the methodological work developed at Lancaster, and of\u000a      the input of Wilson to\u000a      the analysis of the New Zealand data, is attested to by Nigel French. In a\u000a      letter of support, he\u000a      states \"Source attribution models developed by Dr Wilson [...] helped\u000a        us to identify that poultry,\u000a        and one particular supplier, was responsible for the majority of human\u000a        cases in our sentinel site.\u000a        The highly cited paper published by Dr Wilson [et al.] in 2008 in PloS\u000a        Genetics provided a new tool\u000a        that could be applied to multilocus sequence typing data in New Zealand.\u000a        He made the models\u000a        readily available to us and was a highly effective collaborator,\u000a        enabling us to rapidly adopt the\u000a        method and communicate the findings to the New Zealand Food Safety\u000a        Authority. Informed by the\u000a        model outputs, the Campylobacter risk management policy was developed\u000a        and implemented, and\u000a        the public health response was rapid. The models provided the most\u000a        convincing evidence to date\u000a        of the importance of poultry as a source infection, enabling\u000a        interventions to be mandated.\"\u000a    "},{"CaseStudyId":"43599","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"1269750","Name":"India"},{"GeoNamesId":"3017382","Name":"France"},{"GeoNamesId":"798544","Name":"Poland"}],"Funders":[],"ImpactDetails":"\u000d\u000a    Impact on Design of Coastal Flood Defences\u000d\u000a    The North Sea floods of 1953 claimed 307 lives in the UK and 1,800 in\u000d\u000a      Netherlands. Following that event more rigorous methods of data collection\u000d\u000a      and analysis were used in coastal flood defence design. Large budgets are\u000d\u000a      involved, e.g., the total spend on defences on the UK east coast in\u000d\u000a      Lincolnshire, Norfolk, Suffolk and Essex was &#163;250M in 10 years. In\u000d\u000a      designing a sea-wall the key design factor is its height which needs to be\u000d\u000a      estimated to ensure an intended level of protection, which typically\u000d\u000a      corresponds to events larger than those already observed. If the wall is\u000d\u000a      built too high then substantial unnecessary costs can be incurred: 1m of\u000d\u000a      extra height on average costs &#163;150K per 100m length of wall with the UK\u000d\u000a      having about 1000km of walls (JBA 2013). If it is too small then the\u000d\u000a      intended level of protection will not be achieved, with flooding occurring\u000d\u000a      at a greater frequency than anticipated and hence unacceptable risks to\u000d\u000a      human life\/property and effects on longer-term property value. Therefore\u000d\u000a      major economic and societal benefits arise from optimising the wall\u000d\u000a      height.\u000d\u000a    Design standards for a typical coastal town are based on 100-year return\u000d\u000a      levels, which corresponds to the sea-wall height that would be exceeded on\u000d\u000a      average once in 100 years. Prior to the underpinning research for this\u000d\u000a      study, different methods had been used giving estimates for the 100-year\u000d\u000a      level return level at a site that varied by between 0.5-1m for each of the\u000d\u000a      30 sites with data and by much greater larger amounts at intermediate\u000d\u000a      sites without observations (Dixon and Tawn, 1997).\u000d\u000a    Using the new methods for inference for extreme still water levels\u000d\u000a      developed at Lancaster, Dixon and Tawn (1997) produced the first set of\u000d\u000a      estimates of still water extremes for the entire UK coastline. Following\u000d\u000a      strong recommendations from DEFRA and the Proudman Oceanographic\u000d\u000a      Laboratory (now the National Oceanography Centre) these values were used\u000a        in all designs of UK coastal flood defence designs from 1997 to 2010\u000d\u000a      (JBA, 2013). In 2008-10 Tawn provided the statistical expertise in a\u000d\u000a      consortium with JBA Consulting, the National Oceanography Centre and Royal\u000d\u000a      Haskoning to update the methods, exploit new data, and produce a new full\u000d\u000a      set of estimates for the Environment Agency that have been used\u000d\u000a      systematically by the Environment Agency and their clients since 2010\u000d\u000a      (Environment Agency, 2011, Batstone et al 2013). In particular, these\u000d\u000a      estimates have been used to determine the design of over 900 schemes,\u000d\u000a        total spend &#163;900M (JBA 2013), over the REF census period.\u000d\u000a    Even when the still water level does not breach the sea-wall, substantial\u000d\u000a      flooding can occur due to overtopping by waves. Using the methods\u000d\u000a      described in Section 2 for dependence modelling, joint work with HR\u000d\u000a      Wallingford (Bortot and Tawn, 1998; Hawkes et al., 2002) led to the\u000d\u000a      software JOINSEA. This software \"set the standard for joint probability\u000d\u000a        analysis in the UK flood risk studies, and probably remains the industry\u000d\u000a        standard for joint probability assessments in coastal assessments [...]\u000d\u000a        because it is practically the only method used.\" (HRW 2013). It has\u000d\u000a      been used throughout the REF census for determining the designs of all UK\u000d\u000a      coastal flood defences to reduce the wave overtopping to an acceptable\u000d\u000a      level. In 2010, Eastoe (at Lancaster) and Tawn with Royal Haskoning\u000d\u000a      produced for the Environment Agency joint distributions of wave\u000d\u000a      characteristics (wind-sea waves and swell waves) around the UK for more\u000d\u000a      systematic use.\u000d\u000a    Both the still water and wave estimates we produced are specified by the\u000d\u000a      Environment Agency (see e.g., Environment Agency, 2013; which refers to\u000d\u000a      these as Coastal Flood Boundary Data) as the key input into cost-benefit\u000d\u000a      analyses that have to be undertaken before any design for new flood\u000d\u000a      defences is approved. For still water level the estimates differ from\u000d\u000a      earlier values by 20-30 cm, which, if half are lower, corresponds to an\u000d\u000a      estimate of a saving of &#163;22.5M on 450 schemes of 100m long.\u000d\u000a      Additionally the creation of these estimates saves the UK government\u000d\u000a      paying consultant fees for deriving these estimates separately for each\u000d\u000a      site (previously typically costing &#163;15K per schemes, see JBA 2013) leading\u000d\u000a        to a saving since 2010 of over &#163;6M (based on 450 of the 900 schemes\u000d\u000a      in the REF census being developed since 2010). Stefan Laeger (Research\u000d\u000a      Scientist at the Environment Agency), writes:\u000d\u000a    \"The outputs from EA R&amp;D project SC060064 `Coastal Flood Boundary\u000d\u000a        Conditions' produced a new, up-to-date national evidence base and\u000d\u000a        dataset on design sea level conditions for mainland UK. This dataset is\u000d\u000a        now the de facto industry standard in mainland UK and has been used to\u000d\u000a        inform the vast majority of new coastal work (defences, strategies, risk\u000d\u000a        maps etc) since summer 2011. Through Tawn's expert input, we were able\u000d\u000a        to ensure that this dataset was produced by using improved, more\u000d\u000a        scientifically robust statistical methods for analysing these extreme\u000d\u000a        conditions.\"\u000d\u000a    Impact on Risk Assessment of River Flooding\u000d\u000a    The risk of river flooding is managed by society through a combination of\u000d\u000a      governmental agencies and the insurance industry. For both these types of\u000d\u000a      organisation an estimate of the likelihood of the large or widespread\u000d\u000a      flooding events is essential. Such estimates are used by government to\u000d\u000a      help in coordinating flood mitigation activities and by the insurance\u000d\u000a      industry to assess the financial risk of claims associated with their\u000d\u000a      insurance portfolio. These demands call for spatial extreme value methods,\u000d\u000a      for which the only viable option is the novel method developed by\u000d\u000a      Heffernan and Tawn (2004) at Lancaster and described in Section 2. The\u000d\u000a      need for such tools is illustrated in the UK by large-scale floods in\u000d\u000a      2000-1 (&#163;1B insurance loss), 2007 (&#163;3B insurance loss with 55,000\u000d\u000a      properties flooded) and 2012 (&#163;1B insurance loss).\u000d\u000a    The UKs two leading companies working on hydrological risk assessment,\u000d\u000a      JBA Consulting and HR Wallingford (combined annual turnover in excess of\u000d\u000a      &#163;40M), have interfaced Lancaster's conditional spatial extremes\u000d\u000a      statistical work developed for rivers with hydrological models and housing\u000d\u000a      databases to produce tools to quantify the risks of spatial dependence in\u000d\u000a      flooding for the first time (e.g., JBA's portfolio analysis tool `JCALF' http:\/\/www.jbarisk.com\/software,\u000d\u000a      Keef et al., 2013).\u000d\u000a    This software enables users to estimate accurately the distribution of\u000d\u000a      the annual total flood loss for their portfolio of insured properties.\u000d\u000a      This distribution is vital to the insurance companies in determining which\u000d\u000a      new properties to insure and for assessing how much reinsurance they\u000d\u000a      require to satisfy regulators. Without having an accurate model for the\u000d\u000a      joint distribution of different rivers flooding this loss distribution\u000d\u000a      cannot be accurately estimated and hence conservatism is applied in\u000d\u000a      estimates, resulting in substantial over-estimation of the amount of\u000d\u000a      reinsurance required. The key to this software is therefore the high\u000d\u000a      quality of the statistical model for multivariate extremes developed at\u000d\u000a      Lancaster and applied to extreme river levels jointly by research at\u000d\u000a      Lancaster and JBA Consulting,\u000d\u000a    An example of the use of this approach is that JBA Risk Management Ltd.\u000d\u000a      have developed such risk assessment products which have been licensed to\u000d\u000a      several major international clients operating in the catastrophe analysis\u000d\u000a      and reinsurance sector in the UK, France, Poland and India (JBA 2013).\u000d\u000a      Therefore there are substantial economic benefits from the research for\u000d\u000a      the insurance industry and consequently society.\u000d\u000a    ","ImpactSummary":"\u000d\u000a    The UK spends &#163;400-500M per year on flood defence infrastructure with 2\u000d\u000a      million properties exposed to the risk of flooding. Lancaster's research\u000d\u000a      on extreme value methods is fundamental to optimising the design of this\u000d\u000a      infrastructure to protect against coastal and river extreme events. This\u000d\u000a      optimisation minimises costs without jeopardising the level of accepted\u000d\u000a      risk and hence has financial and societal benefits. These methods are the\u000d\u000a      fundamental component in:\u000d\u000a    \u000d\u000a      The design of all UK's coastal flood defences, total spend of &#163;900M on\u000d\u000a        900 schemes over the REF census period.\u000d\u000a      Saving the UK government &#163;6M over this period from consultant fees.\u000d\u000a      International governments' and the insurance industry's assessment of\u000d\u000a        widespread river flooding risk.\u000d\u000a    \u000d\u000a    ","ImpactType":"Environmental","Institution":"\u000d\u000a    Lancaster University\u000d\u000a    ","Institutions":[{"AlternativeName":"Lancaster University","InstitutionName":"Lancaster University","PeerGroup":"B","Region":"North West","UKPRN":10007768}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    Key references\u000d\u000a    \u000aColes, S. G. and Tawn, J. A. (1994). Statistical methods for multivariate\u000d\u000a      extremes: an application to structural design (with discussion), Appl.\u000d\u000a      Statist., 43, 1-48.\u000d\u000a    \u000a\u000aDixon, J. M., Tawn, J. A. and Vassie, J. M. (1998). Spatial modelling of\u000d\u000a      extreme sea-levels, Environmetrics, 9, 283--301.\u000d\u000a    \u000a\u000aHeffernan, J. E. and Tawn, J. A. (2004). A conditional approach to\u000d\u000a      modelling multivariate extreme values (with discussion), J. Roy. Statist.\u000d\u000a      Soc., B, 66, 497--547.\u000d\u000a    \u000aOther references\u000d\u000a    \u000aBatstone, C., Lawless, M., Tawn, J. A., Horsburgh, K., Blackman, D.\u000d\u000a      McMillan, A., Worth, D., Laeger, S., and Hunt, T. (2013). A UK\u000d\u000a      best-practice approach for extreme sea level analysis along complex\u000d\u000a      topographic coastlines. Ocean Engineering, 71, 28-39\u000d\u000a    \u000a\u000aBortot, P, Coles, S. G. and Tawn, J. A. (2000). The multivariate Gaussian\u000d\u000a      tail model: an application to oceanographic data, Appl. Statist., 49,\u000d\u000a      31-49.\u000d\u000a    \u000a\u000aKeef, C., Tawn, J. A. and Svensson, C. (2009). Spatial risk assessment\u000d\u000a      for extreme river flows. Appl. Statist., 58, 601-618.\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"14","Level2":"3","Subject":"Econometrics"}],"Sources":"\u000d\u000a    Batstone, C., Lawless, M., Tawn, J. A., Horsburgh, K., Blackman, D.\u000d\u000a      McMillan, A., Worth, D., Laeger, S., and Hunt, T. (2013). A UK\u000d\u000a      best-practice approach for extreme sea level analysis along complex\u000d\u000a      topographic coastlines. Ocean Engineering, 71, 28-39\u000d\u000a    Bortot, P. and Tawn, J. A. (1998). Joint probability methods for extreme\u000d\u000a      still water levels and waves. HR Wallingford report, SR537, 234 pages.\u000d\u000a    Dixon, M. J. and Tawn, J. A. (1997). Spatial analyses for the UK,\u000d\u000a      Proudman Oceanographic Laboratory report, 112, 200 pages.\u000d\u000a    Environment Agency (2011). Coastal flood boundary conditions for UK\u000d\u000a      mainland and islands. Project: SC060064\/TR2: Design sea-levels.\u000d\u000a      Environment Agency of England and Wales.\u000d\u000a    Environment Agency (2013). Outline brief for Cornwall Coastal Flood Risk\u000d\u000a      Modelling: Plymouth Sound and Tamar Estuary. Environment Agency.\u000d\u000a    Hawkes, P. J., Gouldby, B. P., Tawn, J. A. and Owen, M. W. (2002). The\u000d\u000a      joint probability of waves and water levels in coastal defence design. J.\u000d\u000a      Hydraulic Research, 40, 241--251.\u000d\u000a    Keef, C., Tawn, J. A. and Lamb, R. (2013). Estimating the probability of\u000d\u000a      widespread flood events. Environmetrics, 24, 13-21.\u000d\u000a    Letter of Support from Principal Engineer, HR Wallingford Ltd (HRW 2013)\u000d\u000a    Letter of Support from Chief Scientist, JBA Consulting (JBA 2013).\u000d\u000a    Letter of Support from Research Scientist, Environment Agency. \u000d\u000a    ","Title":"\u000d\u000a    Optimisation of the UK's flood defence infrastructure through the use of\u000d\u000a      innovative statistical research on extreme values\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Overview of Research\u000d\u000a    Estimating the frequency of events that are more extreme than any\u000d\u000a      previous observation is a key element in environmental risk prevention and\u000d\u000a      assessment. Extreme value theory, a core research area at Lancaster for 20\u000d\u000a      years, provides mathematically justified models as the basis for\u000d\u000a      extrapolations from observed large events out to more extreme events. The\u000d\u000a      underpinning research at Lancaster for flooding comprises univariate,\u000d\u000a      multivariate and spatial extreme value analysis and consists of two papers\u000d\u000a      in the prestigious RSS discussion paper series as well as a series of\u000d\u000a      papers in the journal Applied Statistics. Bortot, Coles, Dixon, Heffernan,\u000d\u000a      Keef and Tawn were all based at Lancaster when the cited research was\u000d\u000a      undertaken. Our research provides novel statistical methodologies that\u000d\u000a      integrate knowledge of the structure of oceanographic and hydrological\u000d\u000a      processes with substantial developments in extreme value theory. Indirect\u000d\u000a      evidence of the importance of this research to the UK government is shown\u000d\u000a      by DEFRA and Environment Agency funding of &#163;780K to support Lancaster's\u000d\u000a      research on extreme value methods and their application to assessing the\u000d\u000a      risk of flooding.\u000d\u000a    Coastal Flooding\u000d\u000a    For coastal flood defences the two key design parameters, in order of\u000d\u000a      importance, are still water level (sea level with waves averaged out) and\u000d\u000a      overtopping rate (a combination of wave characteristics and still water\u000d\u000a      level).\u000d\u000a    Still water level is the sum of two components: tide (deterministic) and\u000d\u000a      surge (stochastic). Dixon and Tawn developed the first extreme value\u000d\u000a      methods which accounted for any of the following features: the tide-surge\u000d\u000a      decomposition; interaction between the tide and surge; the spatial\u000d\u000a      coherence of the surge; the use of hydro-dynamical models that predict the\u000d\u000a      tides at intermediate sites; and inference using all extreme events (see\u000d\u000a      e.g. Dixon et al. 1998). These developments required novel theory for\u000d\u000a      temporal extremes and sophisticated covariate and spatial smoothing for\u000d\u000a      extremes. The outcome was the first set of systematic estimates of extreme\u000d\u000a      still water levels for the entire UK coastline that has no bias and much\u000d\u000a      smaller confidence intervals than previous methods. Tawn provided the\u000d\u000a      statistical expertise in a recent adaptation of these methods by Batstone\u000d\u000a      et al. (2013) that updated estimates by using the additional recent data\u000d\u000a      and instead of surge used a characteristic known as skew surge that\u000d\u000a      removed the need to model the tide-surge interaction.\u000d\u000a    For extremes of overtopping rate the joint distribution of still water\u000d\u000a      level, wave height, wave period and direction needs modelling in its\u000d\u000a      extreme tails. This involves univariate modelling of the tails but also\u000d\u000a      multivariate extreme value theory for the dependence structure. Coles and\u000d\u000a      Tawn (1994) identified the benefit of treating these approaches as\u000d\u000a      multivariate, developed a generic Poisson process approach and illustrated\u000d\u000a      its use in the overtopping problem. The methods were later tuned for\u000d\u000a      overtopping by Bortot et al. (2000) using simplified dependence models\u000d\u000a      that gave sufficient flexibility but were robust.\u000d\u000a    River Flooding and Risk Assessment\u000d\u000a    Until Heffernan and Tawn (2004) multivariate extreme value methods were\u000d\u000a      restricted to low dimensional cases with restricted forms of dependence\u000d\u000a      structure, with the methods relying on an underlying assumption of\u000d\u000a      multivariate regular variation. Heffernan and Tawn (2004) addressed the\u000d\u000a      problem from an orthogonal approach by looking at limit theory\u000d\u000a      conditionally on a component of the vector variable being extreme. This\u000d\u000a      opened the methodology to substantive application for high dimensional\u000d\u000a      analyses and a broad range of dependence structures. The work was extended\u000d\u000a      and tailored for application to river flooding in Keef et al. (2009),\u000d\u000a      addressing issues such as how to deal with missing data and temporal\u000d\u000a      dependence.\u000d\u000a    "},{"CaseStudyId":"43600","Continent":[{"GeoNamesId":"6255148","Name":"Europe"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"2750405","Name":"Netherlands"},{"GeoNamesId":"1861060","Name":"Japan"}],"Funders":[],"ImpactDetails":"\u000d\u000a    High Court Re-Opened Formal Investigation of the sinking of the M.V.\u000d\u000a        Derbyshire: 2000\u000d\u000a    The largest UK ship lost at sea is the bulk carrier M.V. Derbyshire that\u000d\u000a      sank in September 1980 when she encountered a typhoon near Japan. All of\u000d\u000a      the 44 people on board were lost and no mayday was signalled. The reason\u000d\u000a      for her sinking became the focus of a series of reports and investigations\u000d\u000a      that were carried out over the following 20 years and culminating in the\u000d\u000a      &#163;11M High Court Re-Opened Formal Investigation in 2000.\u000d\u000a    The judge, Colman (2000) paragraph 6.13, described the contribution that\u000d\u000a      the extremes values group at Lancaster made to identify the cause of the\u000d\u000a      sinking as of `absolutely fundamental importance to the outcome of this\u000d\u000a        Investigation'. Similarly Hansard (2002) reports `Professor Tawn\u000d\u000a        concluded from that data that the flooding of both the stores and the\u000d\u000a        ballast tank&#8212;even the stores alone&#8212;could have produced sufficient loss\u000d\u000a        of freeboard to expose hatch cover No.1 to at least one hatch-breaking\u000d\u000a        wave during the typhoon on 9 September 1980. Therefore, hatch cover\u000d\u000a        strength became crucial, not only to the safety of the Derbyshire, but\u000d\u000a        to all similar vessels, many of which are still navigating the oceans\u000d\u000a        today, thus putting hundreds more lives at risk.'\u000d\u000a    High Court Judge calls for new design standards for hatch covers of\u000d\u000a        carriers: 2000\u000d\u000a    In addition to establishing the likely cause of the M.V. Derbyshire's\u000d\u000a      loss the Re-Opened Formal Investigation's report questioned the adequacy\u000d\u000a      of current regulations governing hatch cover strengths. There were 99 bulk\u000d\u000a      carrier sinkings between 1990 and 1997 with 650 lives lost\u000d\u000a      (http:\/\/en.wikipedia.org\/wiki\/Bulk_carrier)\u000d\u000a      with causes similar to the M.V. Derbyshire.\u000d\u000a    Hatches were found to represent the most significant point of\u000d\u000a      vulnerability for bulk carriers. Colman (2000, Para 62 of the Summary)\u000d\u000a      stated that the current international standard for hatch cover strength is\u000d\u000a      \"seriously deficient in the context of present day concepts of\u000d\u000a        acceptable safety standards.\" He requested further research as a\u000d\u000a      matter of urgency with the work to be carried out by the researchers from\u000d\u000a      the Re-Opened Formal Investigation - specifically by Lancaster's\u000d\u000a      statisticians, Lloyds Registry of Shipping and the Netherlands Maritime\u000d\u000a      Research Institute. He insisted that this study should carry sufficient\u000d\u000a      authority that the International Association of Classification Societies\u000d\u000a      and the International Maritime Organisation should adopt its findings and\u000d\u000a      thus ensure that new design standards would be mandatory globally.\u000d\u000a    Lancaster's statisticians identify new design standard: 2000-2002\u000d\u000a    Funded by the then named Department of Environment, Transport and the\u000d\u000a      Regions, Lancaster's statisticians provided the entire statistical\u000d\u000a      methodology and analysis for the study to develop new design standards and\u000d\u000a      also had input into the design of the study. The research they conducted\u000d\u000a      showed that the strength of hatch covers needed to be increased by 35%\u000d\u000a      from the previous design standards.\u000d\u000a    Ratification of a new Worldwide Mandatory Design Standard: 2003-2004\u000d\u000a    It was agreed that the 35% increase in design standard should become\u000d\u000a      internationally accepted. The process of ratifying this change was\u000d\u000a      complex, passing through a series of meetings of the Maritime Safety\u000d\u000a      Committee and the International Association of Classification Societies,\u000d\u000a      as explained below.\u000d\u000a    Maritime Safety Committee (2002a) shows evidence of progress of the\u000d\u000a      research and Lancaster's contribution. That report documents the committee\u000d\u000a      encouraging the International Association of Classification Societies to\u000d\u000a      rewriting URS21, the design standards for bulk carriers, regarding hatch\u000d\u000a      covers. Maritime Safety Committee (2002b) sets out a framework for the\u000d\u000a      implementation to new and existing ships, with the International\u000d\u000a      Association of Classification Societies updating URS21 in April 2003. The\u000d\u000a      section for URS21 states that its 3rd revision in 2003 came\u000d\u000a      about due to the work undertaken following the Re-Opened Formal\u000d\u000a      Investigation into the loss of the M.V. Derbyshire. The exact changes are\u000d\u000a      a revision of the method for calculating the strength formulation for\u000d\u000a      hatch covers along with the design of the hatch covers themselves. The\u000d\u000a      exact values of this standard vary depending upon the length of the ship,\u000d\u000a      specifically whether it is over, or under, 100m. Finally, the Maritime\u000d\u000a      Safety Committee (2004) adopted the changes and amendment was then made to\u000d\u000a      the SOLAS treaty (an international maritime safety treaty), regulation 7,\u000d\u000a      making this design standard mandatory for U.N. countries.\u000d\u000a    Quantification of Impact: 2004 onwards\u000d\u000a    No amendments to the design standard regulations have been made since\u000d\u000a      2004 and so the global fleet of ocean-going carriers operating through the\u000d\u000a      REF census period follow these new regulations. Specifically, since 1st\u000d\u000a      January 2004, all ship builders that are members of the International\u000d\u000a      Association of Classification Societies must obey these standard\u000d\u000a      requirements and build their ships to this standard. For all 1720 bulk\u000d\u000a      carriers built between 2008 and 12 the strength of hatch covers has been\u000d\u000a      increased by 35% from the previous design standards, and for the 5830\u000d\u000a      previously built bulk carriers hatches were strengthened and new\u000d\u000a      inspection and maintenance procedures were required (see Intercargo,\u000d\u000a      2011). Furthermore, from 2004 the International Association of\u000d\u000a      Classification Societies decided they would apply these rules not just to\u000d\u000a      bulk carriers, but to ore carriers and combination carriers as well.\u000d\u000a    There have been no sinkings of ocean-going bulk carriers since the new\u000d\u000a      design standards were introduced in 2004, whereas on past evidence over\u000d\u000a      100 such sinkings of ocean-going bulk carriers would have been expected in\u000d\u000a      the REF census period. This drastically improved safety record has\u000d\u000a      provided substantial benefits for the shipping industry, insurers and\u000d\u000a      governments (as illustrated by the parties actively involved in the\u000d\u000a      Re-Opened Formal Investigation of the M.V. Derbyshire).\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Research on extreme value methods by Heffernan and Tawn at Lancaster,\u000d\u000a      which proved critical in determining the conclusions of the High Court's\u000d\u000a      investigation of the sinking of the M.V. Derbyshire, also identified that\u000d\u000a      design standards for the strength of hatch covers of ocean-going carriers\u000d\u000a      (bulk carriers, ore carriers and combination carriers) needed to be\u000d\u000a      increased by 35%. This new level was set as a worldwide mandatory standard\u000d\u000a      in 2004. During the REF census period this change has impacted on the\u000d\u000a      design of 1720 new carriers and strengthening for the 5830 in service.\u000d\u000a      There have been no sinkings of ocean-going bulk carriers since the new\u000d\u000a      design standards were introduced in 2004, whereas on past evidence over\u000d\u000a      100 such sinkings of ocean-going bulk carriers would have been expected in\u000d\u000a      the REF census period.\u000d\u000a    ","ImpactType":"Political","Institution":"\u000d\u000a    Lancaster University\u000d\u000a    ","Institutions":[{"AlternativeName":"Lancaster University","InstitutionName":"Lancaster University","PeerGroup":"B","Region":"North West","UKPRN":10007768}],"Panel":"B         ","PlaceName":[],"References":"\u000d\u000a    Key references\u000d\u000a    \u000aHeffernan, J. E. and Tawn, J. A. (2001). Extreme value analysis of a\u000d\u000a      large designed experiment: a case study in bulk carrier safety. Extremes,\u000d\u000a      4, 359--378.\u000d\u000a    \u000a\u000aHeffernan, J.E. and Tawn J.A. (2003) An extreme value analysis for the\u000d\u000a      investigation into the sinking of the M.V. Derbyshire. Appl. Statist. 52,\u000d\u000a      337-354.\u000d\u000a    \u000a\u000aTawn, J.A. and Heffernan, J.E. (2001). Summary of statistical analysis of\u000d\u000a      the seakeeping model tests, p41-54, Proceedings of the Royal Institution\u000d\u000a      of Naval Architects conference Design &amp; Operation of Bulk Carriers\u000d\u000a      Post M.V. Derbyshire. London.\u000d\u000a    \u000aOther references\u000d\u000a    \u000aHeffernan, J. E. and Tawn, J. A. (2004a). A conditional approach to\u000d\u000a      modelling multivariate extreme values (with discussion). J. Roy. Statist.\u000d\u000a      Soc., B, 66, 497-547.\u000d\u000a    \u000a\u000aHeffernan, J. E. and Tawn, J. A. (2004b). Extreme values in the dock.\u000d\u000a      Significance, 1, 13-17.\u000d\u000a    \u000a\u000aNadarajah, S., Anderson, C. W. and Tawn, J. A. (1998). Ordered\u000d\u000a      multivariate extremes, J. Roy. Statist. Soc., B, 60, 473-496.\u000d\u000a    \u000a","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"9","Level2":"11","Subject":"Maritime Engineering"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"}],"Sources":"\u000d\u000a    Colman, Mr Justice (2000) Report of the Re-opened Formal Investigation\u000d\u000a      into the Loss of the M. V. Derbyshire. London: Stationery Office. (Section\u000d\u000a      6.13 and 6.14, 11, 14 and Appendix 17).\u000d\u000a    House of Commons Hansard Debates (2002) 25 Jun 2002: Column 195WH\u000d\u000a    Intercargo (2011) Benchmarking Bulk Carriers, 5th Edition.\u000d\u000a    Maritime Safety Committee (2002a). Bulk Carrier Model Test Progress\u000d\u000a      Report. 75th Session Agenda item 5.\u000d\u000a    Maritime Safety Committee (2002b). Report of The Maritime Safety\u000d\u000a      Committee on its Seventy- Sixth Session. Agenda item 5.\u000d\u000a    Maritime Safety Committee (2004). Report of The Maritime Safety Committee\u000d\u000a      on its Seventy-Ninth Session. Agenda item 3 and Annex 2.\u000d\u000a    International Association of Classification Societies (2011).\u000d\u000a      Requirements concerning strengths of ships. For information on updates of\u000d\u000a      URS21.\u000d\u000a    ","Title":"\u000d\u000a    Investigating the sinking of the M.V. Derbyshire and the setting of\u000d\u000a      global design standards for bulk carriers using statistical extreme value\u000d\u000a      research\u000d\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Research for the MV Derbyshire: 2000\u000d\u000a    The research for this shipping case study stemmed from an approach by Mr\u000d\u000a      Justice Colman who was presiding over the High Court's Re-Opened Formal\u000d\u000a      Investigation into the sinking of the M.V. Derbyshire in 2000. Tawn was\u000d\u000a      appointed the sole expert to deal with statistical issues. Research was\u000d\u000a      required to estimate the probability of the M.V. Derbyshire incurring\u000d\u000a      excessive wave impacts on its hatch covers in the typhoon when it sank. In\u000d\u000a      particular, a series of estimates were required under different scenarios\u000d\u000a      of the running of the ship.\u000d\u000a    By drawing heavily on 20 years of research on extreme value methods at\u000d\u000a      Lancaster, Heffernan and Tawn were able to integrate complex data and\u000d\u000a      knowledge into their estimates of the probability of sinking. This\u000d\u000a      information included: hindcast wave data for the time of the sinking, wave\u000d\u000a      impact data from tank studies performed on a replica of the M.V.\u000d\u000a      Derbyshire in a range of wave conditions, engineering knowledge about the\u000d\u000a      state of the vessel and different operating conditions. Heffernan and Tawn\u000d\u000a      derived estimates and confidence intervals of the probability of the ship\u000d\u000a      sinking under a range of operating scenarios. The results gave a clear\u000d\u000a      delineation into the potential causes of the sinking and a formal\u000d\u000a      quantification of the knowledge of the naval architects involved in the\u000d\u000a      investigation (see Heffernan and Tawn, 2003, 2004b).\u000d\u000a    These modelling approaches required the incorporation of changes in the\u000d\u000a      covariates over the duration of the typhoon that sank the M.V. Derbyshire.\u000d\u000a      The research developed new measures of fit pooled over covariates.\u000d\u000a      Advanced extreme value methods were essential to deriving reliable\u000d\u000a      estimates given that the required wave impact on the ship's hatch cover to\u000d\u000a      have led to the sinking was larger than any derived in the tank studies.\u000d\u000a      Longer running of the tank studies was impossible, so the only way to\u000d\u000a      achieve the required information of the investigation was through extreme\u000d\u000a      value methods. This was the first use of extreme value methods in naval\u000d\u000a      architecture.\u000d\u000a    Research to set new design standards: 2000-2002\u000d\u000a    The report of the High Court Re-Opened Formal Investigation in 2000\u000d\u000a      proposed further study to review the adequacy of the current international\u000d\u000a      design standards for shipping in relation to hatch strength of ocean-going\u000d\u000a      carriers. Heffernan and Tawn undertook all the statistical analysis in\u000d\u000a      that study using tank data supplied by the Netherlands Maritime Research\u000d\u000a      Institute. The data contained wave impacts on the hatch covers and walls\u000d\u000a      for a range of bulk carrier designs and extreme wave conditions. A highly\u000d\u000a      efficient analysis of wave impacts was derived covering all bulk carrier\u000d\u000a      types, including using: extreme value threshold methods; pooling different\u000d\u000a      studies to ensure consistency; and exploiting joint dependence and\u000d\u000a      stochastic ordering between impacts on the hatch covers and the walls.\u000d\u000a      Resulting estimates of the distribution of extreme wave impacts on these\u000d\u000a      hatch covers and walls were then derived over the range of operating\u000d\u000a      conditions. The details of this modelling are described in Heffernan and\u000d\u000a      Tawn (2001) and Tawn and Heffernan (2001) but with key features of the\u000d\u000a      approaches building on research in Heffernan and Tawn (2004a) and\u000d\u000a      Nadarajah et al. (1998), with the former an Royal Statistical Society\u000d\u000a      discussion paper cited in Heffernan and Tawn (2001) but appearing later.\u000d\u000a    "},{"CaseStudyId":"44352","Continent":[{"GeoNamesId":"6255150","Name":"South America"},{"GeoNamesId":"6255147","Name":"Asia"}],"Country":[{"GeoNamesId":"3469034","Name":"Brazil"},{"GeoNamesId":"1861060","Name":"Japan"}],"Funders":[],"ImpactDetails":"\u000a    The impact of the 2001 Mathon and Umerski paper predicting that MgO-based\u000a      systems would exhibit very high MR ratios was immediate. Preliminary\u000a      results were presented at the ICM 2000 Conference in Recife, Brazil, with\u000a      a number of world leading experimentalists in the audience. In addition\u000a      the group were in close communication with experimental groups in Japan\u000a      and the US where high-quality magnetic layer systems were being developed.\u000a      Nevertheless it took three years to manufacture successfully MgO tunnel\u000a      junctions of sufficient quality that a large MR ratio was observed. The\u000a      difficulty arises because the phenomenon is of quantum mechanical origin\u000a      and consequently requires very high quality interfaces. Several\u000a      experimental papers were published during the period 2002-2004, with\u000a      observed MR ratios gradually increasing. In December 2004, the Japanese\u000a      Tsukuba group simultaneously with the US IBM group reported values of\u000a      180%-220% at room temperature, in Nature Materials [3,4]. Both these\u000a      experimental papers cite the predictions of Mathon and Umerski and of\u000a      Butler et al. as motivation and there can be little doubt that the\u000a      results of these theoretical papers both laid the groundwork for success\u000a      and gave experimentalists the impetus required to keep trying until that\u000a      success was achieved.\u000a    Following the experimental success, focus shifted to the creation of a\u000a      commercial product. This required the mass production of MgO-based tunnel\u000a      devices, previously only produced in two world- leading laboratories\u000a      drawing on great experimental expertise. Significant technological\u000a      developments were required in order to do this, principally pioneered by\u000a      the Japanese Tsukuba group and the US IBM group (reviewed in [G] and [H]).\u000a      The first TMR read head reached the market in 2007. Since 2009 all\u000a      manufactured hard disks have been based on this technology. This is\u000a      confirmed by S.S.P. Parkin (head of the research group at IBM and the\u000a      author of reference 4) who states: \"The work of Mathon and Umerski\u000a        clearly played an important role in the development of these materials\u000a        and their subsequent widespread application to recording read heads in\u000a        ~2007. All disk drives manufactured since about 2008-2009 use recording\u000a        read heads based on magnetic tunnel junctions.\" [B]\u000a    Today there are only three major manufacturers of hard disk drives:\u000a      Western Digital, Seagate and Toshiba [A]. In 2008 Western Digital reported\u000a      that \"the industry has made the transition to tunnel-junction magneto\u000a        resistive (\"TMR\") technology for the head reader function. We have\u000a        completed the transition to PMR [Perpendicular Magnetic Recording] and\u000a        TMR in our 2.5-inch products and in the majority of our 3.5-inch\u000a        products\" [C]. In 2009 they reported \"We have completed the\u000a        transition to PMR and TMR across all product platforms\" [D]. An\u000a      example of the use of TMR technology by Toshiba is given in their product\u000a      information for internal notebook hard drives which \"use proven state\u000a        of the art....TMR Head Recording technology for increased capacity,\u000a        reliability and performance\" [E].\u000a    High-tech companies are naturally somewhat reluctant to disclose the\u000a      detailed operation of their products so it is difficult to trace the\u000a      development of MgO tunnel junctions once they left the confines of\u000a      university laboratories. However, the technological benefit of MgO-based\u000a      read heads to the companies which manufacture hard disks is perhaps most\u000a      clearly demonstrated in the following claim of industrial espionage. The\u000a      American Arbitration Association reported on a 5-year dispute between\u000a      Seagate and Western Digital regarding an employee (Dr Mao) who moved from\u000a      Seagate to Western Digital in September 2006 at a critical time when hard\u000a      disk manufacturers were developing the new MgO-based read heads [F]. The\u000a      report provides the following summary of the background to the case\u000a      (Section A5 on page 4): \"Seagate claims that Dr Mao stole Seagate trade\u000a        secrets and confidential information regarding TMR technology and\u000a        provided it to Western Digital, which used trade secrets and\u000a        confidential information to design and manufacture an MgO TMR read head.\u000a        As a result Seagate claims that Western Digital was able to introduce\u000a        products, incorporating an MgO TMR read head, into the market many\u000a        months ahead of when it would have been able to do so without Seagate\u000a        trade secrets and confidential information. Seagate asserts that as a\u000a        result of using stolen Seagate trade secrets and confidential\u000a        information, Western Digital avoided substantial research and\u000a        development costs and made substantial profits.\" The alleged move of\u000a      Dr Mao is likely to have saved Western Digital many months of research and\u000a      development. In 2011 the American Arbitration Association ruled that for\u000a      this infringement Seagate was entitled to recover $525,000,000 plus\u000a      pre-award interest at 10% per annum. This demonstrates the enormous\u000a      financial importance of the new MgO-based technology.\u000a    It is difficult to say precisely what the increase in hard disk capacity\u000a      is as a result of the MgO-based read head. Firstly, because other factors\u000a      like write density are also important. Secondly. because MgO-based read\u000a      heads are still being developed, with improvements in growth morphology\u000a      leading to greater MR ratios. We can say that in 2005, just before the new\u000a      read heads were introduced, Toshiba introduced a hard drive with a\u000a      record-breaking storage density of 179 Gbit\/in2. In 2012 the\u000a      highest density commercially available was about 620Gb\/in2\u000a      although work continues towards a 1Tb\/in2 drive, demonstrated\u000a      by Seagate with great fanfare in March 2012 [J]. An increase by a factor\u000a      of five to date can certainly be claimed. The difficulty at present\u000a      appears to be with stable recording rather than reading, and for the near\u000a      future the MgO based read head looks likely to remain.\u000a    The role of Mathon and Umerski's work in the emergence of MgO-barrier\u000a      magnetic tunnel junctions is emphasised in the reviews cited at [G] and\u000a      [H]. The 20th Tsukuba Prize was awarded to Drs Yuasa and Suzuki\u000a      (authors of reference 3) for \"Giant tunnel magnetoresistance in MgO-based\u000a      magnetic tunnel junctions and its industrial applications\". The prize\u000a      citation traces a direct path from the theoretical prediction to the\u000a      industrial application. It emphasises the significance of the industrial\u000a      application and impact on society of TMR technology, stating: \"The\u000a        giant TMR effect in MgO MTJs (magnetic-tunnel-junctions) is expected to\u000a        contribute to our society by significantly reducing the power\u000a        consumption of electronics devices and improving the performance and\u000a        security of computers\" [I].\u000a    The MR ratio observed in MgO-based systems is now very close to the\u000a      original theoretical prediction of 1000%. The conclusions of the Mathon\u000a      and Umerski theoretical paper, later confirmed experimentally, have\u000a      directly influenced the design of the hard disk reading head commercially\u000a      manufactured since 2008. This has led to increase in hard disk storage\u000a      capacity by more than a factor of five. The original 2001 publication has\u000a      attracted more than 500 citations and, along with that of Butler et\u000a        al., is regarded as a seminal paper in spintronics, one source of\u000a      the explosion of interest in MgO-based systems. Such systems are also the\u000a      basis of magnetic random access memory (MRAM), a new type of non-volatile\u000a      memory, which is being actively developed and may someday replace both\u000a      hard disks and existing random access memory [G, H].\u000a    ","ImpactSummary":"\u000a    The significant increase in hard disk storage capacity in the last few\u000a      years can be in part attributed to theoretical research in Mathematics\u000a      undertaken at City University London. A material or device is said to\u000a      exhibit the property of magnetoresistance if its electrical resistance\u000a      changes when the direction of an external magnetic field is varied. The\u000a      work undertaken at City concluded that devices based on magnesium oxide\u000a      (MgO) would exhibit magnetoresistances very much larger than previously\u000a      observed. In 2004 these conclusions were confirmed experimentally. By 2008\u000a      a new type of disk read head (the device that senses data on a magnetic\u000a      disk) based on this structure was being manufactured commercially,\u000a      enabling a significant increase in hard disk storage capacity. Today all\u000a      computer hard disks use read heads based on this technology in an industry\u000a      with 2012 sales exceeding $28 billion. The increase in hard disk storage\u000a      capacity achieved (from gigabytes to terabytes: 1 terabyte = 1,000\u000a      gigabytes) and the consequent improvement in disk performance for users\u000a      can be partly attributed to the City research.\u000a    ","ImpactType":"Technological","Institution":"\u000a    City University London\u000a    ","Institutions":[{"AlternativeName":"City University, London","InstitutionName":"City University, London","PeerGroup":"C","Region":"London","UKPRN":10001478}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000a[1] Mathon J. &amp; Umerski A. (2001) Theory of tunneling\u000a      magnetoresistance of an epitaxial Fe\/MgO\/Fe(001) junction Phys. Rev.\u000a        B, 63, 220403(R) 10.1103\/PhysRevB.63.220403\u000a    \u000a\u000a[2] Butler W. H. et al. (2001) Spin-dependent tunneling\u000a      conductance of Fe|MgO|Fe sandwiches Phys. Rev. B, 63,\u000a      054416 10.1103\/PhysRevB.63.054416\u000a    \u000a\u000a[3] Yuasa S., Nagahama T., Fukushima A., Suzuki Y., &amp; Ando K. (2004)\u000a      Giant room-temperature magnetoresistance in single-crystal Fe\/MgO\/Fe\u000a      magnetic tunnel junctions Nature Mat., 3, 868 10.1038\/nmat1257\u000a    \u000a\u000a[4] Parkin S. S. P. et al. (2004) Giant tunnelling\u000a      magnetoresistance at room temperature with MgO (100) tunnel barriers Nature\u000a        Mat., 3, 862 10.1038\/nmat1256\u000a    \u000a\u000a[5] Autes G., Mathon J. &amp; Umerski A. (2010) Strong Enhancement of the\u000a      Tunneling Magnetoresistance by Electron Filtering in an\u000a      Fe\/MgO\/Fe\/GaAs(001) Junction Phys. Rev. Lett., 104, 217202\u000a      10.1103\/PhysRevLett.104.217202\u000a    \u000a\u000a[6] Autes G., Mathon J. &amp; Umerski A. (2011) Theory of ultrahigh\u000a      magnetoresistance achieved by k-space filtering without a tunnel barrier Phys.\u000a        Rev. B, 83, 052403 10.1103\/PhysRevB.83.052403\u000a    \u000aPhysical Review B is one of the top journals in its field. Articles\u000a      undergo rigorous double-blind peer review prior to publication.\u000a    ","ResearchSubjectAreas":[{"Level1":"2","Level2":"99","Subject":"Other Physical Sciences"},{"Level1":"3","Level2":"2","Subject":"Inorganic Chemistry"},{"Level1":"9","Level2":"12","Subject":"Materials Engineering"}],"Sources":"\u000a    [A] Bizmology article: `Consolidation in the hard disk drive market: then\u000a      there were three'\u000a      http:\/\/bizmology.hoovers.com\/2012\/03\/19\/consolidation-in-the-hdd-hard-disk-drive-market-then-there-were-three\/\u000a      (Archived by WebCite&#174; at http:\/\/www.webcitation.org\/6BhSl9Ylg)\u000a    [B] Letter from S.S.P. Parkin FRS, Magnetoelectronics Manager, IBM\u000a      Almaden Research Center, confirming the important role of Umerski and\u000a      Mathon in the development of Fe\/MgO\/Fe TMR junctions (Sept 2012).\u000a    [C] Western Digital 2008 Annual Report and Form 10-K\u000a      https:\/\/materials.proxyvote.com\/Approved\/958102\/20080917\/AR_27910\/images\/Western_Digit\u000a        al-AR2008.pdf. (Archived by WebCite&#174; at http:\/\/www.webcitation.org\/6BX5o4wht)\u000a    [D] Western Digital 2009 Annual Report and Form 10-K\u000a      https:\/\/materials.proxyvote.com\/Approved\/958102\/20090916\/AR_46224\/HTML2\/default.htm.\u000a      (Archived by WebCite&#174; at http:\/\/www.webcitation.org\/6BX5xyQ8a).\u000a    [E] Toshiba Storage Products `Internal Notebook Hard Drives' product\u000a      details webpage http:\/\/storage.toshiba.com\/storagesolutions\/archived-models\/internal-notebook-hard-drives.\u000a      (Archived by WebCite&#174; at http:\/\/www.webcitation.org\/6BX6Eic0N)\u000a    [F] http:\/\/amlawdaily.typepad.com\/01302012western_interim.pdf\u000a        (Archived by WebCite&#174; at\u000a      http:\/\/www.webcitation.org\/6DwPHDu1U).\u000a    [G] S Yuasa and D D Djayaprawira, J. Phys. D: Appl. Phys. 40 (2007)\u000a        R337-R354. Particularly the conclusion Section 7 which contains a brief\u000a        summary.\u000a    [H] IEEE Transactions on Electron Devices 54 991 (2007) section\u000a      3A.\u000a    [I] Citation for the 20th Tsukuba prize\u000a      http:\/\/www.suzukiylab.mp.es.osaka-u.ac.jp\/Top\/tsukuba_english.pdf\u000a        (Archived by WebCite&#174; at\u000a      http:\/\/www.webcitation.org\/6BX6MlwDd).\u000a      http:\/\/storageeffect.media.seagate.com\/2012\/03\/storage-effect\/paving-the-way-for-big-hard-drive-capacity-gains\/\u000a        (Archived by WebCite&#174; at http:\/\/www.webcitation.org\/6DzIQc2I7)\u000a    ","Title":"\u000a    Tunneling Magnetoresistance: from theoretical proposal to practical\u000a        application\u000a    ","UKLocation":[],"UKRegion":[],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The modern day hard disk reading head is the first commercial device to\u000a      use the spin of an electron rather than its charge to control the\u000a      resistance. It exploits the phenomenon of magnetoresistance which is an\u000a      important part of a new multidisciplinary field of study called\u000a      spintronics, which attempts to exploit electron spin in solid state\u000a      devices. The Mathematics Department at City University London has been\u000a      involved in the spintronics field from an early stage. The work was led by\u000a      Professor J Mathon (at City since 1970) with Dr A Umerski (Research\u000a      Assistant at City 1996-2000). The initial research addressed interlayer\u000a      exchange coupling and magnetoresistance effects in metallic and model\u000a      tunneling systems. This work set the foundation for the 2001 publication\u000a      on tunneling magnetoresistance in magnesium oxide (MgO) [1], which is\u000a      regarded as a very significant achievement in the field of spintronics\u000a      with over 800 citations. It should be noted that the calculation of this\u000a      effect posed significant technical problems because it required a highly\u000a      accurate calculation of the tiny tunneling current. This demanded\u000a      significant expertise in spintronics.\u000a    In 1989 a magnetoresistance (MR) of quantum mechanical origin, which\u000a      utilises the spin of an electron, was discovered by Albert Fert and Peter\u000a      Grunberg who later received the 2007 Nobel Prize for their work. This\u000a      discovery was quickly developed commercially. In 1997 IBM brought out a\u000a      hard disk drive (HDD) in which the read head used this effect to sense the\u000a      magnetic `bits' of the disk. By the late 1990s, all hard disk reading\u000a      heads were based on this form of MR. This development is primarily\u000a      responsible for the increase in disk storage density from 0.1 to 100\u000a      Gbit\/in2 between 1991 and 2003.\u000a    The efficiency of the modern-day reading head depends on a quantity\u000a      called the magnetoresistance ratio (MR ratio). This can be thought of as\u000a      the maximum percentage change in resistance as the direction of applied\u000a      magnetic field is varied. A reading head with a large MR ratio implies a\u000a      more sensitive device which can read smaller magnetic bits on the hard\u000a      disk, enabling a higher storage density.\u000a    Early metallic-based read heads, used commercially from 1997 until 2005,\u000a      had MR ratios limited to less than 50%. Developments using an amorphous\u000a      insulating barrier produced MR ratios of up to 70% and; this was briefly\u000a      used in Seagate read heads in 2005.\u000a    In 2001 Mathon and Umerski at City [1], simultaneously with a group in\u000a      the US [2], proposed an entirely new system based on a crystalline\u000a      insulator, magnesium oxide (MgO). The underlying physics, based on\u000a      coherent, spin-dependent, quantum electron tunneling through the\u000a      crystalline MgO barrier, is entirely different to the earlier systems. It\u000a      was predicted that the MR ratio of this novel tunneling device could\u000a      exceed 1000%, some 15 times higher than previously achieved. This MR\u000a      effect is called tunneling magnetoresistance (TMR) and the\u000a      magnetoresistive MgO system is referred to as an MgO tunnel junction.\u000a    Nanostructure devices were successfully produced three years later when\u000a      two world-leading experimental groups, one from Tsukuba in Japan and one\u000a      from IBM Almaden in the US, published separate confirmation of large MR\u000a      ratio in MgO-based systems in the same issue of Nature Materials [3, 4].\u000a      The first such TMR read head reached the market in 2007 and since 2009 all\u000a      manufactured hard disks have been based on this technology (see\u000a      corroborating source B).\u000a    Our theoretical research into coherent tunneling in MgO based tunneling\u000a      junctions continues. Our latest proposal to enhance significantly the MR\u000a      ratio is based on the so-called collimation effect achieved by attaching\u000a      the standard MgO junction to a semiconductor lead. This work is described\u000a      in [5] and [6]. The theoretical expectation is that the MR ratio in\u000a      collimated junctions could be enhanced by a factor of 100.\u000a    "},{"CaseStudyId":"44353","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"2635167","Name":"United Kingdom"}],"Funders":[],"ImpactDetails":"\u000d\u000a    The underpinning research demonstrated how complex mathematical\u000d\u000a      optimisation techniques can\u000d\u000a      bring about significant positive impacts on the quality of rostering for\u000d\u000a      staff and employers across\u000d\u000a      areas as far-reaching as call centres, hospitals and care homes.\u000d\u000a    Call centres. Initially the impact was realised within the call\u000d\u000a      centres at Scottish and Southern\u000d\u000a      Energy (SSE). SSE had succeeded in strengthening its customer base\u000d\u000a      substantially by 2008, due\u000d\u000a      in large part to the high quality of its call centre agents. A\u000d\u000a      sophisticated team-working system was\u000d\u000a      being used, based on a complex rotational roster. Glass was approached \"to\u000d\u000a        assess the degree to\u000d\u000a        which SSE's current rostering methodology is `employee friendly', and to\u000d\u000a        offer a `health check' on\u000d\u000a        workforce management practices\"[1].Glass and Knight successfully\u000d\u000a      adapted their implicit ILP\u000d\u000a      optimiser methodology to handle the complexity of team-working, to\u000d\u000a      accommodate fairness\u000d\u000a      between staff and to improve night working arrangements and rest periods\u000d\u000a      between shifts, in line\u000d\u000a      with ergonomic guidelines [2]. The accuracy of the optimiser enabled them\u000d\u000a      to match staff\u000d\u000a      availability to requirements with 12% fewer staff, equivalent to a\u000d\u000a      year-on-year saving of &#163;5.5M p.a.\u000d\u000a      At a follow-on meeting SSE confirmed that: \"City's employee rostering\u000d\u000a        approach fits well with our\u000d\u000a        focus on Customer Experience ... it has the potential to keep employees\u000d\u000a        happier while at the same\u000d\u000a        time reducing employment costs.\"[3]\u000d\u000a    Doctor and nurse rostering in the NHS. Around the time of the SSE\u000d\u000a      work, there were increasing\u000d\u000a      concerns in the NHS with regard to the rostering of nurses and junior\u000d\u000a      doctors and compliance with\u000d\u000a      the newly-introduced European Working Time Directive (EWTD) and the New\u000d\u000a      Deal. The\u000d\u000a      importance of rostering was highlighted by the Royal College of\u000d\u000a      Physicians' 2006 report [4] which\u000d\u000a      identified a 35% reduction in fatigue factors of doctors achieved by good\u000d\u000a      rota-making practice\u000d\u000a      alone. As a result, Glass and Knight started to work on how City's\u000d\u000a      optimiser could enhance the\u000d\u000a      quality of rosters for both doctors and nurses in the NHS. Between 2008\u000d\u000a      and 2009 they worked\u000d\u000a      with collaborators at two NHS hospitals: Dr Landau of Whittington\u000d\u000a      Hospital, London and Dr Todd of\u000d\u000a      the Horton Hospital, Oxfordshire.\u000d\u000a    Glass and Knight worked with Dr Landau to capture the challenge of\u000d\u000a      rostering junior doctors in the\u000d\u000a      Emergency Department. They developed optimised rosters with ergonomic\u000d\u000a      features, within the\u000d\u000a      NHS mode of rotational rolling rosters, known as rotas. Dr Landau was \"delighted\u000d\u000a        with the quality\u000d\u000a        of the rotas and being able to quickly produce a rota for any number of\u000d\u000a        doctors of different levels of\u000d\u000a        seniority. We usually work from a long-standing template, but on this\u000d\u000a        occasion we needed to\u000d\u000a        accommodate an extra doctor into the rota, which is extremely difficult\u000d\u000a        to do manually. In addition,\u000d\u000a        it saves me the many hours of work it would otherwise take to produce a\u000d\u000a        compliant rota.\" [5]\u000d\u000a    Dr Todd was responsible for rostering the Accident &amp; Emergency\u000d\u000a      Department's junior doctors. An\u000d\u000a      initial four-month trial of the City software was undertaken in 2009. Dr\u000d\u000a      Todd's evaluation is that \"the\u000d\u000a        new rosters produced for junior doctors accommodate most holiday\u000d\u000a        requests and distribute fairly\u000d\u000a        the number of hours worked, days on leave, weekends off and bank\u000d\u000a        holidays, none of which are\u000d\u000a        normally considered. The quality of the rosters is greatly improved by\u000d\u000a        the fact that they are able to\u000d\u000a        limit both number of consecutive night duties and the number of\u000d\u000a        consecutive days while still\u000d\u000a        covering the work and complying with the New Deal and EWTD. Feedback\u000d\u000a        from Junior Doctors\u000d\u000a        confirmed that the computer-generated rostering system helpfully\u000d\u000a        improves on fixed rolling rotas.\"\u000d\u000a      [6]\u000d\u000a    A 2011 report by the Economist Intelligence Unit [7] confirmed\u000d\u000a      that City's nurse rostering software\u000d\u000a      demonstrates improved quality and can accommodate flexible working. Dean\u000d\u000a      Fathers, now Chair\u000d\u000a      of Bassetlaw Primary Care Trust, testified that \"City's system would\u000d\u000a        help the NHS to avoid wasting\u000d\u000a        resources and avoid hefty fines for non-compliance with regulations such\u000d\u000a        as the Working Time\u000d\u000a        Directive. Health sector employers are particularly vulnerable to\u000d\u000a        litigation, given the long hours\u000d\u000a        worked and the stressful conditions faced by many medical staff.\"\u000d\u000a      Helen Young, Executive Clinical\u000d\u000a      Director and Chief Nurse at NHS Direct, stated that inefficient rostering\u000d\u000a      systems &#8212; manual\u000d\u000a      or\u000d\u000a      electronic &#8212; can result in financial penalties and clinical safety issues.\u000d\u000a    At this juncture, further research stopped temporarily due to the\u000d\u000a      long-term sickness of Dr Knight\u000d\u000a      and it resumed in 2011. In the intervening years, the NHS adopted\u000d\u000a      so-called nurse e-rostering\u000d\u000a      systems and the market is now dominated by a few large players. These have\u000d\u000a      recently offered\u000d\u000a      some automation but this is limited to heuristics with no account taken of\u000d\u000a      the fatigue and risk\u000d\u000a      aspects of a roster. Glass was approached by the Chief Executive of an NHS\u000d\u000a      Trust in July 2013\u000d\u000a      seeking to investigate ways the City optimiser could be adopted to\u000d\u000a      encourage better rostering\u000d\u000a      practice.\u000d\u000a    Care homes have arguably the greatest need for automated rostering\u000d\u000a      as almost all of the 18,000\u000d\u000a      care homes in the UK currently use labour-intensive manual rostering. In\u000d\u000a      2011 City started to\u000d\u000a      collaborate with Ian Turner, currently National Chairman of the Registered\u000d\u000a      Nursing Homes\u000d\u000a      Association (RNHA), to develop a rostering system for care homes. A\u000d\u000a      rudimentary system was\u000d\u000a      trialled in 2012 by Turner's The Partnership in Care Group of nursing\u000d\u000a      homes in Suffolk. As a result\u000d\u000a      of feedback, an upgraded system, named Roster Care, was piloted in four\u000d\u000a      additional nursing\u000d\u000a      homes from June 2013 and received extremely positive feedback. The system\u000d\u000a      has been found to\u000d\u000a      save 12 days per year in staff time for producing rosters alone. Ian\u000d\u000a      Turner observed: \"This is a very\u000d\u000a        exciting development for the care homes sector. The underlying software\u000d\u000a        is good and we have\u000d\u000a        worked closely on the user interface. Our staff have embraced the idea\u000d\u000a        enthusiastically and they\u000d\u000a        will certainly be looking to use it in our homes. ...the benefits\u000d\u000a        offered by this innovation will apply\u000d\u000a        equally across the whole sector. One of the key issues for us is that\u000d\u000a        the majority of our staff work\u000d\u000a        part-time and have other commitments, making rostering difficult for us.\u000d\u000a        The idea that we can use\u000d\u000a        this feature to our advantage through shift management and recruitment\u000d\u000a        strategy has been a real\u000d\u000a        eye-opener. Being able to prepare the roster so that it is fair to all\u000d\u000a        staff, closely matches hourly\u000d\u000a        requirements and also being able to juggle it in real time for\u000d\u000a        unforeseen changes, helps us to\u000d\u000a        ensure that quality of care is maintained at all times, our staff are\u000d\u000a        not burnt out and that our\u000d\u000a        budgets are being used as efficiently as possible.\" [8] The\u000d\u000a      University is now exploring mechanisms\u000d\u000a      for the commercialisation of Roster Care.\u000d\u000a    City's rostering software has already shown that it can make a\u000d\u000a      significant impact in the health and\u000d\u000a      welfare domain by improving outcomes in the NHS and social care sectors\u000d\u000a      where all key\u000d\u000a      stakeholders (organisations, employees and patients) are beneficiaries. It\u000d\u000a      has improved the\u000d\u000a      workforce planning and management of human resources in organisations\u000d\u000a      where it has been\u000d\u000a      trialled and piloted (the NHS and care homes); has demonstrated reduction\u000d\u000a      in measurements of\u000d\u000a      fatigue and risk (call centres and the NHS) and more conducive rosters to\u000d\u000a      the satisfaction of staff\u000d\u000a      (junior doctors and care homes); has demonstrated to the satisfaction of\u000d\u000a      managers a new efficient\u000d\u000a      way of implementing legislative changes affecting work patterns, hence\u000d\u000a      reducing the number of\u000d\u000a      fines (NHS); has been acclaimed for its cost-effectiveness and efficiency\u000d\u000a      in delivering services\u000d\u000a      through better matched staffing levels (all), for releasing staff time\u000d\u000a      (doctors and care homes) and\u000d\u000a      for its ability to reduce staff turnover through fairer rosters and more\u000d\u000a      even workload (call centres\u000d\u000a      and nurses); and challenged conventional wisdom over the benefits of\u000d\u000a      e-rostering software for\u000d\u000a      organisations (the NHS and care homes).\u000d\u000a    ","ImpactSummary":"\u000d\u000a    Poor staff rosters are at the heart of socially-unacceptable working\u000d\u000a      patterns, inadequate rest times\u000d\u000a      and increased levels of stress. They lead to poor productivity, low levels\u000d\u000a      of engagement and\u000d\u000a      additional costs associated with high levels of staff turnover and\u000d\u000a      absenteeism. Research\u000d\u000a      undertaken at City University London has harnessed the power of\u000d\u000a      `Optimisation' techniques to\u000d\u000a      assist managers to draw up good quality staff rosters in hospitals, call\u000d\u000a      centres and other large\u000d\u000a      workforce organisations. The state-of-the-art electronic rostering\u000d\u000a      programme improves use of\u000d\u000a      resources, reduces reliance on costly agency staff, reduces the risk of\u000d\u000a      fines for breaching legal\u000d\u000a      requirements such as the European Working Time Directive and leads to\u000d\u000a      significant savings in the\u000d\u000a      health and social care sectors.\u000d\u000a    ","ImpactType":"Societal","Institution":"\u000d\u000a    City University London\u000d\u000a    ","Institutions":[{"AlternativeName":"City University, London","InstitutionName":"City University, London","PeerGroup":"C","Region":"London","UKPRN":10001478}],"Panel":"B         ","PlaceName":[{"GeoNamesId":"2643743","Name":"London"}],"References":"\u000d\u000a    \u000a1. Gerodimos A.E., Glass C.A. &amp; Potts C.N. (2001) Scheduling\u000d\u000a        of customized jobs on a single\u000d\u000a        machine under item availability, IIE Transactions, 33(11),\u000d\u000a      975-984\u000d\u000a    \u000a\u000a2. Glass C.A. &amp; Prugel-Bennett A. (2005) A\u000d\u000a        polynomially searchable exponential neighbourhood\u000d\u000a        for graph colouring, Journal of the Operational Research\u000d\u000a        Society, 56(3), 324-330\u000d\u000a    \u000a\u000a3. Knight, R. A., (2008) Optimisation\u000d\u000a        methods for staff scheduling and rostering: an employee-friendly\u000d\u000a        approach. PhD thesis, City University London.\u000d\u000a    \u000a\u000a4. Glass C.A. &amp; Knight R.A. (2013) Call centre tour scheduling with\u000d\u000a      employee preferences,\u000d\u000a      working paper, Cass Business School, City University London.\u000d\u000a    \u000a\u000a5. Glass C.A. &amp; Knight R.A. (2010) The\u000d\u000a        nurse rostering problem: A critical appraisal of the\u000d\u000a        problem structure, European Journal of Operations Research,\u000d\u000a      202(2), 379-389\u000d\u000a    \u000aIIE Transactions, Journal of the Operational Research Society and\u000d\u000a      European Journal of Operations\u000d\u000a        Research are among the highest-rated journals in the field of\u000d\u000a      operations research and apply a\u000d\u000a      stringent peer-review process prior to accepting articles for publication.\u000d\u000a    ","ResearchSubjectAreas":[{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"},{"Level1":"14","Level2":"2","Subject":"Applied Economics"},{"Level1":"11","Level2":"17","Subject":"Public Health and Health Services"}],"Sources":"\u000d\u000a    \u000d\u000a      Scottish and Southern Energy (June 2008) `Licence to Innovate' no 327.\u000d\u000a      Glass, C. A. and Knight, R. A. (March 2009), Case study of\u000d\u000a          employee-friendly rostering at\u000d\u000a          Scottish and Southern Energy Business report for Scottish and\u000d\u000a        Southern Energy, City\u000d\u000a        University London.\u000d\u000a      Scottish and Southern Energy and City University London project\u000d\u000a        meeting 15th October 2009,\u000d\u000a        minutes and progress report written 23rd October 2009.\u000d\u000a      Horrocks, N. and Pounder, R. (2006) `Designing safer rotas for\u000d\u000a          junior doctors (in the 48 hour\u000d\u000a          week)', Table 7, Report of the Royal College of Physicians of\u000d\u000a        London, ISBN 1-86016-288-6.\u000d\u000a      Consultant, Emergency Department, Whittington Hospital, London, user\u000d\u000a        feedback and\u000d\u000a        testimony received April 2009. \u000d\u000a      Associate Specialist in Emergency Medicine, Emergency Department, the\u000d\u000a        Horton Hospital,\u000d\u000a        Oxfordshire, user feedback and testimony, encompassing questionnaire\u000d\u000a        feedback from junior\u000d\u000a        doctors, 17 October 2013.\u000d\u000a      Liz Hall (2011), Software\u000d\u000a          that could save hospitals millions, Economist Intelligence Unit,\u000d\u000a        Feature\u000d\u000a        Report, pages: 22-24.\u000d\u000a      National Chairman, Registered Nursing Homes Association, e-mails June\u000d\u000a        and 20 October\u000d\u000a        2013.\u000d\u000a    \u000d\u000a    ","Title":"\u000d\u000a    Strategic roster planning and control using Mixed Integer Linear\u000d\u000a        Programming with applications to health services and call centres\u000d\u000a    ","UKLocation":[],"UKRegion":[{"GeoNamesId":"2638360","Name":"Scotland"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000d\u000a    Rostering is necessary for the day-to-day operations of many\u000d\u000a      organisations, particularly in the\u000d\u000a      health and social care sectors. A roster informs the employer who is\u000d\u000a      covering each required duty\u000d\u000a      and informs the employee of their work schedule. The nature of rostering\u000d\u000a      can be captured\u000d\u000a      mathematically in the form of a constrained allocation problem. Nurse\u000d\u000a      rostering in particular has\u000d\u000a      become a classic problem and is one of a large family of problems which\u000d\u000a      have been classified as\u000d\u000a      being `Non-deterministic Polynomial-time (NP)-hard' in the field of\u000d\u000a      Combinatorial Optimisation. This\u000d\u000a      classification is key to selecting a suitable computational approach, as\u000d\u000a      it determines whether large\u000d\u000a      instances of the problem are likely to be solvable. For this reason the\u000d\u000a      literature focuses on heuristic\u000d\u000a      approaches, even though these may produce an incomplete or poor quality\u000d\u000a      solution. In addition,\u000d\u000a      these rosters can have a negative impact on people's well-being as the\u000d\u000a      human aspect is ignored.\u000d\u000a      City University London staff adopted an alternative approach based upon\u000d\u000a      Optimisation techniques\u000d\u000a      incorporating human factors, which provides a robust generic rostering\u000d\u000a      solution procedure.\u000d\u000a    The staff rostering project brings together the expertise of Celia Glass\u000d\u000a      (at City since 1999, now a\u000d\u000a      Professor) and Dr Roger Knight, an expert in Call Centre Workforce\u000d\u000a      Management who came to\u000d\u000a      study with Glass for a PhD in 2005. The project began in 2005, although\u000d\u000a      the origins of the\u000d\u000a      approach are derived from Glass' earlier research on comparative\u000d\u000a      algorithms where she studied the\u000d\u000a      performance of contemporary heuristics, such as Simulated Annealing, Tabu\u000d\u000a      Search and Genetic\u000d\u000a      algorithms, to understand the relative performance on a range of\u000d\u000a      combinatorial problems. Later she\u000d\u000a      produced a parsimonious formulation of the problem and then exploited the\u000d\u000a      relative simplicity of its\u000d\u000a      solution space representation using an appropriate algorithm1,2.\u000d\u000a      Knight's research question was\u000d\u000a      whether a rostering programme could be driven for employee well-being. The\u000d\u000a      challenge involved\u000d\u000a      capturing the following competing imperatives: maintaining sufficient\u000d\u000a      staffing levels to cover\u000d\u000a      demand without incurring the additional costs associated with overstaffing\u000d\u000a      or the use of agency\u000d\u000a      staff; satisfying working practices and employment contracts; and having\u000d\u000a      due regard for staff well-being\u000d\u000a      and individual preferences.\u000d\u000a    Shift patterns can have a profound effect on well-being, especially if\u000d\u000a      they include night shifts.\u000d\u000a      Nonetheless, staff well-being had not previously featured within rostering\u000d\u000a      software. Glass and\u000d\u000a      Knight identified fatigue metrics from the Human Resources literature that\u000d\u000a      would suitably capture\u000d\u000a      staff well-being and used them in linear constraints within their\u000d\u000a      rostering model. As it is not\u000d\u000a      generally possible to satisfy all of the constraints simultaneously, the\u000d\u000a      Lagrange multiplier approach\u000d\u000a      was used to reduce selected constraints to soft goals with measurements of\u000d\u000a      their violation. The\u000d\u000a      Call Centre Rostering problem was then amenable to an Integer Linear\u000d\u000a      Programming (ILP)\u000d\u000a      formulation, albeit too slow to handle larger instances. The key advance\u000d\u000a      was to capture succinctly\u000d\u000a      the underlying features of call centre rostering in an implicit\u000d\u000a      formulation which could then be used\u000d\u000a      for optimisation3 4. Glass and Knight were thus able to produce\u000d\u000a      a higher quality roster for\u000d\u000a      benchmark real world call centre data than was found elsewhere in the\u000d\u000a      literature or from available\u000d\u000a      commercial rostering packages4.\u000d\u000a    Their primary achievement was to solve the problem to optimality, in the\u000d\u000a      face of contrary claims in\u000d\u000a      the literature that such large instances of this NP-hard problem could\u000d\u000a      only be solved\u000d\u000a      approximately. The quality objective relating to staff fatigue and\u000d\u000a      workplace risk make the approach\u000d\u000a      particularly attractive. An additional benefit arose from the\u000d\u000a      counter-intuitive outcome of being able\u000d\u000a      to minimise cost simultaneously so that there need be no quality-cost\u000d\u000a      trade off3. Using the\u000d\u000a      optimisation algorithm can provide a company with the cheapest possible\u000d\u000a      roster which\u000d\u000a      simultaneously offers the most employee-friendly low risk roster.\u000d\u000a    Armed with this general approach, Glass and Knight then addressed the\u000d\u000a      most commonly-studied\u000d\u000a      rostering problem in the literature, that of Nurse Rostering. They found\u000d\u000a      that their approach adapted\u000d\u000a      well4. To prove its veracity they focussed on producing a\u000d\u000a      quality roster for three widely-publicised,\u000d\u000a      open benchmark problems, which they solved to optimality5.\u000d\u000a    In 2008, they extended the research in response to various real world\u000d\u000a      situations. Reformulation in\u000d\u000a      each case produced a successful outcome and indicated the transferability\u000d\u000a      of the implicit ILP\u000d\u000a      approach. For trainee doctors, the additional features of fairness\u000d\u000a      criteria, a longer time horizon and\u000d\u000a      legal requirements were added to the roster optimiser. The working\u000d\u000a      practice of using rotational\u000d\u000a      rosters (rotas) in the NHS was reflected in an alternative variant of the\u000d\u000a      ILP formulation. For care\u000d\u000a      homes the different levels of expertise of nurses and carers and\u000d\u000a      pre-existing shift patterns were\u000d\u000a      accommodated by additional constraints.\u000d\u000a    There are two theoretical aspects which allow Glass and Knight's approach\u000d\u000a      to handle larger\u000d\u000a      problem instances than others. One is the depiction of the problem and its\u000d\u000a      consequent\u000d\u000a      representation in an implicit form, which vastly reduces the solution\u000d\u000a      space. The other is the use of a\u000d\u000a      Totally Unimodular constraint matrix in an ILP formulation, for which\u000d\u000a      there are efficient algorithms.\u000d\u000a      The existence of solutions which minimise the staffing costs of the roster\u000d\u000a      without compromising\u000d\u000a      quality could only arise within a(n originally) vast solution space. It is\u000d\u000a      particularly rewarding that the\u000d\u000a      optimisation approach is strong enough to find them.\u000d\u000a    Further insights relate to the importance of grounding the problem\u000d\u000a      formulation firmly in context. For\u000d\u000a      example, their nurse rostering algorithm takes account of month-on-month\u000d\u000a      continuity, while the\u000d\u000a      failure of earlier algorithms to do so provided poor solutions in practice5.\u000d\u000a      The methodology also\u000d\u000a      quantifies both the undesirable effects of a roster such as unconducive\u000d\u000a      night shift patterns which\u000d\u000a      affect both staff fatigue and workplace risk and the desirable effects\u000d\u000a      from satisfying personal\u000d\u000a      preferences and maintaining fairness between staff, both of which affect\u000d\u000a      staff morale. This\u000d\u000a      differentiates the work from both previous academic and other commercial\u000d\u000a      approaches. The impact\u000d\u000a      of the research flows from the robustness of the underpinning mathematical\u000d\u000a      algorithm combined\u000d\u000a      with a careful capturing of contextual aspects of the problem.\u000d\u000a    "},{"CaseStudyId":"44354","Continent":[{"GeoNamesId":"6255148","Name":"Europe"}],"Country":[{"GeoNamesId":"3144096","Name":"Norway"}],"Funders":[],"ImpactDetails":"\u000a    Impact of the software\u000a    The FSS took an interest in the software leading to a licence agreement\u000a      in 2005 with City University London for its use for development and\u000a      commercial sales. The FSS was a government-owned company which provided\u000a      scene-of-crime and forensic investigation services to the police force and\u000a      government agencies of England and Wales, as well as other countries.\u000a      These included the Crown Prosecution Service, HM Revenue and Customs, HM\u000a      Coroners' Service, Ministry of Defence Police and British Transport Police\u000a      in the UK and worldwide forensic services.\u000a    The FSS replaced the user interface of FINEX and added custom features\u000a      such as automated report generation. The version of the software developed\u000a      by the FSS was originally called FSS-ibd, later changed to FSS DNA\u000a      Lineage.1,2 An important development in the functionality of\u000a      the DNA Lineage software by the FSS was the ability to carry out a\u000a      database search for possible partial matches of close relatives to a\u000a      previously given DNA profile, such as might be found at a crime scene.\u000a      This procedure is known as familial searching. Although a crime scene DNA\u000a      sample might not match anyone already in the national DNA database, the\u000a      offender might be a close relative of someone whose information is in the\u000a      database. A partial DNA match may aid in identifying the offender via\u000a      their familial links. The FSS DNA Lineage programme can compare over 2,000\u000a      profiles per minute in such a database search. It was used routinely by\u000a      the FSS in its work and was crucial in winning an immigration contract\u000a      with Norway to provide a service testing the claims people make when they\u000a      apply for visas on behalf of family members. In 2011, the UK police had\u000a      used the software in around one hundred cases.3\u000a    The FSS had a dedicated Familial Searching Service unit. With the use of\u000a      the FSS DNA Lineage software, the FSS was able to identify 19 offenders of\u000a      cold cases leading to their conviction. One of the most notorious\u000a      criminals to be found and convicted with the help of the software was\u000a      David Newton, known as the Ilkley Moor rapist. He was convicted and\u000a      sentenced to life imprisonment in December 2008. DNA evidence linked an\u000a      attack on a dog walker to a rape carried out nine years previously. Within\u000a      36 hours the FSS experts were able to link the Ilkley Moor Attack to the\u000a      earlier crime in Leeds.4\u000a    The FSS lost market share as a result of increased use of competitive\u000a      tendering by police forces for forensic services and it was closed in\u000a      2012. However prior to this it had developed a reputation as a pioneer in\u000a      forensic software and technology, notably DNA interpretation, databasing\u000a      and electronic forensics. The Government decision to close the Service,\u000a      announced in 2010, drew criticism from international forensic scientists\u000a      and victim campaigners for the potential damage that this would do to the\u000a      UK criminal justice system.5\u000a    A Home Office document, `Review of Research and Development in\u000a        Forensic Science'6, outlined the potential future\u000a      of the FSS before the ultimate decision was taken to dissolve the service.\u000a      During the review the question was asked: `Can you give good examples in\u000a      the forensic science field of translation of research into practice?'. The\u000a      answer given relates to the use of the FINEX software and its uptake by\u000a      the FSS ([9] being a reference in the document to the work of Robert\u000a      Cowell):\u000a    \"For example, a well-respected group of academic statisticians [9]\u000a        have published models in peer reviewed journals...\"\u000a    This is followed by an explanation of a new model for DNA mixed profiles:\u000a    \"The FSS R&amp;D statisticians have developed and validated a model\u000a        for de-convolving DNA mixed profiles based on academic research. This\u000a        model has been implemented into the FSS, providing consistency,\u000a        robustness, and cost effectiveness for the taxpayer.\"\u000a    In the same document the FSS also cites its achievements as including: \"Development\u000a        and implementation of FSS DNA LINEAGE for kinship analysis,\u000a        award-winning DNA INSIGHT software for DNA interpretation...and FSS-ibd\u000a        a validated DNA database solution.\"\u000a    The FSS DNA Lineage software was favourably reviewed in a report by the\u000a      Economist Intelligence Unit (EIU) in 2011.3 Part of the\u000a      Economist Group, the EIU is a respected producer of country and industry\u000a      reports. The report says: \"New software is speeding up the work of\u000a        forensic scientists working for police and immigration services across\u000a        the world. Thanks to some elegant mathematics from a lecturer at City\u000a        University London, they can now match up DNA samples to get answers much\u000a        more quickly.\"\u000a    In the article, Dr Maguire, then of the FSS, stated \"the software has\u000a        been used in around a hundred cases so far and would help in crime\u000a        scenes where speed is vital &#8212; such as the identification of multiple\u000a        victims of an incident involving mass fatalities\". He explains that\u000a      even if an exact DNA match is not forthcoming, Dr Cowell's software can be\u000a      used to narrow the search based on near matches commonly found among close\u000a      relatives. Dr Maguire says: \"Based on probabilities, Dr Cowell's model\u000a        identifies the likelihood of genetic markers being passed on from father\u000a        or mother as well as the very close sibling relationships.\"3\u000a    With the closure of the FSS, the FSS DNA Lineage software has been taken\u000a      over by the Home Office who intend to continue to use it in the UK in\u000a      criminal and immigration casework. City University London has recently\u000a      reached an agreement with the Home Office to enable the University to\u000a      continue the development and commercial exploitation of the software. A\u000a      licence agreement is currently being negotiated with a Spanish software\u000a      vendor.\u000a    Other software\u000a    In addition to the commercial software licensed to the FSS, another piece\u000a      of software, FRANz, has been constructed as a result of the underpinning\u000a      research and made available in the public domain.7 This can be\u000a      used to reconstruct pedigrees of related individuals; the algorithm for\u000a      which is described in the journal Theoretical Population Biology.\u000a    ","ImpactSummary":"\u000a    The mathematical calculations for determining the likelihood of kinship\u000a      between two individuals from their DNA profiles can be quite laborious and\u000a      error prone, even when carried out by experts in the field. FINEX, an\u000a      Expert System-based software programme to automate such calculations, was\u000a      developed through research in Bayesian networks undertaken at City\u000a      University London. The software can accurately and efficiently calculate\u000a      kinship likelihoods within a minute or two, calculations that could take\u000a      an expert half a day or more. The software was licensed to the UK Forensic\u000a      Science Service (FSS), who used it among other applications to analyse DNA\u000a      evidence leading to convictions for several serious and high profile\u000a      criminal cases.\u000a    ","ImpactType":"Technological","Institution":"\u000a    City University London\u000a    ","Institutions":[{"AlternativeName":"City University, London","InstitutionName":"City University, London","PeerGroup":"C","Region":"London","UKPRN":10001478}],"Panel":"B         ","PlaceName":[],"References":"\u000a    \u000aCowell R.G., Dawid A.P., Lauritzen S.L. &amp; Spiegelhalter D.J. (1999) Probabilistic\u000a        Networks and Expert Systems, New York: Springer-Verlag\u000a      http:\/\/www.springer.com\/statistics\/statistical+theory+and+methods\/book\/978-0-387-98767-5\u000a    \u000a&#8226;Winner, 2002 DeGroot Prize, International Society for Bayesian Analysis,\u000a      the only book prize in the field of statistics.\u000a    &#8226;1924 Google citations [data retrieved 30\/7\/2013]\u000a    \u000aCowell R.G. (2003). FINEX: a Probabilistic Expert system for forensic\u000a      identification. Forensic Science International, 134(2-3), 196-206\u000a      10.1016\/S0379-0738(03)00164-6\u000a    \u000a\u000aCowell R.G. (2009). Validation of an STR peak area model. Forensic\u000a        Science International: Genetics, 3(3), 193-199 10.1016\/j.fsigen.2009.01.006\u000a    \u000a\u000aCowell R.G., Lauritzen S.L. &amp; Mortera J. (2007). Identification and\u000a      Separation of DNA Mixtures using Peak Area Information, Forensic\u000a        Science International, 166(1), 28-34 10.1016\/j.forsciint.2006.03.021\u000a    \u000a\u000aCowell R.G., Lauritzen S.L. &amp; Mortera J. (2008). Probabilistic\u000a      modelling for DNA mixture analysis, Forensic Science International:\u000a        Genetics Supplement Series, 1(1), 640-642 10.1016\/j.fsigss.2007.10.087\u000a    \u000a\u000aCowell R.G. (2009). Efficient maximum likelihood pedigree reconstruction,\u000a      Theoretical Population Biology, 76, 285-291 10.1016\/j.tpb.2009.09.002\u000a    \u000aThe work has been published in leading international journals in the\u000a      field which apply a rigorous peer review process prior to publication of\u000a      articles.\u000a    ","ResearchSubjectAreas":[{"Level1":"1","Level2":"4","Subject":"Statistics"},{"Level1":"8","Level2":"1","Subject":"Artificial Intelligence and Image Processing"},{"Level1":"8","Level2":"2","Subject":"Computation Theory and Mathematics"}],"Sources":"\u000a    \u000a      \u000awww.promega.com\/~\/media\/files\/resources\/profiles%20in%20dna\/1101\/dna%20based%20kinship%20analysis.pdf?la=en\u000a        (Article dated March 2008 describing the FSS DNA-BasedKinship Analysis\u000a        service)\u000a      Maguire, Christopher, McCullum, L. A., Jones, K. E. and Storey, C. L.\u000a        (2009) Developing a Likelihood Ratio Approach to 'Familial\u000a          Searching' of a DNA Database Using the Advanced Functionality Of\u000a          FSS-ibd. In: 20th International Symposium on Human Identification,\u000a        12-15 October 2009, Las Vegas, NV (Mention of the City University\u000a          London and FSS work in relation to the development of another\u000a          `familial searching tool; paper produced by the Forensic Science\u000a          Service, Wetherby, West Yorkshire, UK)\u000a\u000a      Hoare, Stephen (2011), Formula for Success, The Economist\u000a          Intelligence Unit, Special Report, www.eiu.com\/report_dl.asp?mode=fi&amp;fi=924190877.PDF\u000a        (also published in In Business magazine)\u000a      \u000aWoman,\u000a          52, fought for her life in rape attack, Bradford Telegraph and\u000a        Argus, 18th December 2008\u000a      \u000aAxing\u000a          of Forensic Science Service may lead to rise in miscarriages of\u000a          justice, scientists warn, The Observer , 12 February\u000a        2012; and www.independent.co.uk\/news\/uk\/crime\/csi-\u000a          chief-condemns-forensic-cuts-2179744.html 9 January 2011\u000a      The Home Office, 2011, Review\u000a          of Research and Development in Forensic Science, p.54\u000a      \u000aFRANz\u000a          Beta Pedigree Reconstruction [Information retrieved 21 July\u000a          2013]\u000a\u000a    \u000a    Personal corroboration can also be provided on request from former FSS\u000a      staff.\u000a    ","Title":"\u000a    FINEX: a probabilistic expert system for forensic identification\u000a    ","UKLocation":[{"GeoNamesId":"2643743","Name":"London"},{"GeoNamesId":"2644688","Name":"Leeds"}],"UKRegion":[{"GeoNamesId":"2634895","Name":"Wales"},{"GeoNamesId":"6269131","Name":"England"}],"UOA":"Mathematical Sciences","UnderpinningResearch":"\u000a    The determination of the likelihood that two or more individuals are\u000a      biologically related, based on their DNA profiles is of interest in both\u000a      civil and criminal application. A common civil application occurs in a\u000a      case of disputed paternity in which, for example, a mother claims that the\u000a      father of her child is a certain male, but he denies this. A related\u000a      criminal application is that of incest, for example when a man is\u000a      suspected of fathering a child by his daughter. The establishment of\u000a      relatedness of individuals is also important for immigration cases.\u000a    Research undertaken by Dr Robert Cowell (employed at City since 1995, now\u000a      Senior Lecturer) in collaboration with colleagues has shown how to\u000a      formulate complex problems of forensic DNA identification inference to\u000a      address such questions in terms of Probabilistic Expert Systems (PESs).\u000a      However, general purpose PES software is not particularly well-suited to\u000a      the repetitive tasks of specifying an appropriate set of marker networks\u000a      for a specific problem, editing the many local conditional probability\u000a      tables and combining evidence from several genetic markers to evaluate\u000a      likelihoods. Such software can be time-consuming and error-prone because\u000a      of the number and sizes of the tables requiring specification in the\u000a      Bayesian networks. Co-authors in the research were A. Philip Dawid\u000a      (Professor of Statistics, University of Cambridge, now retired), Steffen\u000a      L. Lauritzen FRS (Professor of Statistics, University of Oxford), David J.\u000a      Spiegelhalter (Winton Professor of the Public Understanding of Risk,\u000a      University of Cambridge) and Julia Mortera (Professor of Statistics, Roma\u000a      Tre University, Italy).\u000a    The creation and development of a novel prototype computer programme,\u000a      FINEX, overcame these problems. FINEX was originally written to automate\u000a      the process of constructing Bayesian networks PESs and to provide a\u000a      user-friendly interface by reproducing the usual genetic tree in the\u000a      computer screen. The Bayesian network is used to structure a definite\u000a      genetic problem (in our case, a disputed relationship) in terms of a\u000a      graphical model (with elementary deterministic relations, probabilistic\u000a      computational nodes and a query node). Later, FINEX was extended to carry\u000a      out the calculations itself, without having to export the Bayesian\u000a      networks to a separate piece of software for analysis.\u000a    FINEX allows a user to express the structure of a forensic identification\u000a      problem in a quick and simple manner through the syntax of a high-level\u000a      graphical specification language. This allows quite complex hypotheses to\u000a      be entertained regarding the relationships of individuals which could be\u000a      so complex that an expert forensic scientist could not do the\u000a      calculations. The user of the programme specifies two or more hypothetical\u000a      relationships and the software evaluates the likelihood of the\u000a      hypothetical relationships between known genetic profiles being actual.\u000a      Assessments are made based on the differences of the likelihoods of the\u000a      hypotheses.\u000a    "}]